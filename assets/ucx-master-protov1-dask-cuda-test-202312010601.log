============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-12-01 06:37:23,168 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:37:23,172 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36543 instead
  warnings.warn(
2023-12-01 06:37:23,176 - distributed.scheduler - INFO - State start
2023-12-01 06:37:23,196 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:37:23,197 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-01 06:37:23,198 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36543/status
2023-12-01 06:37:23,198 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:37:23,475 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40339'
2023-12-01 06:37:23,496 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38471'
2023-12-01 06:37:23,499 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37591'
2023-12-01 06:37:23,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46771'
2023-12-01 06:37:24,838 - distributed.scheduler - INFO - Receive client connection: Client-1495bdc9-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:24,852 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33938
2023-12-01 06:37:25,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:25,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:25,269 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:25,301 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:25,301 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:25,305 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:25,344 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:25,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:25,348 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:25,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:25,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:25,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-12-01 06:37:25,383 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46851
2023-12-01 06:37:25,383 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46851
2023-12-01 06:37:25,383 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43077
2023-12-01 06:37:25,383 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-01 06:37:25,383 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:25,383 - distributed.worker - INFO -               Threads:                          4
2023-12-01 06:37:25,384 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-01 06:37:25,384 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-36z1pzl_
2023-12-01 06:37:25,384 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd54c23a-44ca-4bc2-93e9-143b86df567a
2023-12-01 06:37:25,384 - distributed.worker - INFO - Starting Worker plugin PreImport-76abbce4-291f-4129-950b-3ce027a3fdc0
2023-12-01 06:37:25,385 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3dc28a5f-8ce6-4753-b7d8-5cf32d19cd59
2023-12-01 06:37:25,385 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:25,966 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46851', status: init, memory: 0, processing: 0>
2023-12-01 06:37:25,967 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46851
2023-12-01 06:37:25,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33958
2023-12-01 06:37:25,969 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:25,970 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-01 06:37:25,970 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:25,972 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-01 06:37:27,692 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33725
2023-12-01 06:37:27,692 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33725
2023-12-01 06:37:27,692 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44975
2023-12-01 06:37:27,692 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36609
2023-12-01 06:37:27,692 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-01 06:37:27,692 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36609
2023-12-01 06:37:27,693 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:27,693 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34123
2023-12-01 06:37:27,693 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-01 06:37:27,693 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:27,693 - distributed.worker - INFO -               Threads:                          4
2023-12-01 06:37:27,693 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-01 06:37:27,693 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-jsy_xqql
2023-12-01 06:37:27,693 - distributed.worker - INFO -               Threads:                          4
2023-12-01 06:37:27,693 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-01 06:37:27,693 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ur8pgh3h
2023-12-01 06:37:27,692 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38159
2023-12-01 06:37:27,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c050ba9-81f4-4ff6-8125-c30e246621e5
2023-12-01 06:37:27,693 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38159
2023-12-01 06:37:27,693 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44173
2023-12-01 06:37:27,693 - distributed.worker - INFO - Starting Worker plugin PreImport-f7ceb9b5-59bf-484c-8b99-3878d9f0bf4c
2023-12-01 06:37:27,693 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-01 06:37:27,693 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:27,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d8236ead-c039-448e-890b-623e55af855e
2023-12-01 06:37:27,693 - distributed.worker - INFO - Starting Worker plugin PreImport-c4f3adfc-725e-48cb-99ef-aa2a2e63b724
2023-12-01 06:37:27,693 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32833165-4d97-4ef2-929e-b3003178c2e4
2023-12-01 06:37:27,693 - distributed.worker - INFO -               Threads:                          4
2023-12-01 06:37:27,694 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-12-01 06:37:27,694 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-b1wh1br2
2023-12-01 06:37:27,694 - distributed.worker - INFO - Starting Worker plugin RMMSetup-22011f9a-84a6-465b-939b-8a85d910e351
2023-12-01 06:37:27,694 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:27,694 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:27,694 - distributed.worker - INFO - Starting Worker plugin RMMSetup-01604c4a-6707-4f62-a3ad-e8e2fbacb782
2023-12-01 06:37:27,694 - distributed.worker - INFO - Starting Worker plugin PreImport-773a1705-7baa-469b-b298-df3de934840a
2023-12-01 06:37:27,694 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bdc5cdf0-f80d-4759-9b4e-76c5f7278500
2023-12-01 06:37:27,695 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:27,721 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36609', status: init, memory: 0, processing: 0>
2023-12-01 06:37:27,722 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36609
2023-12-01 06:37:27,722 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33974
2023-12-01 06:37:27,723 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:27,724 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-01 06:37:27,724 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:27,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-01 06:37:27,745 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33725', status: init, memory: 0, processing: 0>
2023-12-01 06:37:27,746 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33725
2023-12-01 06:37:27,746 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33976
2023-12-01 06:37:27,748 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:27,749 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-01 06:37:27,749 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:27,750 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38159', status: init, memory: 0, processing: 0>
2023-12-01 06:37:27,751 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38159
2023-12-01 06:37:27,751 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33980
2023-12-01 06:37:27,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-01 06:37:27,752 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:27,753 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-01 06:37:27,753 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:27,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-01 06:37:27,794 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-01 06:37:27,794 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-01 06:37:27,794 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-01 06:37:27,990 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-12-01 06:37:27,995 - distributed.scheduler - INFO - Remove client Client-1495bdc9-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:27,995 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33938; closing.
2023-12-01 06:37:27,996 - distributed.scheduler - INFO - Remove client Client-1495bdc9-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:27,996 - distributed.scheduler - INFO - Close client connection: Client-1495bdc9-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:27,997 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40339'. Reason: nanny-close
2023-12-01 06:37:27,997 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:27,998 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38471'. Reason: nanny-close
2023-12-01 06:37:27,998 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:27,999 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38159. Reason: nanny-close
2023-12-01 06:37:28,001 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33980; closing.
2023-12-01 06:37:28,001 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-01 06:37:28,001 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37591'. Reason: nanny-close
2023-12-01 06:37:28,001 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38159', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412648.001536')
2023-12-01 06:37:28,001 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:28,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46771'. Reason: nanny-close
2023-12-01 06:37:28,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33725. Reason: nanny-close
2023-12-01 06:37:28,002 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:28,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36609. Reason: nanny-close
2023-12-01 06:37:28,002 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:28,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46851. Reason: nanny-close
2023-12-01 06:37:28,005 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-01 06:37:28,005 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33974; closing.
2023-12-01 06:37:28,005 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36609', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412648.0053391')
2023-12-01 06:37:28,005 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-01 06:37:28,006 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33976; closing.
2023-12-01 06:37:28,006 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-01 06:37:28,006 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:28,006 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33725', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412648.0063725')
2023-12-01 06:37:28,006 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33958; closing.
2023-12-01 06:37:28,007 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46851', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412648.0069833')
2023-12-01 06:37:28,007 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:37:28,008 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:28,008 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:29,364 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:37:29,365 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:37:29,365 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:37:29,366 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-01 06:37:29,367 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-12-01 06:37:31,735 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:37:31,740 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37307 instead
  warnings.warn(
2023-12-01 06:37:31,744 - distributed.scheduler - INFO - State start
2023-12-01 06:37:31,830 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:37:31,831 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:37:31,831 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37307/status
2023-12-01 06:37:31,831 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:37:32,136 - distributed.scheduler - INFO - Receive client connection: Client-19ab0abd-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:32,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39382
2023-12-01 06:37:32,228 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40811'
2023-12-01 06:37:32,243 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35875'
2023-12-01 06:37:32,256 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35917'
2023-12-01 06:37:32,271 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35299'
2023-12-01 06:37:32,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40821'
2023-12-01 06:37:32,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36505'
2023-12-01 06:37:32,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40401'
2023-12-01 06:37:32,301 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41653'
2023-12-01 06:37:34,142 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:34,142 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:34,142 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:34,142 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:34,146 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:34,147 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:34,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:34,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:34,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:34,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:34,167 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:34,169 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:34,170 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:34,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:34,174 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:34,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:34,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:34,188 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:34,242 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:34,242 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:34,246 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:34,254 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:34,254 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:34,258 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:37,084 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46207
2023-12-01 06:37:37,085 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46207
2023-12-01 06:37:37,085 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35627
2023-12-01 06:37:37,085 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,085 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,085 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:37,085 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:37,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vq5d_w7f
2023-12-01 06:37:37,085 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37847c5b-41f7-4a52-82be-d7bd47c24f94
2023-12-01 06:37:37,088 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46821
2023-12-01 06:37:37,089 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46821
2023-12-01 06:37:37,089 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40353
2023-12-01 06:37:37,089 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,089 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,089 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:37,090 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:37,090 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uarrd_qz
2023-12-01 06:37:37,090 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86a5afca-64b7-4189-8421-ae7e9f55cc5f
2023-12-01 06:37:37,093 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38305
2023-12-01 06:37:37,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38305
2023-12-01 06:37:37,094 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40447
2023-12-01 06:37:37,094 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,094 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,093 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46563
2023-12-01 06:37:37,094 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46563
2023-12-01 06:37:37,094 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:37,094 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37525
2023-12-01 06:37:37,094 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:37,094 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,094 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6yzm7zde
2023-12-01 06:37:37,094 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,094 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:37,094 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:37,094 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jpin6m6a
2023-12-01 06:37:37,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7de9d2c3-5481-4e38-bd50-ca8c505d78bc
2023-12-01 06:37:37,095 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-16b40fc0-195a-4749-bd45-a9f36373e9d5
2023-12-01 06:37:37,095 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cbd66c53-57f7-4f9b-910e-2ecfb00a8e2f
2023-12-01 06:37:37,097 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38453
2023-12-01 06:37:37,098 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38453
2023-12-01 06:37:37,098 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38109
2023-12-01 06:37:37,098 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,098 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,098 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:37,098 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:37,098 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sd8perja
2023-12-01 06:37:37,099 - distributed.worker - INFO - Starting Worker plugin PreImport-93aa9ccb-847c-4fbb-891d-0c4e339bb9b1
2023-12-01 06:37:37,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-221cab77-897e-481a-9623-a02def9d923c
2023-12-01 06:37:37,102 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41657
2023-12-01 06:37:37,103 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41657
2023-12-01 06:37:37,103 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43249
2023-12-01 06:37:37,103 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,103 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,103 - distributed.worker - INFO - Starting Worker plugin RMMSetup-346ff3f4-2341-484a-b5c2-ab6aa2080f98
2023-12-01 06:37:37,103 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:37,103 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:37,103 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zqyk4a7c
2023-12-01 06:37:37,104 - distributed.worker - INFO - Starting Worker plugin PreImport-d066f73c-4595-4e42-853d-d666e525590c
2023-12-01 06:37:37,104 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6bd3da7e-9a7a-4e56-a3e1-aaa39af7ac46
2023-12-01 06:37:37,104 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35707
2023-12-01 06:37:37,105 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35707
2023-12-01 06:37:37,105 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44956eff-0d3d-459a-854d-43caed197a49
2023-12-01 06:37:37,105 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41913
2023-12-01 06:37:37,105 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,105 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,105 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:37,105 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:37,105 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pzg95cyn
2023-12-01 06:37:37,106 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca67c071-a35e-44ef-a174-e157dc5d0ae7
2023-12-01 06:37:37,106 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35829
2023-12-01 06:37:37,107 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35829
2023-12-01 06:37:37,107 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45959
2023-12-01 06:37:37,107 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,107 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,107 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:37,108 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:37,108 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h6jofgyy
2023-12-01 06:37:37,108 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72bdd284-5a7b-4d19-9a99-d68b2903f067
2023-12-01 06:37:37,108 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1e5c84ec-8dd1-4caa-8eba-75f2f2dda537
2023-12-01 06:37:37,292 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d1b8035d-38b2-4075-80ba-01bfd0b795f6
2023-12-01 06:37:37,292 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-139be38f-9ff6-4b28-8929-9297dedc5ba8
2023-12-01 06:37:37,292 - distributed.worker - INFO - Starting Worker plugin PreImport-5843107f-0fe2-4a7e-9f89-2e705a00ece7
2023-12-01 06:37:37,292 - distributed.worker - INFO - Starting Worker plugin PreImport-b4961c40-4ebc-4e65-b025-bc171cf08a41
2023-12-01 06:37:37,292 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,292 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,292 - distributed.worker - INFO - Starting Worker plugin PreImport-94b03d73-c734-4c64-8a34-ac8040c01661
2023-12-01 06:37:37,292 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9c4b93c-e948-46fc-8ed1-1af6cd4008e1
2023-12-01 06:37:37,292 - distributed.worker - INFO - Starting Worker plugin PreImport-8bda787c-06ad-4a60-84e9-76fcecffd387
2023-12-01 06:37:37,292 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,292 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56d9a05e-a1f4-44cf-86e6-c2a7b05769d1
2023-12-01 06:37:37,292 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,292 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,292 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,293 - distributed.worker - INFO - Starting Worker plugin PreImport-3a56d9d6-7fae-4731-bbb4-a80b0d6cf026
2023-12-01 06:37:37,293 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,293 - distributed.worker - INFO - Starting Worker plugin PreImport-1715fdbc-6c65-4b22-8c89-8733058c8f18
2023-12-01 06:37:37,294 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,322 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46821', status: init, memory: 0, processing: 0>
2023-12-01 06:37:37,323 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46821
2023-12-01 06:37:37,323 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39508
2023-12-01 06:37:37,324 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38305', status: init, memory: 0, processing: 0>
2023-12-01 06:37:37,324 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:37,324 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38305
2023-12-01 06:37:37,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39494
2023-12-01 06:37:37,325 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,325 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46207', status: init, memory: 0, processing: 0>
2023-12-01 06:37:37,325 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,325 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:37,326 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46207
2023-12-01 06:37:37,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39532
2023-12-01 06:37:37,326 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35829', status: init, memory: 0, processing: 0>
2023-12-01 06:37:37,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:37,327 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,327 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,327 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:37,327 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35829
2023-12-01 06:37:37,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39516
2023-12-01 06:37:37,328 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,328 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,328 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:37,328 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:37,329 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,329 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,329 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:37,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:37,331 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46563', status: init, memory: 0, processing: 0>
2023-12-01 06:37:37,331 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46563
2023-12-01 06:37:37,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39554
2023-12-01 06:37:37,332 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38453', status: init, memory: 0, processing: 0>
2023-12-01 06:37:37,333 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38453
2023-12-01 06:37:37,333 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39544
2023-12-01 06:37:37,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:37,334 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:37,335 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,335 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,335 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41657', status: init, memory: 0, processing: 0>
2023-12-01 06:37:37,336 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41657
2023-12-01 06:37:37,336 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39568
2023-12-01 06:37:37,336 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,337 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35707', status: init, memory: 0, processing: 0>
2023-12-01 06:37:37,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:37,337 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35707
2023-12-01 06:37:37,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39580
2023-12-01 06:37:37,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:37,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:37,339 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,339 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:37,339 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,340 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:37,340 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:37,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:37,342 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:37,351 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:37,351 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:37,352 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:37,352 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:37,352 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:37,352 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:37,352 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:37,352 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:37,357 - distributed.scheduler - INFO - Remove client Client-19ab0abd-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:37,357 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39382; closing.
2023-12-01 06:37:37,357 - distributed.scheduler - INFO - Remove client Client-19ab0abd-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:37,358 - distributed.scheduler - INFO - Close client connection: Client-19ab0abd-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:37,359 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40811'. Reason: nanny-close
2023-12-01 06:37:37,359 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35875'. Reason: nanny-close
2023-12-01 06:37:37,359 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35917'. Reason: nanny-close
2023-12-01 06:37:37,359 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:37,360 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35299'. Reason: nanny-close
2023-12-01 06:37:37,360 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:37,360 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38305. Reason: nanny-close
2023-12-01 06:37:37,361 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40821'. Reason: nanny-close
2023-12-01 06:37:37,361 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:37,361 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46821. Reason: nanny-close
2023-12-01 06:37:37,362 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36505'. Reason: nanny-close
2023-12-01 06:37:37,362 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:37,362 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41657. Reason: nanny-close
2023-12-01 06:37:37,362 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40401'. Reason: nanny-close
2023-12-01 06:37:37,362 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:37,362 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:37,363 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39494; closing.
2023-12-01 06:37:37,363 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35707. Reason: nanny-close
2023-12-01 06:37:37,363 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38305', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412657.363376')
2023-12-01 06:37:37,363 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41653'. Reason: nanny-close
2023-12-01 06:37:37,363 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:37,363 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35829. Reason: nanny-close
2023-12-01 06:37:37,364 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:37,364 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:37,364 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:37,364 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39508; closing.
2023-12-01 06:37:37,364 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46207. Reason: nanny-close
2023-12-01 06:37:37,365 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:37,365 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:37,365 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46821', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412657.3657393')
2023-12-01 06:37:37,365 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:37,365 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:37,365 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38453. Reason: nanny-close
2023-12-01 06:37:37,365 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:37,366 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39568; closing.
2023-12-01 06:37:37,366 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:37,366 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46563. Reason: nanny-close
2023-12-01 06:37:37,366 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:37,367 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41657', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412657.3669665')
2023-12-01 06:37:37,367 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:37,367 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39580; closing.
2023-12-01 06:37:37,367 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:37,367 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:37,368 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35707', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412657.3682742')
2023-12-01 06:37:37,368 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:37,368 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39516; closing.
2023-12-01 06:37:37,368 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:37,369 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35829', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412657.3696916')
2023-12-01 06:37:37,369 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:37,370 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39532; closing.
2023-12-01 06:37:37,370 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39544; closing.
2023-12-01 06:37:37,370 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46207', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412657.3708434')
2023-12-01 06:37:37,370 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:37,371 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38453', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412657.3711967')
2023-12-01 06:37:37,371 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39554; closing.
2023-12-01 06:37:37,372 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46563', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412657.3719273')
2023-12-01 06:37:37,372 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:37:37,372 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:39554>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-12-01 06:37:39,026 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:37:39,026 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:37:39,026 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:37:39,027 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:37:39,028 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-12-01 06:37:41,047 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:37:41,052 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46459 instead
  warnings.warn(
2023-12-01 06:37:41,057 - distributed.scheduler - INFO - State start
2023-12-01 06:37:41,080 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:37:41,081 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:37:41,081 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46459/status
2023-12-01 06:37:41,082 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:37:41,156 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36115'
2023-12-01 06:37:41,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39513'
2023-12-01 06:37:41,175 - distributed.scheduler - INFO - Receive client connection: Client-1f45e012-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:41,180 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44671'
2023-12-01 06:37:41,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49422
2023-12-01 06:37:41,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44095'
2023-12-01 06:37:41,199 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40771'
2023-12-01 06:37:41,208 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40157'
2023-12-01 06:37:41,217 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42883'
2023-12-01 06:37:41,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39673'
2023-12-01 06:37:43,075 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:43,075 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:43,075 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:43,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:43,080 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:43,080 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:43,100 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:43,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:43,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:43,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:43,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:43,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:43,104 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:43,107 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:43,108 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:43,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:43,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:43,112 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:43,123 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:43,123 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:43,128 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:43,138 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:43,138 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:43,142 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:46,007 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35899
2023-12-01 06:37:46,007 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35899
2023-12-01 06:37:46,007 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39163
2023-12-01 06:37:46,008 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,008 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,008 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:46,008 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:46,008 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0re8je9s
2023-12-01 06:37:46,008 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ac63836-73ba-4d93-9d55-c37c32e00766
2023-12-01 06:37:46,019 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36683
2023-12-01 06:37:46,021 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36683
2023-12-01 06:37:46,022 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32937
2023-12-01 06:37:46,022 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,022 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,022 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:46,022 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:46,022 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zj3fu3qz
2023-12-01 06:37:46,022 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89a074d6-49f9-4b12-b85b-4368a3ec31b0
2023-12-01 06:37:46,023 - distributed.worker - INFO - Starting Worker plugin PreImport-84e04684-4b3a-4e8e-a6b5-fc650254ae8d
2023-12-01 06:37:46,023 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,023 - distributed.worker - INFO - Starting Worker plugin RMMSetup-78f43d9d-6ce4-4ea1-8da4-b463a6cd860f
2023-12-01 06:37:46,030 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-043a81d9-8cef-4770-81be-ec2ac63ee3ca
2023-12-01 06:37:46,030 - distributed.worker - INFO - Starting Worker plugin PreImport-a58a8b5d-0a14-40e3-86a5-4f9e457682ff
2023-12-01 06:37:46,030 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,063 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36683', status: init, memory: 0, processing: 0>
2023-12-01 06:37:46,065 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36683
2023-12-01 06:37:46,065 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49428
2023-12-01 06:37:46,066 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35899', status: init, memory: 0, processing: 0>
2023-12-01 06:37:46,066 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:46,067 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35899
2023-12-01 06:37:46,067 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49426
2023-12-01 06:37:46,067 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,067 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:46,069 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:46,069 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,070 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,072 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:46,332 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43053
2023-12-01 06:37:46,334 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43053
2023-12-01 06:37:46,335 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40731
2023-12-01 06:37:46,335 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,335 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,335 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:46,335 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:46,335 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mutaqrdl
2023-12-01 06:37:46,336 - distributed.worker - INFO - Starting Worker plugin PreImport-0c76fb19-8fb8-4c4c-ba16-c8f5cde3d394
2023-12-01 06:37:46,337 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eee7d107-4539-464f-8bc8-de0b9e53be0c
2023-12-01 06:37:46,337 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68874113-0efe-4273-b82f-687cc07736b1
2023-12-01 06:37:46,347 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45703
2023-12-01 06:37:46,349 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45703
2023-12-01 06:37:46,349 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34155
2023-12-01 06:37:46,350 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,350 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,350 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:46,350 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:46,350 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jo0nfg7l
2023-12-01 06:37:46,351 - distributed.worker - INFO - Starting Worker plugin RMMSetup-329ac0bc-1ed9-4b7b-9f26-cb574d772215
2023-12-01 06:37:46,350 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42735
2023-12-01 06:37:46,352 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42735
2023-12-01 06:37:46,353 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40091
2023-12-01 06:37:46,353 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,353 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,353 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:46,353 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:46,353 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ze0pc4k7
2023-12-01 06:37:46,354 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ae5ff69-58af-41ad-850f-96438402759c
2023-12-01 06:37:46,355 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f14b2613-b936-49ca-a784-2a7ff5c43d52
2023-12-01 06:37:46,356 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44269
2023-12-01 06:37:46,357 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44269
2023-12-01 06:37:46,357 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35677
2023-12-01 06:37:46,357 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,357 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,357 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:46,358 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:46,358 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7zvkd1ib
2023-12-01 06:37:46,357 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39627
2023-12-01 06:37:46,358 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39627
2023-12-01 06:37:46,358 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43479
2023-12-01 06:37:46,358 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,358 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,358 - distributed.worker - INFO - Starting Worker plugin PreImport-2a67b74c-f87b-4b37-97e1-66197193c730
2023-12-01 06:37:46,358 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e340b739-ccdb-46a7-b6ce-a122ef9217c7
2023-12-01 06:37:46,358 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:46,358 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:46,358 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jt81878j
2023-12-01 06:37:46,359 - distributed.worker - INFO - Starting Worker plugin RMMSetup-570ecc63-5ee1-4e9a-8f6f-70a40e820e87
2023-12-01 06:37:46,359 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a28bb95-5586-4bee-b541-59bd2cf87328
2023-12-01 06:37:46,364 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42717
2023-12-01 06:37:46,366 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42717
2023-12-01 06:37:46,366 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39647
2023-12-01 06:37:46,366 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,366 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,367 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:46,367 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:46,367 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-28bisa75
2023-12-01 06:37:46,368 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d9c827e-58c1-45a5-8ba0-00197c38cc7b
2023-12-01 06:37:46,369 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4cfa7f8-b6a4-4958-b337-f54e2cd60458
2023-12-01 06:37:46,384 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,386 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3bc1772-7416-49b7-9ff9-85a24664e431
2023-12-01 06:37:46,387 - distributed.worker - INFO - Starting Worker plugin PreImport-d784bf91-eb5d-4465-a159-95c0f123cb33
2023-12-01 06:37:46,387 - distributed.worker - INFO - Starting Worker plugin PreImport-84daee29-d829-4537-b117-d52d5a5e263a
2023-12-01 06:37:46,387 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,387 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,388 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0ce9cf2-02c7-46ed-8c75-b52fd9611aaf
2023-12-01 06:37:46,389 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,391 - distributed.worker - INFO - Starting Worker plugin PreImport-c85992cf-bf09-445c-b201-2012c50d118d
2023-12-01 06:37:46,391 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,391 - distributed.worker - INFO - Starting Worker plugin PreImport-56de649c-a42b-4800-9f3e-4c6cc301be23
2023-12-01 06:37:46,392 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,415 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42735', status: init, memory: 0, processing: 0>
2023-12-01 06:37:46,415 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42735
2023-12-01 06:37:46,416 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49452
2023-12-01 06:37:46,416 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45703', status: init, memory: 0, processing: 0>
2023-12-01 06:37:46,417 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45703
2023-12-01 06:37:46,417 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:46,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49454
2023-12-01 06:37:46,417 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,417 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,418 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42717', status: init, memory: 0, processing: 0>
2023-12-01 06:37:46,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:46,418 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42717
2023-12-01 06:37:46,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49482
2023-12-01 06:37:46,419 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,419 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:46,419 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:46,419 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43053', status: init, memory: 0, processing: 0>
2023-12-01 06:37:46,420 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,420 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43053
2023-12-01 06:37:46,420 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49440
2023-12-01 06:37:46,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:46,422 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:46,422 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:46,423 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,423 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:46,428 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44269', status: init, memory: 0, processing: 0>
2023-12-01 06:37:46,428 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44269
2023-12-01 06:37:46,428 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49466
2023-12-01 06:37:46,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:46,431 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,431 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,433 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39627', status: init, memory: 0, processing: 0>
2023-12-01 06:37:46,434 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39627
2023-12-01 06:37:46,434 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49492
2023-12-01 06:37:46,434 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:46,435 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:46,436 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:46,436 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:46,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:46,536 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:46,537 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:46,537 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:46,537 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:46,537 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:46,537 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:46,537 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:46,538 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:46,542 - distributed.scheduler - INFO - Remove client Client-1f45e012-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:46,542 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49422; closing.
2023-12-01 06:37:46,542 - distributed.scheduler - INFO - Remove client Client-1f45e012-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:46,543 - distributed.scheduler - INFO - Close client connection: Client-1f45e012-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:46,543 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36115'. Reason: nanny-close
2023-12-01 06:37:46,544 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:46,545 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39513'. Reason: nanny-close
2023-12-01 06:37:46,545 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:46,545 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44269. Reason: nanny-close
2023-12-01 06:37:46,545 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44671'. Reason: nanny-close
2023-12-01 06:37:46,546 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:46,546 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35899. Reason: nanny-close
2023-12-01 06:37:46,546 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44095'. Reason: nanny-close
2023-12-01 06:37:46,546 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:46,546 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42717. Reason: nanny-close
2023-12-01 06:37:46,547 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40771'. Reason: nanny-close
2023-12-01 06:37:46,547 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:46,547 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45703. Reason: nanny-close
2023-12-01 06:37:46,547 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40157'. Reason: nanny-close
2023-12-01 06:37:46,547 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:46,548 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:46,548 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49466; closing.
2023-12-01 06:37:46,548 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39627. Reason: nanny-close
2023-12-01 06:37:46,548 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42883'. Reason: nanny-close
2023-12-01 06:37:46,548 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44269', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412666.5487547')
2023-12-01 06:37:46,548 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:46,548 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:46,549 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43053. Reason: nanny-close
2023-12-01 06:37:46,549 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39673'. Reason: nanny-close
2023-12-01 06:37:46,549 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:46,549 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:46,549 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:46,549 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36683. Reason: nanny-close
2023-12-01 06:37:46,550 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42735. Reason: nanny-close
2023-12-01 06:37:46,550 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49454; closing.
2023-12-01 06:37:46,550 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:46,550 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:46,550 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49482; closing.
2023-12-01 06:37:46,550 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49426; closing.
2023-12-01 06:37:46,551 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:46,551 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45703', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412666.5513854')
2023-12-01 06:37:46,551 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:46,551 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:46,551 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42717', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412666.5518894')
2023-12-01 06:37:46,552 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35899', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412666.552252')
2023-12-01 06:37:46,552 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:46,552 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:46,552 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:46,553 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49492; closing.
2023-12-01 06:37:46,553 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:46,553 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:46,553 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49440; closing.
2023-12-01 06:37:46,553 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49428; closing.
2023-12-01 06:37:46,554 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:46,554 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49452; closing.
2023-12-01 06:37:46,554 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39627', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412666.5543704')
2023-12-01 06:37:46,554 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43053', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412666.5547526')
2023-12-01 06:37:46,554 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:46,555 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36683', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412666.5551062')
2023-12-01 06:37:46,555 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42735', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412666.555455')
2023-12-01 06:37:46,555 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:37:48,161 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:37:48,162 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:37:48,162 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:37:48,163 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:37:48,164 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-12-01 06:37:50,154 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:37:50,158 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:37:50,161 - distributed.scheduler - INFO - State start
2023-12-01 06:37:50,182 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:37:50,183 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:37:50,184 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:37:50,184 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:37:50,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34567'
2023-12-01 06:37:50,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34691'
2023-12-01 06:37:50,475 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46701'
2023-12-01 06:37:50,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46617'
2023-12-01 06:37:50,491 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45491'
2023-12-01 06:37:50,500 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33647'
2023-12-01 06:37:50,509 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37889'
2023-12-01 06:37:50,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34185'
2023-12-01 06:37:51,531 - distributed.scheduler - INFO - Receive client connection: Client-24c340d5-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:51,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47172
2023-12-01 06:37:52,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:52,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:52,393 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:52,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:52,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:52,418 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:52,431 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:52,432 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:52,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:52,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:52,435 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:52,435 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:52,436 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:52,438 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:52,439 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:52,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:52,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:52,472 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:52,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:52,474 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:52,477 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:52,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:37:52,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:37:52,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:37:56,835 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41893
2023-12-01 06:37:56,836 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41893
2023-12-01 06:37:56,836 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40183
2023-12-01 06:37:56,836 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:56,836 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:56,836 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:56,836 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:56,836 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u77qpj1i
2023-12-01 06:37:56,837 - distributed.worker - INFO - Starting Worker plugin RMMSetup-92829950-4ebe-4aef-be36-a141e86c35fe
2023-12-01 06:37:56,932 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34083
2023-12-01 06:37:56,933 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34083
2023-12-01 06:37:56,933 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41187
2023-12-01 06:37:56,933 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:56,934 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:56,934 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:56,934 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:56,934 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m7m0xg9w
2023-12-01 06:37:56,934 - distributed.worker - INFO - Starting Worker plugin RMMSetup-23dede42-c971-467b-92c7-b1c9a185f7dc
2023-12-01 06:37:57,116 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46647
2023-12-01 06:37:57,117 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46647
2023-12-01 06:37:57,117 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36573
2023-12-01 06:37:57,117 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,118 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,118 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:57,118 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:57,118 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ne79paee
2023-12-01 06:37:57,118 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b045771-6d62-4b3b-b26f-103e1ede5b06
2023-12-01 06:37:57,118 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ee7e202-0fbd-4589-b115-bbe7b6a62550
2023-12-01 06:37:57,124 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8bf05514-fee3-4a68-926b-08043986cd10
2023-12-01 06:37:57,125 - distributed.worker - INFO - Starting Worker plugin PreImport-9464d8ac-f954-4d17-b72c-77b9823f3a2f
2023-12-01 06:37:57,125 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,126 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40161
2023-12-01 06:37:57,127 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40161
2023-12-01 06:37:57,127 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39617
2023-12-01 06:37:57,127 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,127 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,127 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:57,128 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:57,128 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dfn6gqwd
2023-12-01 06:37:57,128 - distributed.worker - INFO - Starting Worker plugin RMMSetup-737a4fae-2fd1-4880-a8bd-5ce6b2451dd2
2023-12-01 06:37:57,135 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42765
2023-12-01 06:37:57,136 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42765
2023-12-01 06:37:57,136 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40247
2023-12-01 06:37:57,136 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,136 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,136 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:57,136 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:57,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lafnj4cx
2023-12-01 06:37:57,137 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb201343-9cce-4d4e-965c-192c2a2280fa
2023-12-01 06:37:57,138 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5c03745-9122-4e70-8cb2-248072dc8b53
2023-12-01 06:37:57,153 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44687
2023-12-01 06:37:57,155 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44687
2023-12-01 06:37:57,155 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43415
2023-12-01 06:37:57,155 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,155 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,155 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:57,156 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:57,156 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0f0mehr7
2023-12-01 06:37:57,157 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8b2d7f6-62b5-4c14-a218-42f111520df9
2023-12-01 06:37:57,157 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39943
2023-12-01 06:37:57,158 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39943
2023-12-01 06:37:57,158 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43891
2023-12-01 06:37:57,158 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,158 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,158 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:57,158 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:57,158 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b0z1p1ml
2023-12-01 06:37:57,159 - distributed.worker - INFO - Starting Worker plugin RMMSetup-010f4057-5e2e-425b-a303-47cc13607741
2023-12-01 06:37:57,161 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43667
2023-12-01 06:37:57,162 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43667
2023-12-01 06:37:57,162 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36597
2023-12-01 06:37:57,162 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,162 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,162 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:37:57,162 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:37:57,162 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-42gt0s0x
2023-12-01 06:37:57,163 - distributed.worker - INFO - Starting Worker plugin PreImport-ca0dc04d-7bd1-43ca-b21d-5f328b139b36
2023-12-01 06:37:57,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5093006-75f5-4b77-bd14-e6eb41dfd875
2023-12-01 06:37:57,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7bbbb206-49f6-435e-b22a-61faaae831b4
2023-12-01 06:37:57,169 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41893', status: init, memory: 0, processing: 0>
2023-12-01 06:37:57,171 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41893
2023-12-01 06:37:57,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47196
2023-12-01 06:37:57,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:57,174 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,174 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,176 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e8846d38-9e73-4118-9d9f-07776c1cb661
2023-12-01 06:37:57,176 - distributed.worker - INFO - Starting Worker plugin PreImport-9ce48b44-cd77-4d8d-9ac2-d8298335142c
2023-12-01 06:37:57,176 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:57,211 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34083', status: init, memory: 0, processing: 0>
2023-12-01 06:37:57,211 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34083
2023-12-01 06:37:57,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47200
2023-12-01 06:37:57,212 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:57,213 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,213 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:57,308 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ca31e42-debf-4774-83be-b6a08e71be22
2023-12-01 06:37:57,308 - distributed.worker - INFO - Starting Worker plugin PreImport-9d3dafb2-b646-480f-975d-b69872ce59a4
2023-12-01 06:37:57,308 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,309 - distributed.worker - INFO - Starting Worker plugin PreImport-06329596-8235-453c-98c6-4e415b4ead35
2023-12-01 06:37:57,309 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,336 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46647', status: init, memory: 0, processing: 0>
2023-12-01 06:37:57,337 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46647
2023-12-01 06:37:57,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47210
2023-12-01 06:37:57,338 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:57,339 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,339 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:57,346 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40161', status: init, memory: 0, processing: 0>
2023-12-01 06:37:57,347 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40161
2023-12-01 06:37:57,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47224
2023-12-01 06:37:57,348 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:57,349 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,349 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:57,360 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e24ab9da-68bc-4eee-b0d1-9b646954d25c
2023-12-01 06:37:57,360 - distributed.worker - INFO - Starting Worker plugin PreImport-2c8e8f8c-d97b-4347-8d54-34be1bd8a2ef
2023-12-01 06:37:57,360 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,361 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b491983c-b308-4e28-925e-519307f5e2c0
2023-12-01 06:37:57,361 - distributed.worker - INFO - Starting Worker plugin PreImport-e6c08ed8-2f93-4962-98ea-d64afdc43b5e
2023-12-01 06:37:57,362 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,371 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,373 - distributed.worker - INFO - Starting Worker plugin PreImport-65ad6f8c-6189-406d-8914-380c2766caa8
2023-12-01 06:37:57,373 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,386 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39943', status: init, memory: 0, processing: 0>
2023-12-01 06:37:57,387 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39943
2023-12-01 06:37:57,387 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47240
2023-12-01 06:37:57,388 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:57,388 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,389 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,390 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:57,395 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44687', status: init, memory: 0, processing: 0>
2023-12-01 06:37:57,395 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44687
2023-12-01 06:37:57,395 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47250
2023-12-01 06:37:57,396 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:57,397 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,397 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:57,406 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43667', status: init, memory: 0, processing: 0>
2023-12-01 06:37:57,407 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43667
2023-12-01 06:37:57,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47262
2023-12-01 06:37:57,409 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:57,409 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,410 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,410 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42765', status: init, memory: 0, processing: 0>
2023-12-01 06:37:57,410 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42765
2023-12-01 06:37:57,410 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47278
2023-12-01 06:37:57,411 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:37:57,412 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:57,412 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:37:57,412 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:37:57,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:37:57,493 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:57,493 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:57,494 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:57,494 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:57,494 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:57,494 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:57,494 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:57,494 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:37:57,504 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:37:57,505 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:37:57,505 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:37:57,505 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:37:57,505 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:37:57,505 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:37:57,505 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:37:57,505 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:37:57,511 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:37:57,513 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:37:57,515 - distributed.scheduler - INFO - Remove client Client-24c340d5-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:57,515 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47172; closing.
2023-12-01 06:37:57,515 - distributed.scheduler - INFO - Remove client Client-24c340d5-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:57,516 - distributed.scheduler - INFO - Close client connection: Client-24c340d5-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:37:57,517 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34567'. Reason: nanny-close
2023-12-01 06:37:57,517 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:57,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34691'. Reason: nanny-close
2023-12-01 06:37:57,518 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:57,518 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41893. Reason: nanny-close
2023-12-01 06:37:57,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46701'. Reason: nanny-close
2023-12-01 06:37:57,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:57,519 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42765. Reason: nanny-close
2023-12-01 06:37:57,519 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46617'. Reason: nanny-close
2023-12-01 06:37:57,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:57,519 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44687. Reason: nanny-close
2023-12-01 06:37:57,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45491'. Reason: nanny-close
2023-12-01 06:37:57,520 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:57,520 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34083. Reason: nanny-close
2023-12-01 06:37:57,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33647'. Reason: nanny-close
2023-12-01 06:37:57,521 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:57,521 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47196; closing.
2023-12-01 06:37:57,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:57,521 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43667. Reason: nanny-close
2023-12-01 06:37:57,521 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41893', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412677.5213723')
2023-12-01 06:37:57,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37889'. Reason: nanny-close
2023-12-01 06:37:57,521 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:57,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:57,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:57,521 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40161. Reason: nanny-close
2023-12-01 06:37:57,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34185'. Reason: nanny-close
2023-12-01 06:37:57,522 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:57,522 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:37:57,522 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46647. Reason: nanny-close
2023-12-01 06:37:57,522 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39943. Reason: nanny-close
2023-12-01 06:37:57,523 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:57,523 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47200; closing.
2023-12-01 06:37:57,523 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47250; closing.
2023-12-01 06:37:57,523 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:57,523 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47278; closing.
2023-12-01 06:37:57,523 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:57,523 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:57,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:57,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34083', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412677.5239995')
2023-12-01 06:37:57,524 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:57,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44687', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412677.5243926')
2023-12-01 06:37:57,524 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:57,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42765', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412677.5247278')
2023-12-01 06:37:57,524 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:37:57,525 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47262; closing.
2023-12-01 06:37:57,525 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:57,525 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:57,526 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43667', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412677.5262084')
2023-12-01 06:37:57,526 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:57,526 - distributed.nanny - INFO - Worker closed
2023-12-01 06:37:57,526 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47224; closing.
2023-12-01 06:37:57,526 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47210; closing.
2023-12-01 06:37:57,527 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40161', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412677.5271852')
2023-12-01 06:37:57,527 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46647', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412677.5276003')
2023-12-01 06:37:57,527 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47240; closing.
2023-12-01 06:37:57,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412677.5283098')
2023-12-01 06:37:57,528 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:37:59,334 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:37:59,335 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:37:59,335 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:37:59,336 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:37:59,337 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-12-01 06:38:01,578 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:01,583 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:38:01,587 - distributed.scheduler - INFO - State start
2023-12-01 06:38:01,829 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:01,831 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:38:01,831 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:38:01,832 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:38:02,112 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46211'
2023-12-01 06:38:02,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35181'
2023-12-01 06:38:02,143 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36033'
2023-12-01 06:38:02,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36685'
2023-12-01 06:38:02,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33881'
2023-12-01 06:38:02,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38877'
2023-12-01 06:38:02,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44319'
2023-12-01 06:38:02,201 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43571'
2023-12-01 06:38:03,766 - distributed.scheduler - INFO - Receive client connection: Client-2b6e7c3f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:03,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43896
2023-12-01 06:38:04,073 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:04,073 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:04,077 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:04,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:04,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:04,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:04,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:04,094 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:04,094 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:04,094 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:04,094 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:04,098 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:04,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:04,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:04,132 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:04,148 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:04,148 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:04,152 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:04,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:04,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:04,158 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:04,226 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:04,226 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:04,232 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:07,768 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35613
2023-12-01 06:38:07,769 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35613
2023-12-01 06:38:07,769 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44957
2023-12-01 06:38:07,769 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:07,769 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:07,769 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:07,769 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:07,769 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xzr_lled
2023-12-01 06:38:07,770 - distributed.worker - INFO - Starting Worker plugin PreImport-37af071a-c031-4a1f-8603-597d5c181735
2023-12-01 06:38:07,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aecedaf9-1f9e-48ce-960d-2243675e440e
2023-12-01 06:38:07,770 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32a072c1-0f6d-4533-9c94-52834c0bbff6
2023-12-01 06:38:07,775 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40575
2023-12-01 06:38:07,776 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40575
2023-12-01 06:38:07,776 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40497
2023-12-01 06:38:07,776 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:07,776 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:07,776 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:07,777 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:07,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jm928nxj
2023-12-01 06:38:07,777 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f3c73c3-3ca4-4d98-881c-e6e4976a5bf9
2023-12-01 06:38:07,777 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46bc05ee-716d-4f06-b46e-602d8517efb1
2023-12-01 06:38:07,787 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36367
2023-12-01 06:38:07,788 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36367
2023-12-01 06:38:07,788 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43621
2023-12-01 06:38:07,788 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:07,788 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:07,788 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:07,789 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:07,789 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zukzrtbc
2023-12-01 06:38:07,789 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c49a9ae-c8e8-4066-9382-fe0c8c99ac01
2023-12-01 06:38:07,792 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33287
2023-12-01 06:38:07,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33287
2023-12-01 06:38:07,793 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37459
2023-12-01 06:38:07,793 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:07,793 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:07,793 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:07,794 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:07,794 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0d34cfvq
2023-12-01 06:38:07,794 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de6a7777-58df-42c3-b5ec-8e3c298dcb49
2023-12-01 06:38:07,793 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37469
2023-12-01 06:38:07,794 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37469
2023-12-01 06:38:07,794 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43453
2023-12-01 06:38:07,794 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:07,795 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:07,795 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:07,795 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:07,795 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-icmdzntq
2023-12-01 06:38:07,795 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62e1b5a9-aa6d-4570-bac3-7fd1c8b55819
2023-12-01 06:38:07,797 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43563
2023-12-01 06:38:07,798 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43563
2023-12-01 06:38:07,798 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34643
2023-12-01 06:38:07,798 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:07,798 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:07,798 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:07,799 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:07,799 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zgl6apc1
2023-12-01 06:38:07,799 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b0981019-98f2-4ac7-9604-2ebbe2faf8e9
2023-12-01 06:38:07,802 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35329
2023-12-01 06:38:07,802 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35329
2023-12-01 06:38:07,803 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45933
2023-12-01 06:38:07,803 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:07,803 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:07,803 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:07,803 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:07,803 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ii11421j
2023-12-01 06:38:07,803 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-667291e2-f731-4b40-93c0-686e6095030e
2023-12-01 06:38:07,807 - distributed.worker - INFO - Starting Worker plugin RMMSetup-707091d1-c57f-4133-9ff3-c43ae0fbf71b
2023-12-01 06:38:07,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33349
2023-12-01 06:38:07,809 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33349
2023-12-01 06:38:07,809 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37413
2023-12-01 06:38:07,809 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:07,809 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:07,809 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:07,810 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:07,810 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cq76sspm
2023-12-01 06:38:07,810 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84fb1019-bd1b-4812-9330-7ef49f633dfa
2023-12-01 06:38:08,054 - distributed.worker - INFO - Starting Worker plugin PreImport-e0c00f16-9762-4746-9d23-8d96e388bd5a
2023-12-01 06:38:08,055 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,067 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,068 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94ebfbd3-54f2-4f03-95bc-ce293587dd85
2023-12-01 06:38:08,068 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e71fe331-20d5-4a17-abd8-15539e1e8f06
2023-12-01 06:38:08,068 - distributed.worker - INFO - Starting Worker plugin PreImport-6b124ca8-4ee7-4152-86c1-374865e71b8c
2023-12-01 06:38:08,068 - distributed.worker - INFO - Starting Worker plugin PreImport-1f15e860-053e-49a2-a42a-c879ef99d0f4
2023-12-01 06:38:08,068 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,069 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,070 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-094fb03a-7049-4beb-be3b-63b21eba83f8
2023-12-01 06:38:08,071 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22d7e92c-eb3a-4358-8401-508368fea98f
2023-12-01 06:38:08,071 - distributed.worker - INFO - Starting Worker plugin PreImport-dc96e394-5667-4aa3-8318-975e73583fe1
2023-12-01 06:38:08,072 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,072 - distributed.worker - INFO - Starting Worker plugin PreImport-d1bfab7e-0232-4c5d-8e7d-41af3ee09784
2023-12-01 06:38:08,072 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,072 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a4166be1-1619-45b1-bcc7-dcd9e10a3ddb
2023-12-01 06:38:08,072 - distributed.worker - INFO - Starting Worker plugin PreImport-638c516a-e5f4-46ad-bf08-b4fad03c732f
2023-12-01 06:38:08,073 - distributed.worker - INFO - Starting Worker plugin PreImport-fb7d1d7c-2238-4b4d-8538-00b3a7793f42
2023-12-01 06:38:08,073 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,073 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,079 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40575', status: init, memory: 0, processing: 0>
2023-12-01 06:38:08,080 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40575
2023-12-01 06:38:08,080 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43906
2023-12-01 06:38:08,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:08,082 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:08,082 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,084 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:08,099 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43563', status: init, memory: 0, processing: 0>
2023-12-01 06:38:08,100 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43563
2023-12-01 06:38:08,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43952
2023-12-01 06:38:08,101 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33287', status: init, memory: 0, processing: 0>
2023-12-01 06:38:08,101 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:08,101 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33287
2023-12-01 06:38:08,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43926
2023-12-01 06:38:08,102 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:08,102 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,102 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33349', status: init, memory: 0, processing: 0>
2023-12-01 06:38:08,102 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:08,103 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33349
2023-12-01 06:38:08,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43960
2023-12-01 06:38:08,103 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:08,103 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,104 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:08,104 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:08,104 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35613', status: init, memory: 0, processing: 0>
2023-12-01 06:38:08,105 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:08,105 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,105 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35613
2023-12-01 06:38:08,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43916
2023-12-01 06:38:08,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:08,106 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36367', status: init, memory: 0, processing: 0>
2023-12-01 06:38:08,106 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:08,106 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36367
2023-12-01 06:38:08,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43936
2023-12-01 06:38:08,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:08,107 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:08,107 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:08,109 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:08,109 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,110 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35329', status: init, memory: 0, processing: 0>
2023-12-01 06:38:08,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:08,110 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35329
2023-12-01 06:38:08,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43974
2023-12-01 06:38:08,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:08,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:08,113 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:08,113 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,115 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:08,117 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37469', status: init, memory: 0, processing: 0>
2023-12-01 06:38:08,117 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37469
2023-12-01 06:38:08,118 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43968
2023-12-01 06:38:08,119 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:08,120 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:08,120 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:08,122 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:08,174 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:08,174 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:08,174 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:08,174 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:08,174 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:08,175 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:08,175 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:08,175 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:08,186 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:38:08,186 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:38:08,186 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:38:08,186 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:38:08,186 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:38:08,186 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:38:08,186 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:38:08,187 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:38:08,193 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:08,195 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:08,197 - distributed.scheduler - INFO - Remove client Client-2b6e7c3f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:08,197 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43896; closing.
2023-12-01 06:38:08,197 - distributed.scheduler - INFO - Remove client Client-2b6e7c3f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:08,198 - distributed.scheduler - INFO - Close client connection: Client-2b6e7c3f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:08,199 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46211'. Reason: nanny-close
2023-12-01 06:38:08,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:08,200 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35181'. Reason: nanny-close
2023-12-01 06:38:08,200 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:08,200 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36367. Reason: nanny-close
2023-12-01 06:38:08,200 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36033'. Reason: nanny-close
2023-12-01 06:38:08,201 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:08,201 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35329. Reason: nanny-close
2023-12-01 06:38:08,201 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36685'. Reason: nanny-close
2023-12-01 06:38:08,201 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:08,202 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33349. Reason: nanny-close
2023-12-01 06:38:08,202 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33881'. Reason: nanny-close
2023-12-01 06:38:08,202 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:08,202 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43563. Reason: nanny-close
2023-12-01 06:38:08,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:08,203 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38877'. Reason: nanny-close
2023-12-01 06:38:08,203 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43936; closing.
2023-12-01 06:38:08,203 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:08,203 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35613. Reason: nanny-close
2023-12-01 06:38:08,203 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36367', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412688.203706')
2023-12-01 06:38:08,203 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44319'. Reason: nanny-close
2023-12-01 06:38:08,204 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:08,204 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:08,204 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:08,204 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37469. Reason: nanny-close
2023-12-01 06:38:08,204 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43571'. Reason: nanny-close
2023-12-01 06:38:08,204 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:08,204 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40575. Reason: nanny-close
2023-12-01 06:38:08,205 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:08,205 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:08,205 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43974; closing.
2023-12-01 06:38:08,205 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43960; closing.
2023-12-01 06:38:08,205 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:08,206 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:08,206 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33287. Reason: nanny-close
2023-12-01 06:38:08,206 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43952; closing.
2023-12-01 06:38:08,206 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35329', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412688.2064686')
2023-12-01 06:38:08,206 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:08,206 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412688.2068458')
2023-12-01 06:38:08,206 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:08,206 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:08,207 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:08,207 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43563', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412688.2074234')
2023-12-01 06:38:08,207 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:08,208 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:08,208 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43916; closing.
2023-12-01 06:38:08,208 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:08,208 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43968; closing.
2023-12-01 06:38:08,208 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43906; closing.
2023-12-01 06:38:08,209 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35613', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412688.2092798')
2023-12-01 06:38:08,209 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:08,209 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:08,209 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37469', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412688.209708')
2023-12-01 06:38:08,210 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40575', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412688.2100685')
2023-12-01 06:38:08,210 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43926; closing.
2023-12-01 06:38:08,210 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33287', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412688.2108607')
2023-12-01 06:38:08,211 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:38:09,816 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:38:09,817 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:38:09,817 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:38:09,818 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:38:09,819 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-12-01 06:38:11,960 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:11,964 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:38:11,968 - distributed.scheduler - INFO - State start
2023-12-01 06:38:11,988 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:11,989 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:38:11,990 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:38:11,990 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:38:12,155 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34585'
2023-12-01 06:38:12,167 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45129'
2023-12-01 06:38:12,171 - distributed.scheduler - INFO - Receive client connection: Client-31b3e07f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:12,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41869'
2023-12-01 06:38:12,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56060
2023-12-01 06:38:12,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44199'
2023-12-01 06:38:12,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45233'
2023-12-01 06:38:12,204 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41617'
2023-12-01 06:38:12,212 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45357'
2023-12-01 06:38:12,221 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46719'
2023-12-01 06:38:14,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:14,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:14,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:14,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:14,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:14,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:14,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:14,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:14,023 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:14,023 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:14,024 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:14,024 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:14,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:14,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:14,065 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:14,086 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:14,086 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:14,089 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:14,089 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:14,090 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:14,093 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:14,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:14,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:14,131 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:17,036 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36587
2023-12-01 06:38:17,037 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36587
2023-12-01 06:38:17,037 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34295
2023-12-01 06:38:17,037 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,037 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,037 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:17,037 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:17,037 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ejyd3m4z
2023-12-01 06:38:17,038 - distributed.worker - INFO - Starting Worker plugin PreImport-fe8bfae4-4ab0-48b2-9842-cbb3b7bf7bb9
2023-12-01 06:38:17,038 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-348768a5-5ae3-4e8c-af01-e67ca8e3f5c1
2023-12-01 06:38:17,038 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84142474-157e-4a63-8e1c-1afcbd1b912d
2023-12-01 06:38:17,042 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41165
2023-12-01 06:38:17,043 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41165
2023-12-01 06:38:17,043 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33821
2023-12-01 06:38:17,043 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,043 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,043 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:17,044 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:17,044 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5csic549
2023-12-01 06:38:17,044 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59ea7481-320e-4f66-9709-cf607f3ab22b
2023-12-01 06:38:17,179 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-762cb9af-878a-4f5b-9448-436cb5797481
2023-12-01 06:38:17,179 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,180 - distributed.worker - INFO - Starting Worker plugin PreImport-b5820109-52bb-4070-969b-d3470eb53c8a
2023-12-01 06:38:17,181 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,214 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41165', status: init, memory: 0, processing: 0>
2023-12-01 06:38:17,216 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41165
2023-12-01 06:38:17,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56086
2023-12-01 06:38:17,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36587', status: init, memory: 0, processing: 0>
2023-12-01 06:38:17,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:17,218 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36587
2023-12-01 06:38:17,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56084
2023-12-01 06:38:17,218 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,219 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,219 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:17,220 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,220 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:17,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:17,367 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39595
2023-12-01 06:38:17,368 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39595
2023-12-01 06:38:17,368 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39047
2023-12-01 06:38:17,368 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,368 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,368 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:17,369 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:17,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9u8tmbd7
2023-12-01 06:38:17,369 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd148f1a-ba33-4963-aeca-e0738b18c583
2023-12-01 06:38:17,371 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43369
2023-12-01 06:38:17,372 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43369
2023-12-01 06:38:17,372 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40193
2023-12-01 06:38:17,372 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,372 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,372 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:17,373 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:17,373 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7qcocbcf
2023-12-01 06:38:17,373 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aedf9189-0d00-4547-93bb-c04ec9972871
2023-12-01 06:38:17,387 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39949
2023-12-01 06:38:17,388 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39949
2023-12-01 06:38:17,388 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42997
2023-12-01 06:38:17,388 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,388 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,388 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:17,389 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:17,389 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hxi66agw
2023-12-01 06:38:17,389 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd36915d-0d87-41d8-ab48-38fab34ab4a3
2023-12-01 06:38:17,391 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44375
2023-12-01 06:38:17,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44375
2023-12-01 06:38:17,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44341
2023-12-01 06:38:17,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,392 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,392 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:17,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:17,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ztp313rk
2023-12-01 06:38:17,393 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2139b680-7859-4593-a216-487d3a617e73
2023-12-01 06:38:17,397 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41777
2023-12-01 06:38:17,398 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41777
2023-12-01 06:38:17,398 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39803
2023-12-01 06:38:17,398 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,398 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,398 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:17,399 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:17,399 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-78z6spnq
2023-12-01 06:38:17,399 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81739b62-6541-4bac-9ca0-296b45b3af9f
2023-12-01 06:38:17,399 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ce33a19-7650-4919-9e2e-66c76d7e5541
2023-12-01 06:38:17,401 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36199
2023-12-01 06:38:17,402 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36199
2023-12-01 06:38:17,402 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43383
2023-12-01 06:38:17,402 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,402 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,402 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:17,403 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:17,403 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gdiul5sv
2023-12-01 06:38:17,403 - distributed.worker - INFO - Starting Worker plugin PreImport-6919c65f-0494-431d-aa08-4c411addb175
2023-12-01 06:38:17,404 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f2b4254-b588-4589-9a23-762692c58e51
2023-12-01 06:38:17,404 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64cdea3e-589e-4276-bdde-43c64cfe6257
2023-12-01 06:38:17,543 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8c1100af-9fc9-405e-89e7-dad906586b4e
2023-12-01 06:38:17,544 - distributed.worker - INFO - Starting Worker plugin PreImport-06925492-7b4f-427d-adc0-f13e85de1223
2023-12-01 06:38:17,544 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,560 - distributed.worker - INFO - Starting Worker plugin PreImport-3655fc6e-e7ba-4b24-b703-5089dbc7ff98
2023-12-01 06:38:17,560 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bfee29ad-b001-4667-95fe-8284c95c2ae5
2023-12-01 06:38:17,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df421661-ba13-4fea-b872-96feab68c9e5
2023-12-01 06:38:17,560 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,560 - distributed.worker - INFO - Starting Worker plugin PreImport-7959d430-0ac0-432a-ac9e-ba4db7722ccc
2023-12-01 06:38:17,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c597432-f06e-4f45-829b-acd579f8baa4
2023-12-01 06:38:17,560 - distributed.worker - INFO - Starting Worker plugin PreImport-9a4ab2fc-dc4a-43f7-b268-e0118117ad74
2023-12-01 06:38:17,560 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,560 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,562 - distributed.worker - INFO - Starting Worker plugin PreImport-790dc4ea-0e47-474b-acba-c82be42a7c0f
2023-12-01 06:38:17,562 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,570 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39595', status: init, memory: 0, processing: 0>
2023-12-01 06:38:17,571 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39595
2023-12-01 06:38:17,571 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56100
2023-12-01 06:38:17,572 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:17,573 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,573 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,574 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:17,587 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41777', status: init, memory: 0, processing: 0>
2023-12-01 06:38:17,588 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41777
2023-12-01 06:38:17,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56106
2023-12-01 06:38:17,589 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39949', status: init, memory: 0, processing: 0>
2023-12-01 06:38:17,589 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:17,590 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39949
2023-12-01 06:38:17,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56130
2023-12-01 06:38:17,590 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,590 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:17,591 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,591 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:17,592 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36199', status: init, memory: 0, processing: 0>
2023-12-01 06:38:17,592 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36199
2023-12-01 06:38:17,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56144
2023-12-01 06:38:17,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:17,594 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43369', status: init, memory: 0, processing: 0>
2023-12-01 06:38:17,594 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43369
2023-12-01 06:38:17,594 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:17,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56122
2023-12-01 06:38:17,595 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,595 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,595 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44375', status: init, memory: 0, processing: 0>
2023-12-01 06:38:17,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:17,596 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44375
2023-12-01 06:38:17,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56158
2023-12-01 06:38:17,596 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,596 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,597 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:17,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:17,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:17,598 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:17,598 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:17,600 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:17,675 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:38:17,675 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:38:17,675 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:38:17,675 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:38:17,675 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:38:17,675 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:38:17,675 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:38:17,675 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:38:17,680 - distributed.scheduler - INFO - Remove client Client-31b3e07f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:17,680 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56060; closing.
2023-12-01 06:38:17,681 - distributed.scheduler - INFO - Remove client Client-31b3e07f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:17,681 - distributed.scheduler - INFO - Close client connection: Client-31b3e07f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:17,682 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34585'. Reason: nanny-close
2023-12-01 06:38:17,683 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:17,684 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45129'. Reason: nanny-close
2023-12-01 06:38:17,684 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:17,684 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36199. Reason: nanny-close
2023-12-01 06:38:17,684 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41869'. Reason: nanny-close
2023-12-01 06:38:17,685 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:17,685 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36587. Reason: nanny-close
2023-12-01 06:38:17,685 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44199'. Reason: nanny-close
2023-12-01 06:38:17,685 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:17,685 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39595. Reason: nanny-close
2023-12-01 06:38:17,686 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45233'. Reason: nanny-close
2023-12-01 06:38:17,686 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:17,686 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41777. Reason: nanny-close
2023-12-01 06:38:17,686 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41617'. Reason: nanny-close
2023-12-01 06:38:17,687 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:17,687 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56144; closing.
2023-12-01 06:38:17,687 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:17,687 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41165. Reason: nanny-close
2023-12-01 06:38:17,687 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45357'. Reason: nanny-close
2023-12-01 06:38:17,687 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36199', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412697.687374')
2023-12-01 06:38:17,687 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:17,687 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:17,687 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:17,687 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44375. Reason: nanny-close
2023-12-01 06:38:17,687 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46719'. Reason: nanny-close
2023-12-01 06:38:17,688 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:17,688 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39949. Reason: nanny-close
2023-12-01 06:38:17,688 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:17,688 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43369. Reason: nanny-close
2023-12-01 06:38:17,688 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:17,689 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56106; closing.
2023-12-01 06:38:17,689 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:17,689 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56084; closing.
2023-12-01 06:38:17,689 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56100; closing.
2023-12-01 06:38:17,689 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:17,689 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:17,689 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:17,690 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412697.6901228')
2023-12-01 06:38:17,690 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:17,690 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:17,690 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412697.6905334')
2023-12-01 06:38:17,690 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39595', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412697.6908853')
2023-12-01 06:38:17,691 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:17,691 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:17,691 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56086; closing.
2023-12-01 06:38:17,691 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:17,692 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:17,692 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41165', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412697.6923437')
2023-12-01 06:38:17,692 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56158; closing.
2023-12-01 06:38:17,692 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56130; closing.
2023-12-01 06:38:17,693 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:17,693 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44375', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412697.6934474')
2023-12-01 06:38:17,693 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39949', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412697.6938841')
2023-12-01 06:38:17,694 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56122; closing.
2023-12-01 06:38:17,694 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43369', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412697.694617')
2023-12-01 06:38:17,694 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:38:19,501 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:38:19,501 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:38:19,502 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:38:19,503 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:38:19,503 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-12-01 06:38:21,531 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:21,535 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:38:21,538 - distributed.scheduler - INFO - State start
2023-12-01 06:38:21,559 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:21,560 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:38:21,560 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:38:21,561 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:38:21,803 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42437'
2023-12-01 06:38:23,239 - distributed.scheduler - INFO - Receive client connection: Client-376e437b-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:23,252 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41104
2023-12-01 06:38:23,321 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:23,321 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:23,835 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:24,799 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45667
2023-12-01 06:38:24,800 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45667
2023-12-01 06:38:24,800 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-12-01 06:38:24,801 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:24,801 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:24,801 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:24,801 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-01 06:38:24,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-naxcb512
2023-12-01 06:38:24,801 - distributed.worker - INFO - Starting Worker plugin PreImport-1eb92968-b408-49ca-9b99-2f038fd0d8c7
2023-12-01 06:38:24,802 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-159375b5-5a40-4df9-84e9-9f7fb041b63b
2023-12-01 06:38:24,802 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ffa5e24-a8ab-4b9e-b139-693322613f08
2023-12-01 06:38:24,802 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:24,833 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45667', status: init, memory: 0, processing: 0>
2023-12-01 06:38:24,834 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45667
2023-12-01 06:38:24,834 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41116
2023-12-01 06:38:24,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:24,836 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:24,836 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:24,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:24,936 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:24,939 - distributed.scheduler - INFO - Remove client Client-376e437b-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:24,939 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41104; closing.
2023-12-01 06:38:24,939 - distributed.scheduler - INFO - Remove client Client-376e437b-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:24,939 - distributed.scheduler - INFO - Close client connection: Client-376e437b-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:24,941 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42437'. Reason: nanny-close
2023-12-01 06:38:24,941 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:24,943 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45667. Reason: nanny-close
2023-12-01 06:38:24,944 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41116; closing.
2023-12-01 06:38:24,944 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:24,945 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45667', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412704.9452288')
2023-12-01 06:38:24,945 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:38:24,946 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:27,109 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:38:27,110 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:38:27,110 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:38:27,111 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:38:27,112 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-12-01 06:38:31,330 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:31,334 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-12-01 06:38:31,338 - distributed.scheduler - INFO - State start
2023-12-01 06:38:31,361 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:31,362 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:38:31,363 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-12-01 06:38:31,363 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:38:31,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43065'
2023-12-01 06:38:33,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:33,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:33,723 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:34,742 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36131
2023-12-01 06:38:34,742 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36131
2023-12-01 06:38:34,742 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37003
2023-12-01 06:38:34,742 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:34,743 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:34,743 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:34,743 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-01 06:38:34,743 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_r1gv2q4
2023-12-01 06:38:34,743 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-02b28f67-6601-46dd-bd43-f0da8d59627e
2023-12-01 06:38:34,743 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4835a1c8-b7f6-4e8a-af80-8a8d78fe2ea2
2023-12-01 06:38:34,744 - distributed.worker - INFO - Starting Worker plugin PreImport-d811c402-624c-420c-a453-60af84c0d888
2023-12-01 06:38:34,745 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:34,805 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36131', status: init, memory: 0, processing: 0>
2023-12-01 06:38:34,820 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36131
2023-12-01 06:38:34,820 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33248
2023-12-01 06:38:34,822 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:34,823 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:34,824 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:34,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:34,944 - distributed.scheduler - INFO - Receive client connection: Client-3d2ba238-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:34,945 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33264
2023-12-01 06:38:34,952 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:34,957 - distributed.scheduler - INFO - Remove client Client-3d2ba238-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:34,957 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33264; closing.
2023-12-01 06:38:34,957 - distributed.scheduler - INFO - Remove client Client-3d2ba238-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:34,957 - distributed.scheduler - INFO - Close client connection: Client-3d2ba238-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:34,958 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43065'. Reason: nanny-close
2023-12-01 06:38:34,958 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:34,960 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36131. Reason: nanny-close
2023-12-01 06:38:34,962 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33248; closing.
2023-12-01 06:38:34,962 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:38:34,962 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36131', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412714.962706')
2023-12-01 06:38:34,962 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:38:34,964 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:36,075 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:38:36,075 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:38:36,075 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:38:36,076 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:38:36,077 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-12-01 06:38:38,278 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:38,282 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32967 instead
  warnings.warn(
2023-12-01 06:38:38,286 - distributed.scheduler - INFO - State start
2023-12-01 06:38:38,330 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:38,331 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:38:38,331 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32967/status
2023-12-01 06:38:38,332 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:38:44,348 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:33278'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33278>: Stream is closed
2023-12-01 06:38:44,914 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:38:44,914 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:38:44,914 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:38:44,915 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:38:44,916 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-12-01 06:38:47,207 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:47,212 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38399 instead
  warnings.warn(
2023-12-01 06:38:47,216 - distributed.scheduler - INFO - State start
2023-12-01 06:38:47,241 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:47,243 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-12-01 06:38:47,243 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38399/status
2023-12-01 06:38:47,244 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:38:47,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37105'
2023-12-01 06:38:47,939 - distributed.scheduler - INFO - Receive client connection: Client-46a837b6-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:47,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59990
2023-12-01 06:38:49,307 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:49,307 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:49,311 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:50,724 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35691
2023-12-01 06:38:50,724 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35691
2023-12-01 06:38:50,725 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46565
2023-12-01 06:38:50,725 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-12-01 06:38:50,725 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:50,725 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:50,725 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-01 06:38:50,725 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ag57kyp0
2023-12-01 06:38:50,725 - distributed.worker - INFO - Starting Worker plugin PreImport-147137bc-a242-4c76-8e33-0ed297155d85
2023-12-01 06:38:50,726 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75304fa4-e3b4-49d4-8a46-e2e8b1252fa9
2023-12-01 06:38:50,726 - distributed.worker - INFO - Starting Worker plugin RMMSetup-acf34c2f-3fc7-4d5a-a80a-8eb23fdf8c30
2023-12-01 06:38:50,726 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:50,763 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35691', status: init, memory: 0, processing: 0>
2023-12-01 06:38:50,765 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35691
2023-12-01 06:38:50,765 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50528
2023-12-01 06:38:50,766 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:50,767 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-12-01 06:38:50,767 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:50,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-12-01 06:38:50,847 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:38:50,851 - distributed.scheduler - INFO - Remove client Client-46a837b6-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:50,851 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59990; closing.
2023-12-01 06:38:50,852 - distributed.scheduler - INFO - Remove client Client-46a837b6-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:50,852 - distributed.scheduler - INFO - Close client connection: Client-46a837b6-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:50,853 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37105'. Reason: nanny-close
2023-12-01 06:38:50,854 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:38:50,855 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35691. Reason: nanny-close
2023-12-01 06:38:50,857 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50528; closing.
2023-12-01 06:38:50,857 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-12-01 06:38:50,857 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35691', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412730.8578703')
2023-12-01 06:38:50,858 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:38:50,859 - distributed.nanny - INFO - Worker closed
2023-12-01 06:38:51,869 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:38:51,869 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:38:51,870 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:38:51,872 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-12-01 06:38:51,873 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-12-01 06:38:54,381 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:54,386 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33259 instead
  warnings.warn(
2023-12-01 06:38:54,390 - distributed.scheduler - INFO - State start
2023-12-01 06:38:54,419 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:38:54,420 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:38:54,421 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33259/status
2023-12-01 06:38:54,421 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:38:54,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40561'
2023-12-01 06:38:54,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35999'
2023-12-01 06:38:54,830 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44915'
2023-12-01 06:38:54,832 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45243'
2023-12-01 06:38:54,841 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42665'
2023-12-01 06:38:54,850 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40079'
2023-12-01 06:38:54,858 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33569'
2023-12-01 06:38:54,869 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36675'
2023-12-01 06:38:55,146 - distributed.scheduler - INFO - Receive client connection: Client-4aced60f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:38:55,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50032
2023-12-01 06:38:56,638 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:56,638 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:56,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:56,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:56,640 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:56,640 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:56,643 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:56,643 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:56,644 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:56,696 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:56,696 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:56,700 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:56,700 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:56,701 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:56,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:56,701 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:56,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:56,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:56,705 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:56,705 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:56,706 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:56,706 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:38:56,706 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:38:56,710 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:38:59,597 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38327
2023-12-01 06:38:59,598 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38327
2023-12-01 06:38:59,598 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35537
2023-12-01 06:38:59,598 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:59,598 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:59,598 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:59,598 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:59,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z086mrhe
2023-12-01 06:38:59,599 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed6d91cc-904c-42fd-817c-711424296e52
2023-12-01 06:38:59,724 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-510ffb0b-593a-445d-ae98-8b8abfab5d5f
2023-12-01 06:38:59,724 - distributed.worker - INFO - Starting Worker plugin PreImport-2ce200d5-9c45-4dad-9a30-3687c30d4c82
2023-12-01 06:38:59,725 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:59,756 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38327', status: init, memory: 0, processing: 0>
2023-12-01 06:38:59,758 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38327
2023-12-01 06:38:59,758 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50054
2023-12-01 06:38:59,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:38:59,760 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:38:59,760 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:59,761 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:38:59,907 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41839
2023-12-01 06:38:59,908 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41839
2023-12-01 06:38:59,908 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33311
2023-12-01 06:38:59,908 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:59,908 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:59,908 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:59,908 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:59,908 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hloclr2_
2023-12-01 06:38:59,909 - distributed.worker - INFO - Starting Worker plugin PreImport-0fde991d-fb48-468d-8b39-aa2797e7e084
2023-12-01 06:38:59,909 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d434847-36e5-4ab0-b849-f00f6841d746
2023-12-01 06:38:59,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ba81980-af3a-4516-8599-d2d92e20368e
2023-12-01 06:38:59,912 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35799
2023-12-01 06:38:59,913 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35799
2023-12-01 06:38:59,913 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35637
2023-12-01 06:38:59,913 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:59,913 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:59,913 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:59,913 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:59,913 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-171aloq2
2023-12-01 06:38:59,914 - distributed.worker - INFO - Starting Worker plugin RMMSetup-53f2658c-87ac-48ec-a9b0-161bc26dce01
2023-12-01 06:38:59,918 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34357
2023-12-01 06:38:59,919 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34357
2023-12-01 06:38:59,919 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44949
2023-12-01 06:38:59,919 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:38:59,919 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:38:59,919 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:38:59,920 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:38:59,920 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-djacvp2s
2023-12-01 06:38:59,920 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2b902cf-d41c-4ff9-b4ac-9e258e6dd643
2023-12-01 06:39:00,074 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33621
2023-12-01 06:39:00,075 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33621
2023-12-01 06:39:00,075 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35347
2023-12-01 06:39:00,075 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,075 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,075 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:00,076 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:00,076 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4_34qlak
2023-12-01 06:39:00,076 - distributed.worker - INFO - Starting Worker plugin PreImport-45b9c9b7-9243-467d-b2b4-306a16005e9c
2023-12-01 06:39:00,076 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91154fcc-fcb8-4578-a2af-9b310ed5ca50
2023-12-01 06:39:00,078 - distributed.worker - INFO - Starting Worker plugin RMMSetup-98f1ad0c-ff11-46e5-87df-520d5305542e
2023-12-01 06:39:00,088 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35257
2023-12-01 06:39:00,089 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35257
2023-12-01 06:39:00,089 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46381
2023-12-01 06:39:00,089 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,089 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,089 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:00,089 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:00,089 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7f1zkyih
2023-12-01 06:39:00,090 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df247137-de6d-402c-bba4-d6df373b8ae8
2023-12-01 06:39:00,091 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1e3b44f8-2b05-4514-bd4e-0f79d91e3b9b
2023-12-01 06:39:00,092 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33209
2023-12-01 06:39:00,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33209
2023-12-01 06:39:00,093 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43921
2023-12-01 06:39:00,093 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,093 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,093 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:00,094 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:00,094 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fb59sk87
2023-12-01 06:39:00,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7750a8e8-f804-4ebf-b06b-067f1211cd1e
2023-12-01 06:39:00,095 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41871
2023-12-01 06:39:00,096 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41871
2023-12-01 06:39:00,096 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34849
2023-12-01 06:39:00,096 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,096 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,096 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:00,096 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-12-01 06:39:00,096 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8aelpofo
2023-12-01 06:39:00,097 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ac8ef1e-cc0a-46ae-84e5-7356ea27c050
2023-12-01 06:39:00,097 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32f83e75-290f-48ee-a186-c43721cfcd53
2023-12-01 06:39:00,272 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca238d2c-039b-4875-8025-3bf99016cd02
2023-12-01 06:39:00,273 - distributed.worker - INFO - Starting Worker plugin PreImport-1a8d4652-fab4-46f4-b165-d56a7a9ac500
2023-12-01 06:39:00,274 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,276 - distributed.worker - INFO - Starting Worker plugin PreImport-3aa2de6c-50d1-4069-bc88-3c5c805dc832
2023-12-01 06:39:00,277 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,283 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,287 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ad92a5c-d90a-405e-91f4-bf17fe42cc97
2023-12-01 06:39:00,287 - distributed.worker - INFO - Starting Worker plugin PreImport-ec0eb784-5866-4909-9335-d56f0818aa15
2023-12-01 06:39:00,287 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,292 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7dd7f4ef-8779-444f-8303-de25c98b5553
2023-12-01 06:39:00,293 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,293 - distributed.worker - INFO - Starting Worker plugin PreImport-cebff252-6502-47d4-9898-b6c2cf982acf
2023-12-01 06:39:00,294 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,312 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35257', status: init, memory: 0, processing: 0>
2023-12-01 06:39:00,313 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35257
2023-12-01 06:39:00,313 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59442
2023-12-01 06:39:00,315 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:00,316 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,316 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,318 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:00,319 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33209', status: init, memory: 0, processing: 0>
2023-12-01 06:39:00,320 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33209
2023-12-01 06:39:00,320 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59456
2023-12-01 06:39:00,321 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:00,321 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35799', status: init, memory: 0, processing: 0>
2023-12-01 06:39:00,322 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,322 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35799
2023-12-01 06:39:00,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59430
2023-12-01 06:39:00,322 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,323 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41839', status: init, memory: 0, processing: 0>
2023-12-01 06:39:00,323 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41839
2023-12-01 06:39:00,323 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59446
2023-12-01 06:39:00,323 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:00,323 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:00,324 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,324 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,325 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:00,326 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,326 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:00,329 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:00,332 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33621', status: init, memory: 0, processing: 0>
2023-12-01 06:39:00,333 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33621
2023-12-01 06:39:00,333 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59470
2023-12-01 06:39:00,334 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:00,335 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,335 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:00,339 - distributed.worker - INFO - Starting Worker plugin PreImport-3780e886-7f27-4ae5-8bf4-79fad50867e9
2023-12-01 06:39:00,339 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,339 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34357', status: init, memory: 0, processing: 0>
2023-12-01 06:39:00,340 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34357
2023-12-01 06:39:00,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59474
2023-12-01 06:39:00,342 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:00,343 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,343 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,345 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:00,364 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41871', status: init, memory: 0, processing: 0>
2023-12-01 06:39:00,365 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41871
2023-12-01 06:39:00,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59488
2023-12-01 06:39:00,366 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:00,367 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:00,367 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:00,368 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:00,385 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:00,385 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:00,385 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:00,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:00,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:00,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:00,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:00,386 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-12-01 06:39:00,399 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:00,399 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:00,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:00,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:00,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:00,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:00,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:00,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:00,404 - distributed.scheduler - INFO - Remove client Client-4aced60f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:00,404 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50032; closing.
2023-12-01 06:39:00,405 - distributed.scheduler - INFO - Remove client Client-4aced60f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:00,405 - distributed.scheduler - INFO - Close client connection: Client-4aced60f-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:00,406 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40561'. Reason: nanny-close
2023-12-01 06:39:00,407 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:00,408 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35999'. Reason: nanny-close
2023-12-01 06:39:00,408 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:00,408 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33621. Reason: nanny-close
2023-12-01 06:39:00,408 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44915'. Reason: nanny-close
2023-12-01 06:39:00,409 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:00,409 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35799. Reason: nanny-close
2023-12-01 06:39:00,409 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45243'. Reason: nanny-close
2023-12-01 06:39:00,409 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:00,410 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35257. Reason: nanny-close
2023-12-01 06:39:00,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42665'. Reason: nanny-close
2023-12-01 06:39:00,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:00,410 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38327. Reason: nanny-close
2023-12-01 06:39:00,410 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:00,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40079'. Reason: nanny-close
2023-12-01 06:39:00,410 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59470; closing.
2023-12-01 06:39:00,411 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:00,411 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33621', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412740.4112098')
2023-12-01 06:39:00,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34357. Reason: nanny-close
2023-12-01 06:39:00,411 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33569'. Reason: nanny-close
2023-12-01 06:39:00,411 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:00,411 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:00,412 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41839. Reason: nanny-close
2023-12-01 06:39:00,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36675'. Reason: nanny-close
2023-12-01 06:39:00,412 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:00,412 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:00,412 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:00,413 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:00,413 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59430; closing.
2023-12-01 06:39:00,413 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33209. Reason: nanny-close
2023-12-01 06:39:00,413 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59442; closing.
2023-12-01 06:39:00,413 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41871. Reason: nanny-close
2023-12-01 06:39:00,413 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:00,413 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50054; closing.
2023-12-01 06:39:00,414 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35799', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412740.4140232')
2023-12-01 06:39:00,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:00,414 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:00,414 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:00,414 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35257', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412740.414552')
2023-12-01 06:39:00,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:00,414 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412740.4148388')
2023-12-01 06:39:00,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:00,415 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:00,415 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59474; closing.
2023-12-01 06:39:00,415 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59446; closing.
2023-12-01 06:39:00,415 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:00,416 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:00,416 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34357', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412740.416642')
2023-12-01 06:39:00,416 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:00,416 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:00,416 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41839', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412740.4169312')
2023-12-01 06:39:00,417 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59456; closing.
2023-12-01 06:39:00,417 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59488; closing.
2023-12-01 06:39:00,417 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33209', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412740.4178376')
2023-12-01 06:39:00,418 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412740.4182584')
2023-12-01 06:39:00,418 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:39:02,074 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:39:02,075 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:39:02,075 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:39:02,076 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:39:02,077 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-12-01 06:39:04,224 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:04,229 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41625 instead
  warnings.warn(
2023-12-01 06:39:04,232 - distributed.scheduler - INFO - State start
2023-12-01 06:39:04,253 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:04,254 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:39:04,254 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41625/status
2023-12-01 06:39:04,255 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:39:04,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46269'
2023-12-01 06:39:04,657 - distributed.scheduler - INFO - Receive client connection: Client-50e7b831-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:04,670 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59580
2023-12-01 06:39:05,937 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:05,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:05,941 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:07,268 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35397
2023-12-01 06:39:07,268 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35397
2023-12-01 06:39:07,269 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45981
2023-12-01 06:39:07,269 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:07,269 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:07,269 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:07,269 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-01 06:39:07,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cipe84e6
2023-12-01 06:39:07,269 - distributed.worker - INFO - Starting Worker plugin PreImport-e453cd20-4a5c-4142-a46c-a86ab29af662
2023-12-01 06:39:07,269 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fced1999-6e1b-4b02-ab84-dab0af87e0e1
2023-12-01 06:39:07,270 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5685021f-71be-4ebb-8399-ba2714a5118f
2023-12-01 06:39:07,417 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:07,484 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35397', status: init, memory: 0, processing: 0>
2023-12-01 06:39:07,485 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35397
2023-12-01 06:39:07,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59592
2023-12-01 06:39:07,486 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:07,487 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:07,487 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:07,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:07,522 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:39:07,526 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:07,527 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:07,530 - distributed.scheduler - INFO - Remove client Client-50e7b831-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:07,530 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59580; closing.
2023-12-01 06:39:07,530 - distributed.scheduler - INFO - Remove client Client-50e7b831-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:07,531 - distributed.scheduler - INFO - Close client connection: Client-50e7b831-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:07,531 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46269'. Reason: nanny-close
2023-12-01 06:39:07,532 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:07,533 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35397. Reason: nanny-close
2023-12-01 06:39:07,535 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59592; closing.
2023-12-01 06:39:07,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:07,535 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35397', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412747.5354526')
2023-12-01 06:39:07,535 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:39:07,536 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:08,648 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:39:08,648 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:39:08,648 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:39:08,649 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:39:08,650 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-12-01 06:39:10,598 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:10,601 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37637 instead
  warnings.warn(
2023-12-01 06:39:10,605 - distributed.scheduler - INFO - State start
2023-12-01 06:39:10,625 - distributed.scheduler - INFO - -----------------------------------------------
2023-12-01 06:39:10,625 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-12-01 06:39:10,626 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37637/status
2023-12-01 06:39:10,626 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-12-01 06:39:10,713 - distributed.scheduler - INFO - Receive client connection: Client-54b89e4b-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:10,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55086
2023-12-01 06:39:10,965 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46465'
2023-12-01 06:39:12,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-12-01 06:39:12,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-12-01 06:39:12,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-12-01 06:39:13,742 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39033
2023-12-01 06:39:13,743 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39033
2023-12-01 06:39:13,743 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40569
2023-12-01 06:39:13,743 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-12-01 06:39:13,743 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:13,743 - distributed.worker - INFO -               Threads:                          1
2023-12-01 06:39:13,743 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-12-01 06:39:13,743 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5zcsbm3x
2023-12-01 06:39:13,744 - distributed.worker - INFO - Starting Worker plugin PreImport-9a0b42eb-785e-4b2b-80c1-7fc0042c4435
2023-12-01 06:39:13,744 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f75e5d3a-cdd2-481d-9761-111f19ca5254
2023-12-01 06:39:13,744 - distributed.worker - INFO - Starting Worker plugin RMMSetup-471273b1-4b89-40a9-96f4-58798fc66b25
2023-12-01 06:39:13,855 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:13,883 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39033', status: init, memory: 0, processing: 0>
2023-12-01 06:39:13,884 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39033
2023-12-01 06:39:13,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55120
2023-12-01 06:39:13,886 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-12-01 06:39:13,886 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-12-01 06:39:13,887 - distributed.worker - INFO - -------------------------------------------------
2023-12-01 06:39:13,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-12-01 06:39:13,986 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-12-01 06:39:13,990 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-12-01 06:39:13,994 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:13,996 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-12-01 06:39:13,999 - distributed.scheduler - INFO - Remove client Client-54b89e4b-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:13,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55086; closing.
2023-12-01 06:39:14,000 - distributed.scheduler - INFO - Remove client Client-54b89e4b-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:14,000 - distributed.scheduler - INFO - Close client connection: Client-54b89e4b-9014-11ee-84ce-d8c49764f6bb
2023-12-01 06:39:14,001 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46465'. Reason: nanny-close
2023-12-01 06:39:14,002 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-12-01 06:39:14,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39033. Reason: nanny-close
2023-12-01 06:39:14,005 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55120; closing.
2023-12-01 06:39:14,005 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-12-01 06:39:14,005 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39033', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701412754.0055299')
2023-12-01 06:39:14,005 - distributed.scheduler - INFO - Lost all workers
2023-12-01 06:39:14,007 - distributed.nanny - INFO - Worker closed
2023-12-01 06:39:15,068 - distributed._signals - INFO - Received signal SIGINT (2)
2023-12-01 06:39:15,068 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-12-01 06:39:15,069 - distributed.scheduler - INFO - Scheduler closing all comms
2023-12-01 06:39:15,069 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-12-01 06:39:15,070 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40111 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39127 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34269 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36969 instead
  warnings.warn(
2023-12-01 06:40:08,939 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-12-01 06:40:08,938 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-12-01 06:40:08,939 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-12-01 06:40:08,939 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-12-01 06:40:08,940 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-12-01 06:40:08,942 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-12-01 06:40:08,943 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-12-01 06:40:08,945 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1201' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1199' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46605 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40503 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38903 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34683 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] [1701412873.412421] [dgx13:55851:0]            sock.c:470  UCX  ERROR bind(fd=127 addr=0.0.0.0:33378) failed: Address already in use
[1701412874.951965] [dgx13:55940:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:60154) failed: Address already in use
[1701412876.418536] [dgx13:55944:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:54570) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40881 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34629 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36889 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36555 instead
  warnings.warn(
2023-12-01 06:43:16,194 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-12-01 06:43:16,197 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:33291', name: 1, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41145 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46037 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33053 instead
  warnings.warn(
[1701413044.766943] [dgx13:58648:0]            sock.c:470  UCX  ERROR bind(fd=135 addr=0.0.0.0:34248) failed: Address already in use
[1701413046.172963] [dgx13:58737:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:34427) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36199 instead
  warnings.warn(
[1701413062.949661] [dgx13:58973:0]            sock.c:470  UCX  ERROR bind(fd=127 addr=0.0.0.0:44058) failed: Address already in use
[1701413062.949714] [dgx13:58973:0]            sock.c:470  UCX  ERROR bind(fd=127 addr=0.0.0.0:35112) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44473 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44599 instead
  warnings.warn(
[1701413107.264739] [dgx13:59706:0]            sock.c:470  UCX  ERROR bind(fd=160 addr=0.0.0.0:38926) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40825 instead
  warnings.warn(
[1701413135.774383] [dgx13:60114:0]            sock.c:470  UCX  ERROR bind(fd=153 addr=0.0.0.0:47002) failed: Address already in use
[1701413135.890063] [dgx13:60114:0]            sock.c:470  UCX  ERROR bind(fd=160 addr=0.0.0.0:57502) failed: Address already in use
[1701413137.280488] [dgx13:60290:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:54200) failed: Address already in use
[1701413140.347885] [dgx13:60282:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:38746) failed: Address already in use
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-5081' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42135 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46145 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35805 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45845 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43459 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33573 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41015 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43753 instead
  warnings.warn(
[1701413277.596620] [dgx13:63030:0]            sock.c:470  UCX  ERROR bind(fd=135 addr=0.0.0.0:40550) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42177 instead
  warnings.warn(
[1701413297.799988] [dgx13:63448:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:39000) failed: Address already in use
[1701413301.605349] [dgx13:63444:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:48675) failed: Address already in use
[1701413301.605482] [dgx13:63444:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:58852) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34225 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46241 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33395 instead
  warnings.warn(
[1701413376.406869] [dgx13:64499:0]            sock.c:470  UCX  ERROR bind(fd=160 addr=0.0.0.0:35140) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42399 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40901 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33219 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40715 instead
  warnings.warn(
2023-12-01 06:50:32,058 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-01 06:50:32,066 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:35241'.
2023-12-01 06:50:32,067 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:35241'. Shutting down.
2023-12-01 06:50:32,069 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f4bc00ea9a0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-12-01 06:50:34,072 - distributed.nanny - ERROR - Worker process died unexpectedly
