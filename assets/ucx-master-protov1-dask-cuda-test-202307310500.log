============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-07-31 05:41:04,715 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:41:04,720 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34517 instead
  warnings.warn(
2023-07-31 05:41:04,725 - distributed.scheduler - INFO - State start
2023-07-31 05:41:04,785 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:41:04,787 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:41:04,787 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:41:04,788 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-07-31 05:41:04,853 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41373'
2023-07-31 05:41:04,870 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41851'
2023-07-31 05:41:04,873 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37239'
2023-07-31 05:41:04,880 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36965'
2023-07-31 05:41:05,566 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41373'. Reason: nanny-close
2023-07-31 05:41:05,567 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41851'. Reason: nanny-close
2023-07-31 05:41:05,567 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37239'. Reason: nanny-close
2023-07-31 05:41:05,567 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36965'. Reason: nanny-close
2023-07-31 05:41:06,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:06,610 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:06,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:06,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:06,618 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:06,618 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:06,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:06,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:06,678 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:06,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:06,681 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:06,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-07-31 05:41:06,708 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43265
2023-07-31 05:41:06,708 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43265
2023-07-31 05:41:06,708 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46407
2023-07-31 05:41:06,708 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-31 05:41:06,708 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:06,708 - distributed.worker - INFO -               Threads:                          4
2023-07-31 05:41:06,708 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-31 05:41:06,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vvhwbpia
2023-07-31 05:41:06,709 - distributed.worker - INFO - Starting Worker plugin PreImport-b86ae49a-16d1-4b18-979c-da0aae21bb58
2023-07-31 05:41:06,709 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ad1cf05-37d3-4018-bf8e-1f67d7d911a4
2023-07-31 05:41:06,709 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-238bc7c2-844b-4530-8aba-bb2ac6f028f4
2023-07-31 05:41:06,709 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:06,729 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-31 05:41:06,729 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:06,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-31 05:41:06,768 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:06,769 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43265. Reason: nanny-close
2023-07-31 05:41:06,771 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-31 05:41:06,772 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:07,201 - distributed.nanny - WARNING - Restarting worker

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 549, in _on_worker_exit
    await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 711, in start
    await self.process.start()
asyncio.exceptions.CancelledError
2023-07-31 05:41:08,263 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37933
2023-07-31 05:41:08,263 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37933
2023-07-31 05:41:08,263 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37675
2023-07-31 05:41:08,263 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-31 05:41:08,263 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:08,263 - distributed.worker - INFO -               Threads:                          4
2023-07-31 05:41:08,264 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-31 05:41:08,264 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e8y78k_h
2023-07-31 05:41:08,264 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6641ba9d-c56b-4217-90b4-deb27aefe55a
2023-07-31 05:41:08,264 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34169
2023-07-31 05:41:08,264 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34169
2023-07-31 05:41:08,264 - distributed.worker - INFO - Starting Worker plugin PreImport-ae928d9a-3b97-45ab-984f-72063e7cdb33
2023-07-31 05:41:08,264 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41425
2023-07-31 05:41:08,264 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48bc1c32-c728-45a8-96fe-bee8494aead0
2023-07-31 05:41:08,264 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-31 05:41:08,264 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:08,265 - distributed.worker - INFO -               Threads:                          4
2023-07-31 05:41:08,265 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-31 05:41:08,265 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:08,265 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8yrxtzid
2023-07-31 05:41:08,266 - distributed.worker - INFO - Starting Worker plugin PreImport-9a3e5cba-737b-4209-aea0-c85e2cf98f34
2023-07-31 05:41:08,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44d2134b-0c7b-42c0-8494-d8ae1cfec4f4
2023-07-31 05:41:08,266 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e42c648-f537-430e-89dd-ae481ba1a108
2023-07-31 05:41:08,267 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:08,314 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41181
2023-07-31 05:41:08,314 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41181
2023-07-31 05:41:08,314 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39731
2023-07-31 05:41:08,314 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-31 05:41:08,314 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:08,314 - distributed.worker - INFO -               Threads:                          4
2023-07-31 05:41:08,314 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-31 05:41:08,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q9_j5buz
2023-07-31 05:41:08,315 - distributed.worker - INFO - Starting Worker plugin PreImport-395a1acc-b8a7-456f-b2be-a511f8785894
2023-07-31 05:41:08,315 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5373672-4b59-4401-99f6-9eb72638a7d4
2023-07-31 05:41:08,315 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89981e00-5b30-4e44-9b50-74cd81e2720c
2023-07-31 05:41:08,316 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:08,937 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:08,937 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:08,945 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-07-31 05:41:08,959 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35307
2023-07-31 05:41:08,959 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35307
2023-07-31 05:41:08,959 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34149
2023-07-31 05:41:08,959 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-31 05:41:08,959 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:08,959 - distributed.worker - INFO -               Threads:                          4
2023-07-31 05:41:08,959 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-07-31 05:41:08,959 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7vfgz9m4
2023-07-31 05:41:08,960 - distributed.worker - INFO - Starting Worker plugin PreImport-0e02baf8-fe69-46d7-9f15-a0514dbd9cbc
2023-07-31 05:41:08,960 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb9d2d42-9d04-4e53-ab80-e82c38f0f3cd
2023-07-31 05:41:08,960 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8442ecc3-495f-4090-9388-6d84c8e91583
2023-07-31 05:41:08,960 - distributed.worker - INFO - -------------------------------------------------
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 24 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 79, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 56, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 209, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/__init__.py", line 22, in <module>
    from distributed.actor import Actor, ActorFuture, BaseActorFuture
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/actor.py", line 14, in <module>
    from distributed.client import Future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 49, in <module>
    from distributed.core import ErrorMessage
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 27, in <module>
    from distributed.comm import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/__init__.py", line 16, in <module>
    from distributed.comm.utils import get_tcp_server_address, get_tcp_server_addresses
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 29, in <module>
    import numpy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/__init__.py", line 153, in <module>
    from . import ctypeslib
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/ctypeslib.py", line 376, in <module>
    _scalar_type_map = _get_scalar_type_map()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/ctypeslib.py", line 373, in _get_scalar_type_map
    return {_dtype(ctype): ctype for ctype in simple_types}
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/ctypeslib.py", line 373, in <dictcomp>
    return {_dtype(ctype): ctype for ctype in simple_types}
  File "<frozen importlib._bootstrap>", line 217, in _lock_unlock_module
  File "<frozen importlib._bootstrap>", line 123, in release
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 79, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 56, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 209, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/__init__.py", line 37, in <module>
    from distributed.deploy import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/__init__.py", line 7, in <module>
    from distributed.deploy.local import LocalCluster
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 12, in <module>
    from distributed.deploy.spec import SpecCluster
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 24, in <module>
    from distributed.scheduler import Scheduler
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 978, in get_code
  File "<frozen importlib._bootstrap_external>", line 647, in _compile_bytecode
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-07-31 05:41:38,881 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:41:38,886 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40661 instead
  warnings.warn(
2023-07-31 05:41:38,890 - distributed.scheduler - INFO - State start
2023-07-31 05:41:38,911 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:41:38,912 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-31 05:41:38,912 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40661/status
2023-07-31 05:41:39,007 - distributed.scheduler - INFO - Receive client connection: Client-eb8a4acd-2f64-11ee-8acd-d8c49764f6bb
2023-07-31 05:41:39,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55488
2023-07-31 05:41:39,141 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38703'
2023-07-31 05:41:39,160 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38041'
2023-07-31 05:41:39,172 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39905'
2023-07-31 05:41:39,174 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44119'
2023-07-31 05:41:39,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45227'
2023-07-31 05:41:39,195 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39681'
2023-07-31 05:41:39,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40869'
2023-07-31 05:41:39,214 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33667'
2023-07-31 05:41:39,239 - distributed.scheduler - INFO - Receive client connection: Client-ea634102-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:41:39,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55600
2023-07-31 05:41:40,956 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:40,956 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-7vfgz9m4', purging
2023-07-31 05:41:40,956 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:40,956 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-8yrxtzid', purging
2023-07-31 05:41:40,957 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-e8y78k_h', purging
2023-07-31 05:41:40,957 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-worker-space/worker-q9_j5buz', purging
2023-07-31 05:41:40,958 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:40,958 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:40,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:40,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:40,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:40,970 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:40,973 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:40,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:40,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:40,981 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:40,983 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:40,985 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:40,988 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:41,012 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:41,012 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:41,029 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:41,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:41,051 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:41,107 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:41,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:41,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:41,222 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:45,050 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36347
2023-07-31 05:41:45,050 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36347
2023-07-31 05:41:45,050 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40933
2023-07-31 05:41:45,050 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,050 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,050 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:45,050 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:45,051 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2pm9ijg3
2023-07-31 05:41:45,051 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef24440a-57e5-4b27-8a64-ceef2b60ad0b
2023-07-31 05:41:45,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36487
2023-07-31 05:41:45,382 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36487
2023-07-31 05:41:45,382 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34555
2023-07-31 05:41:45,382 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,382 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,382 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:45,382 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:45,382 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lrm5ksu_
2023-07-31 05:41:45,382 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d6e02ec-0b82-460c-a78b-35b796fa1832
2023-07-31 05:41:45,764 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41677
2023-07-31 05:41:45,764 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41677
2023-07-31 05:41:45,764 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38287
2023-07-31 05:41:45,764 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,764 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,765 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:45,765 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:45,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1sa_wopo
2023-07-31 05:41:45,765 - distributed.worker - INFO - Starting Worker plugin PreImport-ece6d882-fc6e-47ac-b391-868a77e45eb8
2023-07-31 05:41:45,765 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7e022a57-bca0-42cf-a846-dd4f8fe95c93
2023-07-31 05:41:45,765 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a94af95-03ba-4407-9a69-cacce5fe62df
2023-07-31 05:41:45,776 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44137
2023-07-31 05:41:45,777 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44137
2023-07-31 05:41:45,777 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38195
2023-07-31 05:41:45,777 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,777 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40995
2023-07-31 05:41:45,777 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,777 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40995
2023-07-31 05:41:45,777 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:45,777 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38305
2023-07-31 05:41:45,777 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:45,777 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rcggqqys
2023-07-31 05:41:45,777 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,777 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:45,777 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:45,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4ct4d07l
2023-07-31 05:41:45,777 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f9dd995-6be4-4ac0-8880-dcc2531c6877
2023-07-31 05:41:45,778 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c6443ab-0616-4dc2-a19d-02a9ff512d29
2023-07-31 05:41:45,778 - distributed.worker - INFO - Starting Worker plugin PreImport-189310d0-1d8d-4362-ad4c-18e00c040020
2023-07-31 05:41:45,778 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24da2def-3eed-4136-9655-1f41502dea6f
2023-07-31 05:41:45,786 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37807
2023-07-31 05:41:45,787 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45833
2023-07-31 05:41:45,787 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37807
2023-07-31 05:41:45,787 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45833
2023-07-31 05:41:45,787 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34595
2023-07-31 05:41:45,787 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37861
2023-07-31 05:41:45,787 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,787 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,787 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,787 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,787 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:45,787 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:45,787 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:45,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2tfacdp0
2023-07-31 05:41:45,787 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:45,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jp6jovd0
2023-07-31 05:41:45,788 - distributed.worker - INFO - Starting Worker plugin PreImport-52e5707e-bdc4-4622-9e1d-e886703a46b0
2023-07-31 05:41:45,788 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-844f58ba-d1d2-418b-8214-109436b2a9dc
2023-07-31 05:41:45,788 - distributed.worker - INFO - Starting Worker plugin PreImport-2c3e1f00-464d-471c-8512-43f6fa072e91
2023-07-31 05:41:45,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9307a20b-1eb0-4228-8ee1-a94d832d4704
2023-07-31 05:41:45,788 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d9dc99e-dada-4eff-a94c-790ea829a09f
2023-07-31 05:41:45,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e3f0e649-2cdd-469f-872f-a1e29878d58a
2023-07-31 05:41:45,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a071b292-cda4-45b5-92c3-3bea80b264e2
2023-07-31 05:41:45,805 - distributed.worker - INFO - Starting Worker plugin PreImport-6212a3c0-1453-436a-acec-9c9d6b89c7ca
2023-07-31 05:41:45,806 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,846 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5dad4bde-c25f-496f-870b-83fc03511ad1
2023-07-31 05:41:45,847 - distributed.worker - INFO - Starting Worker plugin PreImport-ca706dad-b358-496f-ac64-9c816f35c29b
2023-07-31 05:41:45,847 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36347', status: init, memory: 0, processing: 0>
2023-07-31 05:41:45,859 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36347
2023-07-31 05:41:45,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42694
2023-07-31 05:41:45,860 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,860 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:45,869 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,907 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34353
2023-07-31 05:41:45,907 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34353
2023-07-31 05:41:45,907 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37453
2023-07-31 05:41:45,908 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,908 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,908 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:45,908 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:45,908 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-es98fhzg
2023-07-31 05:41:45,908 - distributed.worker - INFO - Starting Worker plugin PreImport-f1d9fe35-dc62-4984-8362-6a502a9cec08
2023-07-31 05:41:45,908 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26f538c2-a509-4022-9593-761b8800636a
2023-07-31 05:41:45,908 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4a389b7-ee94-4ebc-a730-32be74191e7d
2023-07-31 05:41:45,909 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36487', status: init, memory: 0, processing: 0>
2023-07-31 05:41:45,909 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36487
2023-07-31 05:41:45,910 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42708
2023-07-31 05:41:45,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41677', status: init, memory: 0, processing: 0>
2023-07-31 05:41:45,910 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,910 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,911 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41677
2023-07-31 05:41:45,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42712
2023-07-31 05:41:45,911 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,911 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:45,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:45,927 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1e426ff3-14f7-44ac-bd50-08fbf74dfb4e
2023-07-31 05:41:45,927 - distributed.worker - INFO - Starting Worker plugin PreImport-aabea247-4d30-4af0-80b5-054b714a055b
2023-07-31 05:41:45,927 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,931 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,969 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,975 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44137', status: init, memory: 0, processing: 0>
2023-07-31 05:41:45,976 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44137
2023-07-31 05:41:45,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42726
2023-07-31 05:41:45,977 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,977 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,979 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40995', status: init, memory: 0, processing: 0>
2023-07-31 05:41:45,980 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:45,980 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40995
2023-07-31 05:41:45,980 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42718
2023-07-31 05:41:45,980 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:45,981 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:45,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:45,985 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:46,007 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45833', status: init, memory: 0, processing: 0>
2023-07-31 05:41:46,007 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45833
2023-07-31 05:41:46,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42742
2023-07-31 05:41:46,008 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:46,008 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:46,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:46,021 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37807', status: init, memory: 0, processing: 0>
2023-07-31 05:41:46,021 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37807
2023-07-31 05:41:46,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42748
2023-07-31 05:41:46,022 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:46,022 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:46,024 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:46,037 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:46,081 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34353', status: init, memory: 0, processing: 0>
2023-07-31 05:41:46,082 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34353
2023-07-31 05:41:46,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42752
2023-07-31 05:41:46,083 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:46,083 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:46,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:46,109 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,109 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,109 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,109 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,110 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,111 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,111 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,111 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,111 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,111 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:46,117 - distributed.scheduler - INFO - Remove client Client-ea634102-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:41:46,117 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55600; closing.
2023-07-31 05:41:46,118 - distributed.scheduler - INFO - Remove client Client-ea634102-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:41:46,118 - distributed.scheduler - INFO - Remove client Client-eb8a4acd-2f64-11ee-8acd-d8c49764f6bb
2023-07-31 05:41:46,118 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55488; closing.
2023-07-31 05:41:46,119 - distributed.scheduler - INFO - Close client connection: Client-ea634102-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:41:46,119 - distributed.scheduler - INFO - Remove client Client-eb8a4acd-2f64-11ee-8acd-d8c49764f6bb
2023-07-31 05:41:46,119 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38041'. Reason: nanny-close
2023-07-31 05:41:46,120 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:46,120 - distributed.scheduler - INFO - Close client connection: Client-eb8a4acd-2f64-11ee-8acd-d8c49764f6bb
2023-07-31 05:41:46,120 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45227'. Reason: nanny-close
2023-07-31 05:41:46,120 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:46,121 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39681'. Reason: nanny-close
2023-07-31 05:41:46,121 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40995. Reason: nanny-close
2023-07-31 05:41:46,121 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:46,121 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38703'. Reason: nanny-close
2023-07-31 05:41:46,122 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36347. Reason: nanny-close
2023-07-31 05:41:46,122 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:46,122 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45833. Reason: nanny-close
2023-07-31 05:41:46,122 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39905'. Reason: nanny-close
2023-07-31 05:41:46,122 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:46,123 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44119'. Reason: nanny-close
2023-07-31 05:41:46,123 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34353. Reason: nanny-close
2023-07-31 05:41:46,123 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:46,123 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40869'. Reason: nanny-close
2023-07-31 05:41:46,123 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36487. Reason: nanny-close
2023-07-31 05:41:46,123 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:46,124 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33667'. Reason: nanny-close
2023-07-31 05:41:46,124 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42718; closing.
2023-07-31 05:41:46,124 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:46,124 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:46,124 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:46,124 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44137. Reason: nanny-close
2023-07-31 05:41:46,124 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40995', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:46,124 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40995
2023-07-31 05:41:46,124 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41677. Reason: nanny-close
2023-07-31 05:41:46,124 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:46,124 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:46,125 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42742; closing.
2023-07-31 05:41:46,125 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:46,125 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37807. Reason: nanny-close
2023-07-31 05:41:46,125 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45833', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:46,125 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45833
2023-07-31 05:41:46,125 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:46,126 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:46,126 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:46,126 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:46,126 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:46,126 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42694; closing.
2023-07-31 05:41:46,127 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:46,127 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42752; closing.
2023-07-31 05:41:46,127 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40995
2023-07-31 05:41:46,127 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:46,127 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:46,128 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:46,128 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:46,127 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42742>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-31 05:41:46,129 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:46,129 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36347', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:46,129 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36347
2023-07-31 05:41:46,129 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34353', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:46,129 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34353
2023-07-31 05:41:46,130 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42708; closing.
2023-07-31 05:41:46,130 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42712; closing.
2023-07-31 05:41:46,130 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36487', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:46,130 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36487
2023-07-31 05:41:46,131 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41677', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:46,131 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41677
2023-07-31 05:41:46,131 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42726; closing.
2023-07-31 05:41:46,132 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44137', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:46,132 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44137
2023-07-31 05:41:46,132 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42748; closing.
2023-07-31 05:41:46,132 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37807', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:46,132 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37807
2023-07-31 05:41:46,132 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:41:48,139 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-31 05:41:48,139 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:41:48,140 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:41:48,142 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-31 05:41:48,142 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-07-31 05:41:50,549 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:41:50,554 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35207 instead
  warnings.warn(
2023-07-31 05:41:50,559 - distributed.scheduler - INFO - State start
2023-07-31 05:41:50,583 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:41:50,584 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-31 05:41:50,585 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35207/status
2023-07-31 05:41:50,669 - distributed.scheduler - INFO - Receive client connection: Client-f14cef7f-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:41:50,685 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43518
2023-07-31 05:41:50,776 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40447'
2023-07-31 05:41:50,809 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41161'
2023-07-31 05:41:50,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39351'
2023-07-31 05:41:50,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40439'
2023-07-31 05:41:50,839 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40009'
2023-07-31 05:41:50,851 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36473'
2023-07-31 05:41:50,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42609'
2023-07-31 05:41:50,879 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39967'
2023-07-31 05:41:50,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35521', status: init, memory: 0, processing: 0>
2023-07-31 05:41:50,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35521
2023-07-31 05:41:50,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43600
2023-07-31 05:41:50,921 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44905', status: init, memory: 0, processing: 0>
2023-07-31 05:41:50,922 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44905
2023-07-31 05:41:50,922 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43614
2023-07-31 05:41:50,954 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43600; closing.
2023-07-31 05:41:50,954 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35521', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:50,954 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35521
2023-07-31 05:41:50,955 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43614; closing.
2023-07-31 05:41:50,955 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44905', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:50,955 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44905
2023-07-31 05:41:50,955 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:41:51,278 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44785', status: init, memory: 0, processing: 0>
2023-07-31 05:41:51,279 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44785
2023-07-31 05:41:51,279 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43616
2023-07-31 05:41:51,309 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43616; closing.
2023-07-31 05:41:51,310 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44785', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:51,310 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44785
2023-07-31 05:41:51,310 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:41:51,870 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:43644 closed before handshake completed
2023-07-31 05:41:51,918 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44009', status: init, memory: 0, processing: 0>
2023-07-31 05:41:51,919 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44009
2023-07-31 05:41:51,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43628
2023-07-31 05:41:51,975 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46547', status: init, memory: 0, processing: 0>
2023-07-31 05:41:51,976 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46547
2023-07-31 05:41:51,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43646
2023-07-31 05:41:52,101 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43646; closing.
2023-07-31 05:41:52,102 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46547', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:52,102 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46547
2023-07-31 05:41:52,103 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43628; closing.
2023-07-31 05:41:52,103 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:43652 closed before handshake completed
2023-07-31 05:41:52,103 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44009', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:52,103 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44009
2023-07-31 05:41:52,103 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:41:52,104 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:43664 closed before handshake completed
2023-07-31 05:41:52,144 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40483', status: init, memory: 0, processing: 0>
2023-07-31 05:41:52,145 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40483
2023-07-31 05:41:52,145 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43666
2023-07-31 05:41:52,201 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43666; closing.
2023-07-31 05:41:52,201 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40483', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:52,201 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40483
2023-07-31 05:41:52,201 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:41:52,321 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34725', status: init, memory: 0, processing: 0>
2023-07-31 05:41:52,321 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34725
2023-07-31 05:41:52,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43678
2023-07-31 05:41:52,336 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39759', status: init, memory: 0, processing: 0>
2023-07-31 05:41:52,337 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39759
2023-07-31 05:41:52,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43688
2023-07-31 05:41:52,354 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43678; closing.
2023-07-31 05:41:52,354 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34725', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:52,354 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34725
2023-07-31 05:41:52,355 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43688; closing.
2023-07-31 05:41:52,355 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39759', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:52,355 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39759
2023-07-31 05:41:52,355 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:41:52,630 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:52,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:52,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:52,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:52,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:52,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:52,659 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:52,674 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:52,674 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:52,674 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:52,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:52,697 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:52,697 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:52,704 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:52,704 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:52,730 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:52,746 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:52,746 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:52,765 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:52,768 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:52,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:41:52,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:41:52,807 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:52,937 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:41:53,002 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:43692 closed before handshake completed
2023-07-31 05:41:53,124 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:43700 closed before handshake completed
2023-07-31 05:41:53,173 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:43706 closed before handshake completed
2023-07-31 05:41:53,257 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:43708 closed before handshake completed
2023-07-31 05:41:54,549 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:43710 closed before handshake completed
2023-07-31 05:41:55,651 - distributed.scheduler - INFO - Receive client connection: Client-f575fdc6-2f64-11ee-8acd-d8c49764f6bb
2023-07-31 05:41:55,651 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49274
2023-07-31 05:41:56,233 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41455
2023-07-31 05:41:56,233 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41455
2023-07-31 05:41:56,234 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39347
2023-07-31 05:41:56,234 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,234 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,234 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:56,234 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:56,234 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z77351m5
2023-07-31 05:41:56,234 - distributed.worker - INFO - Starting Worker plugin RMMSetup-146c2cd4-7a04-4638-9bf7-14b509e51c9a
2023-07-31 05:41:56,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41863
2023-07-31 05:41:56,273 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41863
2023-07-31 05:41:56,273 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41249
2023-07-31 05:41:56,273 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,273 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,273 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:56,273 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:56,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xq66z0ms
2023-07-31 05:41:56,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e690c6b2-9934-417e-af7d-f47aeed40d80
2023-07-31 05:41:56,461 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37559
2023-07-31 05:41:56,461 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37559
2023-07-31 05:41:56,461 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37733
2023-07-31 05:41:56,461 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,462 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,462 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:56,462 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:56,462 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qd1bt3bs
2023-07-31 05:41:56,462 - distributed.worker - INFO - Starting Worker plugin PreImport-51a40726-eb16-4896-8b74-1b7e5ff6b7f3
2023-07-31 05:41:56,462 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d7824952-8fab-4a5a-97e9-04ef60ac1f10
2023-07-31 05:41:56,462 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3817736e-c096-4d2a-847c-67c4cfc3aff2
2023-07-31 05:41:56,471 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46351
2023-07-31 05:41:56,471 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46351
2023-07-31 05:41:56,471 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45189
2023-07-31 05:41:56,472 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,472 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,472 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:56,472 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:56,472 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-308zyy2x
2023-07-31 05:41:56,472 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e685777-8fa6-453e-af45-0892100a2da0
2023-07-31 05:41:56,519 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41617
2023-07-31 05:41:56,520 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41617
2023-07-31 05:41:56,520 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39901
2023-07-31 05:41:56,520 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,520 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,520 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:56,520 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:56,520 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q1ben4qh
2023-07-31 05:41:56,521 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51e016e8-209b-49ca-942d-2fc83dbef653
2023-07-31 05:41:56,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37141
2023-07-31 05:41:56,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37141
2023-07-31 05:41:56,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37887
2023-07-31 05:41:56,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,536 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,536 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:56,536 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:56,536 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-p8zsx_5s
2023-07-31 05:41:56,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e120bb35-bc36-4efa-9ca4-bfc06eb454c8
2023-07-31 05:41:56,537 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35341
2023-07-31 05:41:56,537 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35341
2023-07-31 05:41:56,537 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44687
2023-07-31 05:41:56,537 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,538 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,538 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:56,538 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:56,538 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sxefm4sw
2023-07-31 05:41:56,539 - distributed.worker - INFO - Starting Worker plugin RMMSetup-645628eb-22c5-4c3c-ba2d-de9b6328f8a9
2023-07-31 05:41:56,554 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45893
2023-07-31 05:41:56,554 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45893
2023-07-31 05:41:56,554 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43409
2023-07-31 05:41:56,554 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,554 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,554 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:41:56,555 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:41:56,555 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dzx2pks6
2023-07-31 05:41:56,555 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0c4c402-73df-46c5-8be0-4eba7ec3e3eb
2023-07-31 05:41:56,582 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f40c12d5-f054-41cf-a238-fb19038e7bf5
2023-07-31 05:41:56,582 - distributed.worker - INFO - Starting Worker plugin PreImport-efc320fb-d0e5-4a05-a8f0-f373e4a24a9e
2023-07-31 05:41:56,583 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,586 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c00273dc-3fb6-4f54-9ffe-5b44d4054f4b
2023-07-31 05:41:56,586 - distributed.worker - INFO - Starting Worker plugin PreImport-977d2a18-7bb1-4ae8-964f-2b098d4048e6
2023-07-31 05:41:56,587 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,614 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41455', status: init, memory: 0, processing: 0>
2023-07-31 05:41:56,615 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41455
2023-07-31 05:41:56,615 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49292
2023-07-31 05:41:56,615 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,615 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,617 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:56,618 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41863', status: init, memory: 0, processing: 0>
2023-07-31 05:41:56,618 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41863
2023-07-31 05:41:56,618 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49302
2023-07-31 05:41:56,619 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,619 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,621 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:56,727 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,733 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f442be28-aa8d-469d-a2cd-f0d5612bc72f
2023-07-31 05:41:56,733 - distributed.worker - INFO - Starting Worker plugin PreImport-4fc4c0bf-2dcc-49da-80c7-a74dba31f149
2023-07-31 05:41:56,734 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ae0c89b-3855-4771-890e-6f342f193590
2023-07-31 05:41:56,745 - distributed.worker - INFO - Starting Worker plugin PreImport-0423fc6f-1db3-45e6-b5ec-ed4147ba5248
2023-07-31 05:41:56,745 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,750 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6bc05ddb-2515-436f-8197-a07bf071329b
2023-07-31 05:41:56,750 - distributed.worker - INFO - Starting Worker plugin PreImport-87fd7061-a27d-4e1f-a39c-cc7934abd090
2023-07-31 05:41:56,750 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ed519a8-5eb7-40bf-8d4b-34bbbd9dba92
2023-07-31 05:41:56,752 - distributed.worker - INFO - Starting Worker plugin PreImport-6381a4df-c018-403f-ba93-e3453575eda5
2023-07-31 05:41:56,752 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,753 - distributed.worker - INFO - Starting Worker plugin PreImport-c8d2889c-b1df-421c-808f-4055d014bb4a
2023-07-31 05:41:56,753 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c2ff7c1-9fe0-496f-b03d-10171e09541c
2023-07-31 05:41:56,753 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,755 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37559', status: init, memory: 0, processing: 0>
2023-07-31 05:41:56,755 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37559
2023-07-31 05:41:56,756 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49316
2023-07-31 05:41:56,756 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,756 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,758 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:56,777 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46351', status: init, memory: 0, processing: 0>
2023-07-31 05:41:56,778 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46351
2023-07-31 05:41:56,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49318
2023-07-31 05:41:56,779 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,779 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,782 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:56,785 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35341', status: init, memory: 0, processing: 0>
2023-07-31 05:41:56,786 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35341
2023-07-31 05:41:56,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49338
2023-07-31 05:41:56,786 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,786 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,787 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45893', status: init, memory: 0, processing: 0>
2023-07-31 05:41:56,787 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45893
2023-07-31 05:41:56,787 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49358
2023-07-31 05:41:56,788 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,788 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,789 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:56,790 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37141', status: init, memory: 0, processing: 0>
2023-07-31 05:41:56,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:56,790 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37141
2023-07-31 05:41:56,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49354
2023-07-31 05:41:56,791 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,791 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,794 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:56,799 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41617', status: init, memory: 0, processing: 0>
2023-07-31 05:41:56,799 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41617
2023-07-31 05:41:56,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49326
2023-07-31 05:41:56,800 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:41:56,800 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:41:56,803 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:41:56,877 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:56,877 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:56,877 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:56,877 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:56,877 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:56,877 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:56,877 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:56,878 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:41:56,889 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-31 05:41:56,889 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-31 05:41:56,889 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-31 05:41:56,889 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-31 05:41:56,890 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-31 05:41:56,890 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-31 05:41:56,890 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-31 05:41:56,890 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-31 05:41:56,896 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:41:56,898 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:41:56,900 - distributed.scheduler - INFO - Remove client Client-f14cef7f-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:41:56,901 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43518; closing.
2023-07-31 05:41:56,901 - distributed.scheduler - INFO - Remove client Client-f14cef7f-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:41:56,901 - distributed.scheduler - INFO - Close client connection: Client-f14cef7f-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:41:56,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40447'. Reason: nanny-close
2023-07-31 05:41:56,903 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:56,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40439'. Reason: nanny-close
2023-07-31 05:41:56,904 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:56,904 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37559. Reason: nanny-close
2023-07-31 05:41:56,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40009'. Reason: nanny-close
2023-07-31 05:41:56,905 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:56,905 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45893. Reason: nanny-close
2023-07-31 05:41:56,905 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39967'. Reason: nanny-close
2023-07-31 05:41:56,905 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:56,905 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41161'. Reason: nanny-close
2023-07-31 05:41:56,906 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37141. Reason: nanny-close
2023-07-31 05:41:56,906 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:56,906 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:56,906 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49316; closing.
2023-07-31 05:41:56,906 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39351'. Reason: nanny-close
2023-07-31 05:41:56,906 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46351. Reason: nanny-close
2023-07-31 05:41:56,906 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:56,906 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37559', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:56,906 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37559
2023-07-31 05:41:56,907 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36473'. Reason: nanny-close
2023-07-31 05:41:56,907 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41863. Reason: nanny-close
2023-07-31 05:41:56,907 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:56,907 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:56,907 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42609'. Reason: nanny-close
2023-07-31 05:41:56,907 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41455. Reason: nanny-close
2023-07-31 05:41:56,907 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:56,907 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:41:56,908 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37559
2023-07-31 05:41:56,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49358; closing.
2023-07-31 05:41:56,908 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37559
2023-07-31 05:41:56,908 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37559
2023-07-31 05:41:56,908 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:56,908 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37559
2023-07-31 05:41:56,908 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37559
2023-07-31 05:41:56,908 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:56,908 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45893', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:56,908 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35341. Reason: nanny-close
2023-07-31 05:41:56,909 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45893
2023-07-31 05:41:56,909 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37559
2023-07-31 05:41:56,909 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:56,909 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:56,909 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41617. Reason: nanny-close
2023-07-31 05:41:56,909 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49354; closing.
2023-07-31 05:41:56,909 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:56,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37141', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:56,910 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37141
2023-07-31 05:41:56,910 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:56,910 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49318; closing.
2023-07-31 05:41:56,910 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:56,910 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49302; closing.
2023-07-31 05:41:56,910 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:56,910 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:56,910 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:56,910 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49292; closing.
2023-07-31 05:41:56,911 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46351', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:56,911 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46351
2023-07-31 05:41:56,911 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:41:56,911 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41863', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:56,911 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41863
2023-07-31 05:41:56,911 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:56,911 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41455', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:56,912 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41455
2023-07-31 05:41:56,912 - distributed.nanny - INFO - Worker closed
2023-07-31 05:41:56,912 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49338; closing.
2023-07-31 05:41:56,912 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35341', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:56,913 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35341
2023-07-31 05:41:56,913 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49326; closing.
2023-07-31 05:41:56,913 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41617', status: closing, memory: 0, processing: 0>
2023-07-31 05:41:56,913 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41617
2023-07-31 05:41:56,913 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:41:58,622 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-31 05:41:58,622 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:41:58,623 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:41:58,625 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-31 05:41:58,626 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-07-31 05:42:00,940 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:42:00,945 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46109 instead
  warnings.warn(
2023-07-31 05:42:00,949 - distributed.scheduler - INFO - State start
2023-07-31 05:42:00,972 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:42:00,973 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-31 05:42:00,974 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46109/status
2023-07-31 05:42:01,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36161'
2023-07-31 05:42:01,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33835'
2023-07-31 05:42:01,200 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42753'
2023-07-31 05:42:01,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36569'
2023-07-31 05:42:01,213 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43569'
2023-07-31 05:42:01,222 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41445'
2023-07-31 05:42:01,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35269'
2023-07-31 05:42:01,239 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40485'
2023-07-31 05:42:01,245 - distributed.scheduler - INFO - Receive client connection: Client-f78bd941-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:42:01,259 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49734
2023-07-31 05:42:01,359 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40187', status: init, memory: 0, processing: 0>
2023-07-31 05:42:01,360 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40187
2023-07-31 05:42:01,360 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49756
2023-07-31 05:42:01,480 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49756; closing.
2023-07-31 05:42:01,480 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40187', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:01,480 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40187
2023-07-31 05:42:01,480 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:42:02,908 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:42:02,908 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:42:02,925 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:42:02,925 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:42:02,934 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:42:02,948 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:42:02,948 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:42:02,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:42:02,959 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:42:02,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:42:02,959 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:42:02,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:42:02,962 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:42:02,962 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:42:02,985 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:42:02,986 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:42:02,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:42:02,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:42:02,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:42:03,004 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:42:03,004 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:42:03,004 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:42:03,045 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:42:03,045 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:42:05,151 - distributed.scheduler - INFO - Receive client connection: Client-fb1f84b1-2f64-11ee-8acd-d8c49764f6bb
2023-07-31 05:42:05,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38722
2023-07-31 05:42:06,908 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46497
2023-07-31 05:42:06,908 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46497
2023-07-31 05:42:06,908 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42021
2023-07-31 05:42:06,908 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:42:06,908 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:06,908 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:42:06,908 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:42:06,908 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gp1jsuh2
2023-07-31 05:42:06,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f20d71db-dcbf-4951-841c-93aca6e57cd9
2023-07-31 05:42:06,926 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33035
2023-07-31 05:42:06,926 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33035
2023-07-31 05:42:06,926 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33237
2023-07-31 05:42:06,926 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:42:06,926 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:06,926 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:42:06,927 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:42:06,927 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qoou3j3j
2023-07-31 05:42:06,927 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe74aa23-ede3-44ae-92ec-6c8782dc70fd
2023-07-31 05:42:06,974 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42043
2023-07-31 05:42:06,974 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42043
2023-07-31 05:42:06,975 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44463
2023-07-31 05:42:06,975 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:42:06,975 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:06,975 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:42:06,975 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:42:06,975 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-k990y0ro
2023-07-31 05:42:06,976 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68d1583d-f621-4702-8a27-ae1a140bf181
2023-07-31 05:42:07,092 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35863
2023-07-31 05:42:07,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35863
2023-07-31 05:42:07,093 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41287
2023-07-31 05:42:07,093 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,093 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,093 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:42:07,093 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:42:07,093 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9pomt6aj
2023-07-31 05:42:07,094 - distributed.worker - INFO - Starting Worker plugin PreImport-26917f08-1c83-41c5-91e5-c2ed9ee676c6
2023-07-31 05:42:07,094 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f56e48e-3ce0-40d2-8a01-ea2aac5a9044
2023-07-31 05:42:07,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-570e892d-2237-4794-abbf-c2f710298abb
2023-07-31 05:42:07,105 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38481
2023-07-31 05:42:07,105 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38481
2023-07-31 05:42:07,105 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35159
2023-07-31 05:42:07,105 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,105 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,106 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:42:07,106 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:42:07,106 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d2ccblzs
2023-07-31 05:42:07,107 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1930215a-61fd-4e96-84f7-fdaaf7900e55
2023-07-31 05:42:07,134 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43327
2023-07-31 05:42:07,134 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43327
2023-07-31 05:42:07,134 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42779
2023-07-31 05:42:07,135 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41619
2023-07-31 05:42:07,135 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42779
2023-07-31 05:42:07,135 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,135 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,135 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35621
2023-07-31 05:42:07,135 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,135 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:42:07,135 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,135 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:42:07,135 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:42:07,135 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o102kczp
2023-07-31 05:42:07,135 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:42:07,135 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lo_vad2_
2023-07-31 05:42:07,135 - distributed.worker - INFO - Starting Worker plugin RMMSetup-011c0d9e-f1c9-4b7d-a65b-76d4f179aee7
2023-07-31 05:42:07,136 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55ae0117-da3e-4b72-b7c3-5d0f0a60c107
2023-07-31 05:42:07,142 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33995
2023-07-31 05:42:07,142 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33995
2023-07-31 05:42:07,142 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36787
2023-07-31 05:42:07,142 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,142 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,143 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:42:07,143 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:42:07,143 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lcf20uub
2023-07-31 05:42:07,143 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d513f75b-a4d2-44e2-86bc-8892d6dc87c5
2023-07-31 05:42:07,308 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9079f84d-71dc-4451-8f28-a6961ad95e3c
2023-07-31 05:42:07,309 - distributed.worker - INFO - Starting Worker plugin PreImport-be99cc93-5b2d-4d84-b6ab-71489f04bf4f
2023-07-31 05:42:07,309 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,310 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cfbad84b-624e-4d56-b44a-3c6509f27bb1
2023-07-31 05:42:07,310 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,311 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-48a823b8-2a58-4280-b745-143200c9d6e7
2023-07-31 05:42:07,311 - distributed.worker - INFO - Starting Worker plugin PreImport-1c5fae70-23b0-4d2a-8f1f-4f03d69a4085
2023-07-31 05:42:07,311 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,311 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94a16e11-41f9-4ccf-b99a-e3ceea448190
2023-07-31 05:42:07,311 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aecf5404-9b36-449e-9d6e-632d8adfe7a8
2023-07-31 05:42:07,311 - distributed.worker - INFO - Starting Worker plugin PreImport-feea2cd5-7050-418f-9998-d08c6617f542
2023-07-31 05:42:07,311 - distributed.worker - INFO - Starting Worker plugin PreImport-71191ba8-e4f1-4988-ad42-612af8178cec
2023-07-31 05:42:07,311 - distributed.worker - INFO - Starting Worker plugin PreImport-1391a34f-3596-4a5e-9710-535c0e883cb0
2023-07-31 05:42:07,311 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-73f784c9-5c1a-4af0-9fad-3047f3af5dc7
2023-07-31 05:42:07,312 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,312 - distributed.worker - INFO - Starting Worker plugin PreImport-e20198e8-570d-4b8e-a9e5-df73b3056a30
2023-07-31 05:42:07,312 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-893f8f2e-6903-4454-8a65-6ed504073dba
2023-07-31 05:42:07,312 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,312 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,312 - distributed.worker - INFO - Starting Worker plugin PreImport-5c6d9865-77a2-44cb-939c-d7549ae997ef
2023-07-31 05:42:07,312 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,312 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,341 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42779', status: init, memory: 0, processing: 0>
2023-07-31 05:42:07,342 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42779
2023-07-31 05:42:07,342 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38788
2023-07-31 05:42:07,343 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,343 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,343 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35863', status: init, memory: 0, processing: 0>
2023-07-31 05:42:07,344 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35863
2023-07-31 05:42:07,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38760
2023-07-31 05:42:07,345 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:42:07,345 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,345 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,345 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33035', status: init, memory: 0, processing: 0>
2023-07-31 05:42:07,345 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33035
2023-07-31 05:42:07,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38774
2023-07-31 05:42:07,346 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,346 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,346 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42043', status: init, memory: 0, processing: 0>
2023-07-31 05:42:07,347 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42043
2023-07-31 05:42:07,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38752
2023-07-31 05:42:07,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:42:07,347 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,348 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:42:07,349 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43327', status: init, memory: 0, processing: 0>
2023-07-31 05:42:07,349 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43327
2023-07-31 05:42:07,349 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38818
2023-07-31 05:42:07,350 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,350 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:42:07,351 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38481', status: init, memory: 0, processing: 0>
2023-07-31 05:42:07,351 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38481
2023-07-31 05:42:07,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38812
2023-07-31 05:42:07,352 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,352 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:42:07,356 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46497', status: init, memory: 0, processing: 0>
2023-07-31 05:42:07,357 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46497
2023-07-31 05:42:07,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38794
2023-07-31 05:42:07,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:42:07,357 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,358 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,358 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33995', status: init, memory: 0, processing: 0>
2023-07-31 05:42:07,359 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33995
2023-07-31 05:42:07,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38798
2023-07-31 05:42:07,359 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:42:07,360 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:07,361 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:42:07,363 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:42:07,391 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:42:07,391 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:42:07,391 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:42:07,391 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:42:07,391 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:42:07,392 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:42:07,392 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:42:07,392 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:42:07,398 - distributed.scheduler - INFO - Remove client Client-f78bd941-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:42:07,398 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49734; closing.
2023-07-31 05:42:07,399 - distributed.scheduler - INFO - Remove client Client-f78bd941-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:42:07,399 - distributed.scheduler - INFO - Close client connection: Client-f78bd941-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:42:07,400 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33835'. Reason: nanny-close
2023-07-31 05:42:07,401 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:42:07,401 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43569'. Reason: nanny-close
2023-07-31 05:42:07,402 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:42:07,402 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41445'. Reason: nanny-close
2023-07-31 05:42:07,403 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35863. Reason: nanny-close
2023-07-31 05:42:07,403 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:42:07,403 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36161'. Reason: nanny-close
2023-07-31 05:42:07,403 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:42:07,403 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38481. Reason: nanny-close
2023-07-31 05:42:07,404 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42043. Reason: nanny-close
2023-07-31 05:42:07,404 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42753'. Reason: nanny-close
2023-07-31 05:42:07,404 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36569'. Reason: nanny-close
2023-07-31 05:42:07,404 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35269'. Reason: nanny-close
2023-07-31 05:42:07,404 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33035. Reason: nanny-close
2023-07-31 05:42:07,404 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:42:07,405 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40485'. Reason: nanny-close
2023-07-31 05:42:07,405 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:42:07,405 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:42:07,405 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38760; closing.
2023-07-31 05:42:07,405 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43327. Reason: nanny-close
2023-07-31 05:42:07,405 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35863', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:07,406 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35863
2023-07-31 05:42:07,406 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:42:07,406 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:42:07,406 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42779. Reason: nanny-close
2023-07-31 05:42:07,406 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:42:07,406 - distributed.nanny - INFO - Worker closed
2023-07-31 05:42:07,407 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:42:07,407 - distributed.nanny - INFO - Worker closed
2023-07-31 05:42:07,407 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:42:07,407 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35863
2023-07-31 05:42:07,408 - distributed.nanny - INFO - Worker closed
2023-07-31 05:42:07,408 - distributed.nanny - INFO - Worker closed
2023-07-31 05:42:07,408 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:42:07,408 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38752; closing.
2023-07-31 05:42:07,408 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35863
2023-07-31 05:42:07,408 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38774; closing.
2023-07-31 05:42:07,408 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35863
2023-07-31 05:42:07,408 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:42:07,408 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38812; closing.
2023-07-31 05:42:07,409 - distributed.nanny - INFO - Worker closed
2023-07-31 05:42:07,409 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33995. Reason: nanny-close
2023-07-31 05:42:07,409 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46497. Reason: nanny-close
2023-07-31 05:42:07,409 - distributed.nanny - INFO - Worker closed
2023-07-31 05:42:07,409 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42043', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:07,409 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42043
2023-07-31 05:42:07,410 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33035', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:07,410 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33035
2023-07-31 05:42:07,411 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38481', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:07,411 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38481
2023-07-31 05:42:07,411 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:42:07,411 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:42:07,412 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38818; closing.
2023-07-31 05:42:07,412 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38788; closing.
2023-07-31 05:42:07,412 - distributed.nanny - INFO - Worker closed
2023-07-31 05:42:07,412 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43327', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:07,413 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43327
2023-07-31 05:42:07,413 - distributed.nanny - INFO - Worker closed
2023-07-31 05:42:07,413 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42779', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:07,413 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42779
2023-07-31 05:42:07,414 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38798; closing.
2023-07-31 05:42:07,414 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38794; closing.
2023-07-31 05:42:07,415 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33995', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:07,415 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33995
2023-07-31 05:42:07,415 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46497', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:07,415 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46497
2023-07-31 05:42:07,415 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:42:07,416 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38798>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-31 05:42:09,019 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-31 05:42:09,020 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:42:09,020 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:42:09,022 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-31 05:42:09,023 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-07-31 05:42:11,161 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:42:11,166 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40915 instead
  warnings.warn(
2023-07-31 05:42:11,170 - distributed.scheduler - INFO - State start
2023-07-31 05:42:11,192 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:42:11,193 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-31 05:42:11,193 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40915/status
2023-07-31 05:42:11,318 - distributed.scheduler - INFO - Receive client connection: Client-fb1f84b1-2f64-11ee-8acd-d8c49764f6bb
2023-07-31 05:42:11,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39256
2023-07-31 05:42:11,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39665'
2023-07-31 05:42:11,596 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39117', status: init, memory: 0, processing: 0>
2023-07-31 05:42:11,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39117
2023-07-31 05:42:11,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39284
2023-07-31 05:42:11,718 - distributed.scheduler - INFO - Receive client connection: Client-fdb0f9c9-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:42:11,719 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39298
2023-07-31 05:42:11,731 - distributed.scheduler - INFO - Remove client Client-fdb0f9c9-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:42:11,731 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39298; closing.
2023-07-31 05:42:11,731 - distributed.scheduler - INFO - Remove client Client-fdb0f9c9-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:42:11,732 - distributed.scheduler - INFO - Close client connection: Client-fdb0f9c9-2f64-11ee-8518-d8c49764f6bb
2023-07-31 05:42:11,733 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39665'. Reason: nanny-close
2023-07-31 05:42:13,120 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:42:13,120 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:42:13,205 - distributed.scheduler - INFO - Remove client Client-fb1f84b1-2f64-11ee-8acd-d8c49764f6bb
2023-07-31 05:42:13,205 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39256; closing.
2023-07-31 05:42:13,205 - distributed.scheduler - INFO - Remove client Client-fb1f84b1-2f64-11ee-8acd-d8c49764f6bb
2023-07-31 05:42:13,206 - distributed.scheduler - INFO - Close client connection: Client-fb1f84b1-2f64-11ee-8acd-d8c49764f6bb
2023-07-31 05:42:13,210 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39284; closing.
2023-07-31 05:42:13,211 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39117', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:13,211 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39117
2023-07-31 05:42:13,211 - distributed.scheduler - INFO - Lost all workers
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-31 05:42:13,694 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:42:14,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44763
2023-07-31 05:42:14,810 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44763
2023-07-31 05:42:14,810 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-07-31 05:42:14,811 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:42:14,811 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:14,811 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:42:14,811 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-31 05:42:14,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2yoj0tmy
2023-07-31 05:42:14,811 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6aa0df5-c7f5-4d3e-b0c7-fb266568ec82
2023-07-31 05:42:14,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1bee875-5836-4800-9a86-edf586544d6e
2023-07-31 05:42:14,811 - distributed.worker - INFO - Starting Worker plugin PreImport-f7948773-1c92-4bd5-b3e1-f12d90c4f931
2023-07-31 05:42:14,812 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:14,848 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44763', status: init, memory: 0, processing: 0>
2023-07-31 05:42:14,849 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44763
2023-07-31 05:42:14,849 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60828
2023-07-31 05:42:14,850 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:42:14,850 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:14,853 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:42:14,888 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:42:14,890 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44763. Reason: nanny-close
2023-07-31 05:42:14,893 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:42:14,893 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60828; closing.
2023-07-31 05:42:14,894 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44763', status: closing, memory: 0, processing: 0>
2023-07-31 05:42:14,894 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44763
2023-07-31 05:42:14,894 - distributed.nanny - INFO - Worker closed
2023-07-31 05:42:14,895 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:42:16,261 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-31 05:42:16,262 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:42:16,262 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:42:16,263 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-31 05:42:16,263 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-07-31 05:42:20,499 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:42:20,504 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46289 instead
  warnings.warn(
2023-07-31 05:42:20,508 - distributed.scheduler - INFO - State start
2023-07-31 05:42:20,528 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:42:20,529 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:42:20,530 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:42:20,530 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-07-31 05:42:20,641 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38555'
2023-07-31 05:42:22,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:42:22,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-07-31 05:42:23,044 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:42:24,249 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46505
2023-07-31 05:42:24,249 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46505
2023-07-31 05:42:24,249 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33813
2023-07-31 05:42:24,249 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:42:24,249 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:24,249 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:42:24,249 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-31 05:42:24,249 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3yhrvr_9
2023-07-31 05:42:24,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-036c18bb-9cdb-4503-813b-1bb7e8634334
2023-07-31 05:42:24,250 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f8c9e026-2a38-4745-8199-72bd8a71a002
2023-07-31 05:42:24,250 - distributed.worker - INFO - Starting Worker plugin PreImport-287f9e70-b789-4282-9432-1c462490cdee
2023-07-31 05:42:24,252 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:37,360 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:42:37,360 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:42:37,363 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:42:37,874 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46505. Reason: scheduler-close
2023-07-31 05:42:37,875 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:9369; closing.
2023-07-31 05:42:37,877 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:39418 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-07-31 05:42:37,881 - distributed.nanny - INFO - Closing Nanny gracefully at 'tcp://127.0.0.1:38555'. Reason: scheduler-close
2023-07-31 05:42:37,883 - distributed.nanny - INFO - Worker closed
2023-07-31 05:42:39,886 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-07-31 05:42:40,873 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38555'. Reason: nanny-close-gracefully
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-07-31 05:42:52,296 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:42:52,301 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40893 instead
  warnings.warn(
2023-07-31 05:42:52,305 - distributed.scheduler - INFO - State start
2023-07-31 05:42:52,327 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:42:52,328 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-31 05:42:52,329 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40893/status
2023-07-31 05:42:57,256 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-31 05:42:57,257 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:42:57,257 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:42:57,258 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-31 05:42:57,258 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-07-31 05:42:59,592 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:42:59,597 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39929 instead
  warnings.warn(
2023-07-31 05:42:59,601 - distributed.scheduler - INFO - State start
2023-07-31 05:42:59,623 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:42:59,624 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-07-31 05:42:59,625 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39929/status
2023-07-31 05:42:59,791 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45889'
2023-07-31 05:43:00,868 - distributed.scheduler - INFO - Receive client connection: Client-1a8579f6-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:00,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49880
2023-07-31 05:43:01,497 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:01,497 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:01,505 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:02,524 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38049
2023-07-31 05:43:02,524 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38049
2023-07-31 05:43:02,525 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33133
2023-07-31 05:43:02,525 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-07-31 05:43:02,525 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:02,525 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:02,525 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-31 05:43:02,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-62enhdd1
2023-07-31 05:43:02,525 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07f64f2a-1ea6-4fc8-92b9-20008bbdf4a3
2023-07-31 05:43:02,525 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0fae061e-f734-407c-8b6a-37b171374b98
2023-07-31 05:43:02,526 - distributed.worker - INFO - Starting Worker plugin PreImport-6153d447-a615-4a8c-b27a-db8ca417cd86
2023-07-31 05:43:02,526 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:02,547 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38049', status: init, memory: 0, processing: 0>
2023-07-31 05:43:02,549 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38049
2023-07-31 05:43:02,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49906
2023-07-31 05:43:02,549 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-07-31 05:43:02,549 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:02,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-07-31 05:43:02,652 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:02,655 - distributed.scheduler - INFO - Remove client Client-1a8579f6-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:02,655 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49880; closing.
2023-07-31 05:43:02,655 - distributed.scheduler - INFO - Remove client Client-1a8579f6-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:02,655 - distributed.scheduler - INFO - Close client connection: Client-1a8579f6-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:02,656 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45889'. Reason: nanny-close
2023-07-31 05:43:02,657 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:02,658 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38049. Reason: nanny-close
2023-07-31 05:43:02,660 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49906; closing.
2023-07-31 05:43:02,660 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-07-31 05:43:02,660 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38049', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:02,660 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38049
2023-07-31 05:43:02,661 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:43:02,661 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:04,576 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-31 05:43:04,576 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:43:04,576 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:43:04,577 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-07-31 05:43:04,577 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-07-31 05:43:06,783 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:43:06,788 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33255 instead
  warnings.warn(
2023-07-31 05:43:06,793 - distributed.scheduler - INFO - State start
2023-07-31 05:43:06,816 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:43:06,817 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-31 05:43:06,818 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33255/status
2023-07-31 05:43:07,065 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38175'
2023-07-31 05:43:07,087 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35919'
2023-07-31 05:43:07,089 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40949'
2023-07-31 05:43:07,097 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37293'
2023-07-31 05:43:07,109 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39119'
2023-07-31 05:43:07,120 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36113'
2023-07-31 05:43:07,131 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44699'
2023-07-31 05:43:07,141 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46539'
2023-07-31 05:43:08,122 - distributed.scheduler - INFO - Receive client connection: Client-1ec48d15-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:08,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46986
2023-07-31 05:43:08,886 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:08,886 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:08,886 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:08,886 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:08,908 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:08,908 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:08,911 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:08,911 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:08,914 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:08,914 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:08,928 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:08,928 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:08,946 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:08,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:08,952 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:08,952 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:08,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:08,963 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:08,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:08,991 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:08,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:08,999 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:09,020 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:09,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:12,837 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41471
2023-07-31 05:43:12,837 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41471
2023-07-31 05:43:12,837 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42569
2023-07-31 05:43:12,837 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:43:12,837 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:12,837 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:12,838 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:43:12,838 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2nysm9bd
2023-07-31 05:43:12,838 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e8edb25-ed5f-480f-91ce-cb8c9a66c346
2023-07-31 05:43:12,845 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35507
2023-07-31 05:43:12,845 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35507
2023-07-31 05:43:12,845 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43191
2023-07-31 05:43:12,845 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:43:12,845 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:12,845 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43343
2023-07-31 05:43:12,845 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:12,845 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43343
2023-07-31 05:43:12,845 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:43:12,845 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39603
2023-07-31 05:43:12,845 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rht27lgh
2023-07-31 05:43:12,845 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:43:12,845 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:12,846 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:12,846 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:43:12,846 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uecj_0ai
2023-07-31 05:43:12,846 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3fc7b4ff-8033-48b4-8018-30c40eddb82e
2023-07-31 05:43:12,846 - distributed.worker - INFO - Starting Worker plugin PreImport-b60b796a-eb03-4698-9178-0c9fa9e1808f
2023-07-31 05:43:12,846 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2cae618-3624-4104-a08e-32a18c8f419c
2023-07-31 05:43:12,846 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2c2c23e2-585c-4b08-83df-83eaaf415248
2023-07-31 05:43:12,947 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41071
2023-07-31 05:43:12,947 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41071
2023-07-31 05:43:12,948 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34155
2023-07-31 05:43:12,948 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:43:12,948 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:12,948 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:12,948 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:43:12,948 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0luxju1a
2023-07-31 05:43:12,948 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da16774d-afe7-4a16-ac2b-4388c7d1079d
2023-07-31 05:43:12,956 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34641
2023-07-31 05:43:12,957 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34641
2023-07-31 05:43:12,957 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33645
2023-07-31 05:43:12,957 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:43:12,957 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:12,957 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:12,957 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:43:12,957 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lwwoab1k
2023-07-31 05:43:12,958 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e16fbe8-7ad9-40a6-b611-9a64b4aacb3e
2023-07-31 05:43:12,966 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43693
2023-07-31 05:43:12,966 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43693
2023-07-31 05:43:12,966 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39759
2023-07-31 05:43:12,966 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:43:12,966 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:12,966 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:12,966 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:43:12,966 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-beo1n1u7
2023-07-31 05:43:12,967 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b078d64c-fbc4-4705-8419-2b7287f1ff2b
2023-07-31 05:43:13,066 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32967
2023-07-31 05:43:13,067 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32967
2023-07-31 05:43:13,067 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33613
2023-07-31 05:43:13,067 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:43:13,067 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,067 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:13,067 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:43:13,067 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-te4s0u1t
2023-07-31 05:43:13,067 - distributed.worker - INFO - Starting Worker plugin RMMSetup-734f2a97-9d3b-45c6-bc5e-65a9e1674cbd
2023-07-31 05:43:13,074 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35413
2023-07-31 05:43:13,075 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35413
2023-07-31 05:43:13,075 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45351
2023-07-31 05:43:13,075 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:43:13,075 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,075 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:13,075 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-07-31 05:43:13,075 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-89ak9imh
2023-07-31 05:43:13,076 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1656754-a8ce-404c-a2c9-8a83bf463f65
2023-07-31 05:43:13,173 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1501131-6e70-4c42-b78a-043f5949a751
2023-07-31 05:43:13,173 - distributed.worker - INFO - Starting Worker plugin PreImport-304a6042-a5e1-47d3-8540-0e1df6030f07
2023-07-31 05:43:13,173 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,173 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04e84fa4-f821-4d1b-b739-be567de57965
2023-07-31 05:43:13,174 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,174 - distributed.worker - INFO - Starting Worker plugin PreImport-36a58066-5497-44ed-bf65-8643b8c97fd8
2023-07-31 05:43:13,179 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,203 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43343', status: init, memory: 0, processing: 0>
2023-07-31 05:43:13,206 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43343
2023-07-31 05:43:13,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47006
2023-07-31 05:43:13,206 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:43:13,207 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,209 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:43:13,213 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0405865f-7c73-4215-9c5f-5c3ea7c6b26f
2023-07-31 05:43:13,213 - distributed.worker - INFO - Starting Worker plugin PreImport-c99a5e2a-e6de-443b-a8ae-c33bf75e7352
2023-07-31 05:43:13,213 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4edf406-be3d-4625-9313-f044cb0aa65c
2023-07-31 05:43:13,213 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,213 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-10fc5aad-d82d-4b6d-b999-eb78033b6e3f
2023-07-31 05:43:13,213 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3e03f1c-6b65-4283-b0e4-8760d3b976c1
2023-07-31 05:43:13,213 - distributed.worker - INFO - Starting Worker plugin PreImport-ac477495-2033-41f4-8bd8-dfbb3e668ef2
2023-07-31 05:43:13,214 - distributed.worker - INFO - Starting Worker plugin PreImport-afc4549d-449f-43a7-b46b-1b9ae819f7d9
2023-07-31 05:43:13,214 - distributed.worker - INFO - Starting Worker plugin PreImport-63141b75-fbb7-46bb-a997-7a4c58d24c8b
2023-07-31 05:43:13,214 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,214 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,214 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,216 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35507', status: init, memory: 0, processing: 0>
2023-07-31 05:43:13,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35507
2023-07-31 05:43:13,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47020
2023-07-31 05:43:13,217 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:43:13,218 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:43:13,223 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41071', status: init, memory: 0, processing: 0>
2023-07-31 05:43:13,224 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41071
2023-07-31 05:43:13,224 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47028
2023-07-31 05:43:13,225 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:43:13,225 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:43:13,236 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-642d6561-7422-4449-b07d-2f2b1117e222
2023-07-31 05:43:13,236 - distributed.worker - INFO - Starting Worker plugin PreImport-8b59bf11-75d3-4a0a-9f25-5ed0abdf0f29
2023-07-31 05:43:13,236 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,243 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41471', status: init, memory: 0, processing: 0>
2023-07-31 05:43:13,244 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41471
2023-07-31 05:43:13,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47036
2023-07-31 05:43:13,244 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:43:13,244 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:43:13,253 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35413', status: init, memory: 0, processing: 0>
2023-07-31 05:43:13,254 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35413
2023-07-31 05:43:13,255 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47050
2023-07-31 05:43:13,255 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:43:13,255 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,256 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43693', status: init, memory: 0, processing: 0>
2023-07-31 05:43:13,257 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43693
2023-07-31 05:43:13,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47066
2023-07-31 05:43:13,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:43:13,257 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:43:13,257 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,260 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:43:13,261 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32967', status: init, memory: 0, processing: 0>
2023-07-31 05:43:13,261 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32967
2023-07-31 05:43:13,261 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47072
2023-07-31 05:43:13,262 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:43:13,262 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:43:13,272 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34641', status: init, memory: 0, processing: 0>
2023-07-31 05:43:13,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34641
2023-07-31 05:43:13,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47086
2023-07-31 05:43:13,274 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:43:13,274 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:13,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:43:13,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:43:13,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:43:13,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:43:13,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:43:13,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:43:13,382 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:43:13,382 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:43:13,382 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-07-31 05:43:13,401 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:13,401 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:13,401 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:13,401 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:13,402 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:13,402 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:13,402 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:13,402 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:13,408 - distributed.scheduler - INFO - Remove client Client-1ec48d15-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:13,408 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46986; closing.
2023-07-31 05:43:13,409 - distributed.scheduler - INFO - Remove client Client-1ec48d15-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:13,409 - distributed.scheduler - INFO - Close client connection: Client-1ec48d15-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:13,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35919'. Reason: nanny-close
2023-07-31 05:43:13,411 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:13,411 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39119'. Reason: nanny-close
2023-07-31 05:43:13,412 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:13,412 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43343. Reason: nanny-close
2023-07-31 05:43:13,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36113'. Reason: nanny-close
2023-07-31 05:43:13,412 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:13,413 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35507. Reason: nanny-close
2023-07-31 05:43:13,413 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38175'. Reason: nanny-close
2023-07-31 05:43:13,413 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:13,413 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40949'. Reason: nanny-close
2023-07-31 05:43:13,413 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32967. Reason: nanny-close
2023-07-31 05:43:13,413 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:13,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:43:13,414 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37293'. Reason: nanny-close
2023-07-31 05:43:13,414 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47006; closing.
2023-07-31 05:43:13,414 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:13,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41071. Reason: nanny-close
2023-07-31 05:43:13,414 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:43:13,414 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44699'. Reason: nanny-close
2023-07-31 05:43:13,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41471. Reason: nanny-close
2023-07-31 05:43:13,414 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43343', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:13,414 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43343
2023-07-31 05:43:13,414 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:13,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46539'. Reason: nanny-close
2023-07-31 05:43:13,415 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:13,415 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35413. Reason: nanny-close
2023-07-31 05:43:13,415 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:13,416 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:13,416 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34641. Reason: nanny-close
2023-07-31 05:43:13,416 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:43:13,416 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47020; closing.
2023-07-31 05:43:13,416 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:43:13,417 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43693. Reason: nanny-close
2023-07-31 05:43:13,417 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43343
2023-07-31 05:43:13,417 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35507', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:13,417 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:13,417 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:13,417 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35507
2023-07-31 05:43:13,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:43:13,418 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43343
2023-07-31 05:43:13,418 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43343
2023-07-31 05:43:13,418 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:43:13,418 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:43:13,418 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47072; closing.
2023-07-31 05:43:13,419 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:13,419 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47036; closing.
2023-07-31 05:43:13,419 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43343
2023-07-31 05:43:13,419 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32967', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:13,419 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32967
2023-07-31 05:43:13,420 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:13,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:43:13,420 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41471', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:13,420 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41471
2023-07-31 05:43:13,420 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:13,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47050; closing.
2023-07-31 05:43:13,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47028; closing.
2023-07-31 05:43:13,421 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:13,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47086; closing.
2023-07-31 05:43:13,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35413', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:13,422 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35413
2023-07-31 05:43:13,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41071', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:13,423 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41071
2023-07-31 05:43:13,423 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34641', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:13,423 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34641
2023-07-31 05:43:13,424 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47066; closing.
2023-07-31 05:43:13,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43693', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:13,425 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43693
2023-07-31 05:43:13,425 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:43:15,279 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-31 05:43:15,280 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:43:15,281 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:43:15,282 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-31 05:43:15,283 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-07-31 05:43:17,661 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:43:17,666 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46527 instead
  warnings.warn(
2023-07-31 05:43:17,670 - distributed.scheduler - INFO - State start
2023-07-31 05:43:17,731 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:43:17,732 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-31 05:43:17,733 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46527/status
2023-07-31 05:43:18,031 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36575'
2023-07-31 05:43:19,727 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:19,727 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:19,752 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:19,776 - distributed.scheduler - INFO - Receive client connection: Client-25431364-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:19,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41834
2023-07-31 05:43:21,061 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39311
2023-07-31 05:43:21,061 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39311
2023-07-31 05:43:21,061 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35447
2023-07-31 05:43:21,061 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:43:21,061 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:21,061 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:21,062 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-31 05:43:21,062 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-urism5a5
2023-07-31 05:43:21,062 - distributed.worker - INFO - Starting Worker plugin PreImport-07bac9be-f185-4054-ac8d-6a2c99870892
2023-07-31 05:43:21,062 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d2491696-2c1f-4e9b-8b3d-630500097055
2023-07-31 05:43:21,062 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f07b279-b52c-493d-b7a4-7b19908b2c98
2023-07-31 05:43:21,184 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:21,212 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39311', status: init, memory: 0, processing: 0>
2023-07-31 05:43:21,213 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39311
2023-07-31 05:43:21,213 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41848
2023-07-31 05:43:21,214 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:43:21,214 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:21,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:43:21,220 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-31 05:43:21,223 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:21,225 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:21,227 - distributed.scheduler - INFO - Remove client Client-25431364-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:21,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41834; closing.
2023-07-31 05:43:21,228 - distributed.scheduler - INFO - Remove client Client-25431364-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:21,229 - distributed.scheduler - INFO - Close client connection: Client-25431364-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:21,229 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36575'. Reason: nanny-close
2023-07-31 05:43:21,230 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:21,231 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39311. Reason: nanny-close
2023-07-31 05:43:21,232 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:43:21,232 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41848; closing.
2023-07-31 05:43:21,233 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39311', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:21,233 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39311
2023-07-31 05:43:21,233 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:43:21,234 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:22,597 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-31 05:43:22,597 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:43:22,598 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:43:22,599 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-31 05:43:22,599 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-07-31 05:43:24,963 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:43:24,968 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40469 instead
  warnings.warn(
2023-07-31 05:43:24,972 - distributed.scheduler - INFO - State start
2023-07-31 05:43:25,015 - distributed.scheduler - INFO - -----------------------------------------------
2023-07-31 05:43:25,016 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-07-31 05:43:25,017 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40469/status
2023-07-31 05:43:25,615 - distributed.scheduler - INFO - Receive client connection: Client-299751c9-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:25,628 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57976
2023-07-31 05:43:25,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40181'
2023-07-31 05:43:27,435 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:27,436 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:27,465 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-07-31 05:43:29,072 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40751
2023-07-31 05:43:29,072 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40751
2023-07-31 05:43:29,072 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45203
2023-07-31 05:43:29,072 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-07-31 05:43:29,072 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:29,072 - distributed.worker - INFO -               Threads:                          1
2023-07-31 05:43:29,073 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-07-31 05:43:29,073 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uix85jq8
2023-07-31 05:43:29,073 - distributed.worker - INFO - Starting Worker plugin PreImport-d9c25537-4fd2-403b-a203-fb28e57bb6be
2023-07-31 05:43:29,073 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-53f67a26-157e-4a6c-9e7d-9a48a37e68c4
2023-07-31 05:43:29,073 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c60045dd-b83e-4358-88b5-c896c4009c74
2023-07-31 05:43:29,199 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:29,237 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40751', status: init, memory: 0, processing: 0>
2023-07-31 05:43:29,239 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40751
2023-07-31 05:43:29,239 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58014
2023-07-31 05:43:29,241 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-07-31 05:43:29,242 - distributed.worker - INFO - -------------------------------------------------
2023-07-31 05:43:29,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-07-31 05:43:29,311 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-07-31 05:43:29,319 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-07-31 05:43:29,323 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:29,325 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-07-31 05:43:29,327 - distributed.scheduler - INFO - Remove client Client-299751c9-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:29,328 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57976; closing.
2023-07-31 05:43:29,328 - distributed.scheduler - INFO - Remove client Client-299751c9-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:29,329 - distributed.scheduler - INFO - Close client connection: Client-299751c9-2f65-11ee-8518-d8c49764f6bb
2023-07-31 05:43:29,329 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40181'. Reason: nanny-close
2023-07-31 05:43:29,330 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-07-31 05:43:29,331 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40751. Reason: nanny-close
2023-07-31 05:43:29,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-07-31 05:43:29,334 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58014; closing.
2023-07-31 05:43:29,335 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40751', status: closing, memory: 0, processing: 0>
2023-07-31 05:43:29,335 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40751
2023-07-31 05:43:29,335 - distributed.scheduler - INFO - Lost all workers
2023-07-31 05:43:29,336 - distributed.nanny - INFO - Worker closed
2023-07-31 05:43:30,647 - distributed._signals - INFO - Received signal SIGINT (2)
2023-07-31 05:43:30,647 - distributed.scheduler - INFO - Scheduler closing...
2023-07-31 05:43:30,648 - distributed.scheduler - INFO - Scheduler closing all comms
2023-07-31 05:43:30,649 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-07-31 05:43:30,649 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35815 instead
  warnings.warn(
2023-07-31 05:43:41,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:41,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:41,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:41,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:41,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:41,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:41,339 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:41,339 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:41,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:41,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:41,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:41,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:41,375 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:41,375 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:41,420 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:41,420 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34501 instead
  warnings.warn(
2023-07-31 05:43:54,254 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:54,254 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:54,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:54,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:54,316 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:54,316 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:54,326 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:54,326 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:54,351 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:54,351 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:54,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:54,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:54,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:54,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:43:54,390 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:43:54,390 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44021 instead
  warnings.warn(
2023-07-31 05:44:05,498 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:05,499 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:05,518 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:05,518 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:05,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:05,541 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:05,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:05,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:05,545 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:05,545 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:05,566 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:05,566 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:05,617 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:05,617 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:05,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:05,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42569 instead
  warnings.warn(
2023-07-31 05:44:17,220 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:17,220 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:17,230 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:17,230 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:17,231 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:17,231 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:17,234 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:17,234 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:17,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:17,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:17,268 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:17,268 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:17,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:17,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:17,286 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:17,286 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37391 instead
  warnings.warn(
2023-07-31 05:44:33,511 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:33,511 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:33,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:33,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:33,579 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:33,579 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:33,580 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:33,580 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:33,585 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:33,585 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:33,589 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:33,589 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:33,604 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:33,604 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:33,615 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:33,615 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36413 instead
  warnings.warn(
2023-07-31 05:44:47,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:47,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:47,934 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:47,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:47,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:47,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:47,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:47,964 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:47,973 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:47,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:47,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:47,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:48,000 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:48,000 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:48,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:44:48,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:44:53,431 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7fd181f65100, tag: 0x72365aea1c6c536, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1265, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7fd181f65100, tag: 0x72365aea1c6c536, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-07-31 05:44:53,441 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f73c142b0c0, tag: 0x640362bbf5445a14, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1265, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f73c142b0c0, tag: 0x640362bbf5445a14, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36925 instead
  warnings.warn(
2023-07-31 05:45:00,712 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:00,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:00,724 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:00,724 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:00,786 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:00,786 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:00,789 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:00,790 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:00,815 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:00,815 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:00,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:00,818 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:00,877 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:00,877 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:00,895 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:00,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38609 instead
  warnings.warn(
2023-07-31 05:45:12,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:12,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:12,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:12,751 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:12,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:12,753 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:12,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:12,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:12,782 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:12,782 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:12,815 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:12,815 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:12,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:12,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-07-31 05:45:12,861 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-07-31 05:45:12,861 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46817 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39713 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40533 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33553 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44273 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40079 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36125 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38965 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35973 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36091 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38993 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34333 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40425 instead
  warnings.warn(
2023-07-31 05:48:33,235 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-4eb0b3a3-cac2-4802-b655-6c10326cc43c
Function:  _run_coroutine_on_worker
args:      (260311473130268233703176184991040487779, <function shuffle_task at 0x7f496c2153a0>, ('explicit-comms-shuffle-e097386536174eabd41defafccacc725', {0: {"('from_pandas-042f56ad2fd1cc4d095a550ce07a5908', 0)"}}, {0: {0}}, ['key'], 1, False, 1, 1))
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

Process SpawnProcess-21:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 153, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 314, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 599, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 3186, in get
    results = self.gather(packed, asynchronous=asynchronous, direct=direct)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2345, in gather
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 349, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 416, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 389, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2208, in _gather
    raise exception.with_traceback(traceback)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 102, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 446, in result
    return self.__get_result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 99, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 446, in result
    return self.__get_result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 376, in shuffle_task
    partitions = create_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 240, in create_partitions
    partition_dataframe(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 192, in partition_dataframe
    df.partition_by_hash(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 4511, in partition_by_hash
    cols = [*self._index._columns, *self._columns]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/index.py", line 813, in _columns
    return self._as_int_index()._columns
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/index.py", line 549, in _as_int_index
    return _dtype_to_index[self.dtype.type]._from_data(self._data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/index.py", line 305, in _data
    {self.name: self._values}
  File "/opt/conda/envs/gdf/lib/python3.9/functools.py", line 993, in __get__
    val = self.func(instance)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/index.py", line 266, in _values
    return column.arange(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2532, in arange
    as_device_scalar(start, dtype=dtype),
  File "scalar.pyx", line 585, in cudf._lib.scalar.as_device_scalar
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/scalar.py", line 157, in device_value
    self._device_value = cudf._lib.scalar.DeviceScalar(
  File "scalar.pyx", line 100, in cudf._lib.scalar.DeviceScalar.__init__
  File "scalar.pyx", line 117, in cudf._lib.scalar.DeviceScalar._set_value
  File "scalar.pyx", line 256, in cudf._lib.scalar._set_numeric_from_np_scalar
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37089 instead
  warnings.warn(
Process SpawnProcess-22:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 132, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5026, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2202, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1985, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 377, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37695 instead
  warnings.warn(
Process SpawnProcess-23:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 132, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5026, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2202, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1985, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 377, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38545 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39009 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41631 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34777 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45813 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45699 instead
  warnings.warn(
2023-07-31 05:49:49,586 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-07-31 05:49:49,588 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-07-31 05:49:49,591 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-07-31 05:49:49,597 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f4de4dbaa60>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-07-31 05:49:49,597 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f4becb87b20>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-07-31 05:49:49,600 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f1eb27a7af0>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 30, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-07-31 05:49:50,304 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-07-31 05:49:50,311 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f6424e2ebe0>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-07-31 05:49:51,601 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-07-31 05:49:52,314 - distributed.nanny - ERROR - Worker process died unexpectedly
