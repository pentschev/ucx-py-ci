============================= test session starts ==============================
platform linux -- Python 3.9.19, pytest-8.1.1, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.6
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-03-28 05:55:03,674 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:03,679 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45515 instead
  warnings.warn(
2024-03-28 05:55:03,684 - distributed.scheduler - INFO - State start
2024-03-28 05:55:03,708 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:03,709 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-28 05:55:03,709 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45515/status
2024-03-28 05:55:03,710 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-28 05:55:03,867 - distributed.scheduler - INFO - Receive client connection: Client-b771e811-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:03,881 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46754
2024-03-28 05:55:03,987 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41527'
2024-03-28 05:55:04,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44067'
2024-03-28 05:55:04,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37537'
2024-03-28 05:55:04,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44625'
2024-03-28 05:55:06,014 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:06,014 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:06,019 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:06,020 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36581
2024-03-28 05:55:06,020 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36581
2024-03-28 05:55:06,020 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38593
2024-03-28 05:55:06,020 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-28 05:55:06,020 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,020 - distributed.worker - INFO -               Threads:                          4
2024-03-28 05:55:06,020 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-28 05:55:06,020 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-me6u89n9
2024-03-28 05:55:06,020 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ede974e7-3733-42f7-ac2c-9b98f6b3ee2a
2024-03-28 05:55:06,021 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d7e2e7e-c528-41ce-956b-8ceeb28311d8
2024-03-28 05:55:06,021 - distributed.worker - INFO - Starting Worker plugin PreImport-3d99af7d-8701-4211-a8d4-e5a6aadd8545
2024-03-28 05:55:06,021 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,044 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:06,044 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:06,051 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:06,053 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41907
2024-03-28 05:55:06,053 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41907
2024-03-28 05:55:06,053 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41399
2024-03-28 05:55:06,053 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-28 05:55:06,053 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,054 - distributed.worker - INFO -               Threads:                          4
2024-03-28 05:55:06,054 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-28 05:55:06,054 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:06,054 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-_6x9_nci
2024-03-28 05:55:06,054 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:06,054 - distributed.worker - INFO - Starting Worker plugin PreImport-8c6b9b10-5cdd-477d-ac45-2c9177b993ba
2024-03-28 05:55:06,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d42c3117-14b1-440d-975e-6035ff3252b9
2024-03-28 05:55:06,054 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b95835ca-51b9-44d2-8979-7afe34d56eec
2024-03-28 05:55:06,055 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,058 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:06,059 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40143
2024-03-28 05:55:06,060 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40143
2024-03-28 05:55:06,060 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41321
2024-03-28 05:55:06,060 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-28 05:55:06,060 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,060 - distributed.worker - INFO -               Threads:                          4
2024-03-28 05:55:06,060 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-28 05:55:06,060 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-a6oz9005
2024-03-28 05:55:06,060 - distributed.worker - INFO - Starting Worker plugin PreImport-0bd09d60-6cbc-4195-a4af-cf87117d216a
2024-03-28 05:55:06,060 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7db5e2b-9319-467e-b4b6-500663ac9d98
2024-03-28 05:55:06,060 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ae76d60-95f1-4fc0-ba48-948b63b266b3
2024-03-28 05:55:06,061 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,081 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:06,081 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:06,087 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:06,088 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39815
2024-03-28 05:55:06,088 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39815
2024-03-28 05:55:06,088 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36431
2024-03-28 05:55:06,088 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-28 05:55:06,088 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,088 - distributed.worker - INFO -               Threads:                          4
2024-03-28 05:55:06,088 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-28 05:55:06,088 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-sk3j8lli
2024-03-28 05:55:06,089 - distributed.worker - INFO - Starting Worker plugin PreImport-22ebabb4-eb5d-41cd-82c3-57d816a6aab0
2024-03-28 05:55:06,089 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90cc0864-8f30-4947-b2f1-b4ca3afa5103
2024-03-28 05:55:06,091 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8f99423-ed80-497d-87a6-47e7f80cf501
2024-03-28 05:55:06,092 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,634 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39815', status: init, memory: 0, processing: 0>
2024-03-28 05:55:06,635 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39815
2024-03-28 05:55:06,635 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46840
2024-03-28 05:55:06,636 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41907', status: init, memory: 0, processing: 0>
2024-03-28 05:55:06,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:06,637 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41907
2024-03-28 05:55:06,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46812
2024-03-28 05:55:06,637 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-28 05:55:06,637 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,638 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:06,639 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-28 05:55:06,639 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,639 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-28 05:55:06,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-28 05:55:06,644 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40143', status: init, memory: 0, processing: 0>
2024-03-28 05:55:06,645 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40143
2024-03-28 05:55:06,645 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46826
2024-03-28 05:55:06,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:06,648 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36581', status: init, memory: 0, processing: 0>
2024-03-28 05:55:06,648 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-28 05:55:06,648 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,648 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36581
2024-03-28 05:55:06,648 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46798
2024-03-28 05:55:06,650 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:06,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-28 05:55:06,651 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-28 05:55:06,651 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:06,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-28 05:55:06,710 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-28 05:55:06,710 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-28 05:55:06,711 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-28 05:55:06,711 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-28 05:55:06,717 - distributed.scheduler - INFO - Remove client Client-b771e811-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:06,718 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46754; closing.
2024-03-28 05:55:06,718 - distributed.scheduler - INFO - Remove client Client-b771e811-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:06,718 - distributed.scheduler - INFO - Close client connection: Client-b771e811-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:06,719 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41527'. Reason: nanny-close
2024-03-28 05:55:06,719 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:06,720 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44067'. Reason: nanny-close
2024-03-28 05:55:06,720 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:06,721 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37537'. Reason: nanny-close
2024-03-28 05:55:06,721 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36581. Reason: nanny-close
2024-03-28 05:55:06,721 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:06,721 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44625'. Reason: nanny-close
2024-03-28 05:55:06,721 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40143. Reason: nanny-close
2024-03-28 05:55:06,721 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:06,722 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41907. Reason: nanny-close
2024-03-28 05:55:06,722 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39815. Reason: nanny-close
2024-03-28 05:55:06,723 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46798; closing.
2024-03-28 05:55:06,723 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-28 05:55:06,724 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605306.724092')
2024-03-28 05:55:06,724 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-28 05:55:06,724 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-28 05:55:06,725 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-28 05:55:06,725 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46840; closing.
2024-03-28 05:55:06,725 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:06,725 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46812; closing.
2024-03-28 05:55:06,725 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46826; closing.
2024-03-28 05:55:06,725 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:06,726 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39815', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605306.7261686')
2024-03-28 05:55:06,726 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:06,726 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41907', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605306.726519')
2024-03-28 05:55:06,726 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:06,726 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40143', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605306.7268646')
2024-03-28 05:55:06,727 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:55:07,435 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-28 05:55:07,435 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-28 05:55:07,435 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:55:07,436 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-28 05:55:07,437 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-03-28 05:55:09,893 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:09,898 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37211 instead
  warnings.warn(
2024-03-28 05:55:09,903 - distributed.scheduler - INFO - State start
2024-03-28 05:55:09,926 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:09,927 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-28 05:55:09,928 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37211/status
2024-03-28 05:55:09,928 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-28 05:55:10,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40765'
2024-03-28 05:55:10,230 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40609'
2024-03-28 05:55:10,244 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37419'
2024-03-28 05:55:10,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45115'
2024-03-28 05:55:10,254 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42199'
2024-03-28 05:55:10,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44629'
2024-03-28 05:55:10,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36719'
2024-03-28 05:55:10,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36245'
2024-03-28 05:55:10,384 - distributed.scheduler - INFO - Receive client connection: Client-bb585a1a-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:10,396 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40894
2024-03-28 05:55:10,665 - distributed.scheduler - INFO - Receive client connection: Client-bb35f390-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:10,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40978
2024-03-28 05:55:12,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:12,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:12,551 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:12,552 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46503
2024-03-28 05:55:12,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46503
2024-03-28 05:55:12,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35661
2024-03-28 05:55:12,552 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:12,552 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:12,552 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:12,552 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:12,552 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-myir_7bo
2024-03-28 05:55:12,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aef5b32b-3c4e-44a7-b6c9-e3b0df01fae2
2024-03-28 05:55:12,553 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d38789eb-0a12-4e98-8062-3090de98c1d3
2024-03-28 05:55:12,554 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:12,554 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:12,560 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:12,561 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41503
2024-03-28 05:55:12,561 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41503
2024-03-28 05:55:12,561 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41225
2024-03-28 05:55:12,561 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:12,561 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:12,561 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:12,561 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:12,561 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u8ex91ya
2024-03-28 05:55:12,561 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e425c0fe-e777-4161-b4d7-00f132fb4784
2024-03-28 05:55:12,619 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:12,619 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:12,625 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:12,626 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34679
2024-03-28 05:55:12,626 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34679
2024-03-28 05:55:12,626 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44149
2024-03-28 05:55:12,626 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:12,626 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:12,626 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:12,626 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:12,626 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6ny5qs2d
2024-03-28 05:55:12,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fcfde502-6861-4a65-9b32-226153de0aab
2024-03-28 05:55:12,627 - distributed.worker - INFO - Starting Worker plugin PreImport-7326a18d-b341-4540-a566-82b0f94e194f
2024-03-28 05:55:12,627 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f35a8df-f07e-4177-a37e-a49d0408dbc4
2024-03-28 05:55:12,634 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:12,634 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:12,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:12,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:12,641 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:12,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:12,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:12,642 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37373
2024-03-28 05:55:12,642 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37373
2024-03-28 05:55:12,642 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45509
2024-03-28 05:55:12,643 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:12,643 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:12,643 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:12,643 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:12,643 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qyficqx9
2024-03-28 05:55:12,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb4ecca5-8a14-4e79-9da3-2fca10a75dae
2024-03-28 05:55:12,643 - distributed.worker - INFO - Starting Worker plugin PreImport-2121f539-e5ce-470c-a3a4-1b83a5aa8d1c
2024-03-28 05:55:12,643 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62f3ed4c-5ae5-4150-84bd-5848c93c2eca
2024-03-28 05:55:12,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:12,644 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:12,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:12,644 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:12,644 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:12,645 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38167
2024-03-28 05:55:12,645 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38167
2024-03-28 05:55:12,645 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41507
2024-03-28 05:55:12,645 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:12,645 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:12,645 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:12,645 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:12,645 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-98lkgqys
2024-03-28 05:55:12,646 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e0ce5cb8-2f1a-486a-8fde-45ce3f82eed6
2024-03-28 05:55:12,647 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:12,648 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43711
2024-03-28 05:55:12,648 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43711
2024-03-28 05:55:12,648 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43085
2024-03-28 05:55:12,648 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:12,649 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:12,649 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:12,649 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:12,649 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y2pilj8d
2024-03-28 05:55:12,649 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:12,649 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:12,649 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6cd03608-2ddb-4afc-add4-da71c456dbcc
2024-03-28 05:55:12,650 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97d72c1c-afa3-4497-9401-884d77cc44cb
2024-03-28 05:55:12,650 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35915
2024-03-28 05:55:12,650 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35915
2024-03-28 05:55:12,650 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38763
2024-03-28 05:55:12,650 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40229
2024-03-28 05:55:12,650 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38763
2024-03-28 05:55:12,650 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:12,650 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:12,650 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34419
2024-03-28 05:55:12,650 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:12,650 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:12,650 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:12,650 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:12,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z8m80ejd
2024-03-28 05:55:12,650 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:12,650 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:12,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6vc7gzjh
2024-03-28 05:55:12,651 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64f9c25a-be74-4d84-9e8d-38ca1d83b628
2024-03-28 05:55:12,651 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f658b4b-179c-4087-b716-70f0bbc9a758
2024-03-28 05:55:12,651 - distributed.worker - INFO - Starting Worker plugin PreImport-bc33bb2f-4ec9-4df4-8045-67a59c6c3c34
2024-03-28 05:55:12,652 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11a6f5ea-e8d2-407a-aa59-1625402b7ce1
2024-03-28 05:55:17,509 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45337', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,510 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45337
2024-03-28 05:55:17,510 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40986
2024-03-28 05:55:17,545 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39171', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,546 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39171
2024-03-28 05:55:17,546 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41000
2024-03-28 05:55:17,605 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,638 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34679', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,638 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34679
2024-03-28 05:55:17,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41006
2024-03-28 05:55:17,640 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:17,641 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:17,641 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,644 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:17,646 - distributed.worker - INFO - Starting Worker plugin PreImport-220828d6-7aa5-48ff-a868-10aaec3981a1
2024-03-28 05:55:17,648 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,648 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,655 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7505ceba-3d84-4193-a1d8-b3a60a36900c
2024-03-28 05:55:17,656 - distributed.worker - INFO - Starting Worker plugin PreImport-ad91b946-8a1e-448b-86c5-5cfa93a9227e
2024-03-28 05:55:17,657 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,666 - distributed.worker - INFO - Starting Worker plugin PreImport-c0ad2c05-d6d8-4db8-abb6-a3ecd5c41e09
2024-03-28 05:55:17,666 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-780e43fe-01f5-4900-b837-c844f4c73a7b
2024-03-28 05:55:17,668 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,679 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46503', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,680 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46503
2024-03-28 05:55:17,680 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41022
2024-03-28 05:55:17,681 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38167', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,681 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:17,682 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38167
2024-03-28 05:55:17,682 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41038
2024-03-28 05:55:17,682 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:17,682 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,683 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:17,683 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35915', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,684 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:17,684 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,684 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35915
2024-03-28 05:55:17,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41028
2024-03-28 05:55:17,685 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:17,685 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:17,686 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:17,687 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:17,687 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:17,703 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41503', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,704 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41503
2024-03-28 05:55:17,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41050
2024-03-28 05:55:17,705 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:17,707 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:17,707 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:17,765 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40385', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,766 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40385
2024-03-28 05:55:17,766 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41052
2024-03-28 05:55:17,771 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45649', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,772 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45649
2024-03-28 05:55:17,772 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41068
2024-03-28 05:55:17,852 - distributed.worker - INFO - Starting Worker plugin PreImport-0a687dd9-63b6-4ed5-9999-2943f668bc0e
2024-03-28 05:55:17,854 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,860 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e490d3b7-0174-4622-ac4e-db6cdea4b6ca
2024-03-28 05:55:17,860 - distributed.worker - INFO - Starting Worker plugin PreImport-ab406c58-9563-4a41-83e3-0104de64ef32
2024-03-28 05:55:17,861 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,884 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38763', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,885 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38763
2024-03-28 05:55:17,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41110
2024-03-28 05:55:17,886 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44569', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,886 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:17,886 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44569
2024-03-28 05:55:17,886 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41080
2024-03-28 05:55:17,887 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:17,887 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,889 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:17,890 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43711', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,890 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43711
2024-03-28 05:55:17,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41096
2024-03-28 05:55:17,892 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:17,893 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:17,893 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:17,898 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42605', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,898 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42605
2024-03-28 05:55:17,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41114
2024-03-28 05:55:17,902 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37091', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,903 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37091
2024-03-28 05:55:17,903 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41116
2024-03-28 05:55:17,903 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,922 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45239', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,923 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45239
2024-03-28 05:55:17,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41122
2024-03-28 05:55:17,928 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37373', status: init, memory: 0, processing: 0>
2024-03-28 05:55:17,928 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37373
2024-03-28 05:55:17,928 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41128
2024-03-28 05:55:17,930 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:17,930 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:17,930 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:17,932 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:26,412 - distributed.scheduler - INFO - Remove client Client-bb585a1a-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:26,412 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40894; closing.
2024-03-28 05:55:26,413 - distributed.scheduler - INFO - Remove client Client-bb585a1a-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:26,413 - distributed.scheduler - INFO - Close client connection: Client-bb585a1a-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:26,420 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41068; closing.
2024-03-28 05:55:26,420 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41114; closing.
2024-03-28 05:55:26,421 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45649', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.4213042')
2024-03-28 05:55:26,422 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41000; closing.
2024-03-28 05:55:26,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42605', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.4227479')
2024-03-28 05:55:26,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40986; closing.
2024-03-28 05:55:26,425 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39171', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.4249864')
2024-03-28 05:55:26,425 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45337', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.4256585')
2024-03-28 05:55:26,429 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41052; closing.
2024-03-28 05:55:26,429 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41122; closing.
2024-03-28 05:55:26,431 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41000>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-28 05:55:26,433 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40986>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-28 05:55:26,434 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40385', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.4345915')
2024-03-28 05:55:26,435 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45239', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.4353812')
2024-03-28 05:55:26,436 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41116; closing.
2024-03-28 05:55:26,436 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41080; closing.
2024-03-28 05:55:26,437 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37091', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.437009')
2024-03-28 05:55:26,437 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44569', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.4375727')
2024-03-28 05:55:26,527 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:26,527 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:26,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:26,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:26,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:26,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:26,529 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:26,529 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:26,536 - distributed.scheduler - INFO - Remove client Client-bb35f390-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:26,536 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40978; closing.
2024-03-28 05:55:26,537 - distributed.scheduler - INFO - Remove client Client-bb35f390-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:26,537 - distributed.scheduler - INFO - Close client connection: Client-bb35f390-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:26,538 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40765'. Reason: nanny-close
2024-03-28 05:55:26,538 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:26,539 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40609'. Reason: nanny-close
2024-03-28 05:55:26,539 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:26,539 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37419'. Reason: nanny-close
2024-03-28 05:55:26,540 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:26,540 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45115'. Reason: nanny-close
2024-03-28 05:55:26,540 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41503. Reason: nanny-close
2024-03-28 05:55:26,540 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:26,540 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37373. Reason: nanny-close
2024-03-28 05:55:26,540 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42199'. Reason: nanny-close
2024-03-28 05:55:26,541 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:26,541 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38167. Reason: nanny-close
2024-03-28 05:55:26,541 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44629'. Reason: nanny-close
2024-03-28 05:55:26,541 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:26,541 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38763. Reason: nanny-close
2024-03-28 05:55:26,541 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36719'. Reason: nanny-close
2024-03-28 05:55:26,542 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:26,542 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36245'. Reason: nanny-close
2024-03-28 05:55:26,542 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:26,542 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46503. Reason: nanny-close
2024-03-28 05:55:26,542 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35915. Reason: nanny-close
2024-03-28 05:55:26,543 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:26,543 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43711. Reason: nanny-close
2024-03-28 05:55:26,543 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:26,543 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34679. Reason: nanny-close
2024-03-28 05:55:26,543 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:26,544 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:26,544 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41038; closing.
2024-03-28 05:55:26,544 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41110; closing.
2024-03-28 05:55:26,545 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41128; closing.
2024-03-28 05:55:26,545 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:26,545 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:26,545 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41050; closing.
2024-03-28 05:55:26,545 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:26,545 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:26,545 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:26,546 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:26,546 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38167', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.5462172')
2024-03-28 05:55:26,546 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:26,546 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:26,546 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38763', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.5468419')
2024-03-28 05:55:26,547 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:26,547 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37373', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.5474267')
2024-03-28 05:55:26,548 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41503', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.5479956')
2024-03-28 05:55:26,548 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:26,548 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:26,548 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:26,550 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41022; closing.
2024-03-28 05:55:26,550 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41028; closing.
2024-03-28 05:55:26,552 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46503', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.552328')
2024-03-28 05:55:26,552 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35915', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.5528748')
2024-03-28 05:55:26,553 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41096; closing.
2024-03-28 05:55:26,553 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41006; closing.
2024-03-28 05:55:26,554 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43711', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.5544899')
2024-03-28 05:55:26,555 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34679', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605326.5550275')
2024-03-28 05:55:26,555 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:55:27,604 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-28 05:55:27,605 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-28 05:55:27,605 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:55:27,607 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-28 05:55:27,607 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-03-28 05:55:29,873 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:29,878 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41295 instead
  warnings.warn(
2024-03-28 05:55:29,881 - distributed.scheduler - INFO - State start
2024-03-28 05:55:29,913 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:29,914 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-28 05:55:29,915 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:55:29,916 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4051, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 629, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-28 05:55:30,016 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39575'
2024-03-28 05:55:30,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44459'
2024-03-28 05:55:30,043 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42995'
2024-03-28 05:55:30,045 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32881'
2024-03-28 05:55:30,054 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38131'
2024-03-28 05:55:30,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45189'
2024-03-28 05:55:30,071 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35069'
2024-03-28 05:55:30,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39577'
2024-03-28 05:55:32,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:32,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:32,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:32,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:32,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:32,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:32,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:32,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:32,190 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:32,190 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:32,190 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:32,191 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40173
2024-03-28 05:55:32,191 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40173
2024-03-28 05:55:32,191 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44057
2024-03-28 05:55:32,191 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:32,191 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:32,191 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:32,191 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:32,191 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35229
2024-03-28 05:55:32,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-36lhm30t
2024-03-28 05:55:32,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:32,191 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35229
2024-03-28 05:55:32,191 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45709
2024-03-28 05:55:32,191 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43077
2024-03-28 05:55:32,192 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45709
2024-03-28 05:55:32,192 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:32,192 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41921
2024-03-28 05:55:32,192 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:32,192 - distributed.worker - INFO - Starting Worker plugin PreImport-ef8e6114-a7b9-4d57-b665-90fab74506a1
2024-03-28 05:55:32,192 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:32,192 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:32,192 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:32,192 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:32,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d70376a-3cdd-48a0-ae28-3d45c1e4f1fa
2024-03-28 05:55:32,192 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:32,192 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dd24z9a0
2024-03-28 05:55:32,192 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:32,192 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d8lrpcmm
2024-03-28 05:55:32,192 - distributed.worker - INFO - Starting Worker plugin PreImport-42cc8e52-58c2-4225-8b74-f520b9d9070c
2024-03-28 05:55:32,192 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9b1c7a1-c0a8-447d-9b30-87b99071356a
2024-03-28 05:55:32,192 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a092f0f-06fa-4c9c-95dd-033eb3a6cbd7
2024-03-28 05:55:32,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a62675d-b316-454f-8d88-732a07a5193f
2024-03-28 05:55:32,192 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40627
2024-03-28 05:55:32,192 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40627
2024-03-28 05:55:32,192 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42231
2024-03-28 05:55:32,192 - distributed.worker - INFO - Starting Worker plugin PreImport-2f1026ee-cf7e-480e-8907-ed28fc8c888a
2024-03-28 05:55:32,193 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:32,193 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:32,193 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:32,193 - distributed.worker - INFO - Starting Worker plugin RMMSetup-78e164cc-40e5-4f54-94fa-1dc72642355a
2024-03-28 05:55:32,193 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:32,193 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-luu7u05u
2024-03-28 05:55:32,193 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbab6d95-45e3-4f6f-a707-273a17b5167d
2024-03-28 05:55:32,193 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:32,193 - distributed.worker - INFO - Starting Worker plugin PreImport-9c47c325-be64-4adb-9e08-24fd049ab1b6
2024-03-28 05:55:32,193 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:32,193 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dda9657c-98e8-4bf7-ae52-03db5b6547d3
2024-03-28 05:55:32,198 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:32,199 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39921
2024-03-28 05:55:32,199 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39921
2024-03-28 05:55:32,199 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37911
2024-03-28 05:55:32,199 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:32,200 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:32,200 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:32,200 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:32,200 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g84b3jr3
2024-03-28 05:55:32,200 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7244eef9-a98a-4742-a853-f4d0f574e36e
2024-03-28 05:55:32,200 - distributed.worker - INFO - Starting Worker plugin PreImport-1c11c270-f915-4ee0-bf6b-684c51ceb998
2024-03-28 05:55:32,200 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4dca0107-611d-4e8a-a1fd-61cf4f80fbcc
2024-03-28 05:55:32,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:32,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:32,298 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:32,299 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46741
2024-03-28 05:55:32,299 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46741
2024-03-28 05:55:32,299 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43713
2024-03-28 05:55:32,299 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:32,300 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:32,300 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:32,300 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:32,300 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jdhraoda
2024-03-28 05:55:32,300 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae6f8d5a-7ebc-48ae-b258-87fe0fd3944c
2024-03-28 05:55:32,300 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad5e5bb3-e8e1-4a93-81fb-27c96757f720
2024-03-28 05:55:32,301 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:32,301 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:32,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:32,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:32,306 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:32,307 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34951
2024-03-28 05:55:32,307 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:32,307 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34951
2024-03-28 05:55:32,307 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37921
2024-03-28 05:55:32,307 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:32,307 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:32,307 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:32,308 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:32,308 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5eup14pf
2024-03-28 05:55:32,308 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dabda18f-d631-46a9-ae30-0658473644c5
2024-03-28 05:55:32,308 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38231
2024-03-28 05:55:32,308 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38231
2024-03-28 05:55:32,308 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37303
2024-03-28 05:55:32,308 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:32,308 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:32,308 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:32,308 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:32,308 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e2vh7rio
2024-03-28 05:55:32,309 - distributed.worker - INFO - Starting Worker plugin PreImport-69d4d850-e54a-41c3-b9c3-fc64fd0863c4
2024-03-28 05:55:32,309 - distributed.worker - INFO - Starting Worker plugin PreImport-da679c0a-a7b1-40ad-af83-d099e5fd1132
2024-03-28 05:55:32,309 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab666667-1135-49e6-8130-cec00a0629f4
2024-03-28 05:55:32,309 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19579c59-a284-4f6a-aaa9-244a5d33e3a9
2024-03-28 05:55:32,311 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5b0d7966-8ab7-4c1d-941c-8d69b2ca9537
2024-03-28 05:55:35,835 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:35,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:35,862 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:35,862 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:35,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:35,943 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:35,975 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:35,977 - distributed.worker - INFO - Starting Worker plugin PreImport-c6c227ba-6c3e-43f9-9bd6-60dbdc4233bc
2024-03-28 05:55:35,978 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:35,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:35,982 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:35,982 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:35,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:36,001 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:36,002 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:36,005 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:36,007 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:36,007 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:36,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:36,011 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:36,012 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:36,012 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:36,014 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:36,029 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-381b6843-1e07-4ed6-8b6d-cae033560205
2024-03-28 05:55:36,030 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:36,036 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:36,038 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:36,038 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:36,040 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:36,040 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:36,041 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:36,041 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:36,043 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:36,055 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:36,059 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:36,060 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:36,060 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:36,061 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:36,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:36,082 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:36,082 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:36,083 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:36,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,146 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,146 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,146 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,146 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,146 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,146 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,146 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,146 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:36,153 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39575'. Reason: nanny-close
2024-03-28 05:55:36,153 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:36,153 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44459'. Reason: nanny-close
2024-03-28 05:55:36,154 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:36,154 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42995'. Reason: nanny-close
2024-03-28 05:55:36,154 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40173. Reason: nanny-close
2024-03-28 05:55:36,154 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:36,154 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32881'. Reason: nanny-close
2024-03-28 05:55:36,155 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35229. Reason: nanny-close
2024-03-28 05:55:36,155 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:36,155 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38131'. Reason: nanny-close
2024-03-28 05:55:36,155 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39921. Reason: nanny-close
2024-03-28 05:55:36,155 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:36,155 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45189'. Reason: nanny-close
2024-03-28 05:55:36,156 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:36,156 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45709. Reason: nanny-close
2024-03-28 05:55:36,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35069'. Reason: nanny-close
2024-03-28 05:55:36,156 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46741. Reason: nanny-close
2024-03-28 05:55:36,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:36,156 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:36,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39577'. Reason: nanny-close
2024-03-28 05:55:36,156 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40627. Reason: nanny-close
2024-03-28 05:55:36,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:36,156 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:36,157 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38231. Reason: nanny-close
2024-03-28 05:55:36,157 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34951. Reason: nanny-close
2024-03-28 05:55:36,157 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:36,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:36,158 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:36,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:36,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:36,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:36,159 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:36,159 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:36,159 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:36,160 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:36,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:36,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:36,161 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:36,162 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-03-28 05:55:39,373 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:39,379 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43763 instead
  warnings.warn(
2024-03-28 05:55:39,384 - distributed.scheduler - INFO - State start
2024-03-28 05:55:39,407 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:39,408 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-28 05:55:39,410 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43763/status
2024-03-28 05:55:39,410 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-28 05:55:39,480 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42411'
2024-03-28 05:55:39,494 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35925'
2024-03-28 05:55:39,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44709'
2024-03-28 05:55:39,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44919'
2024-03-28 05:55:39,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43815'
2024-03-28 05:55:39,529 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44849'
2024-03-28 05:55:39,538 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34737'
2024-03-28 05:55:39,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33259'
2024-03-28 05:55:39,918 - distributed.scheduler - INFO - Receive client connection: Client-cd2f10a4-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:39,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37924
2024-03-28 05:55:40,159 - distributed.scheduler - INFO - Receive client connection: Client-ccc0a119-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:40,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37932
2024-03-28 05:55:41,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:41,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:41,535 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:41,536 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40867
2024-03-28 05:55:41,536 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40867
2024-03-28 05:55:41,536 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46453
2024-03-28 05:55:41,536 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:41,536 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:41,536 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:41,536 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:41,536 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bkd5ov4w
2024-03-28 05:55:41,536 - distributed.worker - INFO - Starting Worker plugin PreImport-493e4d90-7008-4cf3-9484-9e13961da1f1
2024-03-28 05:55:41,536 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de82f7be-706c-496b-a8ed-ec4052cc302b
2024-03-28 05:55:41,537 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2433017e-3a81-430f-ad1e-52f435ac7744
2024-03-28 05:55:41,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:41,764 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:41,771 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:41,772 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45475
2024-03-28 05:55:41,772 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45475
2024-03-28 05:55:41,772 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37729
2024-03-28 05:55:41,773 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:41,773 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:41,773 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:41,773 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:41,773 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kew0rr5f
2024-03-28 05:55:41,773 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a07addbe-f04e-4c5e-a7e6-45f33d3ca673
2024-03-28 05:55:41,773 - distributed.worker - INFO - Starting Worker plugin PreImport-ab79152f-2745-49a0-b80f-777ff2d33278
2024-03-28 05:55:41,773 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad7b034a-c6bc-476c-9cee-08f1d413da6c
2024-03-28 05:55:41,774 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:41,775 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:41,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:41,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:41,777 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:41,777 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:41,777 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:41,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:41,778 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:41,778 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:41,780 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:41,780 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:41,780 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:41,781 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45993
2024-03-28 05:55:41,781 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45993
2024-03-28 05:55:41,781 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44619
2024-03-28 05:55:41,781 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:41,781 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:41,781 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:41,781 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:41,781 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-67nh6u1n
2024-03-28 05:55:41,782 - distributed.worker - INFO - Starting Worker plugin PreImport-1bac12d5-880a-4ada-a031-269c226639f2
2024-03-28 05:55:41,782 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:41,782 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-05ad5d95-1621-40d9-a1b0-97007efa31dc
2024-03-28 05:55:41,782 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:41,783 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34787
2024-03-28 05:55:41,783 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34787
2024-03-28 05:55:41,783 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34999
2024-03-28 05:55:41,783 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:41,783 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:41,783 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:41,783 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:41,783 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:41,783 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q8luvzu8
2024-03-28 05:55:41,783 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:41,783 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dab6d1a9-87c4-4311-850d-e58a99cb622e
2024-03-28 05:55:41,783 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43187
2024-03-28 05:55:41,783 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4069b9c6-fc16-4ec5-b1b0-a35b0c1c881d
2024-03-28 05:55:41,784 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43187
2024-03-28 05:55:41,784 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32949
2024-03-28 05:55:41,784 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:41,784 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37977
2024-03-28 05:55:41,784 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:41,784 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37977
2024-03-28 05:55:41,784 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:41,784 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38855
2024-03-28 05:55:41,784 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:41,784 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:41,784 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:41,784 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nagr0kad
2024-03-28 05:55:41,784 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:41,784 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39905
2024-03-28 05:55:41,784 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:41,784 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3gdkdvm0
2024-03-28 05:55:41,784 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39905
2024-03-28 05:55:41,784 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38623
2024-03-28 05:55:41,784 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:41,784 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:41,784 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c6e7e85-251b-4ff3-afd0-802afa3cfb54
2024-03-28 05:55:41,784 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:41,784 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d63ed8c6-cc1b-4adf-8770-1eb3dcee3b22
2024-03-28 05:55:41,784 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:41,784 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7jg59ztu
2024-03-28 05:55:41,784 - distributed.worker - INFO - Starting Worker plugin RMMSetup-551e37c6-c7bc-405b-9d1f-48f0e9acd54c
2024-03-28 05:55:41,785 - distributed.worker - INFO - Starting Worker plugin PreImport-945f9c32-1749-4d40-b7dc-e58f1af72b22
2024-03-28 05:55:41,785 - distributed.worker - INFO - Starting Worker plugin RMMSetup-576df005-13ea-42e7-bc8b-5ab2aee08293
2024-03-28 05:55:41,785 - distributed.worker - INFO - Starting Worker plugin PreImport-22c9fa04-856b-428b-90e7-82cc11159ee3
2024-03-28 05:55:41,785 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-caa479f8-0cde-46ee-b879-eadd5e6372e2
2024-03-28 05:55:41,785 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:41,786 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43649
2024-03-28 05:55:41,786 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43649
2024-03-28 05:55:41,786 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44397
2024-03-28 05:55:41,786 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:41,786 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:41,786 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:41,786 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:41,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lz3_yasy
2024-03-28 05:55:41,786 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad9735cd-e5f8-4f50-ba55-d86248b9f60d
2024-03-28 05:55:41,787 - distributed.worker - INFO - Starting Worker plugin PreImport-a9f71ee8-383e-4125-b9fa-a1b9909b28ff
2024-03-28 05:55:41,787 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fb860ac-fb6d-477d-846b-9961c7a01236
2024-03-28 05:55:41,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38758462-3804-4df0-b483-89c5e05ce9eb
2024-03-28 05:55:42,019 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:42,045 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40867', status: init, memory: 0, processing: 0>
2024-03-28 05:55:42,046 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40867
2024-03-28 05:55:42,046 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38010
2024-03-28 05:55:42,047 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:42,048 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:42,048 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:42,049 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:43,869 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:43,896 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45475', status: init, memory: 0, processing: 0>
2024-03-28 05:55:43,896 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45475
2024-03-28 05:55:43,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38028
2024-03-28 05:55:43,897 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:43,898 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:43,898 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:43,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:44,007 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,031 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37977', status: init, memory: 0, processing: 0>
2024-03-28 05:55:44,031 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37977
2024-03-28 05:55:44,032 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38036
2024-03-28 05:55:44,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:44,033 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:44,033 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:44,152 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,152 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,153 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,156 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd726089-9e50-463b-8be1-5aeed7d25fb6
2024-03-28 05:55:44,156 - distributed.worker - INFO - Starting Worker plugin PreImport-212fd741-c5a1-404f-b44f-4476275c43ef
2024-03-28 05:55:44,157 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,165 - distributed.worker - INFO - Starting Worker plugin PreImport-b7314f59-0d83-4c9e-9b80-091d9f86f8c7
2024-03-28 05:55:44,166 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,179 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43649', status: init, memory: 0, processing: 0>
2024-03-28 05:55:44,179 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43649
2024-03-28 05:55:44,179 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38062
2024-03-28 05:55:44,180 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:44,181 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:44,181 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,182 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34787', status: init, memory: 0, processing: 0>
2024-03-28 05:55:44,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:44,183 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34787
2024-03-28 05:55:44,183 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38058
2024-03-28 05:55:44,184 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39905', status: init, memory: 0, processing: 0>
2024-03-28 05:55:44,184 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:44,184 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39905
2024-03-28 05:55:44,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38042
2024-03-28 05:55:44,185 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:44,185 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45993', status: init, memory: 0, processing: 0>
2024-03-28 05:55:44,185 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,186 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45993
2024-03-28 05:55:44,185 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:44,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38050
2024-03-28 05:55:44,186 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:44,186 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,187 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:44,187 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:44,187 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43187', status: init, memory: 0, processing: 0>
2024-03-28 05:55:44,187 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:44,188 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:44,188 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43187
2024-03-28 05:55:44,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38078
2024-03-28 05:55:44,189 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:44,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:44,190 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:44,190 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:44,191 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:44,220 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:44,220 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:44,220 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:44,221 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:44,221 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:44,221 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:44,221 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:44,221 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:44,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:44,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:44,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:44,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:44,227 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:44,227 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:44,227 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:44,227 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:44,226 - distributed.worker - WARNING - Run Failed
Function: lambda
args:     ()
kwargs:   {}
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 3182, in run
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dask_cuda_worker.py", line 189, in <lambda>
    type(rmm.mr.get_current_device_resource().get_upstream()),
AttributeError: 'rmm._lib.memory_resource.CudaAsyncMemoryResource' object has no attribute 'get_upstream'
2024-03-28 05:55:44,226 - distributed.worker - WARNING - Run Failed
Function: lambda
args:     ()
kwargs:   {}
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 3182, in run
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dask_cuda_worker.py", line 189, in <lambda>
    type(rmm.mr.get_current_device_resource().get_upstream()),
AttributeError: 'rmm._lib.memory_resource.CudaAsyncMemoryResource' object has no attribute 'get_upstream'
2024-03-28 05:55:44,227 - distributed.worker - WARNING - Run Failed
Function: lambda
args:     ()
kwargs:   {}
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 3182, in run
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dask_cuda_worker.py", line 189, in <lambda>
    type(rmm.mr.get_current_device_resource().get_upstream()),
AttributeError: 'rmm._lib.memory_resource.CudaAsyncMemoryResource' object has no attribute 'get_upstream'
2024-03-28 05:55:44,227 - distributed.worker - WARNING - Run Failed
Function: lambda
args:     ()
kwargs:   {}
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 3182, in run
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dask_cuda_worker.py", line 189, in <lambda>
    type(rmm.mr.get_current_device_resource().get_upstream()),
AttributeError: 'rmm._lib.memory_resource.CudaAsyncMemoryResource' object has no attribute 'get_upstream'
2024-03-28 05:55:44,227 - distributed.worker - WARNING - Run Failed
Function: lambda
args:     ()
kwargs:   {}
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 3182, in run
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dask_cuda_worker.py", line 189, in <lambda>
    type(rmm.mr.get_current_device_resource().get_upstream()),
AttributeError: 'rmm._lib.memory_resource.CudaAsyncMemoryResource' object has no attribute 'get_upstream'
2024-03-28 05:55:44,227 - distributed.worker - WARNING - Run Failed
Function: lambda
args:     ()
kwargs:   {}
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 3182, in run
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dask_cuda_worker.py", line 189, in <lambda>
    type(rmm.mr.get_current_device_resource().get_upstream()),
AttributeError: 'rmm._lib.memory_resource.CudaAsyncMemoryResource' object has no attribute 'get_upstream'
2024-03-28 05:55:44,227 - distributed.worker - WARNING - Run Failed
Function: lambda
args:     ()
kwargs:   {}
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 3182, in run
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dask_cuda_worker.py", line 189, in <lambda>
    type(rmm.mr.get_current_device_resource().get_upstream()),
AttributeError: 'rmm._lib.memory_resource.CudaAsyncMemoryResource' object has no attribute 'get_upstream'
2024-03-28 05:55:44,227 - distributed.worker - WARNING - Run Failed
Function: lambda
args:     ()
kwargs:   {}
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 3182, in run
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dask_cuda_worker.py", line 189, in <lambda>
    type(rmm.mr.get_current_device_resource().get_upstream()),
AttributeError: 'rmm._lib.memory_resource.CudaAsyncMemoryResource' object has no attribute 'get_upstream'
2024-03-28 05:55:44,434 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:44,439 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:44,446 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:44,449 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:44,450 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:44,465 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:44,474 - distributed.scheduler - INFO - Remove client Client-cd2f10a4-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:44,474 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37924; closing.
2024-03-28 05:55:44,475 - distributed.scheduler - INFO - Remove client Client-cd2f10a4-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:44,475 - distributed.scheduler - INFO - Close client connection: Client-cd2f10a4-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:44,475 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:44,726 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:44,734 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:44,736 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:44,738 - distributed.scheduler - INFO - Remove client Client-ccc0a119-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:44,739 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37932; closing.
2024-03-28 05:55:44,739 - distributed.scheduler - INFO - Remove client Client-ccc0a119-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:44,739 - distributed.scheduler - INFO - Close client connection: Client-ccc0a119-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:45,182 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42411'. Reason: nanny-close
2024-03-28 05:55:45,183 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:45,183 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35925'. Reason: nanny-close
2024-03-28 05:55:45,184 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:45,184 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44709'. Reason: nanny-close
2024-03-28 05:55:45,184 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:45,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44919'. Reason: nanny-close
2024-03-28 05:55:45,185 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:45,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43815'. Reason: nanny-close
2024-03-28 05:55:45,185 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45475. Reason: nanny-close
2024-03-28 05:55:45,185 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40867. Reason: nanny-close
2024-03-28 05:55:45,185 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:45,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44849'. Reason: nanny-close
2024-03-28 05:55:45,186 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43187. Reason: nanny-close
2024-03-28 05:55:45,186 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:45,185 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45993. Reason: nanny-close
2024-03-28 05:55:45,186 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34737'. Reason: nanny-close
2024-03-28 05:55:45,186 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:45,186 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33259'. Reason: nanny-close
2024-03-28 05:55:45,187 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:45,187 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34787. Reason: nanny-close
2024-03-28 05:55:45,187 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:45,187 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39905. Reason: nanny-close
2024-03-28 05:55:45,187 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:45,187 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38028; closing.
2024-03-28 05:55:45,187 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:45,188 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45475', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605345.1881652')
2024-03-28 05:55:45,188 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38078; closing.
2024-03-28 05:55:45,188 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:45,188 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38010; closing.
2024-03-28 05:55:45,188 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43649. Reason: nanny-close
2024-03-28 05:55:45,188 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37977. Reason: nanny-close
2024-03-28 05:55:45,189 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:45,189 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:45,189 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43187', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605345.1894073')
2024-03-28 05:55:45,189 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605345.1897874')
2024-03-28 05:55:45,191 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:45,192 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:45,191 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38010>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-28 05:55:45,192 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:45,192 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:38078>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-28 05:55:45,191 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-03-28 05:55:45,193 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38050; closing.
2024-03-28 05:55:45,193 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:38100 closed before handshake completed
2024-03-28 05:55:45,194 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605345.194227')
2024-03-28 05:55:45,194 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:45,194 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38036; closing.
2024-03-28 05:55:45,195 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37977', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605345.1951818')
2024-03-28 05:55:45,195 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-03-28 05:55:45,195 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1558, in _connect
    async def _connect(self, addr: str, timeout: float | None = None) -> Comm:
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-03-28 05:55:45,196 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38062; closing.
2024-03-28 05:55:45,197 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43649', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605345.1971228')
2024-03-28 05:55:45,197 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:45,197 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:45,197 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38042; closing.
2024-03-28 05:55:45,197 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39905', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605345.1978319')
2024-03-28 05:55:45,198 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:45,199 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:45,199 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:39905', status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 939, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-03-28 05:55:45,202 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38058; closing.
2024-03-28 05:55:45,202 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34787', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605345.2022316')
2024-03-28 05:55:45,202 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:45,202 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:55:45,204 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:46,457 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-28 05:55:46,458 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-28 05:55:46,458 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:55:46,460 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-28 05:55:46,461 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-03-28 05:55:48,694 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:48,700 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43457 instead
  warnings.warn(
2024-03-28 05:55:48,704 - distributed.scheduler - INFO - State start
2024-03-28 05:55:48,727 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:48,728 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-28 05:55:48,729 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43457/status
2024-03-28 05:55:48,730 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-28 05:55:48,870 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36233', status: init, memory: 0, processing: 0>
2024-03-28 05:55:48,883 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36233
2024-03-28 05:55:48,883 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38852
2024-03-28 05:55:48,894 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38852; closing.
2024-03-28 05:55:48,895 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36233', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605348.8953865')
2024-03-28 05:55:48,895 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:55:49,010 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35267'
2024-03-28 05:55:49,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43189'
2024-03-28 05:55:49,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35597'
2024-03-28 05:55:49,052 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42677'
2024-03-28 05:55:49,054 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41479'
2024-03-28 05:55:49,063 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41483'
2024-03-28 05:55:49,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42451'
2024-03-28 05:55:49,082 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33601'
2024-03-28 05:55:49,691 - distributed.scheduler - INFO - Receive client connection: Client-d2725d65-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:49,692 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38938
2024-03-28 05:55:50,215 - distributed.scheduler - INFO - Receive client connection: Client-d4743aa3-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:50,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36844
2024-03-28 05:55:51,074 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:51,074 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:51,079 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:51,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:51,080 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:51,083 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39527
2024-03-28 05:55:51,083 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39527
2024-03-28 05:55:51,083 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41813
2024-03-28 05:55:51,083 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:51,084 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:51,084 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:51,084 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:51,084 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6l63qa09
2024-03-28 05:55:51,084 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-242a3adc-d0b1-4528-a05b-62e05287388d
2024-03-28 05:55:51,085 - distributed.worker - INFO - Starting Worker plugin PreImport-b5348bca-8abb-4240-94e2-a6154c62570d
2024-03-28 05:55:51,085 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c70d0c9-38e3-490f-a86f-6fc43c84bd74
2024-03-28 05:55:51,087 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:51,088 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36371
2024-03-28 05:55:51,088 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36371
2024-03-28 05:55:51,088 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42693
2024-03-28 05:55:51,088 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:51,088 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:51,088 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:51,088 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:51,088 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qqatkcgu
2024-03-28 05:55:51,089 - distributed.worker - INFO - Starting Worker plugin PreImport-56b4de9a-3030-4dc0-99c9-592e9a4975c5
2024-03-28 05:55:51,089 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1048192a-ed59-4bfb-8661-a25673454bd1
2024-03-28 05:55:51,095 - distributed.worker - INFO - Starting Worker plugin RMMSetup-543bb846-593b-4a52-a527-43c8832086fd
2024-03-28 05:55:51,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:51,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:51,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:51,152 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:51,156 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:51,156 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:51,157 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:51,158 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:51,158 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33203
2024-03-28 05:55:51,158 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33203
2024-03-28 05:55:51,158 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36297
2024-03-28 05:55:51,158 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:51,158 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:51,158 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:51,158 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:51,159 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pr0t6z21
2024-03-28 05:55:51,159 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4656c1ac-cca4-4529-8758-15b2540ed9ad
2024-03-28 05:55:51,159 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38991
2024-03-28 05:55:51,159 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38991
2024-03-28 05:55:51,159 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43867
2024-03-28 05:55:51,159 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:51,159 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:51,159 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:51,159 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:51,159 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vub547hb
2024-03-28 05:55:51,160 - distributed.worker - INFO - Starting Worker plugin PreImport-84f05a26-5985-4fd8-ba44-799ddb790636
2024-03-28 05:55:51,160 - distributed.worker - INFO - Starting Worker plugin PreImport-4a3f2623-9d2e-4b0a-893c-96dcbe9a8851
2024-03-28 05:55:51,160 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:51,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f84ffdf-6398-48d0-aea6-0e8ae62c936d
2024-03-28 05:55:51,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8619aa1-abda-4a2c-9d45-bf1381c3dc9d
2024-03-28 05:55:51,160 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:51,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f22c41d-2cac-4ef1-a5aa-4122126e2887
2024-03-28 05:55:51,161 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:51,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37389
2024-03-28 05:55:51,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37389
2024-03-28 05:55:51,163 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39813
2024-03-28 05:55:51,163 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:51,163 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:51,163 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:51,163 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:51,163 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-feqzym16
2024-03-28 05:55:51,163 - distributed.worker - INFO - Starting Worker plugin PreImport-727a99b0-cf1e-44ea-b29f-08ce6a3f55e6
2024-03-28 05:55:51,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f290580a-4c7e-4850-872a-59aae1d689ab
2024-03-28 05:55:51,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dca7d32f-1c60-434a-a576-b8ad89a311cd
2024-03-28 05:55:51,166 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:51,167 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46579
2024-03-28 05:55:51,167 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46579
2024-03-28 05:55:51,167 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37369
2024-03-28 05:55:51,167 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:51,167 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:51,167 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:51,168 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:51,168 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q2i3i4kf
2024-03-28 05:55:51,168 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08ed5e13-564c-49db-b4c5-c6b50092c7d7
2024-03-28 05:55:51,168 - distributed.worker - INFO - Starting Worker plugin PreImport-6eb154df-d606-49ec-aa3e-b65308179d73
2024-03-28 05:55:51,168 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c4326d9-c131-415e-b342-54b907041f32
2024-03-28 05:55:51,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:51,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:51,212 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:51,214 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42427
2024-03-28 05:55:51,214 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42427
2024-03-28 05:55:51,214 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46037
2024-03-28 05:55:51,214 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:51,214 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:51,214 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:51,214 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:51,214 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ul1_83ta
2024-03-28 05:55:51,215 - distributed.worker - INFO - Starting Worker plugin PreImport-712ceaf0-bd8c-42cd-a8fa-f77539abc031
2024-03-28 05:55:51,215 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4641d382-7e6c-4769-a3ea-beacba53d315
2024-03-28 05:55:51,215 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ebbfaa6c-6811-471c-aaa7-075374630921
2024-03-28 05:55:51,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:51,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:51,400 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:51,401 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42715
2024-03-28 05:55:51,401 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42715
2024-03-28 05:55:51,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43597
2024-03-28 05:55:51,401 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:51,402 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:51,402 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:51,402 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:51,402 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aubo1do9
2024-03-28 05:55:51,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f5f30a1-437c-4633-aad5-ca14afbaeb4a
2024-03-28 05:55:53,238 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,248 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,266 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36371', status: init, memory: 0, processing: 0>
2024-03-28 05:55:53,267 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36371
2024-03-28 05:55:53,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36938
2024-03-28 05:55:53,269 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:53,270 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:53,270 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,272 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:53,280 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39527', status: init, memory: 0, processing: 0>
2024-03-28 05:55:53,280 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39527
2024-03-28 05:55:53,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36952
2024-03-28 05:55:53,282 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:53,282 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:53,283 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:53,318 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,321 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,322 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,333 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,339 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,345 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c48154d-af48-4b0f-807f-c735edf1a175
2024-03-28 05:55:53,345 - distributed.worker - INFO - Starting Worker plugin PreImport-ac81e881-998e-4267-a456-506cd78a5aaf
2024-03-28 05:55:53,345 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38991', status: init, memory: 0, processing: 0>
2024-03-28 05:55:53,346 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,346 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38991
2024-03-28 05:55:53,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36992
2024-03-28 05:55:53,347 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46579', status: init, memory: 0, processing: 0>
2024-03-28 05:55:53,347 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:53,348 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46579
2024-03-28 05:55:53,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36980
2024-03-28 05:55:53,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:53,348 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,349 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:53,349 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37389', status: init, memory: 0, processing: 0>
2024-03-28 05:55:53,349 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:53,349 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,350 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37389
2024-03-28 05:55:53,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36964
2024-03-28 05:55:53,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:53,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:53,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:53,352 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:53,352 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:53,357 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42427', status: init, memory: 0, processing: 0>
2024-03-28 05:55:53,357 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42427
2024-03-28 05:55:53,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36994
2024-03-28 05:55:53,358 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:53,359 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:53,359 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,361 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:53,365 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33203', status: init, memory: 0, processing: 0>
2024-03-28 05:55:53,366 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33203
2024-03-28 05:55:53,366 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36996
2024-03-28 05:55:53,367 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:53,368 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:53,368 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,369 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:53,373 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42715', status: init, memory: 0, processing: 0>
2024-03-28 05:55:53,374 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42715
2024-03-28 05:55:53,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37010
2024-03-28 05:55:53,375 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:55:53,376 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:55:53,376 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:53,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:55:53,485 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:53,485 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:53,485 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:53,485 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:53,485 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:53,486 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:53,486 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:53,486 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:55:53,491 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:53,491 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:53,491 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:53,491 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:53,491 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:53,491 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:53,492 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:53,492 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:53,495 - distributed.scheduler - INFO - Remove client Client-d4743aa3-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:53,495 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36844; closing.
2024-03-28 05:55:53,495 - distributed.scheduler - INFO - Remove client Client-d4743aa3-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:53,496 - distributed.scheduler - INFO - Close client connection: Client-d4743aa3-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:53,503 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:53,504 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:53,504 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:53,504 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:53,504 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:53,504 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:53,504 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:53,504 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:55:53,512 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:53,514 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:55:53,516 - distributed.scheduler - INFO - Remove client Client-d2725d65-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:53,516 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38938; closing.
2024-03-28 05:55:53,516 - distributed.scheduler - INFO - Remove client Client-d2725d65-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:53,517 - distributed.scheduler - INFO - Close client connection: Client-d2725d65-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:53,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35267'. Reason: nanny-close
2024-03-28 05:55:53,518 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:53,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43189'. Reason: nanny-close
2024-03-28 05:55:53,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:53,519 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35597'. Reason: nanny-close
2024-03-28 05:55:53,519 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36371. Reason: nanny-close
2024-03-28 05:55:53,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:53,519 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42677'. Reason: nanny-close
2024-03-28 05:55:53,520 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:53,520 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39527. Reason: nanny-close
2024-03-28 05:55:53,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41479'. Reason: nanny-close
2024-03-28 05:55:53,520 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37389. Reason: nanny-close
2024-03-28 05:55:53,520 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:53,520 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41483'. Reason: nanny-close
2024-03-28 05:55:53,520 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38991. Reason: nanny-close
2024-03-28 05:55:53,521 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:53,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42451'. Reason: nanny-close
2024-03-28 05:55:53,521 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:53,521 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33203. Reason: nanny-close
2024-03-28 05:55:53,521 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33601'. Reason: nanny-close
2024-03-28 05:55:53,521 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42427. Reason: nanny-close
2024-03-28 05:55:53,521 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:55:53,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:53,522 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46579. Reason: nanny-close
2024-03-28 05:55:53,522 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36938; closing.
2024-03-28 05:55:53,522 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36371', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605353.5225668')
2024-03-28 05:55:53,522 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:53,522 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:53,522 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42715. Reason: nanny-close
2024-03-28 05:55:53,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:53,523 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:53,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:53,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:53,523 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:53,524 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:53,524 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36952; closing.
2024-03-28 05:55:53,524 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:53,524 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:53,524 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36992; closing.
2024-03-28 05:55:53,525 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36964; closing.
2024-03-28 05:55:53,525 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:53,525 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:53,525 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:53,525 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:55:53,526 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36996; closing.
2024-03-28 05:55:53,526 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39527', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605353.526311')
2024-03-28 05:55:53,526 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38991', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605353.5266905')
2024-03-28 05:55:53,527 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37389', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605353.5271447')
2024-03-28 05:55:53,527 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36994; closing.
2024-03-28 05:55:53,527 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36980; closing.
2024-03-28 05:55:53,527 - distributed.nanny - INFO - Worker closed
2024-03-28 05:55:53,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33203', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605353.5281236')
2024-03-28 05:55:53,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42427', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605353.5285382')
2024-03-28 05:55:53,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46579', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605353.5288832')
2024-03-28 05:55:53,529 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37010; closing.
2024-03-28 05:55:53,530 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605353.529952')
2024-03-28 05:55:53,530 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:55:54,584 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-28 05:55:54,584 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-28 05:55:54,585 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:55:54,587 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-28 05:55:54,587 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-03-28 05:55:57,166 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:57,173 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44607 instead
  warnings.warn(
2024-03-28 05:55:57,178 - distributed.scheduler - INFO - State start
2024-03-28 05:55:57,288 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:55:57,290 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-28 05:55:57,291 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44607/status
2024-03-28 05:55:57,292 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-28 05:55:57,413 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45693', status: init, memory: 0, processing: 0>
2024-03-28 05:55:57,425 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45693
2024-03-28 05:55:57,426 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37198
2024-03-28 05:55:57,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40329'
2024-03-28 05:55:57,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34291'
2024-03-28 05:55:57,484 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37198; closing.
2024-03-28 05:55:57,484 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45693', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605357.484591')
2024-03-28 05:55:57,485 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:55:57,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44131'
2024-03-28 05:55:57,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38431'
2024-03-28 05:55:57,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39571'
2024-03-28 05:55:57,516 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40905'
2024-03-28 05:55:57,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37127'
2024-03-28 05:55:57,534 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38587', status: init, memory: 0, processing: 0>
2024-03-28 05:55:57,534 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38587
2024-03-28 05:55:57,534 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37276
2024-03-28 05:55:57,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41759'
2024-03-28 05:55:57,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37276; closing.
2024-03-28 05:55:57,574 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605357.574223')
2024-03-28 05:55:57,574 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:55:57,596 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36137', status: init, memory: 0, processing: 0>
2024-03-28 05:55:57,597 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36137
2024-03-28 05:55:57,597 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37298
2024-03-28 05:55:57,599 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33101', status: init, memory: 0, processing: 0>
2024-03-28 05:55:57,599 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33101
2024-03-28 05:55:57,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37284
2024-03-28 05:55:57,604 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39601', status: init, memory: 0, processing: 0>
2024-03-28 05:55:57,605 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39601
2024-03-28 05:55:57,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37300
2024-03-28 05:55:57,625 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37300; closing.
2024-03-28 05:55:57,625 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39601', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605357.6257913')
2024-03-28 05:55:57,626 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46231', status: init, memory: 0, processing: 0>
2024-03-28 05:55:57,627 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46231
2024-03-28 05:55:57,627 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37316
2024-03-28 05:55:57,627 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37298; closing.
2024-03-28 05:55:57,628 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36137', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605357.628291')
2024-03-28 05:55:57,630 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45415', status: init, memory: 0, processing: 0>
2024-03-28 05:55:57,630 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45415
2024-03-28 05:55:57,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37326
2024-03-28 05:55:57,631 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37284; closing.
2024-03-28 05:55:57,631 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33101', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605357.631309')
2024-03-28 05:55:57,634 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41421', status: init, memory: 0, processing: 0>
2024-03-28 05:55:57,634 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41421
2024-03-28 05:55:57,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37332
2024-03-28 05:55:57,677 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37332; closing.
2024-03-28 05:55:57,677 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41421', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605357.6778536')
2024-03-28 05:55:57,678 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37316; closing.
2024-03-28 05:55:57,678 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37326; closing.
2024-03-28 05:55:57,679 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46231', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605357.6792657')
2024-03-28 05:55:57,679 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45415', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605357.6796186')
2024-03-28 05:55:57,679 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:55:58,464 - distributed.scheduler - INFO - Receive client connection: Client-d7511bff-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:55:58,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37348
2024-03-28 05:55:58,592 - distributed.scheduler - INFO - Receive client connection: Client-d972743b-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:55:58,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37372
2024-03-28 05:55:59,627 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:59,627 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:59,628 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:59,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:59,632 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:59,632 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:59,633 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:59,634 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44867
2024-03-28 05:55:59,634 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44867
2024-03-28 05:55:59,634 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45337
2024-03-28 05:55:59,634 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:59,634 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:59,634 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:59,634 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:59,634 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:59,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v0cogweg
2024-03-28 05:55:59,634 - distributed.worker - INFO - Starting Worker plugin PreImport-ef4976e3-856a-439f-b1a4-518e1b69ab81
2024-03-28 05:55:59,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ca2a2dc-390a-45d0-ac1f-96a08cd3f88f
2024-03-28 05:55:59,635 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38491
2024-03-28 05:55:59,635 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38491
2024-03-28 05:55:59,635 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34429
2024-03-28 05:55:59,635 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:59,635 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:59,635 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:59,636 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:59,636 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-om1l31_i
2024-03-28 05:55:59,636 - distributed.worker - INFO - Starting Worker plugin PreImport-13a7250b-30cd-45c3-ad50-f14a935b1511
2024-03-28 05:55:59,636 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3c2c3e6-7a0d-472a-9f1b-025a28413057
2024-03-28 05:55:59,637 - distributed.worker - INFO - Starting Worker plugin RMMSetup-426d0b30-55eb-4aa7-b778-9cb17eee87dc
2024-03-28 05:55:59,638 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:59,639 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34361
2024-03-28 05:55:59,639 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34361
2024-03-28 05:55:59,639 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43937
2024-03-28 05:55:59,639 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:59,639 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:59,639 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:59,639 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:59,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n9r_jgv3
2024-03-28 05:55:59,640 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37cfa644-1a6e-4ec2-8d64-cfed3a6619d4
2024-03-28 05:55:59,640 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:59,640 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:59,641 - distributed.worker - INFO - Starting Worker plugin PreImport-8116e40c-7f9a-4823-b8dc-5a831d6abda8
2024-03-28 05:55:59,641 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a92e4048-2639-401c-a984-aabbc514104d
2024-03-28 05:55:59,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:59,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:59,646 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:59,647 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43127
2024-03-28 05:55:59,647 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43127
2024-03-28 05:55:59,647 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41853
2024-03-28 05:55:59,647 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:59,647 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:59,647 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:59,647 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:59,647 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yzm38ujm
2024-03-28 05:55:59,647 - distributed.worker - INFO - Starting Worker plugin PreImport-9c180548-62b2-4085-940b-e207e5df2d0e
2024-03-28 05:55:59,648 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f3c67aca-9e8e-4d3b-9a7c-18b2b3d50d9f
2024-03-28 05:55:59,648 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:59,648 - distributed.worker - INFO - Starting Worker plugin RMMSetup-facdd527-95d5-476c-a552-b140e0f287be
2024-03-28 05:55:59,649 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40707
2024-03-28 05:55:59,649 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40707
2024-03-28 05:55:59,649 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36433
2024-03-28 05:55:59,649 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:59,649 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:59,649 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:59,649 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:59,649 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nknv4z8a
2024-03-28 05:55:59,649 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-272bc9b8-a6e9-42b7-a821-92e177298b2d
2024-03-28 05:55:59,650 - distributed.worker - INFO - Starting Worker plugin PreImport-05649f54-f62f-4d83-8e6c-8d5687fff97e
2024-03-28 05:55:59,650 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f1ae2d0-4075-45a7-a711-7d680558d848
2024-03-28 05:55:59,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:59,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:59,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:59,684 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36275
2024-03-28 05:55:59,684 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36275
2024-03-28 05:55:59,684 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33255
2024-03-28 05:55:59,684 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:59,684 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:59,684 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:59,684 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:59,684 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k03mo465
2024-03-28 05:55:59,685 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8fa04e45-fd44-4261-a1b8-c990c6825913
2024-03-28 05:55:59,691 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:59,691 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:59,692 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:55:59,692 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:55:59,696 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:59,697 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:55:59,697 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36623
2024-03-28 05:55:59,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36623
2024-03-28 05:55:59,698 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39667
2024-03-28 05:55:59,698 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:59,698 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:59,698 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:59,698 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:59,698 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xppz1xvd
2024-03-28 05:55:59,698 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36875
2024-03-28 05:55:59,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36875
2024-03-28 05:55:59,698 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43603
2024-03-28 05:55:59,698 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:55:59,698 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ff12f28-dd29-4413-b29d-356fb2a4a37c
2024-03-28 05:55:59,698 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:55:59,698 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:55:59,698 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 05:55:59,698 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bhw80qku
2024-03-28 05:55:59,699 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b691a28-85e4-4f72-9044-9f16df076390
2024-03-28 05:55:59,699 - distributed.worker - INFO - Starting Worker plugin PreImport-e43093ee-69ad-4125-9d37-9047afb9b530
2024-03-28 05:55:59,700 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ad0fca2-1d3f-4f54-98c7-5a75efd10c31
2024-03-28 05:56:01,690 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6bc5e77b-a0c6-474a-be2d-86df1ffe165a
2024-03-28 05:56:01,690 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,713 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,724 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,737 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44867', status: init, memory: 0, processing: 0>
2024-03-28 05:56:01,738 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44867
2024-03-28 05:56:01,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45738
2024-03-28 05:56:01,740 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38491', status: init, memory: 0, processing: 0>
2024-03-28 05:56:01,740 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:01,741 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38491
2024-03-28 05:56:01,741 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45754
2024-03-28 05:56:01,742 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:01,742 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,742 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:01,742 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,743 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:01,743 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:01,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:01,745 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,747 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:01,748 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:01,752 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34361', status: init, memory: 0, processing: 0>
2024-03-28 05:56:01,753 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34361
2024-03-28 05:56:01,753 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45766
2024-03-28 05:56:01,753 - distributed.scheduler - INFO - Remove client Client-d972743b-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:01,754 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37372; closing.
2024-03-28 05:56:01,754 - distributed.scheduler - INFO - Remove client Client-d972743b-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:01,754 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:01,754 - distributed.scheduler - INFO - Close client connection: Client-d972743b-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:01,755 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:01,755 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:01,760 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f3bd9fd4-ccd3-4144-9166-e184f501cb3b
2024-03-28 05:56:01,761 - distributed.worker - INFO - Starting Worker plugin PreImport-3645cf31-9d87-42ac-b54c-0fc09e1f3521
2024-03-28 05:56:01,761 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,765 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40707', status: init, memory: 0, processing: 0>
2024-03-28 05:56:01,766 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40707
2024-03-28 05:56:01,766 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45782
2024-03-28 05:56:01,767 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:01,767 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:01,768 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,768 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43127', status: init, memory: 0, processing: 0>
2024-03-28 05:56:01,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:01,769 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43127
2024-03-28 05:56:01,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45790
2024-03-28 05:56:01,770 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:01,770 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:01,771 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,772 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:01,777 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,783 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e4da1b7-21a1-4773-a03b-164988230e45
2024-03-28 05:56:01,783 - distributed.worker - INFO - Starting Worker plugin PreImport-da68429b-c0d6-4cd3-abea-1a5220c33eed
2024-03-28 05:56:01,783 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,797 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36275', status: init, memory: 0, processing: 0>
2024-03-28 05:56:01,798 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36275
2024-03-28 05:56:01,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45800
2024-03-28 05:56:01,799 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:01,801 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:01,801 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,803 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:01,804 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36875', status: init, memory: 0, processing: 0>
2024-03-28 05:56:01,804 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36875
2024-03-28 05:56:01,804 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45810
2024-03-28 05:56:01,805 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:01,806 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36623', status: init, memory: 0, processing: 0>
2024-03-28 05:56:01,806 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36623
2024-03-28 05:56:01,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45814
2024-03-28 05:56:01,806 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:01,806 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:01,808 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:01,808 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:01,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:01,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:01,850 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:56:01,850 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:56:01,850 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:56:01,850 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:56:01,850 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:56:01,851 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:56:01,851 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:56:01,851 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-28 05:56:01,856 - distributed.scheduler - INFO - Remove client Client-d7511bff-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:01,856 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37348; closing.
2024-03-28 05:56:01,856 - distributed.scheduler - INFO - Remove client Client-d7511bff-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:01,857 - distributed.scheduler - INFO - Close client connection: Client-d7511bff-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:01,858 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40329'. Reason: nanny-close
2024-03-28 05:56:01,858 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:01,859 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34291'. Reason: nanny-close
2024-03-28 05:56:01,859 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:01,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44131'. Reason: nanny-close
2024-03-28 05:56:01,860 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34361. Reason: nanny-close
2024-03-28 05:56:01,860 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:01,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38431'. Reason: nanny-close
2024-03-28 05:56:01,860 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38491. Reason: nanny-close
2024-03-28 05:56:01,860 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:01,861 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39571'. Reason: nanny-close
2024-03-28 05:56:01,861 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36623. Reason: nanny-close
2024-03-28 05:56:01,861 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:01,861 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40905'. Reason: nanny-close
2024-03-28 05:56:01,861 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40707. Reason: nanny-close
2024-03-28 05:56:01,861 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:01,862 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37127'. Reason: nanny-close
2024-03-28 05:56:01,862 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:01,862 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:01,862 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44867. Reason: nanny-close
2024-03-28 05:56:01,862 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41759'. Reason: nanny-close
2024-03-28 05:56:01,862 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45766; closing.
2024-03-28 05:56:01,862 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36875. Reason: nanny-close
2024-03-28 05:56:01,862 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:01,862 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:01,862 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:01,862 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34361', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605361.8629131')
2024-03-28 05:56:01,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:01,863 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36275. Reason: nanny-close
2024-03-28 05:56:01,863 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43127. Reason: nanny-close
2024-03-28 05:56:01,864 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:01,864 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:01,864 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:01,864 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45782; closing.
2024-03-28 05:56:01,864 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:01,865 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:01,865 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45754; closing.
2024-03-28 05:56:01,865 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:01,865 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45814; closing.
2024-03-28 05:56:01,865 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:01,866 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:01,866 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40707', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605361.866815')
2024-03-28 05:56:01,866 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:01,867 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:01,867 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38491', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605361.8674138')
2024-03-28 05:56:01,867 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:01,868 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36623', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605361.8680053')
2024-03-28 05:56:01,870 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:01,869 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45782>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45782>: Stream is closed
2024-03-28 05:56:01,872 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45738; closing.
2024-03-28 05:56:01,873 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45810; closing.
2024-03-28 05:56:01,874 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605361.8740041')
2024-03-28 05:56:01,874 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605361.874597')
2024-03-28 05:56:01,875 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45790; closing.
2024-03-28 05:56:01,875 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45800; closing.
2024-03-28 05:56:01,876 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43127', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605361.8760326')
2024-03-28 05:56:01,876 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36275', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605361.876616')
2024-03-28 05:56:01,876 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:56:02,774 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-28 05:56:02,775 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-28 05:56:02,775 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:56:02,777 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-28 05:56:02,778 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-03-28 05:56:05,103 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:05,108 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42159 instead
  warnings.warn(
2024-03-28 05:56:05,111 - distributed.scheduler - INFO - State start
2024-03-28 05:56:05,132 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:05,133 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-28 05:56:05,134 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42159/status
2024-03-28 05:56:05,134 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-28 05:56:05,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45579'
2024-03-28 05:56:05,340 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46211', status: init, memory: 0, processing: 0>
2024-03-28 05:56:05,352 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46211
2024-03-28 05:56:05,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46034
2024-03-28 05:56:05,374 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46034; closing.
2024-03-28 05:56:05,375 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46211', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605365.3750482')
2024-03-28 05:56:05,375 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:56:06,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:56:06,957 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:56:07,243 - distributed.scheduler - INFO - Receive client connection: Client-dc3752ca-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:07,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46036
2024-03-28 05:56:07,513 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:56:07,514 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44717
2024-03-28 05:56:07,514 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44717
2024-03-28 05:56:07,514 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-03-28 05:56:07,514 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:56:07,515 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:07,515 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:56:07,515 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-28 05:56:07,515 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7ziotqpt
2024-03-28 05:56:07,515 - distributed.worker - INFO - Starting Worker plugin PreImport-fcb954b0-2f70-4112-8bd9-ac828e93910a
2024-03-28 05:56:07,515 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2008ba60-8372-4ce4-a758-9620dc508efb
2024-03-28 05:56:07,515 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86c8aa13-ef81-4d70-903e-211e5aa265c9
2024-03-28 05:56:07,516 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:07,572 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44717', status: init, memory: 0, processing: 0>
2024-03-28 05:56:07,573 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44717
2024-03-28 05:56:07,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46048
2024-03-28 05:56:07,574 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:07,575 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:07,575 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:07,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:07,656 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:07,658 - distributed.scheduler - INFO - Remove client Client-dc3752ca-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:07,658 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46036; closing.
2024-03-28 05:56:07,659 - distributed.scheduler - INFO - Remove client Client-dc3752ca-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:07,659 - distributed.scheduler - INFO - Close client connection: Client-dc3752ca-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:07,660 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45579'. Reason: nanny-close
2024-03-28 05:56:07,660 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:07,661 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44717. Reason: nanny-close
2024-03-28 05:56:07,663 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:07,663 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46048; closing.
2024-03-28 05:56:07,663 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44717', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605367.6639264')
2024-03-28 05:56:07,664 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:56:07,665 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:08,345 - distributed.scheduler - INFO - Receive client connection: Client-df42b32e-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:08,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46062
2024-03-28 05:56:08,425 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-28 05:56:08,426 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-28 05:56:08,426 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:56:08,428 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-28 05:56:08,428 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-03-28 05:56:12,991 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:12,996 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32975 instead
  warnings.warn(
2024-03-28 05:56:13,005 - distributed.scheduler - INFO - State start
2024-03-28 05:56:13,033 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:13,035 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-28 05:56:13,036 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:56:13,037 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4051, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 629, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-28 05:56:13,222 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40843'
2024-03-28 05:56:13,412 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40843'. Reason: nanny-close
2024-03-28 05:56:15,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:56:15,172 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:56:15,821 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:56:15,822 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33057
2024-03-28 05:56:15,822 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33057
2024-03-28 05:56:15,822 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37475
2024-03-28 05:56:15,822 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:56:15,822 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:15,822 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:56:15,822 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-28 05:56:15,823 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hv1dpjcs
2024-03-28 05:56:15,823 - distributed.worker - INFO - Starting Worker plugin PreImport-62a019fd-4cdc-4e85-8177-55eaba0f5333
2024-03-28 05:56:15,824 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e007a7b8-3fe0-4fda-b46e-5c9b7de38f76
2024-03-28 05:56:15,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f693537e-554f-4ffc-81f3-6e6b58ba134d
2024-03-28 05:56:15,824 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:18,826 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:18,826 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:18,826 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:18,828 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:18,876 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:18,878 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33057. Reason: nanny-close
2024-03-28 05:56:18,880 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:18,881 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-03-28 05:56:21,584 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:21,588 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38181 instead
  warnings.warn(
2024-03-28 05:56:21,592 - distributed.scheduler - INFO - State start
2024-03-28 05:56:21,614 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:21,615 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-28 05:56:21,616 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38181/status
2024-03-28 05:56:21,616 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-28 05:56:24,124 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:33216'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4452, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33216>: Stream is closed
2024-03-28 05:56:24,431 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-28 05:56:24,431 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-28 05:56:24,432 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:56:24,433 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-28 05:56:24,433 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-03-28 05:56:26,969 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:26,974 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38857 instead
  warnings.warn(
2024-03-28 05:56:26,978 - distributed.scheduler - INFO - State start
2024-03-28 05:56:27,001 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:27,003 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-28 05:56:27,003 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38857/status
2024-03-28 05:56:27,004 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-28 05:56:27,126 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37365'
2024-03-28 05:56:29,007 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:56:29,007 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:56:29,011 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:56:29,012 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33741
2024-03-28 05:56:29,012 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33741
2024-03-28 05:56:29,012 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39085
2024-03-28 05:56:29,012 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-28 05:56:29,012 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:29,012 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:56:29,012 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-28 05:56:29,012 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-_ztc_9eb
2024-03-28 05:56:29,013 - distributed.worker - INFO - Starting Worker plugin PreImport-4e7286ad-7842-4494-80bb-70f3a088f793
2024-03-28 05:56:29,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3859a1d5-b1b9-4790-af1f-0d6e0eb942b1
2024-03-28 05:56:29,013 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7139804-981c-444b-ba4c-b8e54795d9a5
2024-03-28 05:56:29,013 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:29,061 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33741', status: init, memory: 0, processing: 0>
2024-03-28 05:56:29,075 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33741
2024-03-28 05:56:29,075 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43484
2024-03-28 05:56:29,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:29,076 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-28 05:56:29,077 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:29,078 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-28 05:56:30,821 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:30,936 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37365'. Reason: nanny-close
2024-03-28 05:56:30,936 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:30,937 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33741. Reason: nanny-close
2024-03-28 05:56:30,940 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-28 05:56:30,940 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43484; closing.
2024-03-28 05:56:30,940 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33741', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605390.9405506')
2024-03-28 05:56:30,940 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:56:30,941 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:31,752 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-28 05:56:31,752 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-28 05:56:31,752 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:56:31,753 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-28 05:56:31,754 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 6, in <module>
    from dask.__main__ import main
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 666, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 565, in module_from_spec
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rapids_dask_dependency/dask_loader.py", line 37, in create_module
    mod = importlib.import_module(spec.name)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__init__.py", line 3, in <module>
    from dask import config, datasets
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/config.py", line 8, in <module>
    import pathlib
  File "/opt/conda/envs/gdf/lib/python3.9/pathlib.py", line 13, in <module>
    from urllib.parse import quote_from_bytes as urlquote_from_bytes
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 6, in <module>
    from dask.__main__ import main
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 666, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 565, in module_from_spec
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rapids_dask_dependency/dask_loader.py", line 37, in create_module
    mod = importlib.import_module(spec.name)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__init__.py", line 3, in <module>
    from dask import config, datasets
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/datasets.py", line 7, in <module>
    from dask.utils import import_required
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 26, in <module>
    import tlz as toolz
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tlz/__init__.py", line 9, in <module>
    from . import _build_tlz
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tlz/_build_tlz.py", line 92, in <module>
    tlz_loader.exec_module(sys.modules['tlz'])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tlz/_build_tlz.py", line 86, in exec_module
    submodule = import_module(module_name)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tlz/_build_tlz.py", line 81, in exec_module
    isinstance(v, types.ModuleType)
KeyboardInterrupt
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-03-28 05:56:34,277 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:34,282 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34091 instead
  warnings.warn(
2024-03-28 05:56:34,286 - distributed.scheduler - INFO - State start
2024-03-28 05:56:34,308 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:34,309 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-28 05:56:34,310 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34091/status
2024-03-28 05:56:34,310 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-28 05:56:34,345 - distributed.scheduler - INFO - Receive client connection: Client-ee4dc01a-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:34,356 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42712
2024-03-28 05:56:34,359 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37359'
2024-03-28 05:56:34,532 - distributed.scheduler - INFO - Receive client connection: Client-ed963771-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:34,533 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42752
2024-03-28 05:56:36,236 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:56:36,236 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:56:36,240 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:56:36,240 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33979
2024-03-28 05:56:36,241 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33979
2024-03-28 05:56:36,241 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46353
2024-03-28 05:56:36,241 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:56:36,241 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:36,241 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:56:36,241 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-28 05:56:36,241 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bc16didh
2024-03-28 05:56:36,241 - distributed.worker - INFO - Starting Worker plugin PreImport-bc773433-95c5-4fd9-89a5-6015988c47d0
2024-03-28 05:56:36,241 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a7ec7c06-5bf6-4521-b557-338554c42a7d
2024-03-28 05:56:36,242 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45fb9641-7794-40c7-98c7-aca9d6162471
2024-03-28 05:56:36,543 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:36,666 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33979', status: init, memory: 0, processing: 0>
2024-03-28 05:56:36,669 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33979
2024-03-28 05:56:36,669 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42792
2024-03-28 05:56:36,670 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:36,671 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:36,671 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:36,673 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:36,764 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:56:36,768 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:56:36,769 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:36,771 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:36,772 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:36,773 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:36,774 - distributed.scheduler - INFO - Remove client Client-ed963771-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:36,774 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42752; closing.
2024-03-28 05:56:36,774 - distributed.scheduler - INFO - Remove client Client-ed963771-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:36,775 - distributed.scheduler - INFO - Close client connection: Client-ed963771-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:36,776 - distributed.scheduler - INFO - Remove client Client-ee4dc01a-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:36,776 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37359'. Reason: nanny-close
2024-03-28 05:56:36,776 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42712; closing.
2024-03-28 05:56:36,776 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:36,776 - distributed.scheduler - INFO - Remove client Client-ee4dc01a-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:36,776 - distributed.scheduler - INFO - Close client connection: Client-ee4dc01a-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:36,777 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33979. Reason: nanny-close
2024-03-28 05:56:36,780 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42792; closing.
2024-03-28 05:56:36,780 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:36,780 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33979', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605396.780281')
2024-03-28 05:56:36,780 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:56:36,781 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:37,391 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-28 05:56:37,392 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-28 05:56:37,392 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:56:37,394 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-28 05:56:37,394 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-03-28 05:56:39,694 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:39,699 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33803 instead
  warnings.warn(
2024-03-28 05:56:39,704 - distributed.scheduler - INFO - State start
2024-03-28 05:56:39,727 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-28 05:56:39,728 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-28 05:56:39,729 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33803/status
2024-03-28 05:56:39,729 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-28 05:56:39,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39529'
2024-03-28 05:56:40,126 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45801', status: init, memory: 0, processing: 0>
2024-03-28 05:56:40,139 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45801
2024-03-28 05:56:40,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54718
2024-03-28 05:56:40,179 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54718; closing.
2024-03-28 05:56:40,180 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605400.1803598')
2024-03-28 05:56:40,180 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:56:41,055 - distributed.scheduler - INFO - Receive client connection: Client-f2c1d004-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:41,056 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54720
2024-03-28 05:56:42,172 - distributed.scheduler - INFO - Receive client connection: Client-f0ddaa5c-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:42,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54734
2024-03-28 05:56:42,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:56:42,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:56:42,481 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:56:42,482 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33037
2024-03-28 05:56:42,482 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33037
2024-03-28 05:56:42,482 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34063
2024-03-28 05:56:42,482 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-28 05:56:42,482 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:42,482 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:56:42,482 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-28 05:56:42,482 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i_ukg1fw
2024-03-28 05:56:42,482 - distributed.worker - INFO - Starting Worker plugin PreImport-e6af27d7-2fa5-4f98-b064-7dc7d1092bc7
2024-03-28 05:56:42,482 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-18f80a0b-3057-478f-be2b-00e46e4e88a2
2024-03-28 05:56:42,482 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1664b660-532f-43fa-af58-ee1956749cd4
2024-03-28 05:56:43,668 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:43,724 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33037', status: init, memory: 0, processing: 0>
2024-03-28 05:56:43,726 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33037
2024-03-28 05:56:43,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54754
2024-03-28 05:56:43,726 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:56:43,727 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-28 05:56:43,727 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:56:43,729 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-28 05:56:43,763 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-03-28 05:56:43,765 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-03-28 05:56:43,769 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:56:43,772 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-28 05:56:43,773 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:43,775 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:43,776 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:43,777 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:56:43,778 - distributed.scheduler - INFO - Remove client Client-f0ddaa5c-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:43,778 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54734; closing.
2024-03-28 05:56:43,778 - distributed.scheduler - INFO - Remove client Client-f0ddaa5c-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:43,779 - distributed.scheduler - INFO - Close client connection: Client-f0ddaa5c-ecc7-11ee-ae7b-d8c49764f6bb
2024-03-28 05:56:43,780 - distributed.scheduler - INFO - Remove client Client-f2c1d004-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:43,780 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54720; closing.
2024-03-28 05:56:43,780 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39529'. Reason: nanny-close
2024-03-28 05:56:43,780 - distributed.scheduler - INFO - Remove client Client-f2c1d004-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:43,780 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-28 05:56:43,780 - distributed.scheduler - INFO - Close client connection: Client-f2c1d004-ecc7-11ee-9264-d8c49764f6bb
2024-03-28 05:56:43,781 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33037. Reason: nanny-close
2024-03-28 05:56:43,783 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-28 05:56:43,783 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54754; closing.
2024-03-28 05:56:43,783 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33037', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711605403.7838426')
2024-03-28 05:56:43,784 - distributed.scheduler - INFO - Lost all workers
2024-03-28 05:56:43,789 - distributed.nanny - INFO - Worker closed
2024-03-28 05:56:44,596 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-28 05:56:44,596 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-28 05:56:44,597 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-28 05:56:44,598 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-28 05:56:44,599 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46881 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38289 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] SKIPPED (could ...)
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33667 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33991 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46879 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43163 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36443 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33409 instead
  warnings.warn(
[1711605490.615589] [dgx13:49526:0]            sock.c:481  UCX  ERROR bind(fd=183 addr=0.0.0.0:42748) failed: Address already in use
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers SKIPPED (h...)
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1711605506.224899] [dgx13:49766:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:49874) failed: Address already in use
[1711605506.266694] [dgx13:49792:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:56292) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] SKIPPED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41769 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37469 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39967 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41687 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-03-28 05:59:40,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:59:40,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:59:40,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:59:40,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:59:40,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:59:40,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:59:40,935 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:59:40,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:59:40,950 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:59:40,950 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:59:40,950 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:59:40,950 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:59:40,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:59:40,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:59:41,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 05:59:41,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 05:59:41,168 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:59:41,169 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35597
2024-03-28 05:59:41,169 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35597
2024-03-28 05:59:41,169 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46451
2024-03-28 05:59:41,169 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,169 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,169 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:59:41,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-abfn9w15
2024-03-28 05:59:41,169 - distributed.worker - INFO - Starting Worker plugin PreImport-c5b7cd73-c23f-4e77-9a38-cc1172ba4af8
2024-03-28 05:59:41,169 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4a64e19-7291-4d0c-ba7c-645f21a1a4bf
2024-03-28 05:59:41,169 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7cffeaa2-eddb-4e00-af20-62055027490a
2024-03-28 05:59:41,170 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,250 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:59:41,252 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,252 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41095
2024-03-28 05:59:41,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:59:41,289 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43161
2024-03-28 05:59:41,289 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43161
2024-03-28 05:59:41,289 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37149
2024-03-28 05:59:41,289 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,289 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,289 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:59:41,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p7b82tco
2024-03-28 05:59:41,289 - distributed.worker - INFO - Starting Worker plugin PreImport-4120f80b-8feb-4016-8164-c163e261e14f
2024-03-28 05:59:41,290 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12b9eb76-baa8-48ab-8d59-c9afd1ca9338
2024-03-28 05:59:41,290 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-12401266-fc52-49ba-bf56-333edb8dcafc
2024-03-28 05:59:41,290 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,316 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:59:41,317 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45377
2024-03-28 05:59:41,317 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45377
2024-03-28 05:59:41,317 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33741
2024-03-28 05:59:41,317 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,317 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,317 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:59:41,318 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xy5ifpa7
2024-03-28 05:59:41,318 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe3df3c4-fb33-4ddf-8b06-6ee5f03aa8a8
2024-03-28 05:59:41,318 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-10ae538c-974f-442d-a53f-d7a177b87661
2024-03-28 05:59:41,318 - distributed.worker - INFO - Starting Worker plugin PreImport-a62f2dee-6524-4811-8c47-080516a3fcfb
2024-03-28 05:59:41,318 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,389 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:59:41,390 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,391 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41095
2024-03-28 05:59:41,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:59:41,415 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,415 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41095
2024-03-28 05:59:41,601 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:59:41,602 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40811
2024-03-28 05:59:41,602 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40811
2024-03-28 05:59:41,602 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43591
2024-03-28 05:59:41,602 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,602 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,602 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:59:41,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bqp2a66i
2024-03-28 05:59:41,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9879a6df-8fc0-4111-8cde-4d9159be9827
2024-03-28 05:59:41,603 - distributed.worker - INFO - Starting Worker plugin PreImport-03835fca-ebe5-4261-a091-79e095ecbef5
2024-03-28 05:59:41,603 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45f83e76-49dd-44e7-87e8-e216d4d69de8
2024-03-28 05:59:41,603 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,627 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:59:41,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36451
2024-03-28 05:59:41,628 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36451
2024-03-28 05:59:41,628 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38101
2024-03-28 05:59:41,628 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,628 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,628 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:59:41,628 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-34xeighq
2024-03-28 05:59:41,628 - distributed.worker - INFO - Starting Worker plugin PreImport-bd2a26a2-94fe-41a0-bb37-a4417da80daf
2024-03-28 05:59:41,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4849bca-d360-4982-9917-97e392697c72
2024-03-28 05:59:41,628 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04dc7a45-a3ff-4b6f-b7a9-5691ad3f8884
2024-03-28 05:59:41,628 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,633 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:59:41,634 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41961
2024-03-28 05:59:41,634 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41961
2024-03-28 05:59:41,634 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40569
2024-03-28 05:59:41,634 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,634 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,634 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:59:41,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zty6a1sk
2024-03-28 05:59:41,635 - distributed.worker - INFO - Starting Worker plugin PreImport-26b5d9ab-dc07-485c-bba7-f813e7840047
2024-03-28 05:59:41,635 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e51d3f1-8862-4b6a-858a-63027886377d
2024-03-28 05:59:41,644 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f535c759-eb8c-49d4-beb7-c6abeec6f858
2024-03-28 05:59:41,649 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,675 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:59:41,676 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38749
2024-03-28 05:59:41,676 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38749
2024-03-28 05:59:41,676 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35947
2024-03-28 05:59:41,676 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,676 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,676 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:59:41,676 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gqymfjd9
2024-03-28 05:59:41,676 - distributed.worker - INFO - Starting Worker plugin PreImport-85552786-e654-44dd-9f0f-a4164d02df9d
2024-03-28 05:59:41,676 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9004cac-f5c5-4aa0-a821-bc4754797f6e
2024-03-28 05:59:41,677 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e176f829-0b7e-4840-a664-9439e706a74c
2024-03-28 05:59:41,677 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,695 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:59:41,696 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,696 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41095
2024-03-28 05:59:41,708 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 05:59:41,709 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35715
2024-03-28 05:59:41,709 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35715
2024-03-28 05:59:41,709 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46341
2024-03-28 05:59:41,709 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,709 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,709 - distributed.worker - INFO -               Threads:                          1
2024-03-28 05:59:41,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6wlksztv
2024-03-28 05:59:41,709 - distributed.worker - INFO - Starting Worker plugin RMMSetup-561ab8b7-297e-4f09-998e-ac41a49ac91c
2024-03-28 05:59:41,710 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c99f88cd-5bf0-42cc-ba1c-77f43a90493c
2024-03-28 05:59:41,710 - distributed.worker - INFO - Starting Worker plugin PreImport-7c5c6d96-c59e-48f8-aa18-fa4d961461b4
2024-03-28 05:59:41,710 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,746 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:59:41,747 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,747 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,748 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41095
2024-03-28 05:59:41,826 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:59:41,827 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,827 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,828 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41095
2024-03-28 05:59:41,834 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:59:41,836 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,836 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,839 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41095
2024-03-28 05:59:41,869 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 05:59:41,872 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41095
2024-03-28 05:59:41,873 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 05:59:41,876 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41095
2024-03-28 05:59:41,914 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:59:41,914 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:59:41,914 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:59:41,914 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:59:41,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:59:41,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:59:41,915 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:59:41,916 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-28 05:59:41,926 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35597. Reason: nanny-close
2024-03-28 05:59:41,927 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43161. Reason: nanny-close
2024-03-28 05:59:41,929 - distributed.core - INFO - Connection to tcp://127.0.0.1:41095 has been closed.
2024-03-28 05:59:41,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45377. Reason: nanny-close
2024-03-28 05:59:41,930 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40811. Reason: nanny-close
2024-03-28 05:59:41,930 - distributed.core - INFO - Connection to tcp://127.0.0.1:41095 has been closed.
2024-03-28 05:59:41,930 - distributed.nanny - INFO - Worker closed
2024-03-28 05:59:41,931 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41961. Reason: nanny-close
2024-03-28 05:59:41,931 - distributed.nanny - INFO - Worker closed
2024-03-28 05:59:41,932 - distributed.core - INFO - Connection to tcp://127.0.0.1:41095 has been closed.
2024-03-28 05:59:41,932 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35715. Reason: nanny-close
2024-03-28 05:59:41,932 - distributed.core - INFO - Connection to tcp://127.0.0.1:41095 has been closed.
2024-03-28 05:59:41,932 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36451. Reason: nanny-close
2024-03-28 05:59:41,933 - distributed.nanny - INFO - Worker closed
2024-03-28 05:59:41,933 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38749. Reason: nanny-close
2024-03-28 05:59:41,934 - distributed.core - INFO - Connection to tcp://127.0.0.1:41095 has been closed.
2024-03-28 05:59:41,934 - distributed.nanny - INFO - Worker closed
2024-03-28 05:59:41,935 - distributed.core - INFO - Connection to tcp://127.0.0.1:41095 has been closed.
2024-03-28 05:59:41,935 - distributed.core - INFO - Connection to tcp://127.0.0.1:41095 has been closed.
2024-03-28 05:59:41,936 - distributed.nanny - INFO - Worker closed
2024-03-28 05:59:41,936 - distributed.nanny - INFO - Worker closed
2024-03-28 05:59:41,936 - distributed.nanny - INFO - Worker closed
2024-03-28 05:59:41,938 - distributed.core - INFO - Connection to tcp://127.0.0.1:41095 has been closed.
2024-03-28 05:59:41,941 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-03-28 06:00:24,266 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 06:00:24,266 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 06:00:24,271 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 06:00:24,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37407
2024-03-28 06:00:24,272 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37407
2024-03-28 06:00:24,272 - distributed.worker - INFO -           Worker name:                          0
2024-03-28 06:00:24,272 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35181
2024-03-28 06:00:24,272 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34719
2024-03-28 06:00:24,272 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:24,272 - distributed.worker - INFO -               Threads:                          1
2024-03-28 06:00:24,272 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-28 06:00:24,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ikg7fwbl
2024-03-28 06:00:24,272 - distributed.worker - INFO - Starting Worker plugin PreImport-39032a77-ebd1-47a8-aa20-d736ef6c0443
2024-03-28 06:00:24,277 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-03-28 06:00:24,278 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2aaeecf-6cfb-442a-a10e-4d72037a66a7
2024-03-28 06:00:24,278 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e56d71a-13e2-480d-acf4-1a5b7d7dc940
2024-03-28 06:00:24,278 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37407. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-03-28 06:00:24,278 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-28 06:00:24,281 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-03-28 06:00:29,649 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 06:00:29,649 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 06:00:29,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 06:00:29,773 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 06:00:29,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 06:00:29,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 06:00:29,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 06:00:29,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 06:00:29,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 06:00:29,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 06:00:30,044 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 06:00:30,044 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 06:00:30,077 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 06:00:30,077 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 06:00:30,239 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-28 06:00:30,239 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-28 06:00:30,416 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 06:00:30,417 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33625
2024-03-28 06:00:30,417 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33625
2024-03-28 06:00:30,417 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38967
2024-03-28 06:00:30,417 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,417 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,418 - distributed.worker - INFO -               Threads:                          1
2024-03-28 06:00:30,418 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 06:00:30,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rm2v5z8y
2024-03-28 06:00:30,418 - distributed.worker - INFO - Starting Worker plugin PreImport-0451aba3-87b6-4ea1-b965-85ab2b2e90eb
2024-03-28 06:00:30,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b0e31ea-efab-4004-9dd1-44dbd18de85d
2024-03-28 06:00:30,418 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c6c1aac-09a4-426d-b76a-0030446de8c3
2024-03-28 06:00:30,418 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,505 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 06:00:30,507 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,507 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,509 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36077
2024-03-28 06:00:30,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 06:00:30,518 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42055
2024-03-28 06:00:30,518 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42055
2024-03-28 06:00:30,518 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44943
2024-03-28 06:00:30,518 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,518 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,518 - distributed.worker - INFO -               Threads:                          1
2024-03-28 06:00:30,519 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 06:00:30,519 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8i9s1kad
2024-03-28 06:00:30,519 - distributed.worker - INFO - Starting Worker plugin RMMSetup-129620e0-38c1-4192-803d-849dc4e0c81b
2024-03-28 06:00:30,519 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a65cfe63-7d4d-4a13-bb28-5baa3aafb5d4
2024-03-28 06:00:30,525 - distributed.worker - INFO - Starting Worker plugin PreImport-5af1e671-d2b9-4bb5-b66a-15f63c94a767
2024-03-28 06:00:30,527 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,576 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 06:00:30,577 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39811
2024-03-28 06:00:30,577 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39811
2024-03-28 06:00:30,577 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42497
2024-03-28 06:00:30,577 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,577 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,577 - distributed.worker - INFO -               Threads:                          1
2024-03-28 06:00:30,578 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 06:00:30,578 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qg6q6lft
2024-03-28 06:00:30,578 - distributed.worker - INFO - Starting Worker plugin PreImport-f40d0ba2-ce18-44bc-ac72-557ce476be3c
2024-03-28 06:00:30,578 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d61b617-2cca-4da7-afe3-e96221eeb19a
2024-03-28 06:00:30,578 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0e281e4-6598-4184-a7f9-e5fda61ffa74
2024-03-28 06:00:30,578 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,611 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 06:00:30,612 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,612 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,614 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36077
2024-03-28 06:00:30,644 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 06:00:30,645 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,645 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36077
2024-03-28 06:00:30,657 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 06:00:30,658 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41871
2024-03-28 06:00:30,658 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41871
2024-03-28 06:00:30,658 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41853
2024-03-28 06:00:30,658 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,659 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,659 - distributed.worker - INFO -               Threads:                          1
2024-03-28 06:00:30,659 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 06:00:30,659 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8tf1t73s
2024-03-28 06:00:30,659 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d94e5cc-f62c-4637-bcc2-ac6aa00fc400
2024-03-28 06:00:30,659 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efbce92e-50a7-47d7-9080-1bf3ec9ac16a
2024-03-28 06:00:30,660 - distributed.worker - INFO - Starting Worker plugin PreImport-6ce4ce65-c6a8-4a9f-8079-8a43c9ff8c84
2024-03-28 06:00:30,660 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,699 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 06:00:30,700 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37113
2024-03-28 06:00:30,700 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37113
2024-03-28 06:00:30,700 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41465
2024-03-28 06:00:30,700 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,700 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,700 - distributed.worker - INFO -               Threads:                          1
2024-03-28 06:00:30,700 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 06:00:30,700 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yx6stbad
2024-03-28 06:00:30,701 - distributed.worker - INFO - Starting Worker plugin RMMSetup-214af22e-cc4c-47aa-a180-415a7e0d18df
2024-03-28 06:00:30,701 - distributed.worker - INFO - Starting Worker plugin PreImport-9c792b27-eba8-4e9b-aa39-c2fbc47adad2
2024-03-28 06:00:30,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4eb9c4b0-86e6-4a9e-8757-fdc3ecf7c90a
2024-03-28 06:00:30,701 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,728 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 06:00:30,729 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,729 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36077
2024-03-28 06:00:30,731 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 06:00:30,732 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35265
2024-03-28 06:00:30,732 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35265
2024-03-28 06:00:30,733 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39461
2024-03-28 06:00:30,733 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,733 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,733 - distributed.worker - INFO -               Threads:                          1
2024-03-28 06:00:30,733 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 06:00:30,733 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7zrto_kf
2024-03-28 06:00:30,733 - distributed.worker - INFO - Starting Worker plugin PreImport-a8528b3d-d3ce-400a-a3e5-3329c8c08e60
2024-03-28 06:00:30,733 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b6a91ef-78dc-4281-a645-8e29bee9d6c8
2024-03-28 06:00:30,733 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-622439bf-f0b7-4fca-89b5-6b84f983081f
2024-03-28 06:00:30,734 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,749 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 06:00:30,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43149
2024-03-28 06:00:30,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43149
2024-03-28 06:00:30,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40423
2024-03-28 06:00:30,750 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,750 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,750 - distributed.worker - INFO -               Threads:                          1
2024-03-28 06:00:30,750 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 06:00:30,751 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rtk5n5u4
2024-03-28 06:00:30,751 - distributed.worker - INFO - Starting Worker plugin PreImport-2611841e-4075-4cfc-900f-04a4b70b2d59
2024-03-28 06:00:30,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14e9e7f3-a38a-41fc-a9d7-0f618140f041
2024-03-28 06:00:30,751 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9b1661d9-9a5a-46ec-8939-e8b07f12c5b5
2024-03-28 06:00:30,751 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,766 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 06:00:30,767 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,767 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,768 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36077
2024-03-28 06:00:30,851 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 06:00:30,852 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,852 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,853 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36077
2024-03-28 06:00:30,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 06:00:30,857 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,857 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36077
2024-03-28 06:00:30,943 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-28 06:00:30,944 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33365
2024-03-28 06:00:30,944 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33365
2024-03-28 06:00:30,944 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38547
2024-03-28 06:00:30,944 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36077
2024-03-28 06:00:30,944 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:30,944 - distributed.worker - INFO -               Threads:                          1
2024-03-28 06:00:30,944 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-28 06:00:30,944 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d96v1wfu
2024-03-28 06:00:30,944 - distributed.worker - INFO - Starting Worker plugin PreImport-f7819a05-9643-40d0-b969-8f486a3e4878
2024-03-28 06:00:30,945 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97d3f952-a6e4-4dec-9886-625804b8e69d
2024-03-28 06:00:30,945 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-492bf85d-1970-4cbe-b132-a43a0a06104a
2024-03-28 06:00:30,945 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:31,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-28 06:00:31,010 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36077
2024-03-28 06:00:31,010 - distributed.worker - INFO - -------------------------------------------------
2024-03-28 06:00:31,012 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36077
2024-03-28 06:00:31,044 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33625. Reason: nanny-close
2024-03-28 06:00:31,045 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42055. Reason: nanny-close
2024-03-28 06:00:31,045 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39811. Reason: nanny-close
2024-03-28 06:00:31,046 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37113. Reason: nanny-close
2024-03-28 06:00:31,047 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41871. Reason: nanny-close
2024-03-28 06:00:31,047 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35265. Reason: nanny-close
2024-03-28 06:00:31,048 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43149. Reason: nanny-close
2024-03-28 06:00:31,048 - distributed.core - INFO - Connection to tcp://127.0.0.1:36077 has been closed.
2024-03-28 06:00:31,048 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33365. Reason: nanny-close
2024-03-28 06:00:31,048 - distributed.core - INFO - Connection to tcp://127.0.0.1:36077 has been closed.
2024-03-28 06:00:31,048 - distributed.core - INFO - Connection to tcp://127.0.0.1:36077 has been closed.
2024-03-28 06:00:31,048 - distributed.core - INFO - Connection to tcp://127.0.0.1:36077 has been closed.
2024-03-28 06:00:31,049 - distributed.nanny - INFO - Worker closed
2024-03-28 06:00:31,049 - distributed.core - INFO - Connection to tcp://127.0.0.1:36077 has been closed.
2024-03-28 06:00:31,049 - distributed.nanny - INFO - Worker closed
2024-03-28 06:00:31,050 - distributed.core - INFO - Connection to tcp://127.0.0.1:36077 has been closed.
2024-03-28 06:00:31,050 - distributed.nanny - INFO - Worker closed
2024-03-28 06:00:31,050 - distributed.core - INFO - Connection to tcp://127.0.0.1:36077 has been closed.
2024-03-28 06:00:31,050 - distributed.nanny - INFO - Worker closed
2024-03-28 06:00:31,050 - distributed.core - INFO - Connection to tcp://127.0.0.1:36077 has been closed.
2024-03-28 06:00:31,051 - distributed.nanny - INFO - Worker closed
2024-03-28 06:00:31,051 - distributed.nanny - INFO - Worker closed
2024-03-28 06:00:31,051 - distributed.nanny - INFO - Worker closed
2024-03-28 06:00:31,052 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-03-28 06:00:47,748 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-28 06:00:47,753 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] [1711605647.927456] [dgx13:44667:0]            sock.c:481  UCX  ERROR bind(fd=175 addr=0.0.0.0:56741) failed: Address already in use
[1711605647.927560] [dgx13:44667:0]            sock.c:481  UCX  ERROR bind(fd=175 addr=0.0.0.0:54818) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] 2024-03-28 06:02:47,530 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-28 06:02:47,793 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] 2024-03-28 06:02:54,049 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-28 06:02:54,058 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2024-03-28 06:02:54,060 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x14ef7e3d2d60>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-28 06:02:56,064 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] 2024-03-28 06:03:14,535 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-28 06:03:14,542 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x14b627dd4d90>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-28 06:03:16,546 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-03-28 06:04:15,656 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-28 06:04:15,659 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
