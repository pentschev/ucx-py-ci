============================= test session starts ==============================
platform linux -- Python 3.9.19, pytest-8.1.1, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.6
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-03-22 06:02:53,930 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:02:53,935 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46551 instead
  warnings.warn(
2024-03-22 06:02:53,939 - distributed.scheduler - INFO - State start
2024-03-22 06:02:54,166 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:02:54,168 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-22 06:02:54,169 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46551/status
2024-03-22 06:02:54,170 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:02:54,376 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44073'
2024-03-22 06:02:54,393 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44065'
2024-03-22 06:02:54,396 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37031'
2024-03-22 06:02:54,404 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41037'
2024-03-22 06:02:56,518 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:02:56,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:02:56,523 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:02:56,524 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46539
2024-03-22 06:02:56,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:02:56,524 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46539
2024-03-22 06:02:56,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:02:56,524 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32805
2024-03-22 06:02:56,524 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-22 06:02:56,524 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,524 - distributed.worker - INFO -               Threads:                          4
2024-03-22 06:02:56,524 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-22 06:02:56,524 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-71ezz7py
2024-03-22 06:02:56,525 - distributed.worker - INFO - Starting Worker plugin PreImport-a392c926-7933-49b5-9dd4-db8b4e4b4a14
2024-03-22 06:02:56,525 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9630eaaf-845b-4b01-87e3-7d995585bc0b
2024-03-22 06:02:56,525 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e458939-cdb0-4de1-a835-3b35cef9bb1a
2024-03-22 06:02:56,525 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:02:56,530 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41553
2024-03-22 06:02:56,530 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41553
2024-03-22 06:02:56,530 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43451
2024-03-22 06:02:56,530 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-22 06:02:56,530 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,530 - distributed.worker - INFO -               Threads:                          4
2024-03-22 06:02:56,530 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-22 06:02:56,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-pv2xofd0
2024-03-22 06:02:56,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2511dcde-3305-486f-ac66-f1b52c6433f0
2024-03-22 06:02:56,531 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0fb4614-2c69-4524-b3fc-c3b53cf152f0
2024-03-22 06:02:56,531 - distributed.worker - INFO - Starting Worker plugin PreImport-5dc8056b-e181-4fed-8b51-5db476a9b7bf
2024-03-22 06:02:56,531 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,551 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:02:56,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:02:56,557 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:02:56,558 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40175
2024-03-22 06:02:56,558 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40175
2024-03-22 06:02:56,558 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37825
2024-03-22 06:02:56,558 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-22 06:02:56,558 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,558 - distributed.worker - INFO -               Threads:                          4
2024-03-22 06:02:56,558 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-22 06:02:56,558 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-gl5k05qw
2024-03-22 06:02:56,558 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b65b7007-c02a-4b7c-a886-3ae93a290727
2024-03-22 06:02:56,558 - distributed.worker - INFO - Starting Worker plugin PreImport-305a2b52-4b63-4815-baf0-6150ffafa94c
2024-03-22 06:02:56,558 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-71490355-5a44-40d9-869b-5420a92ee9c8
2024-03-22 06:02:56,559 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,567 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:02:56,567 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:02:56,572 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:02:56,573 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33165
2024-03-22 06:02:56,573 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33165
2024-03-22 06:02:56,573 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42067
2024-03-22 06:02:56,573 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-22 06:02:56,573 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,573 - distributed.worker - INFO -               Threads:                          4
2024-03-22 06:02:56,573 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-22 06:02:56,573 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-719c5fa1
2024-03-22 06:02:56,573 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2583e185-260a-4af8-a11b-cb66d4e1d71e
2024-03-22 06:02:56,573 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8521bc8-bf7a-45cd-bbc6-094129adf2fb
2024-03-22 06:02:56,574 - distributed.worker - INFO - Starting Worker plugin PreImport-a467d61b-a413-46d2-b8b1-8bf83c124d7a
2024-03-22 06:02:56,574 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,590 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46539', status: init, memory: 0, processing: 0>
2024-03-22 06:02:56,602 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46539
2024-03-22 06:02:56,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39792
2024-03-22 06:02:56,603 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:02:56,604 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-22 06:02:56,604 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-22 06:02:56,636 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41553', status: init, memory: 0, processing: 0>
2024-03-22 06:02:56,637 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41553
2024-03-22 06:02:56,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39804
2024-03-22 06:02:56,638 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:02:56,639 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-22 06:02:56,639 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-22 06:02:56,662 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33165', status: init, memory: 0, processing: 0>
2024-03-22 06:02:56,663 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33165
2024-03-22 06:02:56,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39812
2024-03-22 06:02:56,664 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40175', status: init, memory: 0, processing: 0>
2024-03-22 06:02:56,664 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:02:56,665 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40175
2024-03-22 06:02:56,665 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39810
2024-03-22 06:02:56,665 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-22 06:02:56,665 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-22 06:02:56,666 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:02:56,667 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-22 06:02:56,667 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:02:56,669 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-22 06:02:57,138 - distributed.scheduler - INFO - Receive client connection: Client-d16272c7-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:02:57,139 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39814
2024-03-22 06:02:57,149 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-22 06:02:57,149 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-22 06:02:57,149 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-22 06:02:57,149 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-22 06:02:57,154 - distributed.scheduler - INFO - Remove client Client-d16272c7-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:02:57,154 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39814; closing.
2024-03-22 06:02:57,154 - distributed.scheduler - INFO - Remove client Client-d16272c7-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:02:57,155 - distributed.scheduler - INFO - Close client connection: Client-d16272c7-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:02:57,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44073'. Reason: nanny-close
2024-03-22 06:02:57,156 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:02:57,157 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44065'. Reason: nanny-close
2024-03-22 06:02:57,157 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:02:57,157 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37031'. Reason: nanny-close
2024-03-22 06:02:57,158 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41553. Reason: nanny-close
2024-03-22 06:02:57,158 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:02:57,158 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41037'. Reason: nanny-close
2024-03-22 06:02:57,158 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46539. Reason: nanny-close
2024-03-22 06:02:57,158 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:02:57,159 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40175. Reason: nanny-close
2024-03-22 06:02:57,159 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33165. Reason: nanny-close
2024-03-22 06:02:57,160 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39792; closing.
2024-03-22 06:02:57,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-22 06:02:57,160 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-22 06:02:57,160 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46539', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087377.1608164')
2024-03-22 06:02:57,161 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-22 06:02:57,161 - distributed.nanny - INFO - Worker closed
2024-03-22 06:02:57,161 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39812; closing.
2024-03-22 06:02:57,162 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-22 06:02:57,162 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39804; closing.
2024-03-22 06:02:57,162 - distributed.nanny - INFO - Worker closed
2024-03-22 06:02:57,162 - distributed.nanny - INFO - Worker closed
2024-03-22 06:02:57,163 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33165', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087377.1630073')
2024-03-22 06:02:57,163 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41553', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087377.1633713')
2024-03-22 06:02:57,163 - distributed.nanny - INFO - Worker closed
2024-03-22 06:02:57,163 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39810; closing.
2024-03-22 06:02:57,163 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:39812>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:39812>: Stream is closed
2024-03-22 06:02:57,166 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40175', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087377.1668377')
2024-03-22 06:02:57,167 - distributed.scheduler - INFO - Lost all workers
2024-03-22 06:02:57,872 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:02:57,872 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:02:57,873 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:02:57,874 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-22 06:02:57,874 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-03-22 06:03:00,185 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:03:00,190 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41389 instead
  warnings.warn(
2024-03-22 06:03:00,193 - distributed.scheduler - INFO - State start
2024-03-22 06:03:00,737 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:03:00,738 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-22 06:03:00,740 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:03:00,741 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 629, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-22 06:03:01,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37263'
2024-03-22 06:03:01,449 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46137'
2024-03-22 06:03:01,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33457'
2024-03-22 06:03:01,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42177'
2024-03-22 06:03:01,475 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45563'
2024-03-22 06:03:01,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32909'
2024-03-22 06:03:01,494 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43531'
2024-03-22 06:03:01,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44291'
2024-03-22 06:03:01,867 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37263'. Reason: nanny-close
2024-03-22 06:03:01,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46137'. Reason: nanny-close
2024-03-22 06:03:01,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33457'. Reason: nanny-close
2024-03-22 06:03:01,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42177'. Reason: nanny-close
2024-03-22 06:03:01,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45563'. Reason: nanny-close
2024-03-22 06:03:01,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32909'. Reason: nanny-close
2024-03-22 06:03:01,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43531'. Reason: nanny-close
2024-03-22 06:03:01,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44291'. Reason: nanny-close
2024-03-22 06:03:03,520 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:03,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:03,526 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:03,526 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:03,527 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:03,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:03,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:03,528 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33137
2024-03-22 06:03:03,528 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33137
2024-03-22 06:03:03,528 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45487
2024-03-22 06:03:03,528 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:03,528 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:03,528 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:03,528 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:03,528 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8zepiqtx
2024-03-22 06:03:03,529 - distributed.worker - INFO - Starting Worker plugin PreImport-f21edbe8-c771-466a-864a-accf20d0b6c7
2024-03-22 06:03:03,529 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df56e7d6-9054-47b5-b050-8a8f96958b2f
2024-03-22 06:03:03,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:03,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:03,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39235
2024-03-22 06:03:03,534 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39235
2024-03-22 06:03:03,534 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36197
2024-03-22 06:03:03,534 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:03,534 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:03,535 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:03,535 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:03,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u18y2vln
2024-03-22 06:03:03,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41169
2024-03-22 06:03:03,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41169
2024-03-22 06:03:03,535 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8c183051-ce17-480a-85c1-f919506a1ec8
2024-03-22 06:03:03,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40887
2024-03-22 06:03:03,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:03,535 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:03,535 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:03,535 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:03,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_a3918gj
2024-03-22 06:03:03,535 - distributed.worker - INFO - Starting Worker plugin PreImport-766a29e7-4664-46b7-858b-ad1397dd3972
2024-03-22 06:03:03,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cbf5e3f5-f0b8-443d-9cd8-770ce5a2a750
2024-03-22 06:03:03,536 - distributed.worker - INFO - Starting Worker plugin PreImport-546157b5-60cf-4c09-8f62-c819e48877ca
2024-03-22 06:03:03,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6186408-61e9-4d58-a5f1-e8c0b5398eb8
2024-03-22 06:03:03,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:03,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:03,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:03,548 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46631
2024-03-22 06:03:03,548 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46631
2024-03-22 06:03:03,548 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35911
2024-03-22 06:03:03,549 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:03,549 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:03,549 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:03,549 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:03,549 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f5sb5cqx
2024-03-22 06:03:03,549 - distributed.worker - INFO - Starting Worker plugin PreImport-4bdee5ed-ed92-48ac-a1ef-17762692c682
2024-03-22 06:03:03,549 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a94a2bc-4e4d-405c-a6a2-41a265b13478
2024-03-22 06:03:03,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:03,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:03,627 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:03,627 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:03,628 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:03,628 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:03,632 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:03,633 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:03,633 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38989
2024-03-22 06:03:03,633 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38989
2024-03-22 06:03:03,633 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41413
2024-03-22 06:03:03,633 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:03,633 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:03,633 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:03,633 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:03,633 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:03,633 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vupfda4d
2024-03-22 06:03:03,633 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab7be044-a654-4797-81ad-a884893df60c
2024-03-22 06:03:03,633 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e624fac5-b65b-4a5b-8868-933897ba4c38
2024-03-22 06:03:03,634 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45793
2024-03-22 06:03:03,634 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45793
2024-03-22 06:03:03,634 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40781
2024-03-22 06:03:03,634 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:03,634 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:03,634 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:03,634 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:03,634 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37205
2024-03-22 06:03:03,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jv0jtxdi
2024-03-22 06:03:03,634 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37205
2024-03-22 06:03:03,634 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45007
2024-03-22 06:03:03,634 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:03,634 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:03,634 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:03,634 - distributed.worker - INFO - Starting Worker plugin PreImport-b18458fb-98bf-4259-9180-b81699eb5a31
2024-03-22 06:03:03,634 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:03,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_wmo_i2p
2024-03-22 06:03:03,634 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-221fb9a2-704a-4f3f-96a0-5187a1a92300
2024-03-22 06:03:03,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5583d74-7ded-4b3f-aaf2-422a03d9b993
2024-03-22 06:03:03,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14a67c78-154a-4947-8953-245aeade5d61
2024-03-22 06:03:03,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:03,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:03,820 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:03,821 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40215
2024-03-22 06:03:03,821 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40215
2024-03-22 06:03:03,821 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34393
2024-03-22 06:03:03,821 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:03,821 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:03,821 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:03,821 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:03,821 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-647biqri
2024-03-22 06:03:03,822 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3d351df-6415-4fc8-b947-3c4d4c68339e
2024-03-22 06:03:03,822 - distributed.worker - INFO - Starting Worker plugin PreImport-15ccc266-03e1-4fe5-94aa-f4969da26420
2024-03-22 06:03:03,822 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e75d236-622b-4820-8de3-a522a735cde3
2024-03-22 06:03:05,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-30e7c322-b6f0-4a3f-8cd7-fbc07652130c
2024-03-22 06:03:05,414 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:05,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:05,952 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:05,953 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:05,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:05,992 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:03:05,993 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33137. Reason: nanny-close
2024-03-22 06:03:05,995 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:03:05,997 - distributed.nanny - INFO - Worker closed
2024-03-22 06:03:06,178 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8f87f525-76e8-46f7-86a7-37b6bfa0d000
2024-03-22 06:03:06,179 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:06,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:06,216 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:06,216 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:06,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:06,246 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:03:06,247 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41169. Reason: nanny-close
2024-03-22 06:03:06,250 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:03:06,252 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:35838 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:03:07,500 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45091 parent=44892 started daemon>
2024-03-22 06:03:07,500 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45087 parent=44892 started daemon>
2024-03-22 06:03:07,500 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45084 parent=44892 started daemon>
2024-03-22 06:03:07,501 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45079 parent=44892 started daemon>
2024-03-22 06:03:07,501 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45075 parent=44892 started daemon>
2024-03-22 06:03:07,501 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45065 parent=44892 started daemon>
2024-03-22 06:03:07,501 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45060 parent=44892 started daemon>
2024-03-22 06:03:08,060 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45087 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-03-22 06:03:10,793 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:03:10,799 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41927 instead
  warnings.warn(
2024-03-22 06:03:10,804 - distributed.scheduler - INFO - State start
2024-03-22 06:03:10,806 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-647biqri', purging
2024-03-22 06:03:10,807 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-f5sb5cqx', purging
2024-03-22 06:03:10,807 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-u18y2vln', purging
2024-03-22 06:03:10,807 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_wmo_i2p', purging
2024-03-22 06:03:10,808 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vupfda4d', purging
2024-03-22 06:03:10,808 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jv0jtxdi', purging
2024-03-22 06:03:11,079 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:03:11,080 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-22 06:03:11,080 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:03:11,081 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 629, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-22 06:03:11,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43679'
2024-03-22 06:03:11,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38841'
2024-03-22 06:03:11,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42607'
2024-03-22 06:03:11,557 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45001'
2024-03-22 06:03:11,565 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41299'
2024-03-22 06:03:11,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37941'
2024-03-22 06:03:11,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38359'
2024-03-22 06:03:11,591 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34997'
2024-03-22 06:03:13,588 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:13,588 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:13,599 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:13,601 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33195
2024-03-22 06:03:13,601 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33195
2024-03-22 06:03:13,601 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38287
2024-03-22 06:03:13,601 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:13,601 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:13,601 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:13,601 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:13,601 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u18ja1cc
2024-03-22 06:03:13,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2cc3e3f8-3e50-4ff1-9c2e-3a16dd9fb251
2024-03-22 06:03:13,602 - distributed.worker - INFO - Starting Worker plugin PreImport-754bfd65-e4bd-4e5e-aca8-b997ea34ea2e
2024-03-22 06:03:13,602 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0df719c-0e7d-43b2-9a20-4ab1c5001086
2024-03-22 06:03:13,894 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:13,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:13,900 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:13,900 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:13,903 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:13,905 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45275
2024-03-22 06:03:13,905 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45275
2024-03-22 06:03:13,905 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44421
2024-03-22 06:03:13,905 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:13,905 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:13,905 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:13,905 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:13,905 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w98wpnk3
2024-03-22 06:03:13,906 - distributed.worker - INFO - Starting Worker plugin PreImport-39023988-fbbb-4aac-b242-67a0cc7af6d5
2024-03-22 06:03:13,906 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ea81248-839c-41b9-bebd-0fbf3d11f001
2024-03-22 06:03:13,908 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:13,910 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38103
2024-03-22 06:03:13,910 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38103
2024-03-22 06:03:13,910 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41825
2024-03-22 06:03:13,910 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:13,910 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:13,910 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:13,910 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:13,911 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xwe5g_bg
2024-03-22 06:03:13,911 - distributed.worker - INFO - Starting Worker plugin PreImport-ea11413d-8438-490e-92c2-70e1dd9d7b6d
2024-03-22 06:03:13,911 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf72bc3c-102a-4263-9f56-2b4749c01498
2024-03-22 06:03:13,911 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:13,912 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:13,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:13,914 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:13,922 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:13,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:13,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:13,924 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37359
2024-03-22 06:03:13,924 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37359
2024-03-22 06:03:13,924 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40759
2024-03-22 06:03:13,924 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:13,924 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:13,924 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:13,924 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:13,924 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dhpvch3c
2024-03-22 06:03:13,924 - distributed.worker - INFO - Starting Worker plugin PreImport-fe64ebd6-6da8-4e13-93cf-ab75cc927852
2024-03-22 06:03:13,925 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65450e1a-0c18-41d1-97d3-f930ae06f7c4
2024-03-22 06:03:13,926 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:13,928 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37523
2024-03-22 06:03:13,929 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37523
2024-03-22 06:03:13,929 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40621
2024-03-22 06:03:13,929 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:13,929 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:13,929 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:13,929 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:13,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-95c8c8oe
2024-03-22 06:03:13,930 - distributed.worker - INFO - Starting Worker plugin PreImport-e3e9cc72-abfe-4cfc-81cc-0d41f44466fa
2024-03-22 06:03:13,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:13,930 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:13,930 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7bcab84b-12da-4cc9-800f-e6319323c8aa
2024-03-22 06:03:13,930 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:13,930 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ed50665-3118-4abb-bf36-717248d2ecd4
2024-03-22 06:03:13,932 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36171
2024-03-22 06:03:13,932 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36171
2024-03-22 06:03:13,932 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44695
2024-03-22 06:03:13,932 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:13,932 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:13,932 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:13,932 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:13,932 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1s0yn0ot
2024-03-22 06:03:13,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:13,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:13,933 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-68c6bf45-649d-4c61-ae88-67d30cd572e2
2024-03-22 06:03:13,935 - distributed.worker - INFO - Starting Worker plugin PreImport-73f826bb-bbda-4af1-bc43-23ba83bb44a2
2024-03-22 06:03:13,935 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9e42081-4025-4337-b270-e8c05c9d710e
2024-03-22 06:03:13,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:13,940 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37379
2024-03-22 06:03:13,940 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37379
2024-03-22 06:03:13,940 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41681
2024-03-22 06:03:13,940 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:13,940 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:13,940 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:13,940 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:13,940 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6bry907c
2024-03-22 06:03:13,940 - distributed.worker - INFO - Starting Worker plugin PreImport-b667c237-05d1-41f3-a7ec-c5fba17c4be3
2024-03-22 06:03:13,941 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be53b7c7-5141-493b-b667-57de802b3526
2024-03-22 06:03:13,943 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:13,946 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33009
2024-03-22 06:03:13,946 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33009
2024-03-22 06:03:13,946 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39137
2024-03-22 06:03:13,946 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:13,946 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:13,946 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:13,946 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:13,946 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-urpq_tzi
2024-03-22 06:03:13,947 - distributed.worker - INFO - Starting Worker plugin PreImport-384c5b00-af8f-4201-bc63-0850cc85862e
2024-03-22 06:03:13,947 - distributed.worker - INFO - Starting Worker plugin RMMSetup-442d268b-6ec8-47fd-a1cd-87fedde07841
2024-03-22 06:03:14,325 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:14,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:14,352 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:14,352 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:14,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:16,142 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d94a7126-7677-4d5f-bcc7-8d6d678d61d5
2024-03-22 06:03:16,143 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,174 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:16,175 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:16,175 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,176 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:16,209 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cedb436d-60d1-4074-a577-5439c521bc5a
2024-03-22 06:03:16,209 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,213 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7686d30-8364-4fdc-81c1-d572849a0b58
2024-03-22 06:03:16,214 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:16,243 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:16,244 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:16,244 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,245 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:16,245 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:16,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:16,317 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,326 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,338 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f51df522-9d56-4a8b-bcdf-737a2d43e460
2024-03-22 06:03:16,338 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4fafd4e-938e-47a3-befc-baaeb2d0adb7
2024-03-22 06:03:16,338 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,338 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,350 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:16,351 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:16,351 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:16,355 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:16,356 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:16,356 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:16,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:16,366 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:16,366 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,368 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:16,373 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:16,374 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:16,374 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:16,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:16,441 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,441 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,442 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,443 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,445 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,445 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,445 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,445 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,445 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,445 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,446 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,446 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:03:16,455 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43679'. Reason: nanny-close
2024-03-22 06:03:16,455 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:03:16,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33195. Reason: scheduler-close
2024-03-22 06:03:16,456 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38841'. Reason: nanny-close
2024-03-22 06:03:16,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45275. Reason: scheduler-close
2024-03-22 06:03:16,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38103. Reason: scheduler-close
2024-03-22 06:03:16,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37359. Reason: scheduler-close
2024-03-22 06:03:16,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36171. Reason: scheduler-close
2024-03-22 06:03:16,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33009. Reason: scheduler-close
2024-03-22 06:03:16,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37523. Reason: scheduler-close
2024-03-22 06:03:16,456 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:03:16,456 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37379. Reason: scheduler-close
2024-03-22 06:03:16,457 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42607'. Reason: nanny-close
2024-03-22 06:03:16,457 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:03:16,457 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45001'. Reason: nanny-close
2024-03-22 06:03:16,458 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:03:16,458 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41299'. Reason: nanny-close
2024-03-22 06:03:16,458 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:03:16,458 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37941'. Reason: nanny-close
2024-03-22 06:03:16,459 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:03:16,459 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38359'. Reason: nanny-close
2024-03-22 06:03:16,460 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:03:16,458 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53650 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53650 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-03-22 06:03:16,460 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34997'. Reason: nanny-close
2024-03-22 06:03:16,458 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53662 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53662 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-03-22 06:03:16,460 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:03:16,458 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53688 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53688 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-03-22 06:03:16,458 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53666 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53666 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-03-22 06:03:16,458 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53660 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53660 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-03-22 06:03:16,459 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53676 remote=tcp://127.0.0.1:9369>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Worker->Scheduler local=tcp://127.0.0.1:53676 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-03-22 06:03:19,682 - distributed.nanny - WARNING - Worker process still alive after 3.199997711181641 seconds, killing
2024-03-22 06:03:19,682 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2024-03-22 06:03:19,683 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2024-03-22 06:03:19,683 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2024-03-22 06:03:19,683 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2024-03-22 06:03:19,684 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2024-03-22 06:03:19,684 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2024-03-22 06:03:19,684 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2024-03-22 06:03:20,035 - distributed.nanny - INFO - Worker process 45340 was killed by signal 9
2024-03-22 06:03:20,036 - distributed.nanny - INFO - Worker process 45329 was killed by signal 9
2024-03-22 06:03:20,036 - distributed.nanny - INFO - Worker process 45352 was killed by signal 9
2024-03-22 06:03:20,062 - distributed.nanny - INFO - Worker process 45357 was killed by signal 9
2024-03-22 06:03:20,084 - distributed.nanny - INFO - Worker process 45360 was killed by signal 9
2024-03-22 06:03:20,105 - distributed.nanny - INFO - Worker process 45349 was killed by signal 9
2024-03-22 06:03:20,129 - distributed.nanny - INFO - Worker process 45334 was killed by signal 9
2024-03-22 06:03:20,150 - distributed.nanny - INFO - Worker process 45344 was killed by signal 9
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-03-22 06:03:22,596 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:03:22,601 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36859 instead
  warnings.warn(
2024-03-22 06:03:22,605 - distributed.scheduler - INFO - State start
2024-03-22 06:03:22,739 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:03:22,740 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-22 06:03:22,741 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:03:22,742 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 629, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-22 06:03:23,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39741'
2024-03-22 06:03:23,491 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35321'
2024-03-22 06:03:23,500 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38911'
2024-03-22 06:03:23,514 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40785'
2024-03-22 06:03:23,516 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45145'
2024-03-22 06:03:23,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39001'
2024-03-22 06:03:23,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42437'
2024-03-22 06:03:23,546 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33703'
2024-03-22 06:03:25,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:25,536 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:25,540 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:25,541 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37325
2024-03-22 06:03:25,541 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37325
2024-03-22 06:03:25,542 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38549
2024-03-22 06:03:25,542 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:25,542 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:25,542 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:25,542 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:25,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8s9k22bt
2024-03-22 06:03:25,542 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f87428ca-6a47-41bb-8834-0873556868dd
2024-03-22 06:03:25,542 - distributed.worker - INFO - Starting Worker plugin RMMSetup-36912ef3-1401-4868-af57-a80fc5981e02
2024-03-22 06:03:25,551 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:25,551 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:25,556 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:25,557 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42657
2024-03-22 06:03:25,557 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42657
2024-03-22 06:03:25,557 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40315
2024-03-22 06:03:25,557 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:25,557 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:25,557 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:25,557 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:25,557 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rxg9vjkt
2024-03-22 06:03:25,557 - distributed.worker - INFO - Starting Worker plugin PreImport-e81192fe-4dec-45b1-9856-d3b5575573f0
2024-03-22 06:03:25,557 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b725e8ee-ff0e-42bb-908f-2d5406817ff5
2024-03-22 06:03:25,625 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:25,625 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:25,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:25,627 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:25,638 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:25,639 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:25,640 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42221
2024-03-22 06:03:25,640 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42221
2024-03-22 06:03:25,640 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44097
2024-03-22 06:03:25,641 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:25,641 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:25,641 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:25,641 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:25,641 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-expdetkf
2024-03-22 06:03:25,641 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33129
2024-03-22 06:03:25,641 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33129
2024-03-22 06:03:25,641 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37237
2024-03-22 06:03:25,641 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:25,641 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:25,641 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:25,641 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-244e2ae9-5739-477a-8a0c-44bb3f1ac685
2024-03-22 06:03:25,642 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:25,642 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hgb2l5ot
2024-03-22 06:03:25,642 - distributed.worker - INFO - Starting Worker plugin RMMSetup-03314ed2-b858-4b88-9425-699f90f37626
2024-03-22 06:03:25,642 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b890d61-01b4-4eb3-a9d8-91f8924db366
2024-03-22 06:03:25,643 - distributed.worker - INFO - Starting Worker plugin PreImport-6b4500aa-af94-4f85-b6e6-4c2fc0f266ef
2024-03-22 06:03:25,643 - distributed.worker - INFO - Starting Worker plugin RMMSetup-891c1809-8580-4b1d-a4f8-9679d6b522ba
2024-03-22 06:03:25,658 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:25,658 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:25,667 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:25,668 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38231
2024-03-22 06:03:25,669 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38231
2024-03-22 06:03:25,669 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38211
2024-03-22 06:03:25,669 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:25,669 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:25,669 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:25,669 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:25,669 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-82he5gdo
2024-03-22 06:03:25,669 - distributed.worker - INFO - Starting Worker plugin PreImport-75056238-fc95-486f-a91f-0ec50bf756d2
2024-03-22 06:03:25,670 - distributed.worker - INFO - Starting Worker plugin RMMSetup-39a98654-e1d2-49ca-9db4-99b19d9b8460
2024-03-22 06:03:25,690 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:25,690 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:25,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:25,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:25,697 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:25,698 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42237
2024-03-22 06:03:25,699 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42237
2024-03-22 06:03:25,699 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32863
2024-03-22 06:03:25,699 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:25,699 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:25,699 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:25,699 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:25,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f_fvs53g
2024-03-22 06:03:25,699 - distributed.worker - INFO - Starting Worker plugin PreImport-f16e38a2-bac1-4562-864a-7d8c0fd38c3d
2024-03-22 06:03:25,699 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9032187e-7b48-4186-9367-70b6df52540b
2024-03-22 06:03:25,699 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9762bab-9c0e-4684-ad38-e5f126dd72ae
2024-03-22 06:03:25,700 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:25,702 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41549
2024-03-22 06:03:25,702 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41549
2024-03-22 06:03:25,702 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45329
2024-03-22 06:03:25,702 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:25,702 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:25,702 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:25,702 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:25,702 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2d_0ybwt
2024-03-22 06:03:25,702 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cb755f16-8434-47d4-b3a1-9c24a7c73a90
2024-03-22 06:03:25,737 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:25,738 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:25,745 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:25,746 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39179
2024-03-22 06:03:25,746 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39179
2024-03-22 06:03:25,746 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37529
2024-03-22 06:03:25,746 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:25,746 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:25,746 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:25,747 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:25,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w6167foz
2024-03-22 06:03:25,747 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63294a5e-ad0a-47cd-8c3f-6fa3263d9980
2024-03-22 06:03:25,747 - distributed.worker - INFO - Starting Worker plugin PreImport-87129431-7df8-45c8-83ff-afde2d116aca
2024-03-22 06:03:25,748 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12af7a52-8399-4881-9428-fbf64db1f971
2024-03-22 06:03:28,136 - distributed.worker - INFO - Starting Worker plugin PreImport-3180250d-a969-4306-b4db-833cbd869111
2024-03-22 06:03:28,137 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,149 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-51da8590-f81d-4f4d-8ef4-5093a4cc127c
2024-03-22 06:03:28,151 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,167 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:28,166 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-22 06:03:28,167 - distributed.worker - INFO - Starting Worker plugin PreImport-28d439c7-17bd-4014-b416-6802924b61af
2024-03-22 06:03:28,168 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:28,168 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,168 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42221. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 06:03:28,168 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:03:28,169 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:28,171 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,172 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:03:28,178 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9834617d-6c5f-48f7-9a81-97ed71aaa248
2024-03-22 06:03:28,180 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:28,184 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:28,184 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,185 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:28,187 - distributed.worker - INFO - Starting Worker plugin PreImport-066844e7-1ea4-4950-a9c1-f46067d36f78
2024-03-22 06:03:28,187 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a13b876-c493-4131-bbc1-d42817289a3a
2024-03-22 06:03:28,188 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,193 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:03:28,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39741'. Reason: nanny-instantiate-failed
2024-03-22 06:03:28,197 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 06:03:28,198 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:28,199 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:28,199 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:28,201 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,209 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,212 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:28,213 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:28,213 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:28,217 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:28,218 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:28,218 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:28,228 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:28,229 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:28,229 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,231 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:28,233 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:28,233 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:28,233 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:28,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:28,394 - distributed.nanny - INFO - Worker process 45612 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:03:28,398 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45643 parent=45444 started daemon>
2024-03-22 06:03:28,399 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45639 parent=45444 started daemon>
2024-03-22 06:03:28,399 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45635 parent=45444 started daemon>
2024-03-22 06:03:28,399 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45631 parent=45444 started daemon>
2024-03-22 06:03:28,399 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45627 parent=45444 started daemon>
2024-03-22 06:03:28,399 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45622 parent=45444 started daemon>
2024-03-22 06:03:28,399 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45617 parent=45444 started daemon>
2024-03-22 06:03:28,597 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45639 exit status was already read will report exitcode 255
2024-03-22 06:03:28,699 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45627 exit status was already read will report exitcode 255
2024-03-22 06:03:28,742 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45622 exit status was already read will report exitcode 255
2024-03-22 06:03:28,775 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45635 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-03-22 06:03:40,959 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:03:40,964 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35957 instead
  warnings.warn(
2024-03-22 06:03:40,969 - distributed.scheduler - INFO - State start
2024-03-22 06:03:40,970 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2d_0ybwt', purging
2024-03-22 06:03:40,971 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-8s9k22bt', purging
2024-03-22 06:03:40,971 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rxg9vjkt', purging
2024-03-22 06:03:40,971 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-f_fvs53g', purging
2024-03-22 06:03:40,972 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-82he5gdo', purging
2024-03-22 06:03:40,972 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-hgb2l5ot', purging
2024-03-22 06:03:40,972 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-w6167foz', purging
2024-03-22 06:03:41,046 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:03:41,047 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-22 06:03:41,047 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:03:41,048 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 629, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-22 06:03:41,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43785'
2024-03-22 06:03:41,435 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39637'
2024-03-22 06:03:41,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38943'
2024-03-22 06:03:41,457 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43057'
2024-03-22 06:03:41,460 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42233'
2024-03-22 06:03:41,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36245'
2024-03-22 06:03:41,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44115'
2024-03-22 06:03:41,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33085'
2024-03-22 06:03:43,568 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:43,568 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:43,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:43,572 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:43,572 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:43,572 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:43,573 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:43,574 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:43,574 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:43,575 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39303
2024-03-22 06:03:43,575 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39303
2024-03-22 06:03:43,575 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42853
2024-03-22 06:03:43,575 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:43,575 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:43,575 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:43,575 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:43,575 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_8drfz5h
2024-03-22 06:03:43,575 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:43,576 - distributed.worker - INFO - Starting Worker plugin PreImport-d4a5a25b-3240-47c4-bec8-ae3a92408117
2024-03-22 06:03:43,576 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:43,576 - distributed.worker - INFO - Starting Worker plugin RMMSetup-919f6f1b-5fb2-4291-b04b-7a513b243197
2024-03-22 06:03:43,577 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:43,578 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:43,578 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35637
2024-03-22 06:03:43,578 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35637
2024-03-22 06:03:43,578 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44137
2024-03-22 06:03:43,579 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:43,579 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:43,579 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:43,579 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:43,579 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42681
2024-03-22 06:03:43,579 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zypah3o_
2024-03-22 06:03:43,579 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42681
2024-03-22 06:03:43,579 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33059
2024-03-22 06:03:43,579 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:43,579 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:43,579 - distributed.worker - INFO - Starting Worker plugin PreImport-c1f9b6a4-bd25-45b6-adc2-ba81803dbe0a
2024-03-22 06:03:43,579 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:43,579 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:43,579 - distributed.worker - INFO - Starting Worker plugin RMMSetup-799ce959-853d-4b7e-87a4-0c14e09f39b3
2024-03-22 06:03:43,579 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x4_12bt8
2024-03-22 06:03:43,579 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8f7dd1f-8150-4ac1-a8d1-d1e36fc08919
2024-03-22 06:03:43,581 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:43,582 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41605
2024-03-22 06:03:43,582 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41605
2024-03-22 06:03:43,582 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40257
2024-03-22 06:03:43,582 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:43,582 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:43,582 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:43,582 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:43,583 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x4sr5p2y
2024-03-22 06:03:43,583 - distributed.worker - INFO - Starting Worker plugin PreImport-8790cd67-1914-4e4d-b708-182da909d405
2024-03-22 06:03:43,583 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:43,583 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a43890aa-4c89-48cb-b210-740216b9b14b
2024-03-22 06:03:43,584 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44477
2024-03-22 06:03:43,584 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44477
2024-03-22 06:03:43,584 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43313
2024-03-22 06:03:43,584 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:43,585 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:43,585 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:43,585 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:43,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-txh86ysh
2024-03-22 06:03:43,585 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7295afe-00da-4ffd-babe-3bf40a882bec
2024-03-22 06:03:43,585 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a18e04ea-02d4-44d5-8011-ff8a126c3693
2024-03-22 06:03:43,587 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:43,587 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:43,597 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:43,599 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45471
2024-03-22 06:03:43,599 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45471
2024-03-22 06:03:43,599 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39757
2024-03-22 06:03:43,599 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:43,599 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:43,599 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:43,599 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:43,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tcvfmjx2
2024-03-22 06:03:43,600 - distributed.worker - INFO - Starting Worker plugin PreImport-38abc29e-a633-4a82-b330-66b198b6e1d5
2024-03-22 06:03:43,600 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-718eb993-9812-4463-a9d5-1be342331eab
2024-03-22 06:03:43,600 - distributed.worker - INFO - Starting Worker plugin RMMSetup-95855c28-81ba-43ee-8ca2-6108565ae186
2024-03-22 06:03:43,753 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:43,754 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:43,759 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:43,760 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32841
2024-03-22 06:03:43,760 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32841
2024-03-22 06:03:43,760 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37547
2024-03-22 06:03:43,760 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:43,760 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:43,760 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:43,760 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:43,761 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j8748u61
2024-03-22 06:03:43,761 - distributed.worker - INFO - Starting Worker plugin PreImport-62747c14-b397-4f02-882a-6e92455135e8
2024-03-22 06:03:43,761 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c41733f8-ff83-490a-8610-9a36a6860bd2
2024-03-22 06:03:43,822 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:43,822 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:43,829 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:43,830 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38827
2024-03-22 06:03:43,831 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38827
2024-03-22 06:03:43,831 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35273
2024-03-22 06:03:43,831 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:43,831 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:43,831 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:43,831 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:43,831 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pzkehnbm
2024-03-22 06:03:43,831 - distributed.worker - INFO - Starting Worker plugin PreImport-9ef8e38e-2b93-46ba-af52-fc9e4b1b0d27
2024-03-22 06:03:43,832 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-74005d39-05de-4504-9388-af9a1ff171cc
2024-03-22 06:03:43,832 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50e0a0a0-f4c4-4a25-b438-81be994eb3b6
2024-03-22 06:03:46,186 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e763724a-5316-4dca-80b9-eff594d75640
2024-03-22 06:03:46,190 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,194 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,206 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-22 06:03:46,207 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38827. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 06:03:46,207 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:03:46,210 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:03:46,217 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:46,217 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:46,217 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,218 - distributed.worker - INFO - Starting Worker plugin PreImport-eb2f66b9-ff57-4c30-bcdf-44b288278d52
2024-03-22 06:03:46,219 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:46,219 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,222 - distributed.worker - INFO - Starting Worker plugin PreImport-77464e0e-541d-4a0d-aa15-24e4363e90c0
2024-03-22 06:03:46,222 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7ffedaa-d444-47e0-967e-da735cebe991
2024-03-22 06:03:46,224 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,225 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:46,226 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:46,226 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,227 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6233de2e-27c8-48b6-a70f-515a83e82fd7
2024-03-22 06:03:46,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:46,228 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,229 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-53d09510-6bb3-4917-9d28-d534ce5b7eb7
2024-03-22 06:03:46,229 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,233 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7eaa9569-16a0-4074-8226-1da5ba351661
2024-03-22 06:03:46,236 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,244 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:46,245 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:46,245 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:46,248 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:03:46,251 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43785'. Reason: nanny-instantiate-failed
2024-03-22 06:03:46,251 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 06:03:46,254 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:46,255 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:46,255 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,255 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:46,256 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:46,256 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:46,256 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:46,262 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:46,263 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:46,263 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:46,275 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:03:46,276 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:03:46,276 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:46,278 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:03:46,309 - distributed.nanny - INFO - Worker process 45895 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:03:46,313 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45926 parent=45727 started daemon>
2024-03-22 06:03:46,313 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45922 parent=45727 started daemon>
2024-03-22 06:03:46,313 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45918 parent=45727 started daemon>
2024-03-22 06:03:46,313 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45914 parent=45727 started daemon>
2024-03-22 06:03:46,313 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45910 parent=45727 started daemon>
2024-03-22 06:03:46,314 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45905 parent=45727 started daemon>
2024-03-22 06:03:46,314 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=45900 parent=45727 started daemon>
2024-03-22 06:03:46,757 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45910 exit status was already read will report exitcode 255
2024-03-22 06:03:46,792 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 45918 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-03-22 06:03:57,184 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:03:57,189 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 06:03:57,192 - distributed.scheduler - INFO - State start
2024-03-22 06:03:57,194 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-j8748u61', purging
2024-03-22 06:03:57,195 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_8drfz5h', purging
2024-03-22 06:03:57,195 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x4_12bt8', purging
2024-03-22 06:03:57,195 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-txh86ysh', purging
2024-03-22 06:03:57,196 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x4sr5p2y', purging
2024-03-22 06:03:57,196 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-zypah3o_', purging
2024-03-22 06:03:57,196 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-tcvfmjx2', purging
2024-03-22 06:03:57,217 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:03:57,218 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:03:57,218 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 06:03:57,219 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:03:57,227 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40531'
2024-03-22 06:03:57,242 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46571'
2024-03-22 06:03:57,256 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45835'
2024-03-22 06:03:57,259 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42451'
2024-03-22 06:03:57,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40433'
2024-03-22 06:03:57,283 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38871'
2024-03-22 06:03:57,296 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39745'
2024-03-22 06:03:57,304 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37475'
2024-03-22 06:03:58,512 - distributed.scheduler - INFO - Receive client connection: Client-f704ea9e-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:03:58,531 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37724
2024-03-22 06:03:59,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:59,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:59,298 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:59,298 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:59,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:59,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:59,302 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:59,304 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41327
2024-03-22 06:03:59,304 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41327
2024-03-22 06:03:59,304 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45145
2024-03-22 06:03:59,304 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:59,304 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:59,304 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:59,304 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:59,304 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pdwvalx0
2024-03-22 06:03:59,304 - distributed.worker - INFO - Starting Worker plugin PreImport-be0089c7-b522-4d63-a9c8-07a18ac1b010
2024-03-22 06:03:59,305 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f316195-2b4b-4d26-be2a-86b939b6fcaa
2024-03-22 06:03:59,305 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:59,305 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:59,306 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45361
2024-03-22 06:03:59,306 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45361
2024-03-22 06:03:59,306 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40111
2024-03-22 06:03:59,306 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45685
2024-03-22 06:03:59,306 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40111
2024-03-22 06:03:59,306 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:59,306 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:59,306 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36553
2024-03-22 06:03:59,306 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:59,306 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:59,306 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:59,306 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:59,306 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fbppk2di
2024-03-22 06:03:59,306 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:59,306 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c5b3c5e-6985-4480-8f6a-e55bbb5c19b9
2024-03-22 06:03:59,306 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:59,306 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gn3w2vus
2024-03-22 06:03:59,306 - distributed.worker - INFO - Starting Worker plugin PreImport-5e1ce6f9-0465-4f21-b949-51151dc195c6
2024-03-22 06:03:59,306 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3fdcbbd9-93ef-471e-b21d-e5a8be4e9f4a
2024-03-22 06:03:59,306 - distributed.worker - INFO - Starting Worker plugin PreImport-6d5d51a3-28db-4029-94a1-7ac82c25ccf1
2024-03-22 06:03:59,307 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4530a32f-47bd-4242-81bd-5cfec51995df
2024-03-22 06:03:59,307 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2631849e-f4f3-4f87-a4d5-fb8f924a4b5a
2024-03-22 06:03:59,307 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fda48094-d347-4829-b953-b84dc48d2145
2024-03-22 06:03:59,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:59,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:59,321 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:59,321 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:59,322 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:59,323 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42395
2024-03-22 06:03:59,323 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42395
2024-03-22 06:03:59,323 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33781
2024-03-22 06:03:59,323 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:59,324 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:59,324 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:59,324 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:59,324 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l20agwy0
2024-03-22 06:03:59,324 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04d427bb-aca7-443f-a14a-8b847846fc03
2024-03-22 06:03:59,324 - distributed.worker - INFO - Starting Worker plugin PreImport-02369c74-42bb-4520-add7-fece48ea20d6
2024-03-22 06:03:59,324 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62fc1d0f-9235-40ba-b03e-9e6e56b5a617
2024-03-22 06:03:59,329 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:59,331 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42013
2024-03-22 06:03:59,331 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42013
2024-03-22 06:03:59,331 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41569
2024-03-22 06:03:59,331 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:59,331 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:59,331 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:59,331 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:59,331 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y3jn7yws
2024-03-22 06:03:59,332 - distributed.worker - INFO - Starting Worker plugin PreImport-da7777ea-6d45-4be0-a85a-165356cdb940
2024-03-22 06:03:59,332 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aa8504d0-6fad-4848-a409-446512914205
2024-03-22 06:03:59,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:59,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:59,352 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:59,352 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45975
2024-03-22 06:03:59,353 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45975
2024-03-22 06:03:59,353 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37521
2024-03-22 06:03:59,353 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:59,353 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:59,353 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:59,353 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:59,353 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_q08bah8
2024-03-22 06:03:59,353 - distributed.worker - INFO - Starting Worker plugin PreImport-c8395b2d-4a4e-4123-8dd7-124e7058804b
2024-03-22 06:03:59,353 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6bc454d8-d5d8-4217-833f-be5e751b7807
2024-03-22 06:03:59,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:59,367 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:59,373 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:59,374 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:03:59,374 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:03:59,374 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46845
2024-03-22 06:03:59,375 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46845
2024-03-22 06:03:59,375 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35655
2024-03-22 06:03:59,375 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:59,375 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:59,375 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:59,375 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:59,375 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wfcd9yhz
2024-03-22 06:03:59,375 - distributed.worker - INFO - Starting Worker plugin PreImport-09d35c6b-55d4-4a83-a59b-c2bf6667f1b4
2024-03-22 06:03:59,375 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ce041c4-44e9-4a89-bee8-da232c8e46c5
2024-03-22 06:03:59,380 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:03:59,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44365
2024-03-22 06:03:59,381 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44365
2024-03-22 06:03:59,381 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38947
2024-03-22 06:03:59,381 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:03:59,381 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:03:59,381 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:03:59,381 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:03:59,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cb0885i8
2024-03-22 06:03:59,381 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d8ba07e-ee91-4188-be89-db76704c4484
2024-03-22 06:03:59,381 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26c6544f-4da2-44e1-b963-31204799f785
2024-03-22 06:03:59,645 - distributed.scheduler - INFO - Receive client connection: Client-f77f78f4-e811-11ee-ada4-d8c49764f6bb
2024-03-22 06:03:59,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37726
2024-03-22 06:04:02,248 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,278 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45361', status: init, memory: 0, processing: 0>
2024-03-22 06:04:02,281 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45361
2024-03-22 06:04:02,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35136
2024-03-22 06:04:02,282 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:02,283 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:04:02,283 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:04:02,306 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,312 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b0aa83d-6533-46d9-a1c3-9e63a5c1c30e
2024-03-22 06:04:02,312 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,332 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40111', status: init, memory: 0, processing: 0>
2024-03-22 06:04:02,332 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40111
2024-03-22 06:04:02,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35138
2024-03-22 06:04:02,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:02,334 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:04:02,334 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,335 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42013', status: init, memory: 0, processing: 0>
2024-03-22 06:04:02,335 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42013
2024-03-22 06:04:02,335 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35144
2024-03-22 06:04:02,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:04:02,336 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:02,337 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:04:02,337 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:04:02,380 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:04:02,383 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41327. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 06:04:02,384 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:04:02,390 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:04:02,409 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:04:02,413 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46571'. Reason: nanny-instantiate-failed
2024-03-22 06:04:02,413 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 06:04:02,432 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,454 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42395', status: init, memory: 0, processing: 0>
2024-03-22 06:04:02,454 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42395
2024-03-22 06:04:02,454 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35160
2024-03-22 06:04:02,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:02,456 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:04:02,456 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,457 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:04:02,480 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2338f0d1-5f0f-4721-90ab-3cfe2cd43de7
2024-03-22 06:04:02,481 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,485 - distributed.worker - INFO - Starting Worker plugin PreImport-654382c1-7b57-49fa-8158-3fd29fd08faf
2024-03-22 06:04:02,486 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-906327a3-0094-46be-a61b-a1475e9b46a5
2024-03-22 06:04:02,486 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,486 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,492 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44855', status: init, memory: 0, processing: 0>
2024-03-22 06:04:02,492 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44855
2024-03-22 06:04:02,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35124
2024-03-22 06:04:02,507 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45975', status: init, memory: 0, processing: 0>
2024-03-22 06:04:02,508 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45975
2024-03-22 06:04:02,508 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35170
2024-03-22 06:04:02,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:02,508 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44365', status: init, memory: 0, processing: 0>
2024-03-22 06:04:02,509 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44365
2024-03-22 06:04:02,509 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35178
2024-03-22 06:04:02,509 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:04:02,509 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,510 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:02,511 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:04:02,511 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:04:02,511 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,512 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46845', status: init, memory: 0, processing: 0>
2024-03-22 06:04:02,512 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46845
2024-03-22 06:04:02,512 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35164
2024-03-22 06:04:02,513 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:04:02,514 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:02,515 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:04:02,515 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:02,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:04:02,553 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:04:02,553 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:04:02,553 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:04:02,553 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:04:02,554 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:04:02,554 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:04:02,554 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 06:04:02,559 - distributed.scheduler - INFO - Remove client Client-f704ea9e-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:02,560 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37724; closing.
2024-03-22 06:04:02,560 - distributed.scheduler - INFO - Remove client Client-f704ea9e-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:02,561 - distributed.scheduler - INFO - Close client connection: Client-f704ea9e-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:02,561 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39745'. Reason: nanny-close
2024-03-22 06:04:02,561 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:04:02,562 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38871'. Reason: nanny-close
2024-03-22 06:04:02,562 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:04:02,562 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40433'. Reason: nanny-close
2024-03-22 06:04:02,562 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46845. Reason: nanny-close
2024-03-22 06:04:02,563 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:04:02,563 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42451'. Reason: nanny-close
2024-03-22 06:04:02,563 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42013. Reason: nanny-close
2024-03-22 06:04:02,563 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:04:02,563 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44365. Reason: nanny-close
2024-03-22 06:04:02,563 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40531'. Reason: nanny-close
2024-03-22 06:04:02,564 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:04:02,564 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37475'. Reason: nanny-close
2024-03-22 06:04:02,564 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45361. Reason: nanny-close
2024-03-22 06:04:02,564 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:04:02,565 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40111. Reason: nanny-close
2024-03-22 06:04:02,565 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45835'. Reason: nanny-close
2024-03-22 06:04:02,565 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35164; closing.
2024-03-22 06:04:02,565 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:04:02,565 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:04:02,565 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46845', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087442.5655353')
2024-03-22 06:04:02,565 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45975. Reason: nanny-close
2024-03-22 06:04:02,565 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:04:02,565 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:04:02,566 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42395. Reason: nanny-close
2024-03-22 06:04:02,567 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:04:02,567 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:02,567 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:02,567 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:02,567 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35178; closing.
2024-03-22 06:04:02,567 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:04:02,567 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:04:02,567 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35144; closing.
2024-03-22 06:04:02,568 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:02,569 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44365', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087442.5689564')
2024-03-22 06:04:02,569 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:02,569 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:04:02,569 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:02,569 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42013', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087442.5693712')
2024-03-22 06:04:02,569 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35136; closing.
2024-03-22 06:04:02,569 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35138; closing.
2024-03-22 06:04:02,570 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:02,570 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45361', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087442.57057')
2024-03-22 06:04:02,570 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40111', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087442.5709012')
2024-03-22 06:04:02,571 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35170; closing.
2024-03-22 06:04:02,571 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45975', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087442.5718973')
2024-03-22 06:04:02,572 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35160; closing.
2024-03-22 06:04:02,572 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42395', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087442.5727367')
2024-03-22 06:04:02,651 - distributed.scheduler - INFO - Remove client Client-f77f78f4-e811-11ee-ada4-d8c49764f6bb
2024-03-22 06:04:02,651 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37726; closing.
2024-03-22 06:04:02,651 - distributed.scheduler - INFO - Remove client Client-f77f78f4-e811-11ee-ada4-d8c49764f6bb
2024-03-22 06:04:02,652 - distributed.scheduler - INFO - Close client connection: Client-f77f78f4-e811-11ee-ada4-d8c49764f6bb
2024-03-22 06:04:02,656 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35124; closing.
2024-03-22 06:04:02,657 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44855', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087442.657049')
2024-03-22 06:04:02,657 - distributed.scheduler - INFO - Lost all workers
2024-03-22 06:04:03,261 - distributed.nanny - INFO - Worker process 46183 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:04:03,265 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46209 parent=46010 started daemon>
2024-03-22 06:04:03,265 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46205 parent=46010 started daemon>
2024-03-22 06:04:03,266 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46201 parent=46010 started daemon>
2024-03-22 06:04:03,266 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46197 parent=46010 started daemon>
2024-03-22 06:04:03,266 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46193 parent=46010 started daemon>
2024-03-22 06:04:03,266 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46188 parent=46010 started daemon>
2024-03-22 06:04:03,266 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=46178 parent=46010 started daemon>
2024-03-22 06:04:03,263 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:37660'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37660>: Stream is closed
2024-03-22 06:04:03,953 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 46201 exit status was already read will report exitcode 255
2024-03-22 06:04:04,024 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 46178 exit status was already read will report exitcode 255
2024-03-22 06:04:04,105 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 46193 exit status was already read will report exitcode 255
2024-03-22 06:04:04,380 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:04:04,380 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:04:04,381 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:04:04,382 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:04:04,383 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-03-22 06:04:06,973 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:04:06,978 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 06:04:06,982 - distributed.scheduler - INFO - State start
2024-03-22 06:04:07,032 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:04:07,033 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:04:07,034 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 06:04:07,034 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:04:07,130 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34619'
2024-03-22 06:04:07,483 - distributed.scheduler - INFO - Receive client connection: Client-fd751c0b-e811-11ee-ada4-d8c49764f6bb
2024-03-22 06:04:07,496 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35352
2024-03-22 06:04:09,119 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:09,119 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:09,266 - distributed.scheduler - INFO - Receive client connection: Client-fcccbe83-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:09,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35364
2024-03-22 06:04:09,738 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:09,739 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35517
2024-03-22 06:04:09,739 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35517
2024-03-22 06:04:09,739 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-03-22 06:04:09,739 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:04:09,739 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:09,739 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:09,739 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:04:09,739 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hgaplin5
2024-03-22 06:04:09,739 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89689c33-8d88-435f-aba1-3c21127a6648
2024-03-22 06:04:09,740 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c6a6cae-19af-441f-ae95-114f26281af4
2024-03-22 06:04:09,740 - distributed.worker - INFO - Starting Worker plugin PreImport-6289139a-8316-48bb-820d-9f7f592701ca
2024-03-22 06:04:09,740 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:10,268 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35517', status: init, memory: 0, processing: 0>
2024-03-22 06:04:10,269 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35517
2024-03-22 06:04:10,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35378
2024-03-22 06:04:10,270 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:10,271 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:04:10,271 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:10,272 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:04:10,289 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:04:10,292 - distributed.scheduler - INFO - Remove client Client-fcccbe83-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:10,292 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35364; closing.
2024-03-22 06:04:10,292 - distributed.scheduler - INFO - Remove client Client-fcccbe83-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:10,293 - distributed.scheduler - INFO - Close client connection: Client-fcccbe83-e811-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:10,294 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34619'. Reason: nanny-close
2024-03-22 06:04:10,315 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:04:10,316 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35517. Reason: nanny-close
2024-03-22 06:04:10,318 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35378; closing.
2024-03-22 06:04:10,318 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:04:10,318 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35517', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087450.3185654')
2024-03-22 06:04:10,318 - distributed.scheduler - INFO - Lost all workers
2024-03-22 06:04:10,319 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:10,959 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:04:10,959 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:04:10,959 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:04:10,961 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:04:10,961 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-03-22 06:04:15,494 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:04:15,498 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44725 instead
  warnings.warn(
2024-03-22 06:04:15,502 - distributed.scheduler - INFO - State start
2024-03-22 06:04:15,524 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:04:15,525 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:04:15,525 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44725/status
2024-03-22 06:04:15,526 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:04:15,592 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41797'
2024-03-22 06:04:15,668 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44283', status: init, memory: 0, processing: 0>
2024-03-22 06:04:15,679 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44283
2024-03-22 06:04:15,680 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58186
2024-03-22 06:04:15,734 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58186; closing.
2024-03-22 06:04:15,734 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44283', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087455.7346356')
2024-03-22 06:04:15,735 - distributed.scheduler - INFO - Lost all workers
2024-03-22 06:04:16,296 - distributed.scheduler - INFO - Receive client connection: Client-02063a6d-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:16,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58192
2024-03-22 06:04:17,339 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:17,339 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:17,874 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:17,876 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42251
2024-03-22 06:04:17,876 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42251
2024-03-22 06:04:17,876 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41085
2024-03-22 06:04:17,876 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:04:17,876 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:17,876 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:17,876 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:04:17,876 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sgpq7256
2024-03-22 06:04:17,877 - distributed.worker - INFO - Starting Worker plugin PreImport-ac11be21-899b-4491-b283-e4c743408e80
2024-03-22 06:04:17,878 - distributed.worker - INFO - Starting Worker plugin RMMSetup-475eb2aa-788b-40c7-9b32-c0e31900d062
2024-03-22 06:04:17,878 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8132afb2-70a3-4b31-b4fe-9ad956166722
2024-03-22 06:04:17,878 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:17,940 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42251', status: init, memory: 0, processing: 0>
2024-03-22 06:04:17,941 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42251
2024-03-22 06:04:17,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58230
2024-03-22 06:04:17,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:17,943 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:04:17,944 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:17,945 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:04:18,030 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:04:18,033 - distributed.scheduler - INFO - Remove client Client-02063a6d-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:18,033 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58192; closing.
2024-03-22 06:04:18,033 - distributed.scheduler - INFO - Remove client Client-02063a6d-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:18,034 - distributed.scheduler - INFO - Close client connection: Client-02063a6d-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:04:18,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41797'. Reason: nanny-close
2024-03-22 06:04:18,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:04:18,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42251. Reason: nanny-close
2024-03-22 06:04:18,038 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58230; closing.
2024-03-22 06:04:18,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:04:18,039 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42251', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087458.039204')
2024-03-22 06:04:18,039 - distributed.scheduler - INFO - Lost all workers
2024-03-22 06:04:18,040 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:18,800 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:04:18,800 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:04:18,801 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:04:18,802 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:04:18,802 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-03-22 06:04:20,954 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:04:20,959 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44483 instead
  warnings.warn(
2024-03-22 06:04:20,963 - distributed.scheduler - INFO - State start
2024-03-22 06:04:20,983 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:04:20,984 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-22 06:04:20,985 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:04:20,986 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 629, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-03-22 06:04:26,120 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44863'
2024-03-22 06:04:26,211 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:04:26,217 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37213 instead
  warnings.warn(
2024-03-22 06:04:26,223 - distributed.scheduler - INFO - State start
2024-03-22 06:04:26,247 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:04:26,248 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-22 06:04:26,250 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:04:26,251 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 629, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-22 06:04:27,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44863'. Reason: nanny-close
2024-03-22 06:04:28,001 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:28,001 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:28,005 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:28,006 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38043
2024-03-22 06:04:28,006 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38043
2024-03-22 06:04:28,006 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35361
2024-03-22 06:04:28,006 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-22 06:04:28,006 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:28,006 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:28,006 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:04:28,006 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-eczo_rn7
2024-03-22 06:04:28,007 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-77c49c84-0844-48c0-b221-a5477256d08a
2024-03-22 06:04:28,007 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae2542e8-b276-46d8-97bc-6bed473512e6
2024-03-22 06:04:28,007 - distributed.worker - INFO - Starting Worker plugin PreImport-c124ecf7-e7fa-452a-96ff-fc2727b77212
2024-03-22 06:04:28,007 - distributed.worker - INFO - -------------------------------------------------
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-03-22 06:04:59,362 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:04:59,366 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40313 instead
  warnings.warn(
2024-03-22 06:04:59,370 - distributed.scheduler - INFO - State start
2024-03-22 06:04:59,371 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/mockworker-eczo_rn7', purging
2024-03-22 06:04:59,392 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:04:59,392 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-22 06:04:59,393 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:04:59,394 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 629, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-22 06:04:59,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40913'
2024-03-22 06:04:59,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40377'
2024-03-22 06:04:59,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42531'
2024-03-22 06:04:59,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39567'
2024-03-22 06:04:59,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33055'
2024-03-22 06:04:59,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33381'
2024-03-22 06:04:59,590 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38791'
2024-03-22 06:04:59,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36107'
2024-03-22 06:05:01,606 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:01,606 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:01,611 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:01,611 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:01,615 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:01,616 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42403
2024-03-22 06:05:01,616 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42403
2024-03-22 06:05:01,616 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37045
2024-03-22 06:05:01,616 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:05:01,616 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:01,616 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:01,617 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:01,617 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7a3kvehj
2024-03-22 06:05:01,617 - distributed.worker - INFO - Starting Worker plugin PreImport-63530554-ee25-45c0-a7d4-ebe2cb9b1beb
2024-03-22 06:05:01,617 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57f22139-93af-40f2-9194-ba550a0b8b97
2024-03-22 06:05:01,617 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:01,618 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44075
2024-03-22 06:05:01,619 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44075
2024-03-22 06:05:01,619 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38857
2024-03-22 06:05:01,619 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:05:01,619 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:01,619 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:01,619 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:01,619 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4zd7e546
2024-03-22 06:05:01,619 - distributed.worker - INFO - Starting Worker plugin PreImport-7e798a28-a460-4f94-b11b-5ef54022a735
2024-03-22 06:05:01,619 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6679dac4-9a1a-4b6c-8f34-6018573b53e1
2024-03-22 06:05:01,620 - distributed.worker - INFO - Starting Worker plugin RMMSetup-453fc258-5b98-48ba-8181-4cac3435cf51
2024-03-22 06:05:01,631 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:01,632 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:01,636 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:01,636 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:01,638 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:01,640 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46245
2024-03-22 06:05:01,640 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46245
2024-03-22 06:05:01,640 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36287
2024-03-22 06:05:01,640 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:05:01,640 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:01,640 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:01,640 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:01,640 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jh788nf5
2024-03-22 06:05:01,640 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b2b1a290-e291-4847-8287-8dfe81f19a9d
2024-03-22 06:05:01,641 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:01,642 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36965
2024-03-22 06:05:01,642 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36965
2024-03-22 06:05:01,642 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45593
2024-03-22 06:05:01,642 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:05:01,642 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:01,642 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:01,642 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:01,642 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e214bpp_
2024-03-22 06:05:01,643 - distributed.worker - INFO - Starting Worker plugin PreImport-294b3dcc-d68b-476b-bbaf-3997b012addc
2024-03-22 06:05:01,643 - distributed.worker - INFO - Starting Worker plugin RMMSetup-99576c62-4747-46f5-8f32-d99c4ff95d02
2024-03-22 06:05:01,681 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:01,682 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:01,684 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:01,684 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:01,686 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:01,686 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:01,687 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:01,688 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42619
2024-03-22 06:05:01,688 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42619
2024-03-22 06:05:01,688 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41999
2024-03-22 06:05:01,688 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:05:01,688 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:01,688 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:01,688 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:01,688 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x6obry7s
2024-03-22 06:05:01,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e255ae98-bc7c-480e-86af-ba80ca0967dc
2024-03-22 06:05:01,689 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80130a74-91f3-45ad-9d08-0a29b03d06e4
2024-03-22 06:05:01,690 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:01,691 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:01,692 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41433
2024-03-22 06:05:01,692 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41433
2024-03-22 06:05:01,692 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39853
2024-03-22 06:05:01,692 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:05:01,692 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:01,692 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:01,692 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:01,692 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jqf3z46k
2024-03-22 06:05:01,692 - distributed.worker - INFO - Starting Worker plugin PreImport-73bf564f-02d3-4fac-8bff-823224d64382
2024-03-22 06:05:01,693 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c44ce50c-4fab-4211-946d-716c750740f6
2024-03-22 06:05:01,693 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40345
2024-03-22 06:05:01,693 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40345
2024-03-22 06:05:01,693 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39007
2024-03-22 06:05:01,693 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:05:01,693 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:01,693 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:01,693 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:01,693 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fynpk05q
2024-03-22 06:05:01,693 - distributed.worker - INFO - Starting Worker plugin PreImport-6c967454-01f8-4c73-9032-5d50a15dded0
2024-03-22 06:05:01,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f24b9f0-8127-426e-ade0-116d49b6867c
2024-03-22 06:05:01,695 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb49c87d-bc9e-44a7-9ba8-f2f4ad98a25b
2024-03-22 06:05:01,746 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:01,746 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:01,752 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:01,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34759
2024-03-22 06:05:01,753 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34759
2024-03-22 06:05:01,753 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41663
2024-03-22 06:05:01,753 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:05:01,753 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:01,753 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:01,753 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:01,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2j20zmbv
2024-03-22 06:05:01,753 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e9c0087-d398-4b5a-bae5-6e96f15569b3
2024-03-22 06:05:01,753 - distributed.worker - INFO - Starting Worker plugin PreImport-a8bed511-242f-4694-a39b-1f833e61ab11
2024-03-22 06:05:01,754 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e324f76-53ea-4829-9642-88442a819612
2024-03-22 06:05:03,873 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-03f2d13d-1228-48f6-8b57-7616ae100407
2024-03-22 06:05:03,874 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:03,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:05:03,913 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:05:03,913 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:03,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:05:03,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42531'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-22 06:05:03,944 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-22 06:05:03,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42403. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-03-22 06:05:03,947 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:05:03,948 - distributed.nanny - INFO - Worker closed
2024-03-22 06:05:03,986 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:05:03,987 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44075. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 06:05:03,987 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:05:03,991 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:05:04,000 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:05:04,003 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40913'. Reason: nanny-instantiate-failed
2024-03-22 06:05:04,004 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 06:05:04,121 - distributed.worker - INFO - Starting Worker plugin PreImport-aef65753-99c3-42e6-9214-29f3ee68dc36
2024-03-22 06:05:04,122 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-972194ca-82a1-4633-a8ed-27f904daf82e
2024-03-22 06:05:04,123 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:04,156 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:05:04,157 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:05:04,157 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:04,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:05:04,187 - distributed.nanny - INFO - Worker process 47287 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:05:04,192 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47318 parent=47119 started daemon>
2024-03-22 06:05:04,192 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47314 parent=47119 started daemon>
2024-03-22 06:05:04,192 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47310 parent=47119 started daemon>
2024-03-22 06:05:04,192 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47306 parent=47119 started daemon>
2024-03-22 06:05:04,192 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47302 parent=47119 started daemon>
2024-03-22 06:05:04,193 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47297 parent=47119 started daemon>
2024-03-22 06:05:04,193 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=47292 parent=47119 started daemon>
2024-03-22 06:05:04,362 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 47318 exit status was already read will report exitcode 255
2024-03-22 06:05:04,416 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 47306 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-03-22 06:05:15,610 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:05:15,615 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39805 instead
  warnings.warn(
2024-03-22 06:05:15,619 - distributed.scheduler - INFO - State start
2024-03-22 06:05:15,620 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jqf3z46k', purging
2024-03-22 06:05:15,621 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-e214bpp_', purging
2024-03-22 06:05:15,621 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2j20zmbv', purging
2024-03-22 06:05:15,621 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-fynpk05q', purging
2024-03-22 06:05:15,622 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x6obry7s', purging
2024-03-22 06:05:15,622 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jh788nf5', purging
2024-03-22 06:05:15,642 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:05:15,643 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:05:15,644 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39805/status
2024-03-22 06:05:15,644 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:05:15,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45405'
2024-03-22 06:05:15,739 - distributed.scheduler - INFO - Receive client connection: Client-25d20400-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:05:15,753 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44328
2024-03-22 06:05:17,540 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:17,540 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:17,544 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:17,544 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43851
2024-03-22 06:05:17,544 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43851
2024-03-22 06:05:17,544 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40941
2024-03-22 06:05:17,544 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:05:17,544 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:17,544 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:17,545 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:05:17,545 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qs9_1lsh
2024-03-22 06:05:17,545 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fd39f80f-dbce-4877-982e-73001f0d4915
2024-03-22 06:05:17,545 - distributed.worker - INFO - Starting Worker plugin PreImport-b69aee4d-cc7b-4cd6-baab-48bfa14e2360
2024-03-22 06:05:17,545 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3daa5fe7-6ac7-4939-9d87-3e82cde15a12
2024-03-22 06:05:17,917 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:05:17,920 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43851. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 06:05:17,920 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:05:17,924 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:05:17,977 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:05:17,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45405'. Reason: nanny-instantiate-failed
2024-03-22 06:05:17,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 06:05:18,032 - distributed.nanny - INFO - Worker process 47557 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:05:18,033 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44324'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44324>: Stream is closed
2024-03-22 06:05:23,713 - distributed.scheduler - INFO - Remove client Client-25d20400-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:05:23,714 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44328; closing.
2024-03-22 06:05:23,714 - distributed.scheduler - INFO - Remove client Client-25d20400-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:05:23,714 - distributed.scheduler - INFO - Close client connection: Client-25d20400-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:05:23,715 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:05:23,715 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:05:23,716 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:05:23,717 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:05:23,717 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-03-22 06:05:26,074 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:05:26,079 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43225 instead
  warnings.warn(
2024-03-22 06:05:26,083 - distributed.scheduler - INFO - State start
2024-03-22 06:05:26,106 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:05:26,107 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:05:26,108 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43225/status
2024-03-22 06:05:26,108 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:05:26,343 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46139'
2024-03-22 06:05:28,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:28,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:28,159 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:28,160 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37443
2024-03-22 06:05:28,160 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37443
2024-03-22 06:05:28,160 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36867
2024-03-22 06:05:28,160 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:05:28,160 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:28,160 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:28,160 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:05:28,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pzs70yxn
2024-03-22 06:05:28,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eab1de88-537f-4732-b9dc-066ef9509569
2024-03-22 06:05:28,161 - distributed.worker - INFO - Starting Worker plugin PreImport-68fcf5e9-6a32-46f4-8d9e-0619f591a9cd
2024-03-22 06:05:28,161 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76cd6410-018b-431f-89ad-74d18d7b1d2b
2024-03-22 06:05:28,284 - distributed.scheduler - INFO - Receive client connection: Client-2c13599e-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:05:28,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45212
2024-03-22 06:05:28,541 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:05:28,544 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37443. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 06:05:28,544 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:05:28,550 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:05:28,571 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:05:28,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46139'. Reason: nanny-instantiate-failed
2024-03-22 06:05:28,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 06:05:28,626 - distributed.nanny - INFO - Worker process 47741 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:05:28,627 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:45200'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45200>: Stream is closed
2024-03-22 06:05:38,342 - distributed.scheduler - INFO - Remove client Client-2c13599e-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:05:38,342 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45212; closing.
2024-03-22 06:05:38,343 - distributed.scheduler - INFO - Remove client Client-2c13599e-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:05:38,343 - distributed.scheduler - INFO - Close client connection: Client-2c13599e-e812-11ee-adcb-d8c49764f6bb
2024-03-22 06:05:38,344 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:05:38,344 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:05:38,345 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:05:38,346 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:05:38,346 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33323 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36945 instead
  warnings.warn(
2024-03-22 06:05:53,123 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-2:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 82, in _test_tcp_over_ucx
    with LocalCUDACluster(protocol=protocol, enable_tcp_over_ucx=True) as cluster:
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] SKIPPED (could ...)
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42973 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39969 instead
  warnings.warn(
2024-03-22 06:06:02,955 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-4:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 172, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46765 instead
  warnings.warn(
2024-03-22 06:06:07,773 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-5:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 172, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41287 instead
  warnings.warn(
2024-03-22 06:06:12,429 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-6:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 172, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42653 instead
  warnings.warn(
2024-03-22 06:06:17,120 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-7:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 172, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38775 instead
  warnings.warn(
2024-03-22 06:06:20,893 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-8:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 172, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers SKIPPED (h...)
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] 2024-03-22 06:06:28,396 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-22 06:06:28,402 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] SKIPPED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43929 instead
  warnings.warn(
2024-03-22 06:06:39,125 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-25:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 31, in _test_initialize_ucx_tcp
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34753 instead
  warnings.warn(
2024-03-22 06:06:42,183 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-26:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 78, in _test_initialize_ucx_nvlink
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43505 instead
  warnings.warn(
2024-03-22 06:06:45,180 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 126, in _test_initialize_ucx_infiniband
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43659 instead
  warnings.warn(
2024-03-22 06:06:48,253 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4041, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 176, in _test_initialize_ucx_all
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1711087614.391005] [dgx13:44491:0]            sock.c:470  UCX  ERROR bind(fd=170 addr=0.0.0.0:47444) failed: Address already in use
2024-03-22 06:06:59,947 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-22 06:06:59,958 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] 2024-03-22 06:07:07,665 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-22 06:07:07,676 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-03-22 06:07:21,464 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:07:21,465 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:07:21,631 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:07:21,631 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:07:21,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:07:21,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:07:21,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:07:21,753 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:07:21,774 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:07:21,775 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:07:21,812 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:07:21,812 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:07:21,844 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:07:21,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:07:21,847 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:07:21,847 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:07:22,085 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:07:22,086 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35771
2024-03-22 06:07:22,086 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35771
2024-03-22 06:07:22,086 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33069
2024-03-22 06:07:22,086 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,086 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,086 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:07:22,086 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-solai1ji
2024-03-22 06:07:22,086 - distributed.worker - INFO - Starting Worker plugin PreImport-d36ea518-1b69-4d08-8ef0-8f7a1e615676
2024-03-22 06:07:22,087 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6965fe67-6e2d-45bc-8b78-40f0557fc82a
2024-03-22 06:07:22,087 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6a62ec9-9fc9-49d6-a327-ea88cefb8bd6
2024-03-22 06:07:22,088 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,145 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:07:22,146 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,146 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,147 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42509
2024-03-22 06:07:22,224 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:07:22,225 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39201
2024-03-22 06:07:22,225 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39201
2024-03-22 06:07:22,226 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38625
2024-03-22 06:07:22,226 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,226 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,226 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:07:22,226 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r5gpvfd9
2024-03-22 06:07:22,226 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-47428113-20bc-4821-af63-54eb68a3cc5a
2024-03-22 06:07:22,226 - distributed.worker - INFO - Starting Worker plugin PreImport-066d0f12-56ce-4e66-981f-46d4fd88b0d8
2024-03-22 06:07:22,226 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5278aaf7-517b-4c64-af4b-a60ba04cfa99
2024-03-22 06:07:22,227 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,279 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:07:22,280 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,280 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42509
2024-03-22 06:07:22,366 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:07:22,367 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45383
2024-03-22 06:07:22,367 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45383
2024-03-22 06:07:22,367 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37715
2024-03-22 06:07:22,367 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,367 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,367 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:07:22,368 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uq2z0kkg
2024-03-22 06:07:22,368 - distributed.worker - INFO - Starting Worker plugin PreImport-f039a576-16c7-41c1-8b55-9b28f7f4e786
2024-03-22 06:07:22,368 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e2e05ddd-c666-483c-b378-b71d40750a7b
2024-03-22 06:07:22,368 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2737d53-fbf1-40a8-8223-636bd40564ea
2024-03-22 06:07:22,368 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,408 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:07:22,409 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42835
2024-03-22 06:07:22,409 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42835
2024-03-22 06:07:22,409 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34747
2024-03-22 06:07:22,409 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,409 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,409 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:07:22,409 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u_jpqpbv
2024-03-22 06:07:22,409 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c1c8111-5074-4ad8-94f2-71bc36985b73
2024-03-22 06:07:22,410 - distributed.worker - INFO - Starting Worker plugin PreImport-fb486272-8876-40af-bf4b-d74a258de2e3
2024-03-22 06:07:22,410 - distributed.worker - INFO - Starting Worker plugin RMMSetup-531744fb-923e-4fb9-8ddb-b0cde870a56d
2024-03-22 06:07:22,410 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,414 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:07:22,415 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35739
2024-03-22 06:07:22,415 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35739
2024-03-22 06:07:22,415 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44583
2024-03-22 06:07:22,415 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,415 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,415 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:07:22,415 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8rgdqwoo
2024-03-22 06:07:22,415 - distributed.worker - INFO - Starting Worker plugin PreImport-c266c0e7-73ad-47c0-a3fa-5877a30e4771
2024-03-22 06:07:22,415 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df90270d-84dd-4878-bdca-9790310106c7
2024-03-22 06:07:22,415 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14680be9-54f7-4fbb-aee9-ed71751bdbf3
2024-03-22 06:07:22,416 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:07:22,418 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44909
2024-03-22 06:07:22,418 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44909
2024-03-22 06:07:22,418 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37905
2024-03-22 06:07:22,418 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,418 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,418 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:07:22,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-57nrwjtx
2024-03-22 06:07:22,418 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88fc4006-a4ad-4950-a131-5c01c18890d0
2024-03-22 06:07:22,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d63cd027-fa2f-4e4b-9f2a-c08053a700f9
2024-03-22 06:07:22,419 - distributed.worker - INFO - Starting Worker plugin PreImport-7f48f9bb-28cc-4cc9-83a6-fb726f30b78c
2024-03-22 06:07:22,419 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,421 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:07:22,422 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46555
2024-03-22 06:07:22,422 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46555
2024-03-22 06:07:22,422 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38095
2024-03-22 06:07:22,423 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,423 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,423 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:07:22,423 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6b3xu4ne
2024-03-22 06:07:22,423 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-216c8f39-8bc1-4416-aa87-81346e431b38
2024-03-22 06:07:22,423 - distributed.worker - INFO - Starting Worker plugin PreImport-06ca37e1-1eda-48a1-8cb0-deb224b75357
2024-03-22 06:07:22,424 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f962caf-8162-4634-8b81-0353ea381a2b
2024-03-22 06:07:22,424 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:07:22,431 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,431 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42509
2024-03-22 06:07:22,465 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:07:22,467 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35551
2024-03-22 06:07:22,467 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35551
2024-03-22 06:07:22,467 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44779
2024-03-22 06:07:22,467 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,467 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,467 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:07:22,467 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rdn2pn2b
2024-03-22 06:07:22,467 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa4dac09-d26c-40d4-a05e-65cb7bc8c48d
2024-03-22 06:07:22,467 - distributed.worker - INFO - Starting Worker plugin PreImport-5727a3ef-b0d6-44a9-a12b-41d3c4136485
2024-03-22 06:07:22,467 - distributed.worker - INFO - Starting Worker plugin RMMSetup-78c607a2-6e12-4edd-acba-bca4011d8396
2024-03-22 06:07:22,468 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,562 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:07:22,563 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,563 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42509
2024-03-22 06:07:22,569 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:07:22,569 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,569 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,570 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42509
2024-03-22 06:07:22,588 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:07:22,589 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,589 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,590 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:07:22,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42509
2024-03-22 06:07:22,591 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,591 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42509
2024-03-22 06:07:22,598 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:07:22,599 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42509
2024-03-22 06:07:22,599 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:07:22,600 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42509
2024-03-22 06:07:22,639 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:07:22,639 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:07:22,639 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:07:22,639 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:07:22,639 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:07:22,640 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:07:22,640 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:07:22,640 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:07:22,645 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35771. Reason: nanny-close
2024-03-22 06:07:22,646 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42835. Reason: nanny-close
2024-03-22 06:07:22,646 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44909. Reason: nanny-close
2024-03-22 06:07:22,647 - distributed.core - INFO - Connection to tcp://127.0.0.1:42509 has been closed.
2024-03-22 06:07:22,648 - distributed.nanny - INFO - Worker closed
2024-03-22 06:07:22,648 - distributed.core - INFO - Connection to tcp://127.0.0.1:42509 has been closed.
2024-03-22 06:07:22,648 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45383. Reason: nanny-close
2024-03-22 06:07:22,648 - distributed.core - INFO - Connection to tcp://127.0.0.1:42509 has been closed.
2024-03-22 06:07:22,649 - distributed.nanny - INFO - Worker closed
2024-03-22 06:07:22,650 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35551. Reason: nanny-close
2024-03-22 06:07:22,650 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39201. Reason: nanny-close
2024-03-22 06:07:22,650 - distributed.nanny - INFO - Worker closed
2024-03-22 06:07:22,650 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35739. Reason: nanny-close
2024-03-22 06:07:22,650 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46555. Reason: nanny-close
2024-03-22 06:07:22,651 - distributed.core - INFO - Connection to tcp://127.0.0.1:42509 has been closed.
2024-03-22 06:07:22,651 - distributed.core - INFO - Connection to tcp://127.0.0.1:42509 has been closed.
2024-03-22 06:07:22,652 - distributed.core - INFO - Connection to tcp://127.0.0.1:42509 has been closed.
2024-03-22 06:07:22,652 - distributed.nanny - INFO - Worker closed
2024-03-22 06:07:22,652 - distributed.core - INFO - Connection to tcp://127.0.0.1:42509 has been closed.
2024-03-22 06:07:22,652 - distributed.core - INFO - Connection to tcp://127.0.0.1:42509 has been closed.
2024-03-22 06:07:22,653 - distributed.nanny - INFO - Worker closed
2024-03-22 06:07:22,653 - distributed.nanny - INFO - Worker closed
2024-03-22 06:07:22,653 - distributed.nanny - INFO - Worker closed
2024-03-22 06:07:22,653 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool 2024-03-22 06:07:31,069 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:07:31,073 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async 2024-03-22 06:07:43,177 - distributed.worker - ERROR - CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
2024-03-22 06:07:43,181 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size 2024-03-22 06:07:51,494 - distributed.worker - ERROR - CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
2024-03-22 06:07:51,499 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging 2024-03-22 06:07:58,663 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:07:58,667 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-03-22 06:08:03,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:08:03,436 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:08:03,440 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:08:03,442 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42753
2024-03-22 06:08:03,442 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42753
2024-03-22 06:08:03,442 - distributed.worker - INFO -           Worker name:                          0
2024-03-22 06:08:03,442 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35079
2024-03-22 06:08:03,442 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33277
2024-03-22 06:08:03,442 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:03,442 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:08:03,442 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:08:03,442 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6yz633ol
2024-03-22 06:08:03,443 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1348caf4-f033-4919-aa39-d708148e212e
2024-03-22 06:08:03,443 - distributed.worker - INFO - Starting Worker plugin PreImport-a12457ee-6537-4118-b04d-0ff85fc7254a
2024-03-22 06:08:03,446 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-03-22 06:08:03,447 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89d0b836-0ad5-441e-8bf4-d3cb1ac2c785
2024-03-22 06:08:03,447 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42753. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-03-22 06:08:03,447 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:08:03,449 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-03-22 06:08:09,734 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:08:09,734 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:08:09,751 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:08:09,751 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:08:09,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:08:09,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:08:09,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:08:09,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:08:09,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:08:09,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:08:09,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:08:09,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:08:09,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:08:09,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:08:09,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:08:09,777 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:08:10,442 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:08:10,443 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41959
2024-03-22 06:08:10,443 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41959
2024-03-22 06:08:10,443 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43703
2024-03-22 06:08:10,443 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35889
2024-03-22 06:08:10,443 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,444 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:08:10,444 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:08:10,444 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e6lpbzsu
2024-03-22 06:08:10,444 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f08a7d1-2a63-4c48-94ac-735e22de1927
2024-03-22 06:08:10,444 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5865d705-0e2e-45f2-ba36-c69c89ba084d
2024-03-22 06:08:10,444 - distributed.worker - INFO - Starting Worker plugin PreImport-5c676f42-54d6-45ac-91f6-15a1fc06e1ca
2024-03-22 06:08:10,445 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,446 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:08:10,447 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35397
2024-03-22 06:08:10,447 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35397
2024-03-22 06:08:10,447 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36749
2024-03-22 06:08:10,448 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35889
2024-03-22 06:08:10,448 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,448 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:08:10,448 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:08:10,448 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ldra5wz0
2024-03-22 06:08:10,448 - distributed.worker - INFO - Starting Worker plugin PreImport-410553ad-d202-486a-acea-8081bc978c85
2024-03-22 06:08:10,448 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6d59daf-1898-46e8-a046-9629d9e3ee13
2024-03-22 06:08:10,448 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ba3f06c-d8f0-47a6-8b1d-20df81dbb598
2024-03-22 06:08:10,448 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,452 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:08:10,453 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42017
2024-03-22 06:08:10,453 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42017
2024-03-22 06:08:10,453 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44425
2024-03-22 06:08:10,453 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35889
2024-03-22 06:08:10,453 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,453 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:08:10,453 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:08:10,453 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qz6zliu_
2024-03-22 06:08:10,454 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c4ec602-790a-4cc8-ba43-e7b32a30ae94
2024-03-22 06:08:10,454 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:08:10,454 - distributed.worker - INFO - Starting Worker plugin PreImport-a7cbff6b-9f36-4828-8389-a8e69b91efaa
2024-03-22 06:08:10,454 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7db38a8-a268-4b3f-a308-484c51c45dde
2024-03-22 06:08:10,454 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,455 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36797
2024-03-22 06:08:10,455 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36797
2024-03-22 06:08:10,455 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42519
2024-03-22 06:08:10,455 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35889
2024-03-22 06:08:10,455 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,455 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:08:10,455 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:08:10,455 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7b2gavb7
2024-03-22 06:08:10,455 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8f9222dd-c11d-4c6d-bdd1-2cdeda4c6f9b
2024-03-22 06:08:10,456 - distributed.worker - INFO - Starting Worker plugin PreImport-a21f9e17-69d9-4feb-bc91-bead0d3f01c1
2024-03-22 06:08:10,456 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b40b76bc-f8e1-40d9-a79d-2f144def18c2
2024-03-22 06:08:10,457 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,459 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:08:10,460 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36931
2024-03-22 06:08:10,460 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36931
2024-03-22 06:08:10,460 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40103
2024-03-22 06:08:10,460 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35889
2024-03-22 06:08:10,460 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,460 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:08:10,460 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:08:10,460 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5nwkwk4c
2024-03-22 06:08:10,460 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e3bb7ea-9570-403d-ac5b-335f5349671e
2024-03-22 06:08:10,460 - distributed.worker - INFO - Starting Worker plugin PreImport-a6b478b0-1755-4c6a-880d-17ebf6071f7e
2024-03-22 06:08:10,461 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77346bd1-ffaf-4687-a4c5-789511e9b08a
2024-03-22 06:08:10,461 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,476 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:08:10,478 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43075
2024-03-22 06:08:10,478 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43075
2024-03-22 06:08:10,478 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43711
2024-03-22 06:08:10,478 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35889
2024-03-22 06:08:10,478 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,478 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:08:10,478 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:08:10,478 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cu1d91n0
2024-03-22 06:08:10,479 - distributed.worker - INFO - Starting Worker plugin PreImport-73ca7017-8025-45ac-8b88-36c02fe00eff
2024-03-22 06:08:10,479 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81035712-8b7a-42ef-a5d3-0dfe05ccc7f8
2024-03-22 06:08:10,479 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0c7e613-c253-4af6-bb93-e5e8633bccf7
2024-03-22 06:08:10,479 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,482 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:08:10,483 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35589
2024-03-22 06:08:10,483 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35589
2024-03-22 06:08:10,483 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35651
2024-03-22 06:08:10,483 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35889
2024-03-22 06:08:10,483 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,483 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:08:10,484 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:08:10,484 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1_huinrf
2024-03-22 06:08:10,484 - distributed.worker - INFO - Starting Worker plugin PreImport-f0239850-5acf-4996-a696-e0b4546166be
2024-03-22 06:08:10,484 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-627fa1c5-dae1-42b6-8ae1-561092878cda
2024-03-22 06:08:10,484 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9dfd5317-eed1-452f-bd17-3687d2612b57
2024-03-22 06:08:10,485 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,485 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:08:10,486 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36105
2024-03-22 06:08:10,486 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36105
2024-03-22 06:08:10,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39577
2024-03-22 06:08:10,486 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:35889
2024-03-22 06:08:10,486 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:10,486 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:08:10,486 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:08:10,487 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r38zx8f5
2024-03-22 06:08:10,487 - distributed.worker - INFO - Starting Worker plugin PreImport-a98850c0-25d2-4adc-8dc9-32badd87f2b8
2024-03-22 06:08:10,487 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c97a314-d366-4502-aad4-3531aa22405c
2024-03-22 06:08:10,487 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5becde6-6737-41b0-95a3-a79a8d98617c
2024-03-22 06:08:10,487 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:11,220 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:08:11,221 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35889
2024-03-22 06:08:11,221 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:11,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35889
2024-03-22 06:08:11,223 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:08:11,224 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35889
2024-03-22 06:08:11,224 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:11,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35889
2024-03-22 06:08:11,239 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:08:11,239 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35889
2024-03-22 06:08:11,240 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:11,241 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35889
2024-03-22 06:08:11,271 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:08:11,272 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35889
2024-03-22 06:08:11,272 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:11,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35889
2024-03-22 06:08:11,275 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:08:11,276 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35889
2024-03-22 06:08:11,276 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:11,278 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35889
2024-03-22 06:08:11,287 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:08:11,288 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35889
2024-03-22 06:08:11,288 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:11,289 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:08:11,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35889
2024-03-22 06:08:11,290 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35889
2024-03-22 06:08:11,290 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:11,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:08:11,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35889
2024-03-22 06:08:11,292 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:35889
2024-03-22 06:08:11,292 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:08:11,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35889
2024-03-22 06:08:11,308 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36797. Reason: nanny-close
2024-03-22 06:08:11,308 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35589. Reason: nanny-close
2024-03-22 06:08:11,309 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42017. Reason: nanny-close
2024-03-22 06:08:11,309 - distributed.core - INFO - Connection to tcp://127.0.0.1:35889 has been closed.
2024-03-22 06:08:11,310 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35397. Reason: nanny-close
2024-03-22 06:08:11,310 - distributed.core - INFO - Connection to tcp://127.0.0.1:35889 has been closed.
2024-03-22 06:08:11,311 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36105. Reason: nanny-close
2024-03-22 06:08:11,311 - distributed.nanny - INFO - Worker closed
2024-03-22 06:08:11,311 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41959. Reason: nanny-close
2024-03-22 06:08:11,312 - distributed.nanny - INFO - Worker closed
2024-03-22 06:08:11,312 - distributed.core - INFO - Connection to tcp://127.0.0.1:35889 has been closed.
2024-03-22 06:08:11,312 - distributed.core - INFO - Connection to tcp://127.0.0.1:35889 has been closed.
2024-03-22 06:08:11,312 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36931. Reason: nanny-close
2024-03-22 06:08:11,313 - distributed.core - INFO - Connection to tcp://127.0.0.1:35889 has been closed.
2024-03-22 06:08:11,313 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43075. Reason: nanny-close
2024-03-22 06:08:11,314 - distributed.nanny - INFO - Worker closed
2024-03-22 06:08:11,314 - distributed.core - INFO - Connection to tcp://127.0.0.1:35889 has been closed.
2024-03-22 06:08:11,314 - distributed.nanny - INFO - Worker closed
2024-03-22 06:08:11,314 - distributed.core - INFO - Connection to tcp://127.0.0.1:35889 has been closed.
2024-03-22 06:08:11,314 - distributed.nanny - INFO - Worker closed
2024-03-22 06:08:11,315 - distributed.nanny - INFO - Worker closed
2024-03-22 06:08:11,315 - distributed.core - INFO - Connection to tcp://127.0.0.1:35889 has been closed.
2024-03-22 06:08:11,316 - distributed.nanny - INFO - Worker closed
2024-03-22 06:08:11,317 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations 2024-03-22 06:08:20,381 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:08:20,385 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration 2024-03-22 06:08:23,714 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:08:23,718 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-03-22 06:08:26,432 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:08:26,440 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] 2024-03-22 06:08:30,164 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-22 06:08:30,168 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] 2024-03-22 06:10:18,104 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:10:18,131 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] 2024-03-22 06:10:24,746 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:10:24,754 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2024-03-22 06:10:24,755 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x15419ed52280>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:10:26,759 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] 2024-03-22 06:10:45,526 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:10:45,532 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x15235d27c2e0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] 2024-03-22 06:11:15,868 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:11:15,874 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x1476c56be2b0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:11:17,878 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-03-22 06:11:46,371 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:11:46,377 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x1468ef9eb2b0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:11:48,381 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-03-22 06:12:16,879 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:12:16,885 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x152a593d72b0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:12:18,889 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-03-22 06:12:47,387 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:12:47,393 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x14e80be322b0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:12:49,397 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-03-22 06:13:17,994 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:13:18,000 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x1463274b92b0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:13:20,004 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-03-22 06:13:52,934 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:13:52,938 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 12 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
