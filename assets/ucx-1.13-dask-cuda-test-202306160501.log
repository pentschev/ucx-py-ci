============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.2, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-06-16 05:40:12,988 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:12,994 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36329 instead
  warnings.warn(
2023-06-16 05:40:12,999 - distributed.scheduler - INFO - State start
2023-06-16 05:40:13,022 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:13,023 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-16 05:40:13,024 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36329/status
2023-06-16 05:40:13,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45411'
2023-06-16 05:40:13,102 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42581'
2023-06-16 05:40:13,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37109'
2023-06-16 05:40:13,112 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43259'
2023-06-16 05:40:14,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:14,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:14,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:14,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:14,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:14,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:14,701 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:14,701 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:14,701 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:14,724 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:14,724 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:14,731 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-06-16 05:40:14,810 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33543
2023-06-16 05:40:14,810 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33543
2023-06-16 05:40:14,810 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40683
2023-06-16 05:40:14,811 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-16 05:40:14,811 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:14,811 - distributed.worker - INFO -               Threads:                          4
2023-06-16 05:40:14,811 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-16 05:40:14,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6ymx91jm
2023-06-16 05:40:14,811 - distributed.worker - INFO - Starting Worker plugin PreImport-4f1e9484-b4fe-4f73-b3a8-b18fdbabb8b3
2023-06-16 05:40:14,811 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e05ec8d3-8649-4c18-9df3-18fca02fc685
2023-06-16 05:40:14,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6e89391-a191-41dd-9ef9-9fefab818a9a
2023-06-16 05:40:14,811 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:14,824 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33543', status: init, memory: 0, processing: 0>
2023-06-16 05:40:14,835 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33543
2023-06-16 05:40:14,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36696
2023-06-16 05:40:14,835 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-16 05:40:14,836 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:14,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-16 05:40:14,886 - distributed.scheduler - INFO - Receive client connection: Client-42b2ea3a-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:14,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36698
2023-06-16 05:40:17,711 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33815
2023-06-16 05:40:17,711 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33815
2023-06-16 05:40:17,711 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39405
2023-06-16 05:40:17,711 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-16 05:40:17,712 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:17,712 - distributed.worker - INFO -               Threads:                          4
2023-06-16 05:40:17,712 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-16 05:40:17,712 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1dw_g5pk
2023-06-16 05:40:17,712 - distributed.worker - INFO - Starting Worker plugin PreImport-0d171055-07cc-4433-ab79-17034d685056
2023-06-16 05:40:17,712 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3020d87-0a9f-48a2-8e92-373bd9d46d38
2023-06-16 05:40:17,712 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8a565a3-3e4a-47b0-9929-ec55e55e42a6
2023-06-16 05:40:17,713 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:17,735 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33815', status: init, memory: 0, processing: 0>
2023-06-16 05:40:17,736 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33815
2023-06-16 05:40:17,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36742
2023-06-16 05:40:17,737 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-16 05:40:17,737 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:17,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-16 05:40:17,801 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46329
2023-06-16 05:40:17,801 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46329
2023-06-16 05:40:17,801 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36167
2023-06-16 05:40:17,801 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-16 05:40:17,801 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:17,801 - distributed.worker - INFO -               Threads:                          4
2023-06-16 05:40:17,801 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-16 05:40:17,802 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hh9yhnbb
2023-06-16 05:40:17,802 - distributed.worker - INFO - Starting Worker plugin PreImport-e9a8d91e-8f80-44c4-a48a-4d9a21f17635
2023-06-16 05:40:17,802 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-303ff442-face-4509-b5e3-cf7cbe55253a
2023-06-16 05:40:17,802 - distributed.worker - INFO - Starting Worker plugin RMMSetup-477a6776-f434-4257-9f2a-052901661217
2023-06-16 05:40:17,803 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:17,818 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33637
2023-06-16 05:40:17,818 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33637
2023-06-16 05:40:17,818 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36153
2023-06-16 05:40:17,819 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-16 05:40:17,819 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:17,819 - distributed.worker - INFO -               Threads:                          4
2023-06-16 05:40:17,819 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-16 05:40:17,819 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yqayp098
2023-06-16 05:40:17,819 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45d14952-d23f-487b-8b58-e64b9d320a56
2023-06-16 05:40:17,819 - distributed.worker - INFO - Starting Worker plugin PreImport-bcd58430-ac0b-49d1-aff4-18854d854465
2023-06-16 05:40:17,819 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65bc49bc-2ee8-4f38-8cbc-200f0eeae76a
2023-06-16 05:40:17,819 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:17,830 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46329', status: init, memory: 0, processing: 0>
2023-06-16 05:40:17,831 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46329
2023-06-16 05:40:17,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36752
2023-06-16 05:40:17,832 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-16 05:40:17,832 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:17,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-16 05:40:17,838 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33637', status: init, memory: 0, processing: 0>
2023-06-16 05:40:17,839 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33637
2023-06-16 05:40:17,839 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36768
2023-06-16 05:40:17,839 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-16 05:40:17,840 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:17,841 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-16 05:40:17,868 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-16 05:40:17,868 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-16 05:40:17,869 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-16 05:40:17,869 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-16 05:40:17,874 - distributed.scheduler - INFO - Remove client Client-42b2ea3a-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:17,874 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36698; closing.
2023-06-16 05:40:17,874 - distributed.scheduler - INFO - Remove client Client-42b2ea3a-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:17,875 - distributed.scheduler - INFO - Close client connection: Client-42b2ea3a-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:17,876 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45411'. Reason: nanny-close
2023-06-16 05:40:17,876 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37109'. Reason: nanny-close
2023-06-16 05:40:17,876 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43259'. Reason: nanny-close
2023-06-16 05:40:17,876 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:17,877 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42581'. Reason: nanny-close
2023-06-16 05:40:17,878 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:17,878 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33815. Reason: nanny-close
2023-06-16 05:40:17,879 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33543. Reason: nanny-close
2023-06-16 05:40:17,880 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-16 05:40:17,880 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36696; closing.
2023-06-16 05:40:17,880 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33543', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:17,881 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33543
2023-06-16 05:40:17,881 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-16 05:40:17,881 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:17,882 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33543
2023-06-16 05:40:17,882 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36742; closing.
2023-06-16 05:40:17,882 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33543
2023-06-16 05:40:17,882 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33815', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:17,882 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:17,882 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33815
2023-06-16 05:40:17,882 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:17,883 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:17,883 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33637. Reason: nanny-close
2023-06-16 05:40:17,883 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46329. Reason: nanny-close
2023-06-16 05:40:17,885 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-16 05:40:17,885 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36768; closing.
2023-06-16 05:40:17,885 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33637', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:17,885 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33637
2023-06-16 05:40:17,886 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-16 05:40:17,886 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:17,886 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36752; closing.
2023-06-16 05:40:17,887 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46329', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:17,887 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46329
2023-06-16 05:40:17,887 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:40:17,887 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:19,193 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:40:19,193 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:40:19,194 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:40:19,194 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-16 05:40:19,195 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-06-16 05:40:21,080 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:21,083 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35535 instead
  warnings.warn(
2023-06-16 05:40:21,087 - distributed.scheduler - INFO - State start
2023-06-16 05:40:21,105 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:21,106 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-16 05:40:21,107 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35535/status
2023-06-16 05:40:21,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42695'
2023-06-16 05:40:21,289 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39301'
2023-06-16 05:40:21,290 - distributed.scheduler - INFO - Receive client connection: Client-47946c90-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:21,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44339'
2023-06-16 05:40:21,300 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42111'
2023-06-16 05:40:21,303 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53492
2023-06-16 05:40:21,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34769'
2023-06-16 05:40:21,316 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36127'
2023-06-16 05:40:21,324 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41119'
2023-06-16 05:40:21,335 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33325'
2023-06-16 05:40:22,805 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:22,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:22,829 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:22,954 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:22,954 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:22,954 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:22,954 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:22,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:22,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:22,994 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:22,994 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:22,994 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:22,994 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:22,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:22,996 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:23,004 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:23,004 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:23,104 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:23,136 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:23,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:23,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:23,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:23,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:23,139 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:24,454 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42853
2023-06-16 05:40:24,454 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42853
2023-06-16 05:40:24,454 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35443
2023-06-16 05:40:24,454 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:24,455 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:24,455 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:24,455 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:24,455 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-00isupgs
2023-06-16 05:40:24,455 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c57d70a8-36f1-4517-97d9-e39be51c7699
2023-06-16 05:40:24,796 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6746a95c-6f00-434a-b3fc-0122c0d23ae4
2023-06-16 05:40:24,796 - distributed.worker - INFO - Starting Worker plugin PreImport-4351c33e-dc78-4b6a-8655-489377403a6f
2023-06-16 05:40:24,796 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:24,831 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42853', status: init, memory: 0, processing: 0>
2023-06-16 05:40:24,833 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42853
2023-06-16 05:40:24,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53506
2023-06-16 05:40:24,833 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:24,833 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:24,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:25,788 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38841
2023-06-16 05:40:25,788 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38841
2023-06-16 05:40:25,788 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41289
2023-06-16 05:40:25,788 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35509
2023-06-16 05:40:25,788 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41289
2023-06-16 05:40:25,788 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:25,788 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:25,788 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33647
2023-06-16 05:40:25,788 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:25,788 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:25,788 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:25,788 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:25,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qou2zt2p
2023-06-16 05:40:25,788 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:25,788 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:25,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_pm1c6l0
2023-06-16 05:40:25,789 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ffa6b6ab-ff52-477d-b55c-6ad93d030b15
2023-06-16 05:40:25,789 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7079f575-bfac-4311-bb07-3fd975f51a7d
2023-06-16 05:40:25,790 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41657
2023-06-16 05:40:25,790 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41657
2023-06-16 05:40:25,790 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33435
2023-06-16 05:40:25,790 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:25,790 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:25,790 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:25,790 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:25,790 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n2owml82
2023-06-16 05:40:25,791 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0d4b37d-f6d1-46dc-b5dc-d14b6df388e6
2023-06-16 05:40:25,791 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44349
2023-06-16 05:40:25,791 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39295
2023-06-16 05:40:25,791 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44349
2023-06-16 05:40:25,791 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39295
2023-06-16 05:40:25,791 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40151
2023-06-16 05:40:25,791 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36877
2023-06-16 05:40:25,791 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:25,791 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:25,791 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:25,791 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:25,791 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:25,791 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:25,791 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37671
2023-06-16 05:40:25,791 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:25,791 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:25,791 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37671
2023-06-16 05:40:25,791 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pfihqki5
2023-06-16 05:40:25,791 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zr1gevx9
2023-06-16 05:40:25,791 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33491
2023-06-16 05:40:25,791 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37085
2023-06-16 05:40:25,791 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:25,791 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37085
2023-06-16 05:40:25,792 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:25,792 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:25,792 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35359
2023-06-16 05:40:25,792 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:25,792 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:25,792 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wtv0in0m
2023-06-16 05:40:25,792 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e386e325-dd06-488b-8640-fd8d3eb690ad
2023-06-16 05:40:25,792 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:25,792 - distributed.worker - INFO - Starting Worker plugin RMMSetup-604bb6d3-bf99-4fb2-aff0-5ae45bc99865
2023-06-16 05:40:25,792 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:25,792 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:25,792 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sfnqgcfm
2023-06-16 05:40:25,792 - distributed.worker - INFO - Starting Worker plugin PreImport-212ebe41-965a-4253-8555-2ee0ad62d09f
2023-06-16 05:40:25,792 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0749096a-a1ed-4e37-aa76-7e1070f16543
2023-06-16 05:40:25,792 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a85278a5-918a-4143-af4d-728008c71078
2023-06-16 05:40:25,792 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d18519f6-9a4f-4238-b937-4ff5e85e11db
2023-06-16 05:40:26,202 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-831d8873-c68c-4cad-9d52-0f99ab64aef3
2023-06-16 05:40:26,202 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,202 - distributed.worker - INFO - Starting Worker plugin PreImport-45708aa0-0e73-4a9a-b1f8-0cdc48ea6c37
2023-06-16 05:40:26,202 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,239 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37671', status: init, memory: 0, processing: 0>
2023-06-16 05:40:26,240 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37671
2023-06-16 05:40:26,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53516
2023-06-16 05:40:26,240 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:26,241 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,243 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:26,246 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-443e4b26-d9dc-434e-845e-e952c8091cea
2023-06-16 05:40:26,247 - distributed.worker - INFO - Starting Worker plugin PreImport-ab791cd6-838b-452e-b895-c16c389d9a36
2023-06-16 05:40:26,247 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,247 - distributed.worker - INFO - Starting Worker plugin PreImport-69314e3a-162f-4c1f-9757-4a1f9115ad65
2023-06-16 05:40:26,247 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab0a55a4-a08e-4117-99d6-6480bc881b75
2023-06-16 05:40:26,247 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-828527e3-64cc-42b8-bc17-c366cc240d76
2023-06-16 05:40:26,247 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55bd80f0-4e9f-4a53-b30a-0f0e8c03bb94
2023-06-16 05:40:26,247 - distributed.worker - INFO - Starting Worker plugin PreImport-1c1b7d8d-db36-4792-89fa-3277376644ca
2023-06-16 05:40:26,248 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,248 - distributed.worker - INFO - Starting Worker plugin PreImport-b28ae65f-37df-4340-b8b4-cca62613c59c
2023-06-16 05:40:26,248 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,248 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aee3882f-e2f5-4783-9ea5-eb02fc99bf32
2023-06-16 05:40:26,248 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,248 - distributed.worker - INFO - Starting Worker plugin PreImport-ff16227e-2e6c-4581-9e1c-ae5835a098ef
2023-06-16 05:40:26,248 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,249 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41657', status: init, memory: 0, processing: 0>
2023-06-16 05:40:26,250 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41657
2023-06-16 05:40:26,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53522
2023-06-16 05:40:26,250 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:26,250 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:26,411 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41289', status: init, memory: 0, processing: 0>
2023-06-16 05:40:26,412 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41289
2023-06-16 05:40:26,412 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53538
2023-06-16 05:40:26,413 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:26,413 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44349', status: init, memory: 0, processing: 0>
2023-06-16 05:40:26,413 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,413 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44349
2023-06-16 05:40:26,413 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53534
2023-06-16 05:40:26,414 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:26,414 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:26,416 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:26,420 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39295', status: init, memory: 0, processing: 0>
2023-06-16 05:40:26,421 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39295
2023-06-16 05:40:26,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53566
2023-06-16 05:40:26,421 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:26,421 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38841', status: init, memory: 0, processing: 0>
2023-06-16 05:40:26,421 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,422 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38841
2023-06-16 05:40:26,422 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53546
2023-06-16 05:40:26,422 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:26,423 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,423 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37085', status: init, memory: 0, processing: 0>
2023-06-16 05:40:26,423 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37085
2023-06-16 05:40:26,423 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53554
2023-06-16 05:40:26,424 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:26,424 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:26,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:26,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:26,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:26,516 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:26,516 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:26,516 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:26,517 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:26,517 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:26,517 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:26,517 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:26,517 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:26,521 - distributed.scheduler - INFO - Remove client Client-47946c90-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:26,521 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53492; closing.
2023-06-16 05:40:26,522 - distributed.scheduler - INFO - Remove client Client-47946c90-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:26,522 - distributed.scheduler - INFO - Close client connection: Client-47946c90-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:26,523 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42111'. Reason: nanny-close
2023-06-16 05:40:26,523 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:26,524 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42695'. Reason: nanny-close
2023-06-16 05:40:26,524 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:26,525 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39295. Reason: nanny-close
2023-06-16 05:40:26,525 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39301'. Reason: nanny-close
2023-06-16 05:40:26,525 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:26,525 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44339'. Reason: nanny-close
2023-06-16 05:40:26,526 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:26,526 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41289. Reason: nanny-close
2023-06-16 05:40:26,526 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37085. Reason: nanny-close
2023-06-16 05:40:26,526 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34769'. Reason: nanny-close
2023-06-16 05:40:26,526 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:26,527 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42853. Reason: nanny-close
2023-06-16 05:40:26,527 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36127'. Reason: nanny-close
2023-06-16 05:40:26,527 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:26,527 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53566; closing.
2023-06-16 05:40:26,527 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:26,527 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41119'. Reason: nanny-close
2023-06-16 05:40:26,527 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38841. Reason: nanny-close
2023-06-16 05:40:26,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39295', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:26,528 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39295
2023-06-16 05:40:26,528 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:26,528 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:26,528 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33325'. Reason: nanny-close
2023-06-16 05:40:26,528 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41657. Reason: nanny-close
2023-06-16 05:40:26,528 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:26,528 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:26,529 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:26,529 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37671. Reason: nanny-close
2023-06-16 05:40:26,529 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:26,529 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53538; closing.
2023-06-16 05:40:26,529 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:26,529 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39295
2023-06-16 05:40:26,529 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39295
2023-06-16 05:40:26,530 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:26,530 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41289', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:26,530 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44349. Reason: nanny-close
2023-06-16 05:40:26,530 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41289
2023-06-16 05:40:26,530 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:26,530 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:26,530 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39295
2023-06-16 05:40:26,530 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39295
2023-06-16 05:40:26,530 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53554; closing.
2023-06-16 05:40:26,530 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53506; closing.
2023-06-16 05:40:26,531 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:26,531 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:26,531 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37085', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:26,531 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37085
2023-06-16 05:40:26,531 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42853', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:26,531 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:26,531 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42853
2023-06-16 05:40:26,531 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:26,532 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53546; closing.
2023-06-16 05:40:26,532 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:26,532 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:26,532 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38841', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:26,532 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38841
2023-06-16 05:40:26,532 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:26,533 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53522; closing.
2023-06-16 05:40:26,533 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53516; closing.
2023-06-16 05:40:26,533 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41657', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:26,533 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41657
2023-06-16 05:40:26,534 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37671', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:26,534 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37671
2023-06-16 05:40:26,534 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53534; closing.
2023-06-16 05:40:26,534 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44349', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:26,535 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44349
2023-06-16 05:40:26,535 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:40:28,240 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:40:28,241 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:40:28,241 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:40:28,242 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-16 05:40:28,242 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-06-16 05:40:30,160 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:30,164 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40783 instead
  warnings.warn(
2023-06-16 05:40:30,168 - distributed.scheduler - INFO - State start
2023-06-16 05:40:30,187 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:30,188 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-16 05:40:30,188 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40783/status
2023-06-16 05:40:30,373 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46425'
2023-06-16 05:40:30,386 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39457'
2023-06-16 05:40:30,399 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41735'
2023-06-16 05:40:30,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44049'
2023-06-16 05:40:30,408 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45491'
2023-06-16 05:40:30,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32919'
2023-06-16 05:40:30,425 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42461'
2023-06-16 05:40:30,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37531'
2023-06-16 05:40:31,335 - distributed.scheduler - INFO - Receive client connection: Client-4cfa08af-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:31,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33138
2023-06-16 05:40:32,026 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:32,026 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:32,050 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:32,052 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:32,052 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:32,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:32,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:32,063 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:32,063 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:32,080 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:32,080 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:32,088 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:32,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:32,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:32,111 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:32,111 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:32,121 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:32,121 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:32,127 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:32,130 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:32,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:32,147 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:32,156 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:32,176 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:34,019 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42051
2023-06-16 05:40:34,019 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42051
2023-06-16 05:40:34,020 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40015
2023-06-16 05:40:34,020 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,020 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,020 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:34,020 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:34,020 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_3ftzped
2023-06-16 05:40:34,020 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8ed1471-2836-4fe4-9d97-1dc75a5ecf18
2023-06-16 05:40:34,090 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc48db83-ad21-4e88-9f71-67b1fabc78db
2023-06-16 05:40:34,090 - distributed.worker - INFO - Starting Worker plugin PreImport-73af644f-d08a-467e-becd-3e0b692f8604
2023-06-16 05:40:34,090 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,129 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42051', status: init, memory: 0, processing: 0>
2023-06-16 05:40:34,131 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42051
2023-06-16 05:40:34,131 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33158
2023-06-16 05:40:34,132 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,132 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,134 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:34,711 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40943
2023-06-16 05:40:34,712 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40943
2023-06-16 05:40:34,712 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46117
2023-06-16 05:40:34,712 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,712 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,712 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:34,712 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:34,712 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_lshsoes
2023-06-16 05:40:34,712 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73b750a5-4a4a-4cc7-b700-b7c14400acd2
2023-06-16 05:40:34,713 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43707
2023-06-16 05:40:34,713 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43707
2023-06-16 05:40:34,713 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35323
2023-06-16 05:40:34,713 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45237
2023-06-16 05:40:34,713 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35323
2023-06-16 05:40:34,713 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,713 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37159
2023-06-16 05:40:34,713 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,713 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:34,713 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,714 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,714 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:34,714 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:34,714 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1hvionlq
2023-06-16 05:40:34,714 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:34,714 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_9mo9uvj
2023-06-16 05:40:34,714 - distributed.worker - INFO - Starting Worker plugin PreImport-f8e6d4db-d098-4a34-b961-41f35ca25504
2023-06-16 05:40:34,714 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4458e640-28a2-4b12-b6c5-5ee493872e6d
2023-06-16 05:40:34,714 - distributed.worker - INFO - Starting Worker plugin PreImport-a51522be-870c-4b39-bd47-0ba5e3d686a6
2023-06-16 05:40:34,714 - distributed.worker - INFO - Starting Worker plugin RMMSetup-746526a8-8fe7-4dfc-89a4-88bbe16c5080
2023-06-16 05:40:34,714 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cce908c7-d280-4538-9904-e637a48ef694
2023-06-16 05:40:34,714 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a3ac5ea9-874f-431c-b4ff-9d1acf5a094b
2023-06-16 05:40:34,727 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37357
2023-06-16 05:40:34,727 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37357
2023-06-16 05:40:34,728 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37355
2023-06-16 05:40:34,728 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,728 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,728 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:34,728 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:34,728 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_j8fhiqa
2023-06-16 05:40:34,729 - distributed.worker - INFO - Starting Worker plugin PreImport-3aea35dc-d38b-441e-b5d7-3e6600437769
2023-06-16 05:40:34,729 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea3fcf45-b808-4f63-9bff-e37c68e01ca5
2023-06-16 05:40:34,729 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb23ec2c-4159-4360-bb1f-1932e83a1bee
2023-06-16 05:40:34,737 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1540c9ec-d9d7-4480-a992-e99c45de40da
2023-06-16 05:40:34,737 - distributed.worker - INFO - Starting Worker plugin PreImport-baad4de6-825b-420e-b30d-2c8fa3ef9d90
2023-06-16 05:40:34,738 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,738 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,738 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,741 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,767 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43707', status: init, memory: 0, processing: 0>
2023-06-16 05:40:34,768 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43707
2023-06-16 05:40:34,768 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33172
2023-06-16 05:40:34,769 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,769 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,769 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35323', status: init, memory: 0, processing: 0>
2023-06-16 05:40:34,770 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35323
2023-06-16 05:40:34,770 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33188
2023-06-16 05:40:34,771 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:34,771 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,771 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,772 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37357', status: init, memory: 0, processing: 0>
2023-06-16 05:40:34,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:34,773 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37357
2023-06-16 05:40:34,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33202
2023-06-16 05:40:34,774 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,774 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,774 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40943', status: init, memory: 0, processing: 0>
2023-06-16 05:40:34,775 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40943
2023-06-16 05:40:34,775 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33190
2023-06-16 05:40:34,776 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,776 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,776 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:34,779 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:34,892 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37533
2023-06-16 05:40:34,892 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37533
2023-06-16 05:40:34,892 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41093
2023-06-16 05:40:34,892 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,892 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,892 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:34,892 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:34,892 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5ha91pos
2023-06-16 05:40:34,893 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d30a7b4-d003-47aa-b361-ff1ae85c2e6a
2023-06-16 05:40:34,893 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44549
2023-06-16 05:40:34,893 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44549
2023-06-16 05:40:34,893 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42597
2023-06-16 05:40:34,893 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,893 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,893 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:34,893 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:34,893 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xjdlk8ln
2023-06-16 05:40:34,894 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b291e79b-09b7-4b49-a814-447c4a80a21c
2023-06-16 05:40:34,894 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33885
2023-06-16 05:40:34,894 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33885
2023-06-16 05:40:34,894 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37651
2023-06-16 05:40:34,894 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,894 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,894 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:34,894 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:34,894 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c9bd0f71
2023-06-16 05:40:34,895 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fcc44db8-c82a-4455-9666-a622d3c51a7d
2023-06-16 05:40:34,905 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d22db515-dc86-493a-ad87-aaf6d7361cb4
2023-06-16 05:40:34,906 - distributed.worker - INFO - Starting Worker plugin PreImport-282a7fb9-7b86-431a-b2b6-23abfe28d75b
2023-06-16 05:40:34,906 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cef806a2-34d5-484b-9f8b-17fe4d41f4fc
2023-06-16 05:40:34,906 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,906 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-736e07ee-dbb4-47be-897d-eac8e952651f
2023-06-16 05:40:34,906 - distributed.worker - INFO - Starting Worker plugin PreImport-b1b293ca-ebb7-43f2-b1e6-c15511444cb3
2023-06-16 05:40:34,907 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,907 - distributed.worker - INFO - Starting Worker plugin PreImport-a447827b-1937-49f6-a685-7acace5e32d5
2023-06-16 05:40:34,907 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,935 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44549', status: init, memory: 0, processing: 0>
2023-06-16 05:40:34,937 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44549
2023-06-16 05:40:34,937 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33230
2023-06-16 05:40:34,937 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,937 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,938 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33885', status: init, memory: 0, processing: 0>
2023-06-16 05:40:34,939 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33885
2023-06-16 05:40:34,939 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33242
2023-06-16 05:40:34,939 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,940 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:34,940 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37533', status: init, memory: 0, processing: 0>
2023-06-16 05:40:34,941 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37533
2023-06-16 05:40:34,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33216
2023-06-16 05:40:34,942 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:34,942 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:34,942 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:34,945 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:34,965 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:34,965 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:34,965 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:34,965 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:34,966 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:34,966 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:34,966 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:34,966 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:34,971 - distributed.scheduler - INFO - Remove client Client-4cfa08af-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:34,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33138; closing.
2023-06-16 05:40:34,972 - distributed.scheduler - INFO - Remove client Client-4cfa08af-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:34,973 - distributed.scheduler - INFO - Close client connection: Client-4cfa08af-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:34,973 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44049'. Reason: nanny-close
2023-06-16 05:40:34,974 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46425'. Reason: nanny-close
2023-06-16 05:40:34,974 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:34,975 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39457'. Reason: nanny-close
2023-06-16 05:40:34,975 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:34,976 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40943. Reason: nanny-close
2023-06-16 05:40:34,976 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41735'. Reason: nanny-close
2023-06-16 05:40:34,976 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:34,976 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35323. Reason: nanny-close
2023-06-16 05:40:34,976 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45491'. Reason: nanny-close
2023-06-16 05:40:34,976 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32919'. Reason: nanny-close
2023-06-16 05:40:34,977 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42461'. Reason: nanny-close
2023-06-16 05:40:34,977 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37357. Reason: nanny-close
2023-06-16 05:40:34,977 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:34,977 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37531'. Reason: nanny-close
2023-06-16 05:40:34,977 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:34,977 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:34,978 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42051. Reason: nanny-close
2023-06-16 05:40:34,978 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33190; closing.
2023-06-16 05:40:34,978 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:34,978 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40943', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:34,978 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:34,978 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40943
2023-06-16 05:40:34,978 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43707. Reason: nanny-close
2023-06-16 05:40:34,979 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:34,979 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:34,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:34,979 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:34,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:34,979 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:34,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:34,980 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33188; closing.
2023-06-16 05:40:34,980 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:34,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37533. Reason: nanny-close
2023-06-16 05:40:34,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44549. Reason: nanny-close
2023-06-16 05:40:34,980 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:34,981 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40943
2023-06-16 05:40:34,981 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:34,981 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35323', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:34,981 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35323
2023-06-16 05:40:34,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33885. Reason: nanny-close
2023-06-16 05:40:34,982 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33202; closing.
2023-06-16 05:40:34,982 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33158; closing.
2023-06-16 05:40:34,982 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40943
2023-06-16 05:40:34,983 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40943
2023-06-16 05:40:34,983 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37357', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:34,983 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37357
2023-06-16 05:40:34,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:34,983 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42051', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:34,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:34,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:34,983 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42051
2023-06-16 05:40:34,984 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33172; closing.
2023-06-16 05:40:34,984 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:34,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43707', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:34,985 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43707
2023-06-16 05:40:34,985 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:34,985 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:34,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33230; closing.
2023-06-16 05:40:34,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33216; closing.
2023-06-16 05:40:34,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33242; closing.
2023-06-16 05:40:34,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44549', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:34,987 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-06-16 05:40:34,988 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37533', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:34,988 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37533
2023-06-16 05:40:34,988 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33885', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:34,988 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33885
2023-06-16 05:40:34,988 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:40:34,989 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33242>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-16 05:40:34,991 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33216>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-16 05:40:36,341 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:40:36,341 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:40:36,342 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:40:36,343 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-16 05:40:36,343 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-06-16 05:40:38,155 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:38,159 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37851 instead
  warnings.warn(
2023-06-16 05:40:38,163 - distributed.scheduler - INFO - State start
2023-06-16 05:40:38,181 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:38,182 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-16 05:40:38,183 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37851/status
2023-06-16 05:40:38,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43243'
2023-06-16 05:40:38,435 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38035'
2023-06-16 05:40:38,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41357'
2023-06-16 05:40:38,451 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46591'
2023-06-16 05:40:38,459 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38783'
2023-06-16 05:40:38,467 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39237'
2023-06-16 05:40:38,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37697'
2023-06-16 05:40:38,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41347'
2023-06-16 05:40:39,990 - distributed.scheduler - INFO - Receive client connection: Client-51c7c48c-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:40,002 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48874
2023-06-16 05:40:40,029 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:40,029 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:40,053 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:40,091 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:40,092 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:40,116 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:40,118 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:40,118 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:40,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:40,172 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:40,174 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:40,174 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:40,175 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:40,175 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:40,176 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:40,176 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:40,199 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:40,199 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:40,301 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:40,334 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:40,336 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:40,337 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:40,338 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:40,338 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:41,341 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45887
2023-06-16 05:40:41,341 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45887
2023-06-16 05:40:41,341 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42621
2023-06-16 05:40:41,341 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:41,341 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:41,341 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:41,341 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:41,341 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-un5dbr6g
2023-06-16 05:40:41,342 - distributed.worker - INFO - Starting Worker plugin PreImport-f417664f-1b76-4478-9c02-9071f0c96652
2023-06-16 05:40:41,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-139fbcc9-56c6-4d3d-8602-d6552f763783
2023-06-16 05:40:41,342 - distributed.worker - INFO - Starting Worker plugin RMMSetup-073b4a09-ba23-4344-92d3-d273fe2083a1
2023-06-16 05:40:42,190 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,229 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45887', status: init, memory: 0, processing: 0>
2023-06-16 05:40:42,230 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45887
2023-06-16 05:40:42,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48892
2023-06-16 05:40:42,231 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:42,231 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:42,656 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45747
2023-06-16 05:40:42,656 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45747
2023-06-16 05:40:42,656 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44727
2023-06-16 05:40:42,656 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:42,657 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,657 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:42,657 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:42,657 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zkfmdq4h
2023-06-16 05:40:42,658 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3374496-ce9f-4d6d-9a3e-446822dd8584
2023-06-16 05:40:42,658 - distributed.worker - INFO - Starting Worker plugin PreImport-67a5ce18-a704-4e9e-8050-0d837ed7d575
2023-06-16 05:40:42,658 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44819329-2737-4f56-82a0-83cd5d5b60b3
2023-06-16 05:40:42,774 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,805 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45747', status: init, memory: 0, processing: 0>
2023-06-16 05:40:42,806 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45747
2023-06-16 05:40:42,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48896
2023-06-16 05:40:42,806 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:42,806 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:42,855 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33295
2023-06-16 05:40:42,855 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33295
2023-06-16 05:40:42,855 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37275
2023-06-16 05:40:42,855 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:42,856 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,856 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:42,856 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:42,856 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-knbjbp8h
2023-06-16 05:40:42,856 - distributed.worker - INFO - Starting Worker plugin RMMSetup-006217ae-6b8a-4b7d-925f-43b6f12edacb
2023-06-16 05:40:42,861 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45809
2023-06-16 05:40:42,861 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45809
2023-06-16 05:40:42,862 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42385
2023-06-16 05:40:42,862 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:42,862 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,862 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:42,862 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:42,862 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-31e4ng9f
2023-06-16 05:40:42,862 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72f52f90-e128-4f11-b9d8-9408e6a5bb72
2023-06-16 05:40:42,864 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45191
2023-06-16 05:40:42,864 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45191
2023-06-16 05:40:42,864 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34685
2023-06-16 05:40:42,864 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41237
2023-06-16 05:40:42,864 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36283
2023-06-16 05:40:42,864 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34685
2023-06-16 05:40:42,864 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:42,864 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41237
2023-06-16 05:40:42,864 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40815
2023-06-16 05:40:42,864 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,864 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:42,864 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43471
2023-06-16 05:40:42,864 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:42,864 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,864 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:42,864 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:42,864 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,864 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:42,864 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sr7sqj4i
2023-06-16 05:40:42,864 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:42,864 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:42,864 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bvm4nhzi
2023-06-16 05:40:42,864 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:42,865 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w9zg6trb
2023-06-16 05:40:42,865 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc1ac843-49d2-4ec2-903f-a773be09679d
2023-06-16 05:40:42,865 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef1f9de0-66a3-4a49-ba4b-38eea4ddca4f
2023-06-16 05:40:42,865 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a30497f5-1172-4791-8c0f-c8d9fadf365f
2023-06-16 05:40:42,865 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33007
2023-06-16 05:40:42,865 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33007
2023-06-16 05:40:42,865 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46605
2023-06-16 05:40:42,865 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:42,865 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:42,865 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:42,865 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:42,865 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qiq0qfj0
2023-06-16 05:40:42,866 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a970b5ef-3f41-429e-ae58-773821ef34e1
2023-06-16 05:40:43,056 - distributed.worker - INFO - Starting Worker plugin PreImport-b947eaab-cd29-4ce6-9705-a6e301c23445
2023-06-16 05:40:43,056 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-824452d4-5852-4bec-8e9b-e0340e8d9dac
2023-06-16 05:40:43,057 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,059 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88a45e49-bfa6-4796-a9da-50b95d08115c
2023-06-16 05:40:43,059 - distributed.worker - INFO - Starting Worker plugin PreImport-a72cc413-3acf-485d-bef4-78de13f5b332
2023-06-16 05:40:43,060 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,060 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64b3b605-0a2f-4c98-91da-3e4c645e999c
2023-06-16 05:40:43,061 - distributed.worker - INFO - Starting Worker plugin PreImport-61c0f0c2-ebd2-43ba-abaf-c886394bd1ac
2023-06-16 05:40:43,061 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,071 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c5171b4-41f3-47a6-aa21-ff0eab983599
2023-06-16 05:40:43,071 - distributed.worker - INFO - Starting Worker plugin PreImport-5e5fb8d3-f6e0-4a17-bf8c-84b1fb119d88
2023-06-16 05:40:43,071 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d68c43f5-409b-486b-8980-97da3ad27ed5
2023-06-16 05:40:43,071 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0cf8aab-f95f-499b-ba80-d6e8cd150b1a
2023-06-16 05:40:43,071 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,071 - distributed.worker - INFO - Starting Worker plugin PreImport-d2864278-c4cc-4017-8d73-2910859e51e3
2023-06-16 05:40:43,071 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,071 - distributed.worker - INFO - Starting Worker plugin PreImport-402766b6-8a04-4c2e-8831-c98e92331ef8
2023-06-16 05:40:43,071 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,084 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34685', status: init, memory: 0, processing: 0>
2023-06-16 05:40:43,084 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34685
2023-06-16 05:40:43,084 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48924
2023-06-16 05:40:43,085 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:43,085 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,087 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:43,093 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45809', status: init, memory: 0, processing: 0>
2023-06-16 05:40:43,093 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45809
2023-06-16 05:40:43,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48912
2023-06-16 05:40:43,094 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33295', status: init, memory: 0, processing: 0>
2023-06-16 05:40:43,094 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:43,094 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,094 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33295
2023-06-16 05:40:43,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48932
2023-06-16 05:40:43,095 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:43,095 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,095 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45191', status: init, memory: 0, processing: 0>
2023-06-16 05:40:43,096 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45191
2023-06-16 05:40:43,096 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48940
2023-06-16 05:40:43,096 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:43,097 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:43,098 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:43,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:43,099 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41237', status: init, memory: 0, processing: 0>
2023-06-16 05:40:43,100 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41237
2023-06-16 05:40:43,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48944
2023-06-16 05:40:43,100 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:43,100 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:43,106 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33007', status: init, memory: 0, processing: 0>
2023-06-16 05:40:43,107 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33007
2023-06-16 05:40:43,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48950
2023-06-16 05:40:43,107 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:43,108 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:43,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:43,141 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:43,141 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:43,141 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:43,141 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:43,141 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:43,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:43,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:43,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:43,152 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-16 05:40:43,152 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-16 05:40:43,152 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-16 05:40:43,153 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-16 05:40:43,153 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-16 05:40:43,153 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-16 05:40:43,153 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-16 05:40:43,153 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-16 05:40:43,158 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:40:43,160 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:40:43,162 - distributed.scheduler - INFO - Remove client Client-51c7c48c-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:43,162 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48874; closing.
2023-06-16 05:40:43,163 - distributed.scheduler - INFO - Remove client Client-51c7c48c-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:43,163 - distributed.scheduler - INFO - Close client connection: Client-51c7c48c-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:43,164 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41357'. Reason: nanny-close
2023-06-16 05:40:43,164 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:43,165 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39237'. Reason: nanny-close
2023-06-16 05:40:43,165 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:43,166 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43243'. Reason: nanny-close
2023-06-16 05:40:43,166 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45747. Reason: nanny-close
2023-06-16 05:40:43,166 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:43,166 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45809. Reason: nanny-close
2023-06-16 05:40:43,166 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38035'. Reason: nanny-close
2023-06-16 05:40:43,167 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:43,167 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41237. Reason: nanny-close
2023-06-16 05:40:43,167 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46591'. Reason: nanny-close
2023-06-16 05:40:43,167 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:43,168 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48896; closing.
2023-06-16 05:40:43,168 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:43,168 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45191. Reason: nanny-close
2023-06-16 05:40:43,168 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38783'. Reason: nanny-close
2023-06-16 05:40:43,168 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45747', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:43,168 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45747
2023-06-16 05:40:43,168 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:43,168 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:43,168 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37697'. Reason: nanny-close
2023-06-16 05:40:43,168 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33007. Reason: nanny-close
2023-06-16 05:40:43,169 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:43,169 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:43,169 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:43,169 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33295. Reason: nanny-close
2023-06-16 05:40:43,169 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41347'. Reason: nanny-close
2023-06-16 05:40:43,169 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45747
2023-06-16 05:40:43,169 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:43,169 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45747
2023-06-16 05:40:43,169 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48912; closing.
2023-06-16 05:40:43,170 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:43,170 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:43,170 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45887. Reason: nanny-close
2023-06-16 05:40:43,170 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:43,170 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45809', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:43,170 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45809
2023-06-16 05:40:43,170 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34685. Reason: nanny-close
2023-06-16 05:40:43,170 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45747
2023-06-16 05:40:43,170 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48944; closing.
2023-06-16 05:40:43,171 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:43,171 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45747
2023-06-16 05:40:43,171 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41237', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:43,171 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:43,171 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41237
2023-06-16 05:40:43,171 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48940; closing.
2023-06-16 05:40:43,172 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45747
2023-06-16 05:40:43,172 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:43,172 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45191', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:43,172 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45191
2023-06-16 05:40:43,172 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:43,172 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48950; closing.
2023-06-16 05:40:43,172 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:43,173 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:43,173 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33007', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:43,173 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33007
2023-06-16 05:40:43,173 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:43,173 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48932; closing.
2023-06-16 05:40:43,173 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:43,174 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33295', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:43,174 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33295
2023-06-16 05:40:43,174 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48924; closing.
2023-06-16 05:40:43,174 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:43,175 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48892; closing.
2023-06-16 05:40:43,175 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34685', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:43,175 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34685
2023-06-16 05:40:43,176 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45887', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:43,176 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45887
2023-06-16 05:40:43,176 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:40:44,531 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:40:44,531 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:40:44,532 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:40:44,532 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-16 05:40:44,533 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-06-16 05:40:46,343 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:46,348 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38829 instead
  warnings.warn(
2023-06-16 05:40:46,352 - distributed.scheduler - INFO - State start
2023-06-16 05:40:46,371 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:46,372 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-16 05:40:46,372 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38829/status
2023-06-16 05:40:46,504 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33811'
2023-06-16 05:40:46,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45429'
2023-06-16 05:40:46,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33091'
2023-06-16 05:40:46,534 - distributed.scheduler - INFO - Receive client connection: Client-56a940e7-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:46,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42089'
2023-06-16 05:40:46,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39515'
2023-06-16 05:40:46,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49098
2023-06-16 05:40:46,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42663'
2023-06-16 05:40:46,561 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46289'
2023-06-16 05:40:46,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36691'
2023-06-16 05:40:48,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:48,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:48,114 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:48,116 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:48,116 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:48,141 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:48,158 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:48,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:48,158 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:48,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:48,161 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:48,161 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:48,189 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:48,189 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:48,192 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:48,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:48,207 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:48,208 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:48,208 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:48,219 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:48,219 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:48,248 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:48,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:48,260 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:49,866 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38065
2023-06-16 05:40:49,867 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38065
2023-06-16 05:40:49,867 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34203
2023-06-16 05:40:49,867 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:49,867 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:49,867 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:49,867 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:49,867 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y9sffyh2
2023-06-16 05:40:49,867 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a557786d-66ad-4f4e-ac24-4e4535486714
2023-06-16 05:40:50,046 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5252b67f-eeff-4ee1-aca8-e9a2ccea545b
2023-06-16 05:40:50,046 - distributed.worker - INFO - Starting Worker plugin PreImport-05a5b33c-b035-4a6c-b381-333b65a97630
2023-06-16 05:40:50,046 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,075 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38065', status: init, memory: 0, processing: 0>
2023-06-16 05:40:50,077 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38065
2023-06-16 05:40:50,077 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60516
2023-06-16 05:40:50,077 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,077 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,079 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:50,375 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38089
2023-06-16 05:40:50,375 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38089
2023-06-16 05:40:50,376 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36285
2023-06-16 05:40:50,376 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,376 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,376 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:50,376 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:50,376 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nv4k6fj4
2023-06-16 05:40:50,377 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd2cca70-e73d-413f-b56d-9f1404cca29b
2023-06-16 05:40:50,493 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-52640f76-d0f2-42f2-b755-8dcd5a17fcef
2023-06-16 05:40:50,493 - distributed.worker - INFO - Starting Worker plugin PreImport-ae99a302-187e-4698-8042-92a9d60b3956
2023-06-16 05:40:50,494 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,523 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38089', status: init, memory: 0, processing: 0>
2023-06-16 05:40:50,524 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38089
2023-06-16 05:40:50,524 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60528
2023-06-16 05:40:50,525 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,525 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,528 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:50,730 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36309
2023-06-16 05:40:50,730 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36309
2023-06-16 05:40:50,730 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42767
2023-06-16 05:40:50,730 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,730 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,730 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:50,730 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:50,730 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0j2oq6oc
2023-06-16 05:40:50,731 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e4b07e4-6f10-4cca-b4ff-c0d9b24b905d
2023-06-16 05:40:50,734 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41699
2023-06-16 05:40:50,734 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41699
2023-06-16 05:40:50,734 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38439
2023-06-16 05:40:50,734 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35391
2023-06-16 05:40:50,734 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38439
2023-06-16 05:40:50,734 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,734 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37465
2023-06-16 05:40:50,734 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,734 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,734 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:50,734 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,734 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:50,734 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:50,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-12w095g7
2023-06-16 05:40:50,734 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:50,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-geif682e
2023-06-16 05:40:50,735 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1dbb739b-7706-45f3-9388-609709097ed6
2023-06-16 05:40:50,735 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f396f57-53fd-45ca-ae35-c41f1216c7a5
2023-06-16 05:40:50,735 - distributed.worker - INFO - Starting Worker plugin PreImport-ea3980cd-94bb-4798-a0f5-601d4518ac1e
2023-06-16 05:40:50,735 - distributed.worker - INFO - Starting Worker plugin RMMSetup-872c0e3f-a456-4242-ba01-b57fcfeab089
2023-06-16 05:40:50,757 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39863
2023-06-16 05:40:50,757 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39863
2023-06-16 05:40:50,757 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38891
2023-06-16 05:40:50,757 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40821
2023-06-16 05:40:50,757 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,757 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38891
2023-06-16 05:40:50,757 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,757 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42435
2023-06-16 05:40:50,757 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:50,757 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,757 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:50,757 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,757 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-t2u01w7x
2023-06-16 05:40:50,757 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:50,757 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:50,758 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-r3ow7f67
2023-06-16 05:40:50,758 - distributed.worker - INFO - Starting Worker plugin RMMSetup-319af223-d1cb-48cb-a220-993f3a7fa141
2023-06-16 05:40:50,758 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de377859-da6e-4e41-a74a-6ee4a49b8f7f
2023-06-16 05:40:50,760 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42027
2023-06-16 05:40:50,761 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42027
2023-06-16 05:40:50,761 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45601
2023-06-16 05:40:50,761 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,761 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,761 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:50,761 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:40:50,761 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_jsja3yo
2023-06-16 05:40:50,762 - distributed.worker - INFO - Starting Worker plugin PreImport-f0d417bd-b1d6-4e46-b78d-70583d0904bb
2023-06-16 05:40:50,762 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-da213bcc-1562-4d44-a2c2-17209d64ac89
2023-06-16 05:40:50,762 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7bcf67c4-47b2-4484-b8b6-9790af7e70df
2023-06-16 05:40:50,870 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,884 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9989e66-2e8a-4773-bf30-a731556af4eb
2023-06-16 05:40:50,884 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b67661c-d51b-4891-9af7-f9fe4646a35f
2023-06-16 05:40:50,885 - distributed.worker - INFO - Starting Worker plugin PreImport-46788485-f3b2-4a52-902d-138741f95b5f
2023-06-16 05:40:50,885 - distributed.worker - INFO - Starting Worker plugin PreImport-9718268e-327e-492c-b085-81511cfc3775
2023-06-16 05:40:50,885 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,885 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,890 - distributed.worker - INFO - Starting Worker plugin PreImport-a2c32184-a0b7-4cc9-899e-a9f3cfa5fbf7
2023-06-16 05:40:50,891 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf7df835-0e62-4151-9495-0f9ab9077341
2023-06-16 05:40:50,891 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,896 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64d2a1d8-f93b-4b45-a75e-3659e39508e7
2023-06-16 05:40:50,896 - distributed.worker - INFO - Starting Worker plugin PreImport-67709361-7bb3-4af1-a33e-c08dfdfba883
2023-06-16 05:40:50,896 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,896 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,898 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38439', status: init, memory: 0, processing: 0>
2023-06-16 05:40:50,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38439
2023-06-16 05:40:50,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60532
2023-06-16 05:40:50,899 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,900 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,902 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:50,913 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41699', status: init, memory: 0, processing: 0>
2023-06-16 05:40:50,913 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41699
2023-06-16 05:40:50,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60550
2023-06-16 05:40:50,914 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,914 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,914 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36309', status: init, memory: 0, processing: 0>
2023-06-16 05:40:50,915 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36309
2023-06-16 05:40:50,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60548
2023-06-16 05:40:50,916 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,916 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:50,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:50,922 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39863', status: init, memory: 0, processing: 0>
2023-06-16 05:40:50,923 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39863
2023-06-16 05:40:50,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60578
2023-06-16 05:40:50,924 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,924 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,924 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38891', status: init, memory: 0, processing: 0>
2023-06-16 05:40:50,925 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38891
2023-06-16 05:40:50,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60580
2023-06-16 05:40:50,925 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,925 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,926 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42027', status: init, memory: 0, processing: 0>
2023-06-16 05:40:50,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:50,926 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42027
2023-06-16 05:40:50,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60594
2023-06-16 05:40:50,927 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:50,927 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:50,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:50,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:51,029 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:51,029 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:51,030 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:51,030 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:51,030 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:51,030 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:51,030 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:51,125 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:40:51,129 - distributed.scheduler - INFO - Remove client Client-56a940e7-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:51,129 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49098; closing.
2023-06-16 05:40:51,129 - distributed.scheduler - INFO - Remove client Client-56a940e7-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:51,130 - distributed.scheduler - INFO - Close client connection: Client-56a940e7-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:51,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33091'. Reason: nanny-close
2023-06-16 05:40:51,131 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:51,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39515'. Reason: nanny-close
2023-06-16 05:40:51,132 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:51,133 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38439. Reason: nanny-close
2023-06-16 05:40:51,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33811'. Reason: nanny-close
2023-06-16 05:40:51,133 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:51,133 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39863. Reason: nanny-close
2023-06-16 05:40:51,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45429'. Reason: nanny-close
2023-06-16 05:40:51,134 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:51,134 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38891. Reason: nanny-close
2023-06-16 05:40:51,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42089'. Reason: nanny-close
2023-06-16 05:40:51,134 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:51,135 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36309. Reason: nanny-close
2023-06-16 05:40:51,135 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42663'. Reason: nanny-close
2023-06-16 05:40:51,135 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60532; closing.
2023-06-16 05:40:51,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:51,135 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:51,135 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38439', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:51,135 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46289'. Reason: nanny-close
2023-06-16 05:40:51,135 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38439
2023-06-16 05:40:51,135 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38089. Reason: nanny-close
2023-06-16 05:40:51,135 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:51,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:51,136 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:51,136 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36691'. Reason: nanny-close
2023-06-16 05:40:51,136 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38065. Reason: nanny-close
2023-06-16 05:40:51,136 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:51,136 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:51,136 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:51,136 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42027. Reason: nanny-close
2023-06-16 05:40:51,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60578; closing.
2023-06-16 05:40:51,137 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:51,137 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38439
2023-06-16 05:40:51,137 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:51,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60580; closing.
2023-06-16 05:40:51,137 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38439
2023-06-16 05:40:51,137 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:51,137 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41699. Reason: nanny-close
2023-06-16 05:40:51,138 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38439
2023-06-16 05:40:51,138 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:51,138 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39863', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:51,138 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38439
2023-06-16 05:40:51,138 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39863
2023-06-16 05:40:51,138 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38891', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:51,138 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38891
2023-06-16 05:40:51,138 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:51,139 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:51,139 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60548; closing.
2023-06-16 05:40:51,139 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:51,139 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:51,139 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36309', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:51,139 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36309
2023-06-16 05:40:51,140 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60528; closing.
2023-06-16 05:40:51,140 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:51,140 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:51,140 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:51,140 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38089', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:51,141 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38089
2023-06-16 05:40:51,141 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60516; closing.
2023-06-16 05:40:51,141 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60594; closing.
2023-06-16 05:40:51,142 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60550; closing.
2023-06-16 05:40:51,142 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38065', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:51,142 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38065
2023-06-16 05:40:51,143 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42027', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:51,143 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42027
2023-06-16 05:40:51,143 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41699', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:51,143 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41699
2023-06-16 05:40:51,143 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:40:52,448 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:40:52,448 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:40:52,448 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:40:52,449 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-16 05:40:52,450 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-06-16 05:40:54,253 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:54,257 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35663 instead
  warnings.warn(
2023-06-16 05:40:54,261 - distributed.scheduler - INFO - State start
2023-06-16 05:40:54,279 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:40:54,280 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-16 05:40:54,280 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35663/status
2023-06-16 05:40:54,315 - distributed.scheduler - INFO - Receive client connection: Client-5b68ad59-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:54,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60670
2023-06-16 05:40:54,384 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40695'
2023-06-16 05:40:55,732 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:40:55,732 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:40:55,996 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:40:56,978 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37061
2023-06-16 05:40:56,978 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37061
2023-06-16 05:40:56,978 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-06-16 05:40:56,978 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:40:56,978 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:56,978 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:40:56,978 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-16 05:40:56,978 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eud8yj81
2023-06-16 05:40:56,979 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6dac2811-8e3c-45b3-ba13-649dbe8ceea3
2023-06-16 05:40:56,979 - distributed.worker - INFO - Starting Worker plugin PreImport-ad7c05dd-1249-4df0-9026-a43d70f88c68
2023-06-16 05:40:56,979 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-356424a2-ac47-4947-a6ec-2836d863dc96
2023-06-16 05:40:56,979 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:57,007 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37061', status: init, memory: 0, processing: 0>
2023-06-16 05:40:57,008 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37061
2023-06-16 05:40:57,008 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60690
2023-06-16 05:40:57,008 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:40:57,008 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:40:57,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:40:57,072 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:40:57,075 - distributed.scheduler - INFO - Remove client Client-5b68ad59-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:57,075 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60670; closing.
2023-06-16 05:40:57,075 - distributed.scheduler - INFO - Remove client Client-5b68ad59-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:57,075 - distributed.scheduler - INFO - Close client connection: Client-5b68ad59-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:40:57,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40695'. Reason: nanny-close
2023-06-16 05:40:57,077 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:40:57,078 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37061. Reason: nanny-close
2023-06-16 05:40:57,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60690; closing.
2023-06-16 05:40:57,079 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:40:57,079 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37061', status: closing, memory: 0, processing: 0>
2023-06-16 05:40:57,080 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37061
2023-06-16 05:40:57,080 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:40:57,080 - distributed.nanny - INFO - Worker closed
2023-06-16 05:40:57,992 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:40:57,992 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:40:57,993 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:40:57,993 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-16 05:40:57,993 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-06-16 05:41:01,484 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:01,489 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44967 instead
  warnings.warn(
2023-06-16 05:41:01,492 - distributed.scheduler - INFO - State start
2023-06-16 05:41:01,511 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:01,512 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-16 05:41:01,513 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44967/status
2023-06-16 05:41:01,562 - distributed.scheduler - INFO - Receive client connection: Client-5faebb91-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:01,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37982
2023-06-16 05:41:01,622 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38023'
2023-06-16 05:41:03,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:03,102 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:03,388 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:04,414 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34727
2023-06-16 05:41:04,414 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34727
2023-06-16 05:41:04,414 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37121
2023-06-16 05:41:04,414 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:04,414 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:04,414 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:04,414 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-16 05:41:04,414 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-d21u5i4p
2023-06-16 05:41:04,415 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2ccffb0-02e2-4e0c-9501-fa1a276aefab
2023-06-16 05:41:04,415 - distributed.worker - INFO - Starting Worker plugin PreImport-514a8583-adda-4f74-943d-62f2af246942
2023-06-16 05:41:04,416 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-855a696a-48db-4f14-98ad-ae73cba33d61
2023-06-16 05:41:04,416 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:04,437 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34727', status: init, memory: 0, processing: 0>
2023-06-16 05:41:04,438 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34727
2023-06-16 05:41:04,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38006
2023-06-16 05:41:04,439 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:04,439 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:04,440 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:04,521 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:04,523 - distributed.scheduler - INFO - Remove client Client-5faebb91-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:04,523 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37982; closing.
2023-06-16 05:41:04,524 - distributed.scheduler - INFO - Remove client Client-5faebb91-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:04,524 - distributed.scheduler - INFO - Close client connection: Client-5faebb91-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:04,525 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38023'. Reason: nanny-close
2023-06-16 05:41:04,525 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:04,527 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34727. Reason: nanny-close
2023-06-16 05:41:04,528 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38006; closing.
2023-06-16 05:41:04,528 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:04,529 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34727', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:04,529 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34727
2023-06-16 05:41:04,529 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:41:04,530 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:05,491 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:41:05,491 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:41:05,492 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:41:05,492 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-16 05:41:05,493 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-06-16 05:41:07,356 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:07,360 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33391 instead
  warnings.warn(
2023-06-16 05:41:07,364 - distributed.scheduler - INFO - State start
2023-06-16 05:41:07,383 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:07,384 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-16 05:41:07,384 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33391/status
2023-06-16 05:41:10,757 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:41:10,758 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:41:10,758 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:41:10,758 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-16 05:41:10,759 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-06-16 05:41:12,548 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:12,552 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38939 instead
  warnings.warn(
2023-06-16 05:41:12,555 - distributed.scheduler - INFO - State start
2023-06-16 05:41:12,574 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:12,575 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-16 05:41:12,575 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38939/status
2023-06-16 05:41:12,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43431'
2023-06-16 05:41:13,317 - distributed.scheduler - INFO - Receive client connection: Client-664b8908-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:13,329 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38146
2023-06-16 05:41:14,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:14,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:14,258 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:15,061 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33379
2023-06-16 05:41:15,061 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33379
2023-06-16 05:41:15,061 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44959
2023-06-16 05:41:15,061 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-16 05:41:15,061 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:15,061 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:15,061 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-16 05:41:15,061 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jgw890kl
2023-06-16 05:41:15,062 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4679da9-a945-4ca1-a077-9715c93d99a7
2023-06-16 05:41:15,062 - distributed.worker - INFO - Starting Worker plugin PreImport-17c4d520-b112-4db2-acd3-0489d2ac4855
2023-06-16 05:41:15,062 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2c0fe8dd-3cec-4921-9db8-9c26c502464e
2023-06-16 05:41:15,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:15,081 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33379', status: init, memory: 0, processing: 0>
2023-06-16 05:41:15,082 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33379
2023-06-16 05:41:15,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38166
2023-06-16 05:41:15,083 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-16 05:41:15,083 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:15,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-16 05:41:15,174 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:15,177 - distributed.scheduler - INFO - Remove client Client-664b8908-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:15,177 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38146; closing.
2023-06-16 05:41:15,177 - distributed.scheduler - INFO - Remove client Client-664b8908-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:15,178 - distributed.scheduler - INFO - Close client connection: Client-664b8908-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:15,179 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43431'. Reason: nanny-close
2023-06-16 05:41:15,179 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:15,181 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33379. Reason: nanny-close
2023-06-16 05:41:15,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38166; closing.
2023-06-16 05:41:15,182 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-16 05:41:15,183 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33379', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:15,183 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33379
2023-06-16 05:41:15,183 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:41:15,184 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:16,145 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:41:16,145 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:41:16,145 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:41:16,146 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-16 05:41:16,146 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-06-16 05:41:18,032 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:18,036 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36183 instead
  warnings.warn(
2023-06-16 05:41:18,039 - distributed.scheduler - INFO - State start
2023-06-16 05:41:18,057 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:18,058 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-16 05:41:18,058 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36183/status
2023-06-16 05:41:18,271 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45423'
2023-06-16 05:41:18,284 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35687'
2023-06-16 05:41:18,298 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42779'
2023-06-16 05:41:18,299 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33223'
2023-06-16 05:41:18,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45121'
2023-06-16 05:41:18,314 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42951'
2023-06-16 05:41:18,322 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44839'
2023-06-16 05:41:18,334 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39919'
2023-06-16 05:41:19,772 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:19,772 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:19,796 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:19,825 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:19,825 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:19,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:19,892 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:19,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:19,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:19,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:19,923 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:19,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:19,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:19,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:19,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:19,937 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:19,937 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:19,938 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:19,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:19,942 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:20,118 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:20,121 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:20,123 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:20,130 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:20,224 - distributed.scheduler - INFO - Receive client connection: Client-698af676-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:20,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43898
2023-06-16 05:41:21,625 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45813
2023-06-16 05:41:21,625 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45813
2023-06-16 05:41:21,625 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32893
2023-06-16 05:41:21,625 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:21,625 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:21,625 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:21,626 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:41:21,626 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e1brd098
2023-06-16 05:41:21,626 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a8b404e-d69c-48de-a7c8-2e69571c7616
2023-06-16 05:41:21,626 - distributed.worker - INFO - Starting Worker plugin PreImport-b0ab7d32-b348-480d-894d-a03fb61473d3
2023-06-16 05:41:21,626 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e25924b-5370-4ab8-9a2c-f1794d8e1617
2023-06-16 05:41:21,947 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:21,976 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45813', status: init, memory: 0, processing: 0>
2023-06-16 05:41:21,977 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45813
2023-06-16 05:41:21,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43914
2023-06-16 05:41:21,977 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:21,977 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:21,979 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:22,429 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36527
2023-06-16 05:41:22,429 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36527
2023-06-16 05:41:22,430 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36135
2023-06-16 05:41:22,430 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:22,430 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:22,430 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:22,430 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:41:22,430 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bqtb4ydq
2023-06-16 05:41:22,431 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9249fbb2-e79c-4351-92f4-737965fdf200
2023-06-16 05:41:22,561 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a22f4768-990c-4475-b3cc-4a45d5e39582
2023-06-16 05:41:22,562 - distributed.worker - INFO - Starting Worker plugin PreImport-7a08bbd5-346c-43e0-ad2a-fe4454f0b1ea
2023-06-16 05:41:22,562 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:22,591 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36527', status: init, memory: 0, processing: 0>
2023-06-16 05:41:22,592 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36527
2023-06-16 05:41:22,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43924
2023-06-16 05:41:22,592 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:22,592 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:22,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:22,903 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40347
2023-06-16 05:41:22,903 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40347
2023-06-16 05:41:22,903 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35259
2023-06-16 05:41:22,903 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:22,903 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:22,903 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:22,903 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:41:22,903 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2l271sgf
2023-06-16 05:41:22,903 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5db7bcb6-9a13-4cf5-907d-8889648a61ab
2023-06-16 05:41:22,906 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45193
2023-06-16 05:41:22,906 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45193
2023-06-16 05:41:22,906 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40909
2023-06-16 05:41:22,907 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:22,907 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:22,907 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:22,907 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:41:22,907 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5r1fapm5
2023-06-16 05:41:22,907 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e9137b3-af5f-41ef-b2b6-6f9ec8e0be37
2023-06-16 05:41:22,907 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34527
2023-06-16 05:41:22,908 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34527
2023-06-16 05:41:22,908 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45137
2023-06-16 05:41:22,908 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33369
2023-06-16 05:41:22,908 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45137
2023-06-16 05:41:22,908 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:22,908 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:22,908 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46273
2023-06-16 05:41:22,908 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:22,908 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:22,908 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:41:22,908 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:22,908 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4uwgsqq1
2023-06-16 05:41:22,908 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:22,908 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:41:22,908 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w5g5sz6_
2023-06-16 05:41:22,908 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ad7b8c5-48dc-4c46-b491-ca0992f35ad4
2023-06-16 05:41:22,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f92e551-6eb5-4b94-b39a-0da20e01d3f1
2023-06-16 05:41:22,909 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45817
2023-06-16 05:41:22,910 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45817
2023-06-16 05:41:22,910 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45647
2023-06-16 05:41:22,910 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:22,910 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:22,910 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:22,910 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:41:22,910 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sxxap6m_
2023-06-16 05:41:22,911 - distributed.worker - INFO - Starting Worker plugin PreImport-1ff0835e-7415-4b71-b868-7b8f851563d3
2023-06-16 05:41:22,911 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1451a17e-2fa2-49b7-9fec-4ad747cfc377
2023-06-16 05:41:22,911 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86d08a36-ac2d-4a5c-850b-f9f571f5bc00
2023-06-16 05:41:22,927 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44401
2023-06-16 05:41:22,927 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44401
2023-06-16 05:41:22,927 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35963
2023-06-16 05:41:22,927 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:22,927 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:22,927 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:22,927 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-16 05:41:22,927 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-l3bwm4oz
2023-06-16 05:41:22,928 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9ebf296-2be2-4e25-a49c-9f6e212b0a2c
2023-06-16 05:41:23,062 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f5973fe-b9b6-4fdc-8cdb-6835f9d5b19d
2023-06-16 05:41:23,062 - distributed.worker - INFO - Starting Worker plugin PreImport-3d167f83-4ba6-4581-af07-654dc9bd829d
2023-06-16 05:41:23,062 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4651c145-7018-43e6-aad6-7723b60c55cf
2023-06-16 05:41:23,063 - distributed.worker - INFO - Starting Worker plugin PreImport-ee4d4ef9-efc7-4aab-a5f7-1898081aae07
2023-06-16 05:41:23,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6a3ba7a-5528-4b0a-9b3c-7cd1c7385f67
2023-06-16 05:41:23,063 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-895bb958-ae1d-41dc-b876-c71494b3585b
2023-06-16 05:41:23,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5945b2f2-6795-4a6e-9771-a3ff7d6f21a0
2023-06-16 05:41:23,063 - distributed.worker - INFO - Starting Worker plugin PreImport-7b89f0c5-9b01-4c75-8acd-98ebfdc0027d
2023-06-16 05:41:23,063 - distributed.worker - INFO - Starting Worker plugin PreImport-3ae2b57f-5c37-4697-b07d-bdd83d01f2e6
2023-06-16 05:41:23,063 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,063 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,064 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,066 - distributed.worker - INFO - Starting Worker plugin PreImport-dc1220b1-033c-4f00-a253-eec2fdc33f6c
2023-06-16 05:41:23,066 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,088 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40347', status: init, memory: 0, processing: 0>
2023-06-16 05:41:23,089 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40347
2023-06-16 05:41:23,089 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43944
2023-06-16 05:41:23,089 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:23,089 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,089 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45817', status: init, memory: 0, processing: 0>
2023-06-16 05:41:23,090 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45817
2023-06-16 05:41:23,090 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43954
2023-06-16 05:41:23,091 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:23,091 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,091 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45193', status: init, memory: 0, processing: 0>
2023-06-16 05:41:23,091 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45193
2023-06-16 05:41:23,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:23,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43960
2023-06-16 05:41:23,092 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:23,092 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:23,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:23,098 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45137', status: init, memory: 0, processing: 0>
2023-06-16 05:41:23,099 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45137
2023-06-16 05:41:23,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43974
2023-06-16 05:41:23,099 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:23,099 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,099 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34527', status: init, memory: 0, processing: 0>
2023-06-16 05:41:23,100 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34527
2023-06-16 05:41:23,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43982
2023-06-16 05:41:23,101 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:23,101 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,101 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44401', status: init, memory: 0, processing: 0>
2023-06-16 05:41:23,101 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44401
2023-06-16 05:41:23,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43986
2023-06-16 05:41:23,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:23,102 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:23,102 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:23,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:23,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:23,198 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:41:23,198 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:41:23,198 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:41:23,198 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:41:23,198 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:41:23,198 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:41:23,198 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:41:23,198 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-16 05:41:23,212 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:23,212 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:23,212 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:23,212 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:23,213 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:23,213 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:23,213 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:23,213 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:23,217 - distributed.scheduler - INFO - Remove client Client-698af676-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:23,217 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43898; closing.
2023-06-16 05:41:23,217 - distributed.scheduler - INFO - Remove client Client-698af676-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:23,217 - distributed.scheduler - INFO - Close client connection: Client-698af676-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:23,219 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35687'. Reason: nanny-close
2023-06-16 05:41:23,219 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:23,220 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45121'. Reason: nanny-close
2023-06-16 05:41:23,220 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:23,221 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45813. Reason: nanny-close
2023-06-16 05:41:23,221 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45423'. Reason: nanny-close
2023-06-16 05:41:23,221 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:23,221 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45137. Reason: nanny-close
2023-06-16 05:41:23,221 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42779'. Reason: nanny-close
2023-06-16 05:41:23,222 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:23,222 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36527. Reason: nanny-close
2023-06-16 05:41:23,222 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33223'. Reason: nanny-close
2023-06-16 05:41:23,222 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43914; closing.
2023-06-16 05:41:23,222 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:23,222 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:23,223 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45813', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:23,223 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45193. Reason: nanny-close
2023-06-16 05:41:23,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42951'. Reason: nanny-close
2023-06-16 05:41:23,223 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45813
2023-06-16 05:41:23,223 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:23,223 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44839'. Reason: nanny-close
2023-06-16 05:41:23,223 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:23,223 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34527. Reason: nanny-close
2023-06-16 05:41:23,223 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:23,224 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:23,224 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39919'. Reason: nanny-close
2023-06-16 05:41:23,224 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:23,224 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44401. Reason: nanny-close
2023-06-16 05:41:23,224 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45813
2023-06-16 05:41:23,224 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:23,224 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45813
2023-06-16 05:41:23,224 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45813
2023-06-16 05:41:23,224 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:23,224 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43974; closing.
2023-06-16 05:41:23,225 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45817. Reason: nanny-close
2023-06-16 05:41:23,225 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:23,225 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45137', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:23,225 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45137
2023-06-16 05:41:23,225 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40347. Reason: nanny-close
2023-06-16 05:41:23,225 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:23,225 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43924; closing.
2023-06-16 05:41:23,225 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45813
2023-06-16 05:41:23,226 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:23,226 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45813
2023-06-16 05:41:23,226 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43960; closing.
2023-06-16 05:41:23,226 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36527', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:23,226 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36527
2023-06-16 05:41:23,226 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:23,226 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:23,226 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:23,226 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45193', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:23,226 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45193
2023-06-16 05:41:23,227 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:23,227 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43982; closing.
2023-06-16 05:41:23,227 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:23,227 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34527', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:23,228 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34527
2023-06-16 05:41:23,228 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:23,228 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:23,228 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:23,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43986; closing.
2023-06-16 05:41:23,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43954; closing.
2023-06-16 05:41:23,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43944; closing.
2023-06-16 05:41:23,229 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44401', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:23,229 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44401
2023-06-16 05:41:23,229 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45817', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:23,229 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45817
2023-06-16 05:41:23,230 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40347', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:23,230 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40347
2023-06-16 05:41:23,230 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:41:24,636 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:41:24,637 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:41:24,637 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:41:24,638 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-16 05:41:24,639 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-06-16 05:41:26,473 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:26,477 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45815 instead
  warnings.warn(
2023-06-16 05:41:26,481 - distributed.scheduler - INFO - State start
2023-06-16 05:41:26,600 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:26,601 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-16 05:41:26,602 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45815/status
2023-06-16 05:41:26,688 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33189'
2023-06-16 05:41:28,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:28,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:28,206 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:29,070 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37289
2023-06-16 05:41:29,070 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37289
2023-06-16 05:41:29,070 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45989
2023-06-16 05:41:29,071 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:29,071 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:29,071 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:29,071 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-16 05:41:29,071 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-af_g5tkj
2023-06-16 05:41:29,071 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aa10242e-0481-4339-9ba4-e2ffeeaad7c9
2023-06-16 05:41:29,166 - distributed.worker - INFO - Starting Worker plugin PreImport-8770f443-0791-4e30-b316-531100537f87
2023-06-16 05:41:29,166 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5895d9b0-7079-4d8a-a5cf-b03640647f41
2023-06-16 05:41:29,167 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:29,192 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37289', status: init, memory: 0, processing: 0>
2023-06-16 05:41:29,204 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37289
2023-06-16 05:41:29,204 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44098
2023-06-16 05:41:29,204 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:29,204 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:29,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:30,662 - distributed.scheduler - INFO - Receive client connection: Client-6e999217-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:30,662 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57454
2023-06-16 05:41:30,669 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-16 05:41:30,672 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:30,674 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:30,676 - distributed.scheduler - INFO - Remove client Client-6e999217-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:30,676 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57454; closing.
2023-06-16 05:41:30,677 - distributed.scheduler - INFO - Remove client Client-6e999217-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:30,677 - distributed.scheduler - INFO - Close client connection: Client-6e999217-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:30,678 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33189'. Reason: nanny-close
2023-06-16 05:41:30,679 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:30,680 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37289. Reason: nanny-close
2023-06-16 05:41:30,681 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:30,681 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44098; closing.
2023-06-16 05:41:30,682 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37289', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:30,682 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37289
2023-06-16 05:41:30,682 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:41:30,682 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:31,594 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:41:31,594 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:41:31,595 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:41:31,596 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-16 05:41:31,596 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-06-16 05:41:33,364 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:33,369 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46765 instead
  warnings.warn(
2023-06-16 05:41:33,373 - distributed.scheduler - INFO - State start
2023-06-16 05:41:33,430 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-16 05:41:33,431 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-16 05:41:33,432 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46765/status
2023-06-16 05:41:33,594 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34687'
2023-06-16 05:41:33,664 - distributed.scheduler - INFO - Receive client connection: Client-72b73b7b-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:33,678 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57578
2023-06-16 05:41:35,144 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:35,145 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:35,170 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-16 05:41:35,985 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46331
2023-06-16 05:41:35,985 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46331
2023-06-16 05:41:35,985 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37265
2023-06-16 05:41:35,985 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-16 05:41:35,985 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:35,985 - distributed.worker - INFO -               Threads:                          1
2023-06-16 05:41:35,985 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-16 05:41:35,985 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-__fyqfx7
2023-06-16 05:41:35,986 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ff1e498-663a-463c-8873-bace40dbd8ad
2023-06-16 05:41:36,085 - distributed.worker - INFO - Starting Worker plugin PreImport-ae4a7b4a-a137-4add-97bd-5ecbdae02249
2023-06-16 05:41:36,086 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df01faad-3cca-49de-8aa0-b088c1e62c39
2023-06-16 05:41:36,086 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:36,113 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46331', status: init, memory: 0, processing: 0>
2023-06-16 05:41:36,114 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46331
2023-06-16 05:41:36,114 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57602
2023-06-16 05:41:36,114 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-16 05:41:36,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-16 05:41:36,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-16 05:41:36,129 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-06-16 05:41:36,133 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-16 05:41:36,136 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:36,137 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-16 05:41:36,140 - distributed.scheduler - INFO - Remove client Client-72b73b7b-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:36,140 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57578; closing.
2023-06-16 05:41:36,140 - distributed.scheduler - INFO - Remove client Client-72b73b7b-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:36,140 - distributed.scheduler - INFO - Close client connection: Client-72b73b7b-0c08-11ee-8bbd-d8c49764f6bb
2023-06-16 05:41:36,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34687'. Reason: nanny-close
2023-06-16 05:41:36,142 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-16 05:41:36,143 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46331. Reason: nanny-close
2023-06-16 05:41:36,144 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57602; closing.
2023-06-16 05:41:36,144 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-16 05:41:36,145 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46331', status: closing, memory: 0, processing: 0>
2023-06-16 05:41:36,145 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46331
2023-06-16 05:41:36,145 - distributed.scheduler - INFO - Lost all workers
2023-06-16 05:41:36,145 - distributed.nanny - INFO - Worker closed
2023-06-16 05:41:37,158 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-16 05:41:37,158 - distributed.scheduler - INFO - Scheduler closing...
2023-06-16 05:41:37,159 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-16 05:41:37,159 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-16 05:41:37,160 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34933 instead
  warnings.warn(
2023-06-16 05:41:45,671 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:45,671 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:45,797 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:45,797 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:45,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:45,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:45,830 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:45,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:45,831 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:45,831 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:45,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:45,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:45,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:45,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:45,840 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:45,840 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44079 instead
  warnings.warn(
2023-06-16 05:41:54,267 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:54,267 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:54,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:54,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:54,304 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:54,304 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:54,304 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:54,304 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:54,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:54,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:54,343 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:54,343 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:54,382 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:54,382 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:41:54,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:41:54,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43239 instead
  warnings.warn(
2023-06-16 05:42:02,215 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:02,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:02,287 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:02,287 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:02,349 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:02,349 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:02,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:02,361 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:02,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:02,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:02,411 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:02,412 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:02,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:02,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:02,465 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:02,465 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40341 instead
  warnings.warn(
2023-06-16 05:42:10,984 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:10,985 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:11,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:11,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:11,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:11,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:11,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:11,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:11,167 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:11,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:11,191 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:11,191 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:11,192 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:11,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:11,195 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:11,195 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46577 instead
  warnings.warn(
2023-06-16 05:42:21,583 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:21,584 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:21,609 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:21,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:21,638 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:21,638 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:21,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:21,663 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:21,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:21,664 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:21,664 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:21,664 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:21,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:21,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:21,720 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:21,720 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36763 instead
  warnings.warn(
2023-06-16 05:42:31,605 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:31,606 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:31,630 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:31,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:31,646 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:31,646 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:31,690 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:31,691 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:31,691 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:31,691 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:31,692 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:31,692 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:31,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:31,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:31,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:31,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45239 instead
  warnings.warn(
2023-06-16 05:42:42,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:42,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:42,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:42,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:42,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:42,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:42,277 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:42,277 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:42,328 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:42,329 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:42,329 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:42,329 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:42,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:42,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:42,350 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:42,350 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45161 instead
  warnings.warn(
2023-06-16 05:42:52,268 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:52,268 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:52,268 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:52,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:52,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:52,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:52,272 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:52,272 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:52,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:52,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:52,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:52,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:52,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:52,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:42:52,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:42:52,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41221 instead
  warnings.warn(
2023-06-16 05:43:03,646 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1265, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:47824 remote=tcp://127.0.0.1:34971>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32957 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46677 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35637 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42059 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42097 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42635 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39827 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35827 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39049 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35699 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34819 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34091 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45685 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39337 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35953 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41911 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36717 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43921 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37957 instead
  warnings.warn(
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-16 05:48:27,320 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-16 05:48:27,326 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-16 05:48:27,330 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-16 05:48:27,329 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f502f4d3460>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-16 05:48:27,333 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-16 05:48:27,333 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fee4286f520>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-16 05:48:27,337 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f981548b430>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-16 05:48:27,341 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f15f1569370>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-16 05:48:29,332 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-06-16 05:48:29,337 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-06-16 05:48:29,340 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-06-16 05:48:29,344 - distributed.nanny - ERROR - Worker process died unexpectedly
