============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-05-21 05:39:26,594 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:26,599 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33405 instead
  warnings.warn(
2023-05-21 05:39:26,603 - distributed.scheduler - INFO - State start
2023-05-21 05:39:26,624 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:26,626 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-21 05:39:26,626 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33405/status
2023-05-21 05:39:26,868 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37833'
2023-05-21 05:39:26,886 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44273'
2023-05-21 05:39:26,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40929'
2023-05-21 05:39:26,902 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39745'
2023-05-21 05:39:28,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:28,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:28,669 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:28,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:28,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:28,683 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:28,691 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:28,691 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:28,700 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:28,712 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:28,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:28,721 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-05-21 05:39:28,745 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45169
2023-05-21 05:39:28,746 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45169
2023-05-21 05:39:28,746 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36177
2023-05-21 05:39:28,746 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-21 05:39:28,746 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:28,746 - distributed.worker - INFO -               Threads:                          4
2023-05-21 05:39:28,746 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-21 05:39:28,746 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ok1bnx8s
2023-05-21 05:39:28,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63a9bcf3-f2e0-44b8-a65a-d47c0096fe77
2023-05-21 05:39:28,746 - distributed.worker - INFO - Starting Worker plugin PreImport-47fe1172-544b-46d5-8d1e-d7a9e64d640d
2023-05-21 05:39:28,746 - distributed.worker - INFO - Starting Worker plugin RMMSetup-390ddeb4-00f4-4fac-bbd9-8c88b7431c3e
2023-05-21 05:39:28,747 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:28,768 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45169', status: init, memory: 0, processing: 0>
2023-05-21 05:39:28,784 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45169
2023-05-21 05:39:28,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59050
2023-05-21 05:39:28,784 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-21 05:39:28,784 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:28,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-21 05:39:30,118 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39277
2023-05-21 05:39:30,118 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39277
2023-05-21 05:39:30,118 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43803
2023-05-21 05:39:30,118 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-21 05:39:30,118 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:30,118 - distributed.worker - INFO -               Threads:                          4
2023-05-21 05:39:30,118 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-21 05:39:30,118 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-69yx_9qb
2023-05-21 05:39:30,119 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8133d8a5-d244-460b-b470-6adf4eeeadbd
2023-05-21 05:39:30,119 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0a9312b-97f4-4b36-a074-af3c2d41688d
2023-05-21 05:39:30,119 - distributed.worker - INFO - Starting Worker plugin PreImport-be1695f1-6b10-4404-ba4c-34543419e653
2023-05-21 05:39:30,119 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:30,120 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44383
2023-05-21 05:39:30,120 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44383
2023-05-21 05:39:30,120 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45707
2023-05-21 05:39:30,121 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-21 05:39:30,121 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:30,121 - distributed.worker - INFO -               Threads:                          4
2023-05-21 05:39:30,121 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-21 05:39:30,121 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-drrv2x19
2023-05-21 05:39:30,121 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e29bc45-585b-4db1-bebf-a89517a46bb4
2023-05-21 05:39:30,121 - distributed.worker - INFO - Starting Worker plugin PreImport-a8824544-3dcd-4673-a78e-f939bd741c69
2023-05-21 05:39:30,122 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26ee5925-6622-4324-b9a5-e46d90231152
2023-05-21 05:39:30,122 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:30,148 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39277', status: init, memory: 0, processing: 0>
2023-05-21 05:39:30,150 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39277
2023-05-21 05:39:30,151 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33536
2023-05-21 05:39:30,151 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-21 05:39:30,151 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:30,152 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44383', status: init, memory: 0, processing: 0>
2023-05-21 05:39:30,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-21 05:39:30,153 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44383
2023-05-21 05:39:30,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33550
2023-05-21 05:39:30,154 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-21 05:39:30,154 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:30,157 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-21 05:39:30,217 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38057
2023-05-21 05:39:30,217 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38057
2023-05-21 05:39:30,217 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40429
2023-05-21 05:39:30,217 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-21 05:39:30,217 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:30,217 - distributed.worker - INFO -               Threads:                          4
2023-05-21 05:39:30,217 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-05-21 05:39:30,217 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jfi15z4y
2023-05-21 05:39:30,218 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a1414d61-bc40-4685-ad90-3ba6398820fd
2023-05-21 05:39:30,218 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-471fb42f-8321-4452-b60f-56ec0d1cbf25
2023-05-21 05:39:30,218 - distributed.worker - INFO - Starting Worker plugin PreImport-c2da64e2-8df4-4bbe-bb6a-98e1e01f9fed
2023-05-21 05:39:30,218 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:30,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38057', status: init, memory: 0, processing: 0>
2023-05-21 05:39:30,248 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38057
2023-05-21 05:39:30,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33552
2023-05-21 05:39:30,248 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-21 05:39:30,248 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:30,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-21 05:39:31,007 - distributed.scheduler - INFO - Receive client connection: Client-d839b30a-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:31,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33560
2023-05-21 05:39:31,018 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-21 05:39:31,018 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-21 05:39:31,018 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-21 05:39:31,019 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-05-21 05:39:31,024 - distributed.scheduler - INFO - Remove client Client-d839b30a-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:31,024 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33560; closing.
2023-05-21 05:39:31,024 - distributed.scheduler - INFO - Remove client Client-d839b30a-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:31,025 - distributed.scheduler - INFO - Close client connection: Client-d839b30a-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:31,026 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37833'. Reason: nanny-close
2023-05-21 05:39:31,026 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:31,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40929'. Reason: nanny-close
2023-05-21 05:39:31,027 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:31,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39745'. Reason: nanny-close
2023-05-21 05:39:31,028 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44383. Reason: nanny-close
2023-05-21 05:39:31,028 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:31,028 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39277. Reason: nanny-close
2023-05-21 05:39:31,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44273'. Reason: nanny-close
2023-05-21 05:39:31,029 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:31,029 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38057. Reason: nanny-close
2023-05-21 05:39:31,030 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45169. Reason: nanny-close
2023-05-21 05:39:31,030 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33550; closing.
2023-05-21 05:39:31,030 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-21 05:39:31,030 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-21 05:39:31,030 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44383', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:31,030 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44383
2023-05-21 05:39:31,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33536; closing.
2023-05-21 05:39:31,031 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:31,031 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-21 05:39:31,031 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-21 05:39:31,032 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:31,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39277', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:31,032 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39277
2023-05-21 05:39:31,032 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33552; closing.
2023-05-21 05:39:31,033 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59050; closing.
2023-05-21 05:39:31,033 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:31,033 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38057', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:31,033 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:31,033 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38057
2023-05-21 05:39:31,033 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45169', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:31,033 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45169
2023-05-21 05:39:31,033 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:33,146 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:39:33,146 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:39:33,147 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:39:33,148 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-21 05:39:33,148 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 79, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 56, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 210, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 4, in <module>
    from dask.dataframe import backends, dispatch, rolling
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/backends.py", line 18, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/__init__.py", line 2, in <module>
    from dask.array import backends, fft, lib, linalg, ma, overlap, random
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/backends.py", line 6, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/core.py", line 30, in <module>
    from numpy.typing import ArrayLike
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/typing/__init__.py", line 158, in <module>
    from numpy._typing import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/_typing/__init__.py", line 164, in <module>
    from ._dtype_like import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/_typing/_dtype_like.py", line 149, in <module>
    Type[np.unsignedinteger],
  File "/opt/conda/envs/gdf/lib/python3.9/typing.py", line 274, in inner
    return cached(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/typing.py", line 839, in __getitem__
    return self.copy_with(params)
  File "/opt/conda/envs/gdf/lib/python3.9/typing.py", line 842, in copy_with
    return _GenericAlias(self.__origin__, params,
  File "/opt/conda/envs/gdf/lib/python3.9/typing.py", line 740, in __init__
    super().__init__(origin, inst=inst, name=name)
  File "/opt/conda/envs/gdf/lib/python3.9/typing.py", line 679, in __init__
    self._name = name
  File "/opt/conda/envs/gdf/lib/python3.9/typing.py", line 714, in __setattr__
    if _is_dunder(attr) or attr in ('_name', '_inst', '_nparams'):
  File "/opt/conda/envs/gdf/lib/python3.9/typing.py", line 665, in _is_dunder
    def _is_dunder(attr):
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 79, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 56, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 210, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 4, in <module>
    from dask.dataframe import backends, dispatch, rolling
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/backends.py", line 18, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/__init__.py", line 2, in <module>
    from dask.array import backends, fft, lib, linalg, ma, overlap, random
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/backends.py", line 6, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/core.py", line 37, in <module>
    from dask.array.chunk_types import is_valid_array_chunk, is_valid_chunk_type
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/chunk_types.py", line 108, in <module>
    import cupy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/__init__.py", line 47, in <module>
    from cupy import testing  # NOQA  # NOQA
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/testing/__init__.py", line 1, in <module>
    from cupy.testing._array import assert_allclose  # NOQA
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/testing/_array.py", line 1, in <module>
    import numpy.testing
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/testing/__init__.py", line 8, in <module>
    from unittest import TestCase
  File "/opt/conda/envs/gdf/lib/python3.9/unittest/__init__.py", line 60, in <module>
    from .case import (addModuleCleanup, TestCase, FunctionTestCase, SkipTest, skip,
  File "/opt/conda/envs/gdf/lib/python3.9/unittest/case.py", line 5, in <module>
    import difflib
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 982, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 925, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1423, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1395, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1522, in find_spec
  File "<frozen importlib._bootstrap_external>", line 142, in _path_stat
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-05-21 05:39:36,982 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:36,987 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34763 instead
  warnings.warn(
2023-05-21 05:39:36,991 - distributed.scheduler - INFO - State start
2023-05-21 05:39:37,012 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:37,013 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:39:37,014 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34763/status
2023-05-21 05:39:37,127 - distributed.scheduler - INFO - Receive client connection: Client-df6ed9fa-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:37,144 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52810
2023-05-21 05:39:37,304 - distributed.scheduler - INFO - Receive client connection: Client-de609b6a-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:37,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52842
2023-05-21 05:39:37,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35137'
2023-05-21 05:39:37,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40233'
2023-05-21 05:39:37,553 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39891'
2023-05-21 05:39:37,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43129'
2023-05-21 05:39:37,576 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33585'
2023-05-21 05:39:37,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43329'
2023-05-21 05:39:37,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40371'
2023-05-21 05:39:37,609 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39427'
2023-05-21 05:39:39,275 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:39,275 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:39,278 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:39,279 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:39,294 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:39,294 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:39,302 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:39,305 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:39,322 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:39,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:39,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:39,370 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:39,370 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:39,378 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:39,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:39,390 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:39,390 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:39,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:39,402 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:39,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:39,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:39,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:39,428 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:39,465 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:42,404 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36209
2023-05-21 05:39:42,405 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36209
2023-05-21 05:39:42,405 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38711
2023-05-21 05:39:42,405 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:42,405 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:42,405 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:42,405 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:42,405 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5rbkq41x
2023-05-21 05:39:42,406 - distributed.worker - INFO - Starting Worker plugin PreImport-ece9a9d0-97e7-4c02-8da5-2649d145c004
2023-05-21 05:39:42,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a089d5a-6493-41b6-ab99-3fb800e4471a
2023-05-21 05:39:42,439 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-469d7f33-1c68-4034-b92e-6ea958cc3cd0
2023-05-21 05:39:42,439 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:42,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36209', status: init, memory: 0, processing: 0>
2023-05-21 05:39:42,479 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36209
2023-05-21 05:39:42,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37906
2023-05-21 05:39:42,479 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:42,480 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:42,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:42,733 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43521
2023-05-21 05:39:42,733 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43521
2023-05-21 05:39:42,733 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45637
2023-05-21 05:39:42,733 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:42,733 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:42,734 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:42,734 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:42,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jestczso
2023-05-21 05:39:42,735 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8df2a1c8-aa52-44b4-8315-6cfc3ac346e9
2023-05-21 05:39:42,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-806ab504-08b3-4417-b87e-c2be9b6bdfc2
2023-05-21 05:39:42,752 - distributed.worker - INFO - Starting Worker plugin PreImport-105d211f-be06-45a6-9f53-ba6645700261
2023-05-21 05:39:42,752 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:42,788 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43521', status: init, memory: 0, processing: 0>
2023-05-21 05:39:42,789 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43521
2023-05-21 05:39:42,789 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37914
2023-05-21 05:39:42,789 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:42,789 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:42,792 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:42,873 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37375
2023-05-21 05:39:42,873 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37375
2023-05-21 05:39:42,874 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36767
2023-05-21 05:39:42,874 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:42,873 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40779
2023-05-21 05:39:42,874 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:42,874 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:42,874 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40779
2023-05-21 05:39:42,874 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:42,874 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40567
2023-05-21 05:39:42,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bokz2wpz
2023-05-21 05:39:42,874 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:42,874 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:42,874 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:42,874 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:42,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fm18wg15
2023-05-21 05:39:42,874 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efe2711f-04aa-4298-ab92-6e6ffe1432ef
2023-05-21 05:39:42,875 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7cb96fe5-5105-4221-b1ca-1b81485f5af3
2023-05-21 05:39:42,875 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b2b2b04b-c834-4922-a058-ee3a9b70ce71
2023-05-21 05:39:42,895 - distributed.worker - INFO - Starting Worker plugin PreImport-eda4fdf8-4e10-474f-b514-98aa06064665
2023-05-21 05:39:42,895 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:42,902 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-705662c0-9ddd-4a07-b8f6-e88c51537eb9
2023-05-21 05:39:42,903 - distributed.worker - INFO - Starting Worker plugin PreImport-360a0b16-7e7d-4dd6-89a1-a69e74872055
2023-05-21 05:39:42,903 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,080 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37375', status: init, memory: 0, processing: 0>
2023-05-21 05:39:43,081 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37375
2023-05-21 05:39:43,081 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37924
2023-05-21 05:39:43,082 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40779', status: init, memory: 0, processing: 0>
2023-05-21 05:39:43,081 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:43,082 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,082 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40779
2023-05-21 05:39:43,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37928
2023-05-21 05:39:43,083 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:43,083 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,084 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:43,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:43,373 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38365
2023-05-21 05:39:43,373 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38365
2023-05-21 05:39:43,373 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40137
2023-05-21 05:39:43,373 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:43,373 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,373 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:43,374 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:43,374 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-75bilmd5
2023-05-21 05:39:43,374 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e29b45fc-daa9-4ca5-b570-de50268970ee
2023-05-21 05:39:43,386 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42821
2023-05-21 05:39:43,386 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42821
2023-05-21 05:39:43,386 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32977
2023-05-21 05:39:43,386 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:43,386 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,386 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:43,387 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:43,387 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ep_2qdnv
2023-05-21 05:39:43,387 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-275ad974-7427-4923-a495-0a00919c1b30
2023-05-21 05:39:43,387 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65f7721e-622e-4a89-a9e0-abb9023bea11
2023-05-21 05:39:43,633 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40013
2023-05-21 05:39:43,633 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40013
2023-05-21 05:39:43,633 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35995
2023-05-21 05:39:43,633 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:43,634 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,634 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:43,634 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:43,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gv_1ahuy
2023-05-21 05:39:43,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2991f884-e8e8-4183-b79a-0db0545fdd66
2023-05-21 05:39:43,635 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44193
2023-05-21 05:39:43,636 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44193
2023-05-21 05:39:43,636 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43395
2023-05-21 05:39:43,636 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:43,636 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,636 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:43,636 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:43,636 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2cz2d3z8
2023-05-21 05:39:43,637 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d58b6e0b-5ccd-481d-95bd-b4660b252248
2023-05-21 05:39:43,654 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26ed8373-a282-47dc-8636-db4a98445fb6
2023-05-21 05:39:43,655 - distributed.worker - INFO - Starting Worker plugin PreImport-a2c5c658-977c-4368-9f90-dd663e61d1f4
2023-05-21 05:39:43,655 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,658 - distributed.worker - INFO - Starting Worker plugin PreImport-3626730d-14fc-4503-974a-ddf5291d6c24
2023-05-21 05:39:43,658 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,827 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42821', status: init, memory: 0, processing: 0>
2023-05-21 05:39:43,827 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42821
2023-05-21 05:39:43,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37944
2023-05-21 05:39:43,828 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:43,828 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,828 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38365', status: init, memory: 0, processing: 0>
2023-05-21 05:39:43,829 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38365
2023-05-21 05:39:43,829 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37942
2023-05-21 05:39:43,829 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:43,829 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:43,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:43,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-abb15f50-72e7-4037-9ece-5afae5f480f6
2023-05-21 05:39:43,859 - distributed.worker - INFO - Starting Worker plugin PreImport-9be23754-d783-43ab-9932-66dd020f97cf
2023-05-21 05:39:43,859 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:43,875 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f260c0c7-d4a1-4c36-8a64-90986bee388e
2023-05-21 05:39:43,875 - distributed.worker - INFO - Starting Worker plugin PreImport-84886cfd-eb52-4679-86d9-7ef55738ae31
2023-05-21 05:39:43,876 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:44,216 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40013', status: init, memory: 0, processing: 0>
2023-05-21 05:39:44,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40013
2023-05-21 05:39:44,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37968
2023-05-21 05:39:44,217 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:44,218 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:44,219 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44193', status: init, memory: 0, processing: 0>
2023-05-21 05:39:44,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:44,220 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44193
2023-05-21 05:39:44,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37980
2023-05-21 05:39:44,220 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:44,220 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:44,223 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:44,328 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,328 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,328 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,328 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,329 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,329 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,329 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,331 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,384 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,385 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:44,388 - distributed.scheduler - INFO - Remove client Client-de609b6a-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:44,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52842; closing.
2023-05-21 05:39:44,389 - distributed.scheduler - INFO - Remove client Client-de609b6a-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:44,389 - distributed.scheduler - INFO - Remove client Client-df6ed9fa-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:44,389 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52810; closing.
2023-05-21 05:39:44,390 - distributed.scheduler - INFO - Remove client Client-df6ed9fa-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:44,390 - distributed.scheduler - INFO - Close client connection: Client-de609b6a-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:44,390 - distributed.scheduler - INFO - Close client connection: Client-df6ed9fa-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:44,580 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43129'. Reason: nanny-close
2023-05-21 05:39:44,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:44,581 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33585'. Reason: nanny-close
2023-05-21 05:39:44,581 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:44,582 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35137'. Reason: nanny-close
2023-05-21 05:39:44,582 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36209. Reason: nanny-close
2023-05-21 05:39:44,582 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:44,583 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40233'. Reason: nanny-close
2023-05-21 05:39:44,583 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44193. Reason: nanny-close
2023-05-21 05:39:44,583 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:44,583 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39891'. Reason: nanny-close
2023-05-21 05:39:44,583 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40779. Reason: nanny-close
2023-05-21 05:39:44,583 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:44,584 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43329'. Reason: nanny-close
2023-05-21 05:39:44,584 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43521. Reason: nanny-close
2023-05-21 05:39:44,584 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:44,584 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38365. Reason: nanny-close
2023-05-21 05:39:44,585 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40371'. Reason: nanny-close
2023-05-21 05:39:44,585 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:44,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37906; closing.
2023-05-21 05:39:44,585 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:44,585 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:44,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36209', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:44,585 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:44,585 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36209
2023-05-21 05:39:44,585 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39427'. Reason: nanny-close
2023-05-21 05:39:44,585 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:44,586 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:44,586 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40013. Reason: nanny-close
2023-05-21 05:39:44,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37980; closing.
2023-05-21 05:39:44,586 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:44,586 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:44,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37928; closing.
2023-05-21 05:39:44,587 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:44,587 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:44,587 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42821. Reason: nanny-close
2023-05-21 05:39:44,587 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:44,587 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44193', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:44,587 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44193
2023-05-21 05:39:44,588 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36209
2023-05-21 05:39:44,588 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40779', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:44,588 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40779
2023-05-21 05:39:44,588 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37375. Reason: nanny-close
2023-05-21 05:39:44,588 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:44,588 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37914; closing.
2023-05-21 05:39:44,588 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:44,589 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36209
2023-05-21 05:39:44,589 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43521', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:44,589 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43521
2023-05-21 05:39:44,589 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:44,589 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:44,590 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37942; closing.
2023-05-21 05:39:44,590 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38365', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:44,590 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36209
2023-05-21 05:39:44,590 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38365
2023-05-21 05:39:44,591 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37968; closing.
2023-05-21 05:39:44,591 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:44,591 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40013', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:44,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:44,591 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40013
2023-05-21 05:39:44,591 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37944; closing.
2023-05-21 05:39:44,592 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42821', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:44,592 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42821
2023-05-21 05:39:44,593 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37924; closing.
2023-05-21 05:39:44,593 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37375', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:44,593 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37375
2023-05-21 05:39:44,593 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:44,592 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1265, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 241, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 144, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:37994 remote=tcp://127.0.0.1:9369>: Stream is closed
2023-05-21 05:39:44,594 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:46,409 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:39:46,410 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:39:46,410 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:39:46,412 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:39:46,412 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-05-21 05:39:48,800 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:48,805 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37945 instead
  warnings.warn(
2023-05-21 05:39:48,810 - distributed.scheduler - INFO - State start
2023-05-21 05:39:48,832 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:48,833 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:39:48,834 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37945/status
2023-05-21 05:39:48,959 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38313', status: init, memory: 0, processing: 0>
2023-05-21 05:39:48,977 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38313
2023-05-21 05:39:48,978 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38736
2023-05-21 05:39:48,985 - distributed.scheduler - INFO - Receive client connection: Client-e561d2ee-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:48,985 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38738
2023-05-21 05:39:49,026 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38736; closing.
2023-05-21 05:39:49,026 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38313', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:49,026 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38313
2023-05-21 05:39:49,027 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:49,130 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36753'
2023-05-21 05:39:49,151 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45933'
2023-05-21 05:39:49,162 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38071'
2023-05-21 05:39:49,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37887'
2023-05-21 05:39:49,174 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36619'
2023-05-21 05:39:49,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40015'
2023-05-21 05:39:49,197 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33565'
2023-05-21 05:39:49,208 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45393'
2023-05-21 05:39:49,467 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41969', status: init, memory: 0, processing: 0>
2023-05-21 05:39:49,468 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41969
2023-05-21 05:39:49,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38828
2023-05-21 05:39:49,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38828; closing.
2023-05-21 05:39:49,482 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41969', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:49,482 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41969
2023-05-21 05:39:49,482 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:49,500 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34405', status: init, memory: 0, processing: 0>
2023-05-21 05:39:49,501 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34405
2023-05-21 05:39:49,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38830
2023-05-21 05:39:49,533 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38830; closing.
2023-05-21 05:39:49,533 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34405', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:49,534 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34405
2023-05-21 05:39:49,534 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:49,739 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41025', status: init, memory: 0, processing: 0>
2023-05-21 05:39:49,740 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41025
2023-05-21 05:39:49,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38832
2023-05-21 05:39:49,771 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38832; closing.
2023-05-21 05:39:49,772 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41025', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:49,772 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41025
2023-05-21 05:39:49,772 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:49,900 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:38504 closed before handshake completed
2023-05-21 05:39:49,965 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43735', status: init, memory: 0, processing: 0>
2023-05-21 05:39:49,966 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43735
2023-05-21 05:39:49,966 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38520
2023-05-21 05:39:50,146 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38520; closing.
2023-05-21 05:39:50,147 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43735', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:50,147 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43735
2023-05-21 05:39:50,147 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:50,325 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:38534 closed before handshake completed
2023-05-21 05:39:50,343 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32943', status: init, memory: 0, processing: 0>
2023-05-21 05:39:50,344 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32943
2023-05-21 05:39:50,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38530
2023-05-21 05:39:50,382 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:38542 closed before handshake completed
2023-05-21 05:39:50,403 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38530; closing.
2023-05-21 05:39:50,404 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32943', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:50,404 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32943
2023-05-21 05:39:50,404 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:50,533 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:38548 closed before handshake completed
2023-05-21 05:39:50,929 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:38558 closed before handshake completed
2023-05-21 05:39:50,959 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:50,959 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:50,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:50,981 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:50,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:50,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:50,989 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:51,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:51,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:51,013 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:51,021 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:51,021 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:51,022 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:51,023 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:51,025 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:51,025 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:51,028 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:51,044 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:51,062 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:51,063 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:51,063 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:51,078 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:39:51,078 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:39:51,094 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44207', status: init, memory: 0, processing: 0>
2023-05-21 05:39:51,094 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44207
2023-05-21 05:39:51,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38568
2023-05-21 05:39:51,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38568; closing.
2023-05-21 05:39:51,137 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44207', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:51,138 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44207
2023-05-21 05:39:51,138 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:51,145 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:39:51,342 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:38582 closed before handshake completed
2023-05-21 05:39:52,956 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42059', status: init, memory: 0, processing: 0>
2023-05-21 05:39:52,957 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42059
2023-05-21 05:39:52,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38586
2023-05-21 05:39:53,006 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38586; closing.
2023-05-21 05:39:53,007 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42059', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:53,007 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42059
2023-05-21 05:39:53,007 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:53,124 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:38602 closed before handshake completed
2023-05-21 05:39:53,945 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:38606 closed before handshake completed
2023-05-21 05:39:54,525 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36131
2023-05-21 05:39:54,525 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36131
2023-05-21 05:39:54,525 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45067
2023-05-21 05:39:54,525 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,525 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,525 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:54,525 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:54,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7sglpzba
2023-05-21 05:39:54,526 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b0872ac-c749-418c-b21e-80d5e4a5681a
2023-05-21 05:39:54,526 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f85a0b8-4eca-4423-bde4-200adaceee52
2023-05-21 05:39:54,533 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43273
2023-05-21 05:39:54,534 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43273
2023-05-21 05:39:54,534 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35141
2023-05-21 05:39:54,534 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,534 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,534 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:54,534 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:54,534 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-b95d_5z0
2023-05-21 05:39:54,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7f1edd5-5976-4b84-be82-62da41c4eb04
2023-05-21 05:39:54,539 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34377
2023-05-21 05:39:54,539 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34377
2023-05-21 05:39:54,539 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38713
2023-05-21 05:39:54,539 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,539 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,539 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:54,540 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:54,540 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pg90n_gc
2023-05-21 05:39:54,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-48520454-b467-4ac6-887b-5c0563c31e19
2023-05-21 05:39:54,540 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df915f39-6116-45c2-b900-5b5d4cf601cb
2023-05-21 05:39:54,596 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40581
2023-05-21 05:39:54,596 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40581
2023-05-21 05:39:54,596 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44729
2023-05-21 05:39:54,596 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,596 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,596 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:54,596 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:54,596 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kpjgv262
2023-05-21 05:39:54,597 - distributed.worker - INFO - Starting Worker plugin RMMSetup-34635861-c9bf-4313-b51e-89dbb8219cfa
2023-05-21 05:39:54,604 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43889
2023-05-21 05:39:54,604 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43889
2023-05-21 05:39:54,604 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46435
2023-05-21 05:39:54,605 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,605 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,605 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:54,605 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:54,605 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-axjmukbz
2023-05-21 05:39:54,605 - distributed.worker - INFO - Starting Worker plugin PreImport-57db6a5e-0f82-4c3c-a03e-8b70c5c7a1eb
2023-05-21 05:39:54,606 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-76cfea90-9a0b-4959-a81c-fcfb65bad0c4
2023-05-21 05:39:54,606 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da82e029-373e-45a0-892d-f527b4a5a716
2023-05-21 05:39:54,607 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37361
2023-05-21 05:39:54,607 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37361
2023-05-21 05:39:54,607 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36073
2023-05-21 05:39:54,607 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,607 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,608 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:54,608 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:54,608 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gy5d_9xm
2023-05-21 05:39:54,608 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc89c94a-b449-4f69-9e4e-801b20bd9f18
2023-05-21 05:39:54,609 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45643
2023-05-21 05:39:54,609 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45643
2023-05-21 05:39:54,609 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44159
2023-05-21 05:39:54,609 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,609 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,609 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:54,609 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:54,609 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-v7sx2t76
2023-05-21 05:39:54,610 - distributed.worker - INFO - Starting Worker plugin PreImport-b0ad3f9d-d991-4f2e-b23a-c3f10ae41c7a
2023-05-21 05:39:54,610 - distributed.worker - INFO - Starting Worker plugin PreImport-f74a0291-b1fc-4655-bee8-d625494f786b
2023-05-21 05:39:54,610 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4ed78ce-ae55-46e3-b0d6-569b1f69138a
2023-05-21 05:39:54,610 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d94e91c-9a03-41ad-984c-ffde0ee3ea10
2023-05-21 05:39:54,610 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d915ffb-8cdf-470b-a3d2-2f462b747b71
2023-05-21 05:39:54,611 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35505
2023-05-21 05:39:54,612 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35505
2023-05-21 05:39:54,612 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34351
2023-05-21 05:39:54,612 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,612 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,612 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:39:54,612 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:39:54,612 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tncywgc5
2023-05-21 05:39:54,612 - distributed.worker - INFO - Starting Worker plugin PreImport-eec2e7da-9218-45a4-840a-ac723209735c
2023-05-21 05:39:54,612 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6632f936-5b31-46bf-871e-f77115133047
2023-05-21 05:39:54,613 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b33ac4e5-6d05-4e30-8d3d-b984ccb18a62
2023-05-21 05:39:54,832 - distributed.worker - INFO - Starting Worker plugin PreImport-b5447e7a-fab1-4772-ae8f-054ea76bef3e
2023-05-21 05:39:54,832 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,839 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e08a33e9-7c0f-4db8-b08e-d40c72a5151c
2023-05-21 05:39:54,842 - distributed.worker - INFO - Starting Worker plugin PreImport-c6357ae8-1402-49eb-babd-6d72247a7748
2023-05-21 05:39:54,842 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,848 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-152d91e8-9677-4e36-9f21-6b5d7893d326
2023-05-21 05:39:54,848 - distributed.worker - INFO - Starting Worker plugin PreImport-837dd3f7-4020-46b2-95e5-91e344458310
2023-05-21 05:39:54,848 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,853 - distributed.worker - INFO - Starting Worker plugin PreImport-9c254261-1ba4-4591-9801-eaf979c1a010
2023-05-21 05:39:54,854 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,856 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,858 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,858 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34377', status: init, memory: 0, processing: 0>
2023-05-21 05:39:54,858 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,859 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34377
2023-05-21 05:39:54,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38612
2023-05-21 05:39:54,859 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,860 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:54,874 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40581', status: init, memory: 0, processing: 0>
2023-05-21 05:39:54,874 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40581
2023-05-21 05:39:54,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38626
2023-05-21 05:39:54,875 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,875 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:54,879 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43273', status: init, memory: 0, processing: 0>
2023-05-21 05:39:54,880 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43273
2023-05-21 05:39:54,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38618
2023-05-21 05:39:54,881 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,881 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:54,885 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43889', status: init, memory: 0, processing: 0>
2023-05-21 05:39:54,885 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43889
2023-05-21 05:39:54,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38646
2023-05-21 05:39:54,886 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,886 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:54,888 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35505', status: init, memory: 0, processing: 0>
2023-05-21 05:39:54,889 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35505
2023-05-21 05:39:54,889 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38656
2023-05-21 05:39:54,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,889 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:54,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36131', status: init, memory: 0, processing: 0>
2023-05-21 05:39:54,893 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36131
2023-05-21 05:39:54,893 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38634
2023-05-21 05:39:54,894 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,894 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45643', status: init, memory: 0, processing: 0>
2023-05-21 05:39:54,894 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,895 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45643
2023-05-21 05:39:54,895 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38672
2023-05-21 05:39:54,895 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37361', status: init, memory: 0, processing: 0>
2023-05-21 05:39:54,895 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,895 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,896 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37361
2023-05-21 05:39:54,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38668
2023-05-21 05:39:54,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:39:54,897 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:39:54,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:54,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:54,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:39:54,937 - distributed.scheduler - INFO - Receive client connection: Client-ea2e20dd-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:39:54,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38680
2023-05-21 05:39:54,974 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:54,974 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:54,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:54,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:54,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:54,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:54,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:54,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:39:54,987 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:54,987 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:54,987 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:54,987 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:54,987 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:54,987 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:54,987 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:54,987 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:39:54,995 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:39:54,997 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:39:55,000 - distributed.scheduler - INFO - Remove client Client-e561d2ee-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:55,000 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38738; closing.
2023-05-21 05:39:55,000 - distributed.scheduler - INFO - Remove client Client-e561d2ee-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:55,000 - distributed.scheduler - INFO - Close client connection: Client-e561d2ee-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:39:55,001 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36753'. Reason: nanny-close
2023-05-21 05:39:55,002 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:55,002 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37887'. Reason: nanny-close
2023-05-21 05:39:55,003 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:55,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37361. Reason: nanny-close
2023-05-21 05:39:55,003 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36619'. Reason: nanny-close
2023-05-21 05:39:55,004 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:55,004 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45393'. Reason: nanny-close
2023-05-21 05:39:55,004 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36131. Reason: nanny-close
2023-05-21 05:39:55,004 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:55,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34377. Reason: nanny-close
2023-05-21 05:39:55,005 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45933'. Reason: nanny-close
2023-05-21 05:39:55,005 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:55,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35505. Reason: nanny-close
2023-05-21 05:39:55,006 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38071'. Reason: nanny-close
2023-05-21 05:39:55,006 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38668; closing.
2023-05-21 05:39:55,006 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:55,006 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:55,006 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37361', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:55,006 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37361
2023-05-21 05:39:55,006 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45643. Reason: nanny-close
2023-05-21 05:39:55,006 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40015'. Reason: nanny-close
2023-05-21 05:39:55,007 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:55,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:55,007 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43273. Reason: nanny-close
2023-05-21 05:39:55,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:55,007 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33565'. Reason: nanny-close
2023-05-21 05:39:55,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:55,007 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:55,007 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:39:55,007 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37361
2023-05-21 05:39:55,008 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38634; closing.
2023-05-21 05:39:55,008 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43889. Reason: nanny-close
2023-05-21 05:39:55,008 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:55,008 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36131', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:55,008 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:55,008 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:55,008 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36131
2023-05-21 05:39:55,008 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40581. Reason: nanny-close
2023-05-21 05:39:55,008 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37361
2023-05-21 05:39:55,009 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38612; closing.
2023-05-21 05:39:55,009 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38656; closing.
2023-05-21 05:39:55,009 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37361
2023-05-21 05:39:55,009 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:55,009 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37361
2023-05-21 05:39:55,009 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34377', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:55,009 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34377
2023-05-21 05:39:55,009 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35505', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:55,010 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35505
2023-05-21 05:39:55,010 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:55,010 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:55,010 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:39:55,010 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38672; closing.
2023-05-21 05:39:55,010 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:55,010 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38618; closing.
2023-05-21 05:39:55,011 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:55,011 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45643', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:55,011 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45643
2023-05-21 05:39:55,011 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:55,011 - distributed.nanny - INFO - Worker closed
2023-05-21 05:39:55,011 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43273', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:55,011 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43273
2023-05-21 05:39:55,012 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38646; closing.
2023-05-21 05:39:55,012 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38626; closing.
2023-05-21 05:39:55,012 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43889', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:55,012 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43889
2023-05-21 05:39:55,013 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40581', status: closing, memory: 0, processing: 0>
2023-05-21 05:39:55,013 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40581
2023-05-21 05:39:55,013 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:39:56,670 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:39:56,670 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:39:56,671 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:39:56,673 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:39:56,674 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-05-21 05:39:58,948 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:58,953 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45893 instead
  warnings.warn(
2023-05-21 05:39:58,957 - distributed.scheduler - INFO - State start
2023-05-21 05:39:58,983 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:39:58,984 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:39:58,984 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:39:58,985 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-21 05:39:59,458 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45915'
2023-05-21 05:39:59,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42287'
2023-05-21 05:39:59,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39497'
2023-05-21 05:39:59,498 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44123'
2023-05-21 05:39:59,501 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41705'
2023-05-21 05:39:59,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45493'
2023-05-21 05:39:59,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45295'
2023-05-21 05:39:59,537 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37709'
2023-05-21 05:40:01,223 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:01,223 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:01,250 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:01,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:01,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:01,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:01,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:01,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:01,287 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:01,287 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:01,288 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:01,288 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:01,288 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:01,288 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:01,292 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:01,292 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:01,300 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:01,335 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:01,350 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:01,351 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:01,351 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:01,352 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:01,352 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:01,421 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:04,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36119
2023-05-21 05:40:04,381 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36119
2023-05-21 05:40:04,382 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37461
2023-05-21 05:40:04,382 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:04,382 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:04,382 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:04,382 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:04,382 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-__k10kyk
2023-05-21 05:40:04,382 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e668f51-4da9-4ffe-91f1-2deb797fe1ab
2023-05-21 05:40:04,382 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5309b195-41fc-4432-a1e5-98a2d7373bbc
2023-05-21 05:40:04,576 - distributed.worker - INFO - Starting Worker plugin PreImport-e7ead2bb-0bc5-495e-bf70-cc070a2e73ce
2023-05-21 05:40:04,576 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:04,834 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38953
2023-05-21 05:40:04,834 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38953
2023-05-21 05:40:04,835 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36805
2023-05-21 05:40:04,835 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:04,835 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:04,835 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:04,835 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:04,835 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0grwrc_v
2023-05-21 05:40:04,835 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d497ed61-0c5f-4bdd-99ba-9d7f04f8539f
2023-05-21 05:40:04,836 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d484381f-32b2-4446-9664-084305aeb964
2023-05-21 05:40:05,002 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33509
2023-05-21 05:40:05,003 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33509
2023-05-21 05:40:05,003 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33497
2023-05-21 05:40:05,003 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:05,003 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,003 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:05,003 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:05,003 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_wjoor17
2023-05-21 05:40:05,003 - distributed.worker - INFO - Starting Worker plugin PreImport-22c9087b-bd47-4423-892e-53976971428a
2023-05-21 05:40:05,004 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab2f4faa-84dc-4c0b-a174-214252e19ee3
2023-05-21 05:40:05,004 - distributed.worker - INFO - Starting Worker plugin RMMSetup-34108224-1794-47dd-a2ea-6d7ad7217cc5
2023-05-21 05:40:05,007 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43397
2023-05-21 05:40:05,007 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43397
2023-05-21 05:40:05,007 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35599
2023-05-21 05:40:05,007 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:05,007 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,007 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:05,008 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:05,008 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tt34oxbv
2023-05-21 05:40:05,008 - distributed.worker - INFO - Starting Worker plugin PreImport-648b3974-9b9a-477b-a44d-849ee9cde18e
2023-05-21 05:40:05,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eccaeca3-f168-4541-a891-7f96c23e82cd
2023-05-21 05:40:05,008 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f04bfd4c-0b46-4e94-b5a0-b9a10233f547
2023-05-21 05:40:05,016 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35351
2023-05-21 05:40:05,017 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35351
2023-05-21 05:40:05,017 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34473
2023-05-21 05:40:05,017 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:05,017 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,017 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:05,017 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:05,017 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vnscmkr7
2023-05-21 05:40:05,017 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1640adf-9d80-4357-acc4-1f8934a7d5d6
2023-05-21 05:40:05,018 - distributed.worker - INFO - Starting Worker plugin PreImport-ebf72781-ac65-4ad2-a5b4-1384a31e6fb6
2023-05-21 05:40:05,018 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b24ce60d-7e94-4603-a7c1-0b227cec9d83
2023-05-21 05:40:05,021 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40923
2023-05-21 05:40:05,021 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40923
2023-05-21 05:40:05,021 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45595
2023-05-21 05:40:05,021 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:05,021 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,021 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:05,021 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:05,021 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_m4dao_3
2023-05-21 05:40:05,022 - distributed.worker - INFO - Starting Worker plugin PreImport-d99355b4-37b5-4095-ab3a-55cc6b0722e2
2023-05-21 05:40:05,022 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-39472ddb-2c4f-40b6-84e2-3d334fd20017
2023-05-21 05:40:05,022 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0058168-da83-4258-9c88-a48aa5e57e67
2023-05-21 05:40:05,027 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43245
2023-05-21 05:40:05,027 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43245
2023-05-21 05:40:05,027 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32895
2023-05-21 05:40:05,027 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:05,027 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,027 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:05,027 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:05,027 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-afv6p1sa
2023-05-21 05:40:05,028 - distributed.worker - INFO - Starting Worker plugin RMMSetup-672be719-806e-428a-a6fc-418e239124dd
2023-05-21 05:40:05,033 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38391
2023-05-21 05:40:05,033 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38391
2023-05-21 05:40:05,033 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34595
2023-05-21 05:40:05,033 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:05,033 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,033 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:05,033 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:05,033 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-graeebiq
2023-05-21 05:40:05,034 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7bd1f1f4-5942-441a-b6ad-574368742692
2023-05-21 05:40:05,185 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,185 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,186 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,186 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dbbe4854-b460-4c1b-b82f-54a063155d4d
2023-05-21 05:40:05,186 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb2057e7-1795-4448-a2d6-d536a61b91da
2023-05-21 05:40:05,186 - distributed.worker - INFO - Starting Worker plugin PreImport-9afc56ce-747f-4a9b-b337-f2cd1f8eacbd
2023-05-21 05:40:05,186 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,186 - distributed.worker - INFO - Starting Worker plugin PreImport-c50f8bdb-74d7-4eb3-936b-e7f414991ad9
2023-05-21 05:40:05,187 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,191 - distributed.worker - INFO - Starting Worker plugin PreImport-e5aa0468-3914-4126-9be0-1f9a2b6e31cd
2023-05-21 05:40:05,192 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:05,200 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:06,273 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:06,273 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:06,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:06,275 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:06,275 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:06,278 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:06,498 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:06,499 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:06,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:06,721 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:06,721 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:06,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:06,785 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:06,785 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:06,787 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:06,814 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:06,814 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:06,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:06,941 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:06,942 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:06,945 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:06,987 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:06,987 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:06,989 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:07,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:07,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:07,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:07,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:07,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:07,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:07,568 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:07,568 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:07,575 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45915'. Reason: nanny-close
2023-05-21 05:40:07,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:07,576 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42287'. Reason: nanny-close
2023-05-21 05:40:07,577 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:07,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39497'. Reason: nanny-close
2023-05-21 05:40:07,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35351. Reason: nanny-close
2023-05-21 05:40:07,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:07,578 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38953. Reason: nanny-close
2023-05-21 05:40:07,578 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44123'. Reason: nanny-close
2023-05-21 05:40:07,579 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:07,579 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41705'. Reason: nanny-close
2023-05-21 05:40:07,579 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36119. Reason: nanny-close
2023-05-21 05:40:07,579 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:07,580 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45493'. Reason: nanny-close
2023-05-21 05:40:07,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40923. Reason: nanny-close
2023-05-21 05:40:07,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:07,580 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:07,580 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45295'. Reason: nanny-close
2023-05-21 05:40:07,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33509. Reason: nanny-close
2023-05-21 05:40:07,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:07,581 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37709'. Reason: nanny-close
2023-05-21 05:40:07,581 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:07,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38391. Reason: nanny-close
2023-05-21 05:40:07,581 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:07,581 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:07,581 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:07,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43397. Reason: nanny-close
2023-05-21 05:40:07,581 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:07,582 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:07,582 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38953
2023-05-21 05:40:07,583 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38953
2023-05-21 05:40:07,583 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:07,583 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38953
2023-05-21 05:40:07,583 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:07,583 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43245. Reason: nanny-close
2023-05-21 05:40:07,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:07,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:07,584 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38953
2023-05-21 05:40:07,584 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:07,584 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:07,585 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:07,585 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:07,586 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:07,586 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 79, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 56, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 210, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 2, in <module>
    import dask.dataframe._pyarrow_compat
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/_pyarrow_compat.py", line 3, in <module>
    import pandas as pd
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/__init__.py", line 22, in <module>
    from pandas.compat import is_numpy_dev as _is_numpy_dev  # pyright: ignore # noqa:F401
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/compat/__init__.py", line 22, in <module>
    from pandas.compat.pyarrow import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/compat/pyarrow.py", line 8, in <module>
    import pyarrow as pa
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pyarrow/__init__.py", line 65, in <module>
    import pyarrow.lib as _lib
  File "<frozen importlib._bootstrap>", line 398, in parent
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 79, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 56, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 210, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 2, in <module>
    import dask.dataframe._pyarrow_compat
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/_pyarrow_compat.py", line 3, in <module>
    import pandas as pd
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/__init__.py", line 48, in <module>
    from pandas.core.api import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/api.py", line 27, in <module>
    from pandas.core.arrays import Categorical
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/arrays/__init__.py", line 8, in <module>
    from pandas.core.arrays.categorical import Categorical
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/arrays/categorical.py", line 255, in <module>
    class Categorical(NDArrayBackedExtensionArray, PandasObject, ObjectStringArrayMixin):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/core/arrays/categorical.py", line 1893, in Categorical
    def sort_values(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pandas/util/_decorators.py", line 293, in decorate
    old_sig = inspect.signature(func)
  File "/opt/conda/envs/gdf/lib/python3.9/inspect.py", line 3113, in signature
    return Signature.from_callable(obj, follow_wrapped=follow_wrapped)
  File "/opt/conda/envs/gdf/lib/python3.9/inspect.py", line 2862, in from_callable
    return _signature_from_callable(obj, sigcls=cls,
  File "/opt/conda/envs/gdf/lib/python3.9/inspect.py", line 2325, in _signature_from_callable
    return _signature_from_function(sigcls, obj,
  File "/opt/conda/envs/gdf/lib/python3.9/inspect.py", line 2205, in _signature_from_function
    parameters.append(Parameter(name, annotation=annotation,
  File "/opt/conda/envs/gdf/lib/python3.9/inspect.py", line 2500, in __init__
    self._kind = _ParameterKind(kind)
  File "/opt/conda/envs/gdf/lib/python3.9/enum.py", line 384, in __call__
    return cls.__new__(cls, value)
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-05-21 05:40:14,195 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:14,200 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45797 instead
  warnings.warn(
2023-05-21 05:40:14,204 - distributed.scheduler - INFO - State start
2023-05-21 05:40:14,226 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:14,227 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:14,227 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:14,228 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-21 05:40:14,311 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33053'
2023-05-21 05:40:16,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:16,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:16,410 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:17,478 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40331
2023-05-21 05:40:17,479 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40331
2023-05-21 05:40:17,479 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34911
2023-05-21 05:40:17,479 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:17,479 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:17,479 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:17,479 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-21 05:40:17,479 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9xh0f9vv
2023-05-21 05:40:17,479 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce0eac8e-aec5-4068-b324-b1466a649589
2023-05-21 05:40:17,479 - distributed.worker - INFO - Starting Worker plugin PreImport-9c2127bd-b055-422b-b5e4-5a46e7598c1f
2023-05-21 05:40:17,481 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8aca7738-619b-4b57-8e84-905d59440136
2023-05-21 05:40:17,481 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:17,530 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:17,530 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:17,532 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:17,587 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:17,592 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33053'. Reason: nanny-close
2023-05-21 05:40:17,592 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:17,594 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40331. Reason: nanny-close
2023-05-21 05:40:17,596 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:17,598 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-05-21 05:40:20,893 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:20,898 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42991 instead
  warnings.warn(
2023-05-21 05:40:20,902 - distributed.scheduler - INFO - State start
2023-05-21 05:40:20,923 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:20,924 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:40:20,924 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42991/status
2023-05-21 05:40:25,111 - distributed.scheduler - INFO - Receive client connection: Client-fc2a52b0-f799-11ed-b842-d8c49764f6bb
2023-05-21 05:40:25,124 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35780
2023-05-21 05:40:25,429 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:40:25,429 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:25,430 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:25,431 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:40:25,432 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-05-21 05:40:27,506 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:27,510 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43407 instead
  warnings.warn(
2023-05-21 05:40:27,515 - distributed.scheduler - INFO - State start
2023-05-21 05:40:27,535 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:27,536 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-21 05:40:27,537 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43407/status
2023-05-21 05:40:27,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43839'
2023-05-21 05:40:28,643 - distributed.scheduler - INFO - Receive client connection: Client-fc99d2fa-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:28,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57074
2023-05-21 05:40:29,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:29,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:29,440 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:32,326 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33727
2023-05-21 05:40:32,327 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33727
2023-05-21 05:40:32,327 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39019
2023-05-21 05:40:32,327 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-21 05:40:32,327 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,327 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:32,327 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-21 05:40:32,327 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ql93hjcy
2023-05-21 05:40:32,327 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f74d853d-203e-42ce-b9e4-b9bcf0112ec8
2023-05-21 05:40:32,328 - distributed.worker - INFO - Starting Worker plugin PreImport-cfed5f70-590b-49f4-9c2a-24b51e7ac075
2023-05-21 05:40:32,328 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-065e7495-c8e7-420b-8c9d-e78c9ba64529
2023-05-21 05:40:32,328 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,356 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33727', status: init, memory: 0, processing: 0>
2023-05-21 05:40:32,358 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33727
2023-05-21 05:40:32,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51890
2023-05-21 05:40:32,359 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-21 05:40:32,359 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:32,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-21 05:40:32,453 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:32,456 - distributed.scheduler - INFO - Remove client Client-fc99d2fa-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:32,457 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57074; closing.
2023-05-21 05:40:32,457 - distributed.scheduler - INFO - Remove client Client-fc99d2fa-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:32,457 - distributed.scheduler - INFO - Close client connection: Client-fc99d2fa-f799-11ed-b058-d8c49764f6bb
2023-05-21 05:40:32,458 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43839'. Reason: nanny-close
2023-05-21 05:40:32,459 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:32,461 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33727. Reason: nanny-close
2023-05-21 05:40:32,463 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51890; closing.
2023-05-21 05:40:32,463 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-21 05:40:32,463 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33727', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:32,463 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33727
2023-05-21 05:40:32,464 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:32,464 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:33,576 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:40:33,576 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:33,577 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:33,577 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-21 05:40:33,578 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-05-21 05:40:35,794 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:35,798 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45307 instead
  warnings.warn(
2023-05-21 05:40:35,802 - distributed.scheduler - INFO - State start
2023-05-21 05:40:35,822 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:35,823 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:40:35,823 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45307/status
2023-05-21 05:40:36,018 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34679'
2023-05-21 05:40:36,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38013'
2023-05-21 05:40:36,038 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46721'
2023-05-21 05:40:36,046 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32929'
2023-05-21 05:40:36,054 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37189'
2023-05-21 05:40:36,062 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43157'
2023-05-21 05:40:36,070 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40471'
2023-05-21 05:40:36,077 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46879'
2023-05-21 05:40:36,850 - distributed.scheduler - INFO - Receive client connection: Client-01d08efa-f79a-11ed-b842-d8c49764f6bb
2023-05-21 05:40:36,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35370
2023-05-21 05:40:37,480 - distributed.scheduler - INFO - Receive client connection: Client-017afebc-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:37,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35404
2023-05-21 05:40:37,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:37,806 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:37,832 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:37,833 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:37,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:37,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:37,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:37,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:37,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:37,852 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:37,852 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:37,862 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:37,862 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:37,876 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:37,876 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:37,877 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:37,877 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:37,884 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:37,885 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:37,886 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:37,887 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:37,938 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:37,939 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:37,939 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:41,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37033
2023-05-21 05:40:41,322 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37033
2023-05-21 05:40:41,322 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45345
2023-05-21 05:40:41,322 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,322 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,322 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:41,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39131
2023-05-21 05:40:41,323 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:41,323 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39131
2023-05-21 05:40:41,323 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-cwwmuu2a
2023-05-21 05:40:41,323 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42353
2023-05-21 05:40:41,323 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,323 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,323 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:41,323 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:41,323 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mnh_967w
2023-05-21 05:40:41,323 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07e2df32-8bdf-49f1-9316-849db998418d
2023-05-21 05:40:41,323 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a52d675-e172-4769-93e8-dacf1d84d514
2023-05-21 05:40:41,342 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46193
2023-05-21 05:40:41,342 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46193
2023-05-21 05:40:41,343 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42523
2023-05-21 05:40:41,343 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,343 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,343 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:41,343 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:41,343 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i8kpm_4q
2023-05-21 05:40:41,343 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d8e94b6-d23a-4a74-b8b0-b26c38ff495d
2023-05-21 05:40:41,349 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45591
2023-05-21 05:40:41,349 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45591
2023-05-21 05:40:41,349 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36897
2023-05-21 05:40:41,349 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,349 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,349 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:41,350 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:41,350 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-w5qpoenv
2023-05-21 05:40:41,351 - distributed.worker - INFO - Starting Worker plugin RMMSetup-daea8dd7-ea20-4647-84d9-bc7261201aeb
2023-05-21 05:40:41,353 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35197
2023-05-21 05:40:41,353 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35197
2023-05-21 05:40:41,354 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44455
2023-05-21 05:40:41,354 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,354 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,354 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:41,354 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:41,354 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5gyto3zj
2023-05-21 05:40:41,355 - distributed.worker - INFO - Starting Worker plugin RMMSetup-63ad1692-39fe-44e5-bc61-a5cc9f3af961
2023-05-21 05:40:41,411 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36625
2023-05-21 05:40:41,411 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36625
2023-05-21 05:40:41,411 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35545
2023-05-21 05:40:41,411 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,411 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,412 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:41,412 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:41,412 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sf26rutg
2023-05-21 05:40:41,412 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1accdc96-c9dd-4ad5-9575-e378b7359fae
2023-05-21 05:40:41,417 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36537
2023-05-21 05:40:41,418 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36537
2023-05-21 05:40:41,418 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43227
2023-05-21 05:40:41,418 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,418 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,418 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:41,418 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:41,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-f4w2qw76
2023-05-21 05:40:41,419 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9df1850-12a5-4afd-a801-64f72673938b
2023-05-21 05:40:41,419 - distributed.worker - INFO - Starting Worker plugin PreImport-b7bf1e43-a0f1-4450-8f0e-216b65e961bc
2023-05-21 05:40:41,420 - distributed.worker - INFO - Starting Worker plugin RMMSetup-273af54b-0a4b-4e43-844d-ad4bc85d4561
2023-05-21 05:40:41,423 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40229
2023-05-21 05:40:41,423 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40229
2023-05-21 05:40:41,423 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38479
2023-05-21 05:40:41,423 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,424 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,424 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:41,424 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-21 05:40:41,424 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jizkp_gk
2023-05-21 05:40:41,425 - distributed.worker - INFO - Starting Worker plugin RMMSetup-185a8b12-496a-48ec-94af-17cad80ca214
2023-05-21 05:40:41,567 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8fe24988-6207-4789-9066-33be47ba10a1
2023-05-21 05:40:41,567 - distributed.worker - INFO - Starting Worker plugin PreImport-f38036db-87ef-4b7b-8cfa-8c16aae555d2
2023-05-21 05:40:41,568 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,587 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2bd304be-c5e1-4c87-bea3-de4478bde2b6
2023-05-21 05:40:41,587 - distributed.worker - INFO - Starting Worker plugin PreImport-132fdf20-c01a-4542-92d8-9ef1bbd9f947
2023-05-21 05:40:41,588 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,601 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-384958fd-e4dd-4f51-b177-c508316c74b3
2023-05-21 05:40:41,601 - distributed.worker - INFO - Starting Worker plugin PreImport-fc939b3f-9b92-4a4d-a8e8-a675f1acc677
2023-05-21 05:40:41,602 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,609 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c180ee49-c30b-4c78-b3c0-4bb4159f9c81
2023-05-21 05:40:41,609 - distributed.worker - INFO - Starting Worker plugin PreImport-f1993613-e361-4652-a8a8-9963b1b61a0d
2023-05-21 05:40:41,609 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,609 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cdc3e210-a4da-4c9b-a4dc-5fa2032d5e1a
2023-05-21 05:40:41,610 - distributed.worker - INFO - Starting Worker plugin PreImport-7a2daef3-3982-44f9-8473-124f20b80ecb
2023-05-21 05:40:41,610 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-866d8b7f-e124-411a-a7fe-2500c220c8d2
2023-05-21 05:40:41,610 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,610 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17f3043a-c138-4090-843f-0fd4cc94d82e
2023-05-21 05:40:41,610 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,610 - distributed.worker - INFO - Starting Worker plugin PreImport-7e50265a-5c8a-4eec-b4bb-cf4f8c84ace4
2023-05-21 05:40:41,610 - distributed.worker - INFO - Starting Worker plugin PreImport-f1fbfd49-6ec8-4268-ab16-5a150c4a371e
2023-05-21 05:40:41,610 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,611 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,612 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37033', status: init, memory: 0, processing: 0>
2023-05-21 05:40:41,613 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37033
2023-05-21 05:40:41,613 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38670
2023-05-21 05:40:41,614 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,614 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:41,624 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:40:41,628 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:41,630 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46193', status: init, memory: 0, processing: 0>
2023-05-21 05:40:41,630 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46193
2023-05-21 05:40:41,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38694
2023-05-21 05:40:41,631 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,631 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,631 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39131', status: init, memory: 0, processing: 0>
2023-05-21 05:40:41,631 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39131
2023-05-21 05:40:41,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38680
2023-05-21 05:40:41,632 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:41,632 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,632 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,633 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:41,635 - distributed.scheduler - INFO - Remove client Client-01d08efa-f79a-11ed-b842-d8c49764f6bb
2023-05-21 05:40:41,635 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35370; closing.
2023-05-21 05:40:41,635 - distributed.scheduler - INFO - Remove client Client-01d08efa-f79a-11ed-b842-d8c49764f6bb
2023-05-21 05:40:41,635 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:41,636 - distributed.scheduler - INFO - Close client connection: Client-01d08efa-f79a-11ed-b842-d8c49764f6bb
2023-05-21 05:40:41,644 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45591', status: init, memory: 0, processing: 0>
2023-05-21 05:40:41,645 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45591
2023-05-21 05:40:41,645 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38730
2023-05-21 05:40:41,645 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,645 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:41,649 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36625', status: init, memory: 0, processing: 0>
2023-05-21 05:40:41,649 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36625
2023-05-21 05:40:41,649 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38702
2023-05-21 05:40:41,650 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,650 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40229', status: init, memory: 0, processing: 0>
2023-05-21 05:40:41,650 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,651 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40229
2023-05-21 05:40:41,651 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38738
2023-05-21 05:40:41,652 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35197', status: init, memory: 0, processing: 0>
2023-05-21 05:40:41,651 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,652 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,652 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35197
2023-05-21 05:40:41,652 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38714
2023-05-21 05:40:41,653 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,653 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,653 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36537', status: init, memory: 0, processing: 0>
2023-05-21 05:40:41,653 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36537
2023-05-21 05:40:41,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:41,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38732
2023-05-21 05:40:41,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:41,654 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:41,654 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:41,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:41,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:41,733 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:41,733 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:41,733 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:41,733 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:41,733 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:41,733 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:41,733 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:41,734 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-21 05:40:41,748 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:41,748 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:41,748 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:41,748 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:41,749 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:41,749 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:41,749 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:41,749 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:41,753 - distributed.scheduler - INFO - Remove client Client-017afebc-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:41,753 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35404; closing.
2023-05-21 05:40:41,754 - distributed.scheduler - INFO - Remove client Client-017afebc-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:41,754 - distributed.scheduler - INFO - Close client connection: Client-017afebc-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:41,755 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32929'. Reason: nanny-close
2023-05-21 05:40:41,756 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:41,756 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37189'. Reason: nanny-close
2023-05-21 05:40:41,757 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:41,757 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36537. Reason: nanny-close
2023-05-21 05:40:41,757 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34679'. Reason: nanny-close
2023-05-21 05:40:41,758 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:41,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38013'. Reason: nanny-close
2023-05-21 05:40:41,758 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39131. Reason: nanny-close
2023-05-21 05:40:41,758 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:41,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46721'. Reason: nanny-close
2023-05-21 05:40:41,758 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40229. Reason: nanny-close
2023-05-21 05:40:41,759 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:41,759 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43157'. Reason: nanny-close
2023-05-21 05:40:41,759 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45591. Reason: nanny-close
2023-05-21 05:40:41,759 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:41,760 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38732; closing.
2023-05-21 05:40:41,760 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:41,760 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40471'. Reason: nanny-close
2023-05-21 05:40:41,760 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35197. Reason: nanny-close
2023-05-21 05:40:41,760 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36537', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:41,760 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:41,760 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36537
2023-05-21 05:40:41,760 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46879'. Reason: nanny-close
2023-05-21 05:40:41,760 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:41,760 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:41,760 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36625. Reason: nanny-close
2023-05-21 05:40:41,761 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:41,761 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:41,761 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37033. Reason: nanny-close
2023-05-21 05:40:41,761 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:41,762 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38738; closing.
2023-05-21 05:40:41,762 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:41,762 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38680; closing.
2023-05-21 05:40:41,762 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:41,762 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36537
2023-05-21 05:40:41,762 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:41,762 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36537
2023-05-21 05:40:41,762 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40229', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:41,762 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46193. Reason: nanny-close
2023-05-21 05:40:41,762 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36537
2023-05-21 05:40:41,762 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40229
2023-05-21 05:40:41,763 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:41,763 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39131', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:41,763 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36537
2023-05-21 05:40:41,763 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39131
2023-05-21 05:40:41,763 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:41,763 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38730; closing.
2023-05-21 05:40:41,763 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:41,764 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45591', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:41,764 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45591
2023-05-21 05:40:41,764 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:41,764 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38714; closing.
2023-05-21 05:40:41,764 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:41,764 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:41,764 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35197', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:41,765 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35197
2023-05-21 05:40:41,765 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:41,765 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38670; closing.
2023-05-21 05:40:41,765 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38702; closing.
2023-05-21 05:40:41,765 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:41,765 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37033', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:41,766 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37033
2023-05-21 05:40:41,766 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36625', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:41,766 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36625
2023-05-21 05:40:41,766 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38694; closing.
2023-05-21 05:40:41,767 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46193', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:41,767 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46193
2023-05-21 05:40:41,767 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:41,820 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38169', status: init, memory: 0, processing: 0>
2023-05-21 05:40:41,821 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38169
2023-05-21 05:40:41,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38742
2023-05-21 05:40:41,847 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38742; closing.
2023-05-21 05:40:41,847 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38169', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:41,848 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38169
2023-05-21 05:40:41,848 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:43,474 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:40:43,474 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:43,475 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:43,476 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:40:43,476 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-05-21 05:40:45,857 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:45,863 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44233 instead
  warnings.warn(
2023-05-21 05:40:45,867 - distributed.scheduler - INFO - State start
2023-05-21 05:40:45,893 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:45,894 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:45,894 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:45,895 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-21 05:40:46,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33089'
2023-05-21 05:40:47,870 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:47,870 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:47,908 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:48,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33089'. Reason: nanny-close
2023-05-21 05:40:48,868 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43659
2023-05-21 05:40:48,868 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43659
2023-05-21 05:40:48,868 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35963
2023-05-21 05:40:48,868 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:48,868 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:48,868 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:48,869 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-21 05:40:48,869 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-skzz8_b8
2023-05-21 05:40:48,869 - distributed.worker - INFO - Starting Worker plugin PreImport-b9db8fec-0578-4920-a097-61539888eddc
2023-05-21 05:40:48,869 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f72b0cd-c5f0-4c22-95e4-9d6bd86b0115
2023-05-21 05:40:48,869 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fbfdef3e-d123-4bf7-98e2-bc938096fa73
2023-05-21 05:40:49,001 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:49,041 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:49,041 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:49,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:49,090 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:49,092 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43659. Reason: nanny-close
2023-05-21 05:40:49,094 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:49,096 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-05-21 05:40:52,340 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:52,345 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34617 instead
  warnings.warn(
2023-05-21 05:40:52,349 - distributed.scheduler - INFO - State start
2023-05-21 05:40:52,371 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-21 05:40:52,372 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-21 05:40:52,373 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34617/status
2023-05-21 05:40:52,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41473'
2023-05-21 05:40:53,239 - distributed.scheduler - INFO - Receive client connection: Client-0b488cbe-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:53,253 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36650
2023-05-21 05:40:54,227 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:40:54,227 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:40:54,253 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-21 05:40:55,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46781
2023-05-21 05:40:55,249 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46781
2023-05-21 05:40:55,249 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37265
2023-05-21 05:40:55,249 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-21 05:40:55,249 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:55,249 - distributed.worker - INFO -               Threads:                          1
2023-05-21 05:40:55,250 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-21 05:40:55,250 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ycr58z2o
2023-05-21 05:40:55,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2947d3f-5638-4b17-ae43-eed76a3ca65f
2023-05-21 05:40:55,404 - distributed.worker - INFO - Starting Worker plugin PreImport-5f78bfe7-1ab2-4cc1-a6a1-1f252b42771b
2023-05-21 05:40:55,404 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a83d2ae-3e68-4592-a6d5-f3f9fbff5e0a
2023-05-21 05:40:55,404 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:55,445 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46781', status: init, memory: 0, processing: 0>
2023-05-21 05:40:55,446 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46781
2023-05-21 05:40:55,446 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36674
2023-05-21 05:40:55,447 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-21 05:40:55,447 - distributed.worker - INFO - -------------------------------------------------
2023-05-21 05:40:55,449 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-21 05:40:55,509 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-05-21 05:40:55,514 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-21 05:40:55,518 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:55,520 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-21 05:40:55,522 - distributed.scheduler - INFO - Remove client Client-0b488cbe-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:55,523 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36650; closing.
2023-05-21 05:40:55,523 - distributed.scheduler - INFO - Remove client Client-0b488cbe-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:55,523 - distributed.scheduler - INFO - Close client connection: Client-0b488cbe-f79a-11ed-b058-d8c49764f6bb
2023-05-21 05:40:55,524 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41473'. Reason: nanny-close
2023-05-21 05:40:55,525 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-21 05:40:55,526 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46781. Reason: nanny-close
2023-05-21 05:40:55,528 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-21 05:40:55,528 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36674; closing.
2023-05-21 05:40:55,529 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46781', status: closing, memory: 0, processing: 0>
2023-05-21 05:40:55,529 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46781
2023-05-21 05:40:55,529 - distributed.scheduler - INFO - Lost all workers
2023-05-21 05:40:55,530 - distributed.nanny - INFO - Worker closed
2023-05-21 05:40:56,893 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-21 05:40:56,894 - distributed.scheduler - INFO - Scheduler closing...
2023-05-21 05:40:56,895 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-21 05:40:56,896 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-21 05:40:56,897 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44149 instead
  warnings.warn(
2023-05-21 05:41:07,551 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:07,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:07,552 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:07,552 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:07,575 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:07,575 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:07,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:07,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:07,591 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:07,591 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:07,615 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:07,616 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:07,652 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:07,652 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:07,668 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:07,668 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40897 instead
  warnings.warn(
2023-05-21 05:41:19,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:19,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:19,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:19,393 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:19,393 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:19,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:19,394 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:19,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:19,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:19,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:19,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:19,453 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:19,495 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:19,495 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:19,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:19,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45885 instead
  warnings.warn(
2023-05-21 05:41:30,369 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:30,369 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:30,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:30,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:30,434 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:30,434 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:30,434 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:30,434 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:30,441 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:30,441 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:30,471 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:30,471 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:30,474 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:30,475 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:30,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:30,477 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35815 instead
  warnings.warn(
2023-05-21 05:41:42,443 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:42,443 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:42,465 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:42,465 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:42,516 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:42,516 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:42,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:42,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:42,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:42,538 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:42,578 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:42,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:42,583 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:42,583 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:42,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:42,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35809 instead
  warnings.warn(
2023-05-21 05:41:55,859 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:55,859 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:55,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:55,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:55,973 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:55,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:55,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:55,981 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:55,997 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:55,997 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:56,010 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:56,010 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:56,050 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:56,050 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:41:56,160 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:41:56,160 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36427 instead
  warnings.warn(
2023-05-21 05:42:09,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:09,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:09,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:09,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:09,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:09,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:09,374 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:09,375 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:09,389 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:09,389 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:09,421 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:09,421 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:09,450 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:09,450 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:09,468 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:09,468 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38173 instead
  warnings.warn(
2023-05-21 05:42:22,615 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:22,616 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:22,647 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:22,647 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:22,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:22,655 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:22,666 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:22,666 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:22,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:22,701 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:22,705 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:22,705 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:22,707 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:22,707 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:22,712 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:22,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41393 instead
  warnings.warn(
2023-05-21 05:42:35,852 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:35,852 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:35,898 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:35,898 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:35,898 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:35,898 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:35,908 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:35,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:35,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:35,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:35,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:35,933 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:35,940 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:35,940 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-21 05:42:36,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-21 05:42:36,096 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34303 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40849 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46879 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46743 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42519 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40023 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38287 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35223 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37509 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41419 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39677 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36979 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42033 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36403 instead
  warnings.warn(
Process SpawnProcess-22:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 132, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 101, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5109, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2199, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1985, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 374, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "interop.pyx", line 178, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34321 instead
  warnings.warn(
2023-05-21 05:46:15,283 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1389, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 525, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 664, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-05-21 05:46:15,305 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-33' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:118> exception=RuntimeError('Nanny failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 119, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-05-21 05:46:15,865 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 55698 exit status was already read will report exitcode 255
Process SpawnProcess-23:
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 288, in __init__
    self.sync(self._correct_state)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 349, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 416, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 389, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 382, in _correct_state_internal
    await w  # for tornado gen.coroutine support
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 525, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 119, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 290, in __init__
    self.sync(self.close)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 349, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 416, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 389, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 455, in _close
    assert w.status in {
AssertionError: Status.running
2023-05-21 05:46:25,842 - distributed.nanny - WARNING - Restarting worker
2023-05-21 05:46:25,868 - distributed.nanny - WARNING - Restarting worker
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45157 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36855 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43439 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33401 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39323 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34159 instead
  warnings.warn(
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-21 05:47:22,699 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-21 05:47:22,706 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-21 05:47:22,709 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fa5416beac0>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-21 05:47:22,713 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-21 05:47:22,714 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fceb6089ac0>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-21 05:47:22,721 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f810d6a8ac0>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-21 05:47:22,731 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-21 05:47:22,738 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f6cb8191ac0>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-21 05:47:24,718 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-05-21 05:47:24,724 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-05-21 05:47:24,741 - distributed.nanny - ERROR - Worker process died unexpectedly
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 12 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
