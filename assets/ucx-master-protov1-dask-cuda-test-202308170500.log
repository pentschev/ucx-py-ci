============================= test session starts ==============================
platform linux -- Python 3.9.17, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-08-17 05:36:45,140 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:36:45,144 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45317 instead
  warnings.warn(
2023-08-17 05:36:45,148 - distributed.scheduler - INFO - State start
2023-08-17 05:36:45,394 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:36:45,395 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-08-17 05:36:45,396 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45317/status
2023-08-17 05:36:45,549 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33791'
2023-08-17 05:36:45,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37187'
2023-08-17 05:36:45,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41921'
2023-08-17 05:36:45,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40213'
2023-08-17 05:36:47,409 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:47,409 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:47,409 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:47,409 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:47,411 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:47,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:47,411 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:47,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:47,413 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:47,413 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:47,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:47,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-08-17 05:36:47,597 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45219
2023-08-17 05:36:47,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45219
2023-08-17 05:36:47,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39343
2023-08-17 05:36:47,597 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-17 05:36:47,597 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:47,597 - distributed.worker - INFO -               Threads:                          4
2023-08-17 05:36:47,597 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-17 05:36:47,597 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-25qdu3dn
2023-08-17 05:36:47,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f931b46b-4a2d-404b-b0ca-311e20a0ef81
2023-08-17 05:36:47,598 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45584632-dd2b-4994-957e-5e6d3ce8388f
2023-08-17 05:36:47,598 - distributed.worker - INFO - Starting Worker plugin PreImport-3dc3057b-bf4d-42be-86ae-ddf2d83d6018
2023-08-17 05:36:47,598 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:48,842 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45219', status: init, memory: 0, processing: 0>
2023-08-17 05:36:48,856 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45219
2023-08-17 05:36:48,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39128
2023-08-17 05:36:48,858 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:36:48,858 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-17 05:36:48,859 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:48,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-17 05:36:49,517 - distributed.scheduler - INFO - Receive client connection: Client-0c49ed68-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:36:49,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39134
2023-08-17 05:36:50,314 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44039
2023-08-17 05:36:50,314 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44039
2023-08-17 05:36:50,315 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41417
2023-08-17 05:36:50,315 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-17 05:36:50,315 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:50,315 - distributed.worker - INFO -               Threads:                          4
2023-08-17 05:36:50,315 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-17 05:36:50,315 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-gyelgkla
2023-08-17 05:36:50,315 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5cd6be46-6693-406d-bcd4-76eab4db58ea
2023-08-17 05:36:50,316 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59653f35-823f-4d34-a26c-1dd5526d8cb7
2023-08-17 05:36:50,316 - distributed.worker - INFO - Starting Worker plugin PreImport-c53f0a8c-87cf-4ef5-8d8a-c525b41fd49a
2023-08-17 05:36:50,316 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:50,315 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46683
2023-08-17 05:36:50,317 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46683
2023-08-17 05:36:50,317 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42067
2023-08-17 05:36:50,317 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-17 05:36:50,317 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:50,317 - distributed.worker - INFO -               Threads:                          4
2023-08-17 05:36:50,317 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-17 05:36:50,317 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-nw66tuey
2023-08-17 05:36:50,318 - distributed.worker - INFO - Starting Worker plugin RMMSetup-884ef31a-63f3-4c92-8417-05f8a2eee861
2023-08-17 05:36:50,318 - distributed.worker - INFO - Starting Worker plugin PreImport-8df5832e-7abe-4ee6-90bb-c7d6c9f2a6ea
2023-08-17 05:36:50,318 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3be4de48-c809-42c5-9ca9-2498bf30654a
2023-08-17 05:36:50,319 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:50,321 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44187
2023-08-17 05:36:50,321 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44187
2023-08-17 05:36:50,321 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45539
2023-08-17 05:36:50,321 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-17 05:36:50,321 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:50,322 - distributed.worker - INFO -               Threads:                          4
2023-08-17 05:36:50,322 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-17 05:36:50,322 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-o3lddbk8
2023-08-17 05:36:50,322 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6825a2a3-9c7d-4dac-b3bd-f017ca01ff87
2023-08-17 05:36:50,322 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f453c0d2-600f-4fb3-90b1-b17650df7796
2023-08-17 05:36:50,322 - distributed.worker - INFO - Starting Worker plugin PreImport-edf39e2e-7a5f-4c23-8b18-998276332509
2023-08-17 05:36:50,323 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:50,349 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44187', status: init, memory: 0, processing: 0>
2023-08-17 05:36:50,350 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44187
2023-08-17 05:36:50,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39178
2023-08-17 05:36:50,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:36:50,351 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46683', status: init, memory: 0, processing: 0>
2023-08-17 05:36:50,351 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-17 05:36:50,351 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:50,352 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46683
2023-08-17 05:36:50,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39170
2023-08-17 05:36:50,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-17 05:36:50,353 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:36:50,353 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44039', status: init, memory: 0, processing: 0>
2023-08-17 05:36:50,353 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-17 05:36:50,354 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:50,354 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44039
2023-08-17 05:36:50,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39168
2023-08-17 05:36:50,355 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-17 05:36:50,355 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:36:50,356 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-17 05:36:50,356 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:50,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-17 05:36:50,449 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-17 05:36:50,449 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-17 05:36:50,449 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-17 05:36:50,449 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-17 05:36:50,455 - distributed.scheduler - INFO - Remove client Client-0c49ed68-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:36:50,455 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39134; closing.
2023-08-17 05:36:50,456 - distributed.scheduler - INFO - Remove client Client-0c49ed68-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:36:50,456 - distributed.scheduler - INFO - Close client connection: Client-0c49ed68-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:36:50,457 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33791'. Reason: nanny-close
2023-08-17 05:36:50,457 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:36:50,458 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37187'. Reason: nanny-close
2023-08-17 05:36:50,458 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:36:50,458 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46683. Reason: nanny-close
2023-08-17 05:36:50,458 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41921'. Reason: nanny-close
2023-08-17 05:36:50,459 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:36:50,459 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44039. Reason: nanny-close
2023-08-17 05:36:50,459 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40213'. Reason: nanny-close
2023-08-17 05:36:50,459 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:36:50,460 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44187. Reason: nanny-close
2023-08-17 05:36:50,460 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-17 05:36:50,460 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39170; closing.
2023-08-17 05:36:50,460 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45219. Reason: nanny-close
2023-08-17 05:36:50,460 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46683', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250610.460808')
2023-08-17 05:36:50,461 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-17 05:36:50,461 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-17 05:36:50,461 - distributed.nanny - INFO - Worker closed
2023-08-17 05:36:50,462 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-17 05:36:50,462 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39168; closing.
2023-08-17 05:36:50,463 - distributed.nanny - INFO - Worker closed
2023-08-17 05:36:50,463 - distributed.nanny - INFO - Worker closed
2023-08-17 05:36:50,463 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250610.4636216')
2023-08-17 05:36:50,463 - distributed.nanny - INFO - Worker closed
2023-08-17 05:36:50,464 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39178; closing.
2023-08-17 05:36:50,464 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39128; closing.
2023-08-17 05:36:50,464 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44187', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250610.464886')
2023-08-17 05:36:50,465 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45219', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250610.4655051')
2023-08-17 05:36:50,465 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:36:51,924 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:36:51,924 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:36:51,925 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:36:51,926 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-08-17 05:36:51,926 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-08-17 05:36:54,201 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:36:54,206 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34547 instead
  warnings.warn(
2023-08-17 05:36:54,212 - distributed.scheduler - INFO - State start
2023-08-17 05:36:54,234 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:36:54,235 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-17 05:36:54,235 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34547/status
2023-08-17 05:36:54,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38435'
2023-08-17 05:36:54,436 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46549'
2023-08-17 05:36:54,439 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37207'
2023-08-17 05:36:54,446 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39391'
2023-08-17 05:36:54,454 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33845'
2023-08-17 05:36:54,462 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38079'
2023-08-17 05:36:54,471 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38001'
2023-08-17 05:36:54,480 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33221'
2023-08-17 05:36:54,813 - distributed.scheduler - INFO - Receive client connection: Client-11bfb89f-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:36:54,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56664
2023-08-17 05:36:56,253 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:56,254 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:56,258 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:56,305 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:56,305 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:56,309 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:56,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:56,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:56,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:56,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:56,317 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:56,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:56,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:56,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:56,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:56,350 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:56,350 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:56,354 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:56,396 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:56,396 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:56,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:56,407 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:36:56,407 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:36:56,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:36:58,283 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45845
2023-08-17 05:36:58,284 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45845
2023-08-17 05:36:58,284 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36833
2023-08-17 05:36:58,284 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:36:58,284 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:58,284 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:36:58,284 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:36:58,284 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bfll6g4i
2023-08-17 05:36:58,285 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-36a58745-785b-498e-82d7-10bddbb27df6
2023-08-17 05:36:58,286 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5309bb23-143c-4228-9f8a-fc148f1769c9
2023-08-17 05:36:58,587 - distributed.worker - INFO - Starting Worker plugin PreImport-ef896225-8508-4a15-ac02-a43ba30bbe6f
2023-08-17 05:36:58,588 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:58,639 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45845', status: init, memory: 0, processing: 0>
2023-08-17 05:36:58,640 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45845
2023-08-17 05:36:58,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56676
2023-08-17 05:36:58,642 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:36:58,644 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:36:58,644 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:36:58,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:00,684 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45393
2023-08-17 05:37:00,686 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45393
2023-08-17 05:37:00,686 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45037
2023-08-17 05:37:00,686 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:00,686 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:00,686 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:00,686 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:00,686 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3pyd77ee
2023-08-17 05:37:00,686 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41587
2023-08-17 05:37:00,687 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41587
2023-08-17 05:37:00,687 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33419
2023-08-17 05:37:00,687 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:00,687 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:00,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa81bb4d-83ed-4635-8559-b0143fd4b486
2023-08-17 05:37:00,687 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:00,687 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:00,688 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vy3hcur_
2023-08-17 05:37:00,688 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81d53359-94ff-49b5-9b8b-0f45e8e8a273
2023-08-17 05:37:00,688 - distributed.worker - INFO - Starting Worker plugin RMMSetup-887fe797-dd98-47f5-a23c-1f1da8cf947d
2023-08-17 05:37:00,698 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34941
2023-08-17 05:37:00,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34941
2023-08-17 05:37:00,698 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40809
2023-08-17 05:37:00,699 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:00,699 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:00,699 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:00,699 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:00,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mf8z8qh3
2023-08-17 05:37:00,699 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0648cc32-3072-4767-b479-d96e5b374bf5
2023-08-17 05:37:00,700 - distributed.worker - INFO - Starting Worker plugin PreImport-c35d2c91-a8db-4bfc-9ee6-9f2ccaae014a
2023-08-17 05:37:00,700 - distributed.worker - INFO - Starting Worker plugin RMMSetup-18e1537e-6456-44a1-8e8d-73b8e08e0d33
2023-08-17 05:37:00,698 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43115
2023-08-17 05:37:00,701 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43115
2023-08-17 05:37:00,701 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40591
2023-08-17 05:37:00,701 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:00,701 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:00,701 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:00,701 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:00,701 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3e7asri6
2023-08-17 05:37:00,702 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ec31d4b-885e-4eca-ab9d-2b65efe4e781
2023-08-17 05:37:00,702 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43607
2023-08-17 05:37:00,703 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43607
2023-08-17 05:37:00,703 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34411
2023-08-17 05:37:00,703 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:00,703 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:00,703 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:00,703 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:00,703 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4o7o29gy
2023-08-17 05:37:00,703 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43895
2023-08-17 05:37:00,704 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43895
2023-08-17 05:37:00,704 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ddae672-4795-4dd4-9497-0e06f7a3ac9b
2023-08-17 05:37:00,704 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34811
2023-08-17 05:37:00,704 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:00,704 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:00,703 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35763
2023-08-17 05:37:00,704 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35763
2023-08-17 05:37:00,704 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45683
2023-08-17 05:37:00,704 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:00,704 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:00,704 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:00,704 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:00,704 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qkghxtki
2023-08-17 05:37:00,704 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:00,705 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:00,705 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g3qmtg4x
2023-08-17 05:37:00,705 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-079ed1b0-940b-4739-b62e-7ddd4c5130e8
2023-08-17 05:37:00,705 - distributed.worker - INFO - Starting Worker plugin RMMSetup-542ea112-cf37-495d-926e-be4485481dde
2023-08-17 05:37:00,705 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bba6cffe-b62b-46b3-bd21-248217b695ef
2023-08-17 05:37:00,942 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d9f1c9c-aa7f-4639-b6c0-fc200ff76bbc
2023-08-17 05:37:00,942 - distributed.worker - INFO - Starting Worker plugin PreImport-39ebf824-6123-4fb8-b61b-9c56d1cb8746
2023-08-17 05:37:00,942 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:00,961 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-996c4d17-6c7c-4138-a808-3a848b9bf51c
2023-08-17 05:37:00,962 - distributed.worker - INFO - Starting Worker plugin PreImport-1b247ecd-1bf4-4ee1-91a0-51fdec8176f1
2023-08-17 05:37:00,963 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:00,991 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41587', status: init, memory: 0, processing: 0>
2023-08-17 05:37:00,992 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41587
2023-08-17 05:37:00,992 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56694
2023-08-17 05:37:00,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:00,993 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:00,993 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:00,995 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:01,014 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35763', status: init, memory: 0, processing: 0>
2023-08-17 05:37:01,015 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35763
2023-08-17 05:37:01,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56706
2023-08-17 05:37:01,016 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:01,017 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:01,017 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:01,051 - distributed.worker - INFO - Starting Worker plugin PreImport-1a370f10-9666-40d8-8039-dc68819a5ac7
2023-08-17 05:37:01,051 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,055 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,055 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06bdefa0-ff42-46ce-8910-e2372e7dd222
2023-08-17 05:37:01,058 - distributed.worker - INFO - Starting Worker plugin PreImport-1a5c7b5c-f0a0-48b6-89ab-52055afb2d5a
2023-08-17 05:37:01,058 - distributed.worker - INFO - Starting Worker plugin PreImport-8b9f22c8-50eb-4a79-910a-8640be7136f5
2023-08-17 05:37:01,059 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c30d4dc-4451-42ea-b0c9-41c86a9dbfba
2023-08-17 05:37:01,059 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,059 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,059 - distributed.worker - INFO - Starting Worker plugin PreImport-7e443f5a-8c42-4271-a66d-173992c291c4
2023-08-17 05:37:01,060 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,091 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45393', status: init, memory: 0, processing: 0>
2023-08-17 05:37:01,092 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45393
2023-08-17 05:37:01,092 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56718
2023-08-17 05:37:01,093 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:01,093 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:01,093 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:01,097 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43115', status: init, memory: 0, processing: 0>
2023-08-17 05:37:01,098 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43115
2023-08-17 05:37:01,098 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56736
2023-08-17 05:37:01,099 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:01,099 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34941', status: init, memory: 0, processing: 0>
2023-08-17 05:37:01,099 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:01,100 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,100 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34941
2023-08-17 05:37:01,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56720
2023-08-17 05:37:01,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:01,101 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:01,102 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43895', status: init, memory: 0, processing: 0>
2023-08-17 05:37:01,102 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:01,102 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43895
2023-08-17 05:37:01,102 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56748
2023-08-17 05:37:01,104 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:01,104 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:01,104 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:01,105 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,105 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43607', status: init, memory: 0, processing: 0>
2023-08-17 05:37:01,106 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43607
2023-08-17 05:37:01,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56754
2023-08-17 05:37:01,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:01,107 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:01,108 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:01,108 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:01,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:01,211 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:01,211 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:01,211 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:01,211 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:01,211 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:01,211 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:01,211 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:01,212 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:01,216 - distributed.scheduler - INFO - Remove client Client-11bfb89f-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:01,216 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56664; closing.
2023-08-17 05:37:01,217 - distributed.scheduler - INFO - Remove client Client-11bfb89f-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:01,217 - distributed.scheduler - INFO - Close client connection: Client-11bfb89f-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:01,218 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39391'. Reason: nanny-close
2023-08-17 05:37:01,219 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:01,219 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38435'. Reason: nanny-close
2023-08-17 05:37:01,219 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:01,220 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46549'. Reason: nanny-close
2023-08-17 05:37:01,220 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34941. Reason: nanny-close
2023-08-17 05:37:01,220 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:01,220 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37207'. Reason: nanny-close
2023-08-17 05:37:01,220 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35763. Reason: nanny-close
2023-08-17 05:37:01,221 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:01,221 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33845'. Reason: nanny-close
2023-08-17 05:37:01,221 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45845. Reason: nanny-close
2023-08-17 05:37:01,221 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:01,221 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38079'. Reason: nanny-close
2023-08-17 05:37:01,221 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43115. Reason: nanny-close
2023-08-17 05:37:01,222 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:01,222 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56720; closing.
2023-08-17 05:37:01,222 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:01,222 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43607. Reason: nanny-close
2023-08-17 05:37:01,222 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38001'. Reason: nanny-close
2023-08-17 05:37:01,222 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34941', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250621.2225351')
2023-08-17 05:37:01,222 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:01,222 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:01,222 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33221'. Reason: nanny-close
2023-08-17 05:37:01,223 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43895. Reason: nanny-close
2023-08-17 05:37:01,223 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:01,223 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:01,223 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45393. Reason: nanny-close
2023-08-17 05:37:01,223 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:01,224 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:01,224 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56676; closing.
2023-08-17 05:37:01,224 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:01,224 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56706; closing.
2023-08-17 05:37:01,224 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:01,224 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41587. Reason: nanny-close
2023-08-17 05:37:01,225 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:01,225 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:01,225 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45845', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250621.2252834')
2023-08-17 05:37:01,225 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:01,225 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:01,225 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35763', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250621.225671')
2023-08-17 05:37:01,226 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56736; closing.
2023-08-17 05:37:01,226 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:01,226 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:01,226 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250621.226759')
2023-08-17 05:37:01,227 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56754; closing.
2023-08-17 05:37:01,227 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:01,227 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:01,228 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43607', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250621.2280183')
2023-08-17 05:37:01,228 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:01,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56748; closing.
2023-08-17 05:37:01,228 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56718; closing.
2023-08-17 05:37:01,229 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43895', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250621.2292693')
2023-08-17 05:37:01,229 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45393', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250621.229689')
2023-08-17 05:37:01,230 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56694; closing.
2023-08-17 05:37:01,230 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250621.2306592')
2023-08-17 05:37:01,230 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:37:04,138 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:37:04,139 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:37:04,139 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:37:04,140 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-17 05:37:04,141 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-08-17 05:37:06,374 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:06,379 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40035 instead
  warnings.warn(
2023-08-17 05:37:06,383 - distributed.scheduler - INFO - State start
2023-08-17 05:37:06,405 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:06,406 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-17 05:37:06,406 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40035/status
2023-08-17 05:37:06,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33085'
2023-08-17 05:37:06,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39139'
2023-08-17 05:37:06,736 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37701'
2023-08-17 05:37:06,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34017'
2023-08-17 05:37:06,755 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39175'
2023-08-17 05:37:06,765 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46501'
2023-08-17 05:37:06,778 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46777'
2023-08-17 05:37:06,793 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40419'
2023-08-17 05:37:08,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:08,539 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:08,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:08,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:08,543 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:08,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:08,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:08,543 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:08,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:08,593 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:08,593 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:08,595 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:08,595 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:08,595 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:08,595 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:08,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:08,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:08,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:08,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:08,597 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:08,599 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:08,599 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:08,601 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:08,601 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:11,255 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33435
2023-08-17 05:37:11,255 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33435
2023-08-17 05:37:11,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37249
2023-08-17 05:37:11,256 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,256 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,256 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:11,256 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:11,256 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9w7tmv93
2023-08-17 05:37:11,256 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-666accf3-898e-416d-b164-dad49bc53107
2023-08-17 05:37:11,257 - distributed.worker - INFO - Starting Worker plugin PreImport-a5754b98-a309-4b6d-a4ba-6a492b19d9a0
2023-08-17 05:37:11,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee7d860a-5e74-4692-88db-057bc11e1f88
2023-08-17 05:37:11,260 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45415
2023-08-17 05:37:11,260 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45415
2023-08-17 05:37:11,260 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41743
2023-08-17 05:37:11,261 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,261 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,261 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:11,261 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:11,261 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iktujlsp
2023-08-17 05:37:11,261 - distributed.worker - INFO - Starting Worker plugin RMMSetup-23c4ec24-8643-44e1-8f3a-d1a4f1ecd381
2023-08-17 05:37:11,262 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41777
2023-08-17 05:37:11,263 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41777
2023-08-17 05:37:11,263 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32883
2023-08-17 05:37:11,264 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,264 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,264 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:11,264 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:11,264 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2gjyqqtn
2023-08-17 05:37:11,265 - distributed.worker - INFO - Starting Worker plugin PreImport-a03eb3ab-a217-43b3-9d3c-3d00eca5e6ca
2023-08-17 05:37:11,265 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5b58382e-70dc-468d-827e-6e00a1c1b921
2023-08-17 05:37:11,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09fdcf9d-d133-46d7-80cf-33c6647b571d
2023-08-17 05:37:11,270 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,272 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e42427ff-6c72-49a9-bb18-15e9f42515d3
2023-08-17 05:37:11,272 - distributed.worker - INFO - Starting Worker plugin PreImport-b6db171d-e139-472c-9797-bceea6eedc6a
2023-08-17 05:37:11,273 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,274 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,301 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45415', status: init, memory: 0, processing: 0>
2023-08-17 05:37:11,316 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45415
2023-08-17 05:37:11,316 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54408
2023-08-17 05:37:11,317 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:11,318 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,318 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,318 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33435', status: init, memory: 0, processing: 0>
2023-08-17 05:37:11,319 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33435
2023-08-17 05:37:11,319 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54406
2023-08-17 05:37:11,320 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41777', status: init, memory: 0, processing: 0>
2023-08-17 05:37:11,320 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:11,320 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41777
2023-08-17 05:37:11,320 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54420
2023-08-17 05:37:11,320 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:11,321 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:11,321 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,322 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,322 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,322 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,323 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:11,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:11,431 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43535
2023-08-17 05:37:11,432 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43535
2023-08-17 05:37:11,432 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35143
2023-08-17 05:37:11,432 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,432 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,432 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:11,432 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:11,432 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_aw28m77
2023-08-17 05:37:11,432 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7716c0bc-88e3-4539-b23a-cfb135cb318e
2023-08-17 05:37:11,433 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0484a077-5b4a-40d5-8beb-7429097a883a
2023-08-17 05:37:11,442 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39825
2023-08-17 05:37:11,442 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39825
2023-08-17 05:37:11,442 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38321
2023-08-17 05:37:11,443 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,443 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,443 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:11,443 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:11,443 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p0m0is06
2023-08-17 05:37:11,443 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d99d399-8c98-45ad-b985-817a89152ecc
2023-08-17 05:37:11,448 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46485
2023-08-17 05:37:11,448 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46485
2023-08-17 05:37:11,448 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38815
2023-08-17 05:37:11,448 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,448 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,449 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:11,449 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:11,449 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uu3q80y_
2023-08-17 05:37:11,449 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a5c96d7-b3a3-4f30-aa26-1867085875bb
2023-08-17 05:37:11,450 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef997a41-6cfb-4544-90a0-1c680e45f639
2023-08-17 05:37:11,451 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38031
2023-08-17 05:37:11,451 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38031
2023-08-17 05:37:11,452 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46487
2023-08-17 05:37:11,452 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,452 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,452 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:11,452 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:11,452 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kmn00lq9
2023-08-17 05:37:11,452 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b73798f0-447e-47b6-970e-b2ec093c80ac
2023-08-17 05:37:11,453 - distributed.worker - INFO - Starting Worker plugin RMMSetup-caa1aef5-3cef-4816-bb1d-f21828e23891
2023-08-17 05:37:11,463 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34985
2023-08-17 05:37:11,463 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34985
2023-08-17 05:37:11,464 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35999
2023-08-17 05:37:11,464 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,464 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,464 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:11,464 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:11,464 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_jrmuf62
2023-08-17 05:37:11,464 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ac8757e-6b6f-4dfb-8a95-172e19ff3a7a
2023-08-17 05:37:11,470 - distributed.worker - INFO - Starting Worker plugin PreImport-61d5c92b-c902-4c53-acc7-9bc3cdd9595d
2023-08-17 05:37:11,470 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,472 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ccad1910-4a33-4244-9cbf-49285b61fd8a
2023-08-17 05:37:11,472 - distributed.worker - INFO - Starting Worker plugin PreImport-bbb8eb0a-32f1-4cd0-bd8f-04b6a37107a6
2023-08-17 05:37:11,472 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,473 - distributed.worker - INFO - Starting Worker plugin PreImport-5f2387ed-c6c4-4305-83fa-789cb4fb2276
2023-08-17 05:37:11,474 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,477 - distributed.worker - INFO - Starting Worker plugin PreImport-368dcf04-1408-4343-af78-a4e0806ad750
2023-08-17 05:37:11,477 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,477 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90c7b869-d71e-4104-a9dd-a9422aadb7ef
2023-08-17 05:37:11,478 - distributed.worker - INFO - Starting Worker plugin PreImport-8d2c9390-5432-4228-81d7-362dab1d3f07
2023-08-17 05:37:11,478 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,494 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43535', status: init, memory: 0, processing: 0>
2023-08-17 05:37:11,494 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43535
2023-08-17 05:37:11,494 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54428
2023-08-17 05:37:11,495 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:11,495 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39825', status: init, memory: 0, processing: 0>
2023-08-17 05:37:11,496 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39825
2023-08-17 05:37:11,496 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,496 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54438
2023-08-17 05:37:11,496 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,497 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:11,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:11,497 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,497 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,498 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:11,512 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34985', status: init, memory: 0, processing: 0>
2023-08-17 05:37:11,513 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34985
2023-08-17 05:37:11,513 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54450
2023-08-17 05:37:11,514 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:11,515 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46485', status: init, memory: 0, processing: 0>
2023-08-17 05:37:11,515 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46485
2023-08-17 05:37:11,515 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,515 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54446
2023-08-17 05:37:11,516 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,517 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:11,517 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38031', status: init, memory: 0, processing: 0>
2023-08-17 05:37:11,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:11,518 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,518 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,518 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38031
2023-08-17 05:37:11,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54440
2023-08-17 05:37:11,519 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:11,520 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:11,520 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:11,520 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:11,522 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:11,968 - distributed.scheduler - INFO - Receive client connection: Client-18f551f6-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:11,968 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54452
2023-08-17 05:37:11,980 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:11,981 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:11,981 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:11,981 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:11,981 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:11,981 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:11,981 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:11,982 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:11,985 - distributed.scheduler - INFO - Remove client Client-18f551f6-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:11,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54452; closing.
2023-08-17 05:37:11,986 - distributed.scheduler - INFO - Remove client Client-18f551f6-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:11,986 - distributed.scheduler - INFO - Close client connection: Client-18f551f6-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:11,987 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39139'. Reason: nanny-close
2023-08-17 05:37:11,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:11,988 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39175'. Reason: nanny-close
2023-08-17 05:37:11,988 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:11,989 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33435. Reason: nanny-close
2023-08-17 05:37:11,989 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33085'. Reason: nanny-close
2023-08-17 05:37:11,989 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:11,989 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38031. Reason: nanny-close
2023-08-17 05:37:11,989 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37701'. Reason: nanny-close
2023-08-17 05:37:11,990 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:11,990 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45415. Reason: nanny-close
2023-08-17 05:37:11,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34017'. Reason: nanny-close
2023-08-17 05:37:11,990 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:11,991 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43535. Reason: nanny-close
2023-08-17 05:37:11,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46501'. Reason: nanny-close
2023-08-17 05:37:11,991 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:11,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:11,991 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54406; closing.
2023-08-17 05:37:11,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46777'. Reason: nanny-close
2023-08-17 05:37:11,991 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46485. Reason: nanny-close
2023-08-17 05:37:11,992 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:11,992 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33435', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250631.9922142')
2023-08-17 05:37:11,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:11,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:11,992 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40419'. Reason: nanny-close
2023-08-17 05:37:11,992 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34985. Reason: nanny-close
2023-08-17 05:37:11,992 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:11,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:11,992 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54408; closing.
2023-08-17 05:37:11,992 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39825. Reason: nanny-close
2023-08-17 05:37:11,993 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45415', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250631.993406')
2023-08-17 05:37:11,993 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:11,993 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54440; closing.
2023-08-17 05:37:11,993 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:11,994 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41777. Reason: nanny-close
2023-08-17 05:37:11,994 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:11,994 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:11,994 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:11,994 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:11,994 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:11,994 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54428; closing.
2023-08-17 05:37:11,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38031', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250631.9949954')
2023-08-17 05:37:11,996 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:11,996 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:11,996 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:11,996 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:11,995 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54408>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-17 05:37:11,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43535', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250631.997644')
2023-08-17 05:37:11,998 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54446; closing.
2023-08-17 05:37:11,998 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:11,998 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46485', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250631.9987788')
2023-08-17 05:37:11,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54450; closing.
2023-08-17 05:37:11,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54438; closing.
2023-08-17 05:37:11,999 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34985', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250631.9998155')
2023-08-17 05:37:12,000 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39825', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250632.000271')
2023-08-17 05:37:12,000 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54420; closing.
2023-08-17 05:37:12,001 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250632.001151')
2023-08-17 05:37:12,001 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:37:12,001 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54420>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-17 05:37:13,505 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:37:13,505 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:37:13,506 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:37:13,507 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-17 05:37:13,508 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-08-17 05:37:15,576 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:15,581 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33717 instead
  warnings.warn(
2023-08-17 05:37:15,585 - distributed.scheduler - INFO - State start
2023-08-17 05:37:15,606 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:15,607 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-17 05:37:15,608 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33717/status
2023-08-17 05:37:15,781 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37481'
2023-08-17 05:37:15,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39271'
2023-08-17 05:37:15,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45461'
2023-08-17 05:37:15,812 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44215'
2023-08-17 05:37:15,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34517'
2023-08-17 05:37:15,828 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34181'
2023-08-17 05:37:15,836 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34305'
2023-08-17 05:37:15,846 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34113'
2023-08-17 05:37:16,534 - distributed.scheduler - INFO - Receive client connection: Client-1e86f2b8-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:16,546 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52928
2023-08-17 05:37:17,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:17,538 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:17,542 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:17,609 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:17,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:17,613 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:17,620 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:17,621 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:17,625 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:17,625 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:17,625 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:17,629 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:17,662 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:17,662 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:17,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:17,663 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:17,666 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:17,667 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:17,674 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:17,674 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:17,676 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:17,676 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:17,678 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:17,680 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:19,442 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45853
2023-08-17 05:37:19,443 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45853
2023-08-17 05:37:19,443 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34033
2023-08-17 05:37:19,443 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:19,443 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:19,443 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:19,443 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:19,443 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2z2yhd_q
2023-08-17 05:37:19,443 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dce1ca1e-9b47-45e2-891a-dfc2b0168c20
2023-08-17 05:37:19,444 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e41fd598-92db-4155-9da8-7a5b95652904
2023-08-17 05:37:19,806 - distributed.worker - INFO - Starting Worker plugin PreImport-d75f3886-de58-414e-8dd3-8dadc351bd69
2023-08-17 05:37:19,807 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:19,842 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45853', status: init, memory: 0, processing: 0>
2023-08-17 05:37:19,844 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45853
2023-08-17 05:37:19,844 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52954
2023-08-17 05:37:19,845 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:19,846 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:19,846 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:19,847 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:20,524 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33077
2023-08-17 05:37:20,525 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33077
2023-08-17 05:37:20,525 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46369
2023-08-17 05:37:20,525 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,525 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,525 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:20,525 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:20,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cm35zs0d
2023-08-17 05:37:20,526 - distributed.worker - INFO - Starting Worker plugin PreImport-cf42e298-64a4-461b-a092-87160b2a86f7
2023-08-17 05:37:20,526 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c627abf-d0e7-4cf8-bea0-4590cc385811
2023-08-17 05:37:20,526 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36639
2023-08-17 05:37:20,527 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36639
2023-08-17 05:37:20,527 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36453
2023-08-17 05:37:20,527 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,527 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,527 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:20,527 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:20,527 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1r6mu_38
2023-08-17 05:37:20,527 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6bc2b670-506e-4ef7-adbf-9f83af41fffc
2023-08-17 05:37:20,526 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36477
2023-08-17 05:37:20,528 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36477
2023-08-17 05:37:20,527 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33643
2023-08-17 05:37:20,528 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40787
2023-08-17 05:37:20,528 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33643
2023-08-17 05:37:20,528 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,528 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,528 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38921
2023-08-17 05:37:20,528 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,528 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,528 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:20,529 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:20,529 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:20,529 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1w_ozoxg
2023-08-17 05:37:20,529 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:20,529 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sa1s0gtd
2023-08-17 05:37:20,529 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5edf7aa2-bc6c-4492-983e-a9d4fbdab427
2023-08-17 05:37:20,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-890814ab-0c85-45e6-8f20-acc7423167cb
2023-08-17 05:37:20,530 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48943845-6d41-4546-9548-62eb7a884d4a
2023-08-17 05:37:20,678 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35373
2023-08-17 05:37:20,679 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35373
2023-08-17 05:37:20,679 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36351
2023-08-17 05:37:20,679 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,679 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,679 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:20,680 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:20,680 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-valjq68a
2023-08-17 05:37:20,680 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1742fa50-6e48-400a-bb68-d205b026a425
2023-08-17 05:37:20,691 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37803
2023-08-17 05:37:20,692 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37803
2023-08-17 05:37:20,691 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37983
2023-08-17 05:37:20,692 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42197
2023-08-17 05:37:20,692 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37983
2023-08-17 05:37:20,692 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,692 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38825
2023-08-17 05:37:20,692 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,692 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,692 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,692 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:20,692 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:20,692 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:20,692 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nl2gnzsj
2023-08-17 05:37:20,692 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:20,692 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kfr1_2is
2023-08-17 05:37:20,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-909243eb-bcfa-4227-abf6-ca6a13d031af
2023-08-17 05:37:20,693 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7110910a-7ec7-4a39-8e30-181a22aad909
2023-08-17 05:37:20,693 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65750337-b6a4-4138-9935-27fabc9f612e
2023-08-17 05:37:20,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb211eb4-e52b-4675-a572-05314485d992
2023-08-17 05:37:20,746 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,747 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef1f020e-99fd-4bae-9c2f-eeaabacaad42
2023-08-17 05:37:20,748 - distributed.worker - INFO - Starting Worker plugin PreImport-a9c3aa53-7f38-4d43-80d0-206863374cf4
2023-08-17 05:37:20,748 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,768 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20dd02f5-fbe3-48f5-ab52-7a200ec0ea38
2023-08-17 05:37:20,768 - distributed.worker - INFO - Starting Worker plugin PreImport-88b6ee30-53f2-4d4d-b38e-9be9e29aa8c2
2023-08-17 05:37:20,768 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,769 - distributed.worker - INFO - Starting Worker plugin PreImport-b99cbf2a-46e2-473c-9a81-e34b01ccd9ac
2023-08-17 05:37:20,769 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,793 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33077', status: init, memory: 0, processing: 0>
2023-08-17 05:37:20,794 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33077
2023-08-17 05:37:20,794 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52956
2023-08-17 05:37:20,794 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33643', status: init, memory: 0, processing: 0>
2023-08-17 05:37:20,795 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33643
2023-08-17 05:37:20,795 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52966
2023-08-17 05:37:20,795 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:20,796 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,796 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,797 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:20,798 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,798 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:20,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:20,807 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36477', status: init, memory: 0, processing: 0>
2023-08-17 05:37:20,808 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36477
2023-08-17 05:37:20,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52974
2023-08-17 05:37:20,809 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:20,810 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,810 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:20,813 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36639', status: init, memory: 0, processing: 0>
2023-08-17 05:37:20,813 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36639
2023-08-17 05:37:20,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52990
2023-08-17 05:37:20,815 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:20,816 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,816 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,818 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:20,860 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01ffafeb-2295-45fb-b8dd-cc401a568e9d
2023-08-17 05:37:20,860 - distributed.worker - INFO - Starting Worker plugin PreImport-1a11d8c4-27a3-4d2f-bb26-dfa95a2be29c
2023-08-17 05:37:20,860 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,865 - distributed.worker - INFO - Starting Worker plugin PreImport-228f9637-4d30-4cbd-803b-bd816c2473db
2023-08-17 05:37:20,865 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,865 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2eea554e-e378-4441-8d3f-6c25054a69ee
2023-08-17 05:37:20,865 - distributed.worker - INFO - Starting Worker plugin PreImport-b6d6a690-166c-45c3-b51e-0bb696946793
2023-08-17 05:37:20,866 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,890 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35373', status: init, memory: 0, processing: 0>
2023-08-17 05:37:20,891 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35373
2023-08-17 05:37:20,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53006
2023-08-17 05:37:20,892 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:20,893 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,893 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:20,895 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37983', status: init, memory: 0, processing: 0>
2023-08-17 05:37:20,896 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37983
2023-08-17 05:37:20,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53012
2023-08-17 05:37:20,897 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:20,898 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,898 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,898 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37803', status: init, memory: 0, processing: 0>
2023-08-17 05:37:20,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37803
2023-08-17 05:37:20,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53024
2023-08-17 05:37:20,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:20,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:20,901 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:20,901 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:20,903 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:20,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:20,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:20,961 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:20,962 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:20,962 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:20,962 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:20,962 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:20,962 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:20,973 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-17 05:37:20,973 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-17 05:37:20,973 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-17 05:37:20,973 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-17 05:37:20,973 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-17 05:37:20,973 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-17 05:37:20,973 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-17 05:37:20,973 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-17 05:37:20,979 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:37:20,981 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:37:20,984 - distributed.scheduler - INFO - Remove client Client-1e86f2b8-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:20,984 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52928; closing.
2023-08-17 05:37:20,984 - distributed.scheduler - INFO - Remove client Client-1e86f2b8-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:20,984 - distributed.scheduler - INFO - Close client connection: Client-1e86f2b8-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:20,985 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45461'. Reason: nanny-close
2023-08-17 05:37:20,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:20,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37481'. Reason: nanny-close
2023-08-17 05:37:20,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:20,987 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39271'. Reason: nanny-close
2023-08-17 05:37:20,987 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33077. Reason: nanny-close
2023-08-17 05:37:20,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:20,988 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44215'. Reason: nanny-close
2023-08-17 05:37:20,988 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33643. Reason: nanny-close
2023-08-17 05:37:20,988 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:20,988 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34517'. Reason: nanny-close
2023-08-17 05:37:20,988 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36477. Reason: nanny-close
2023-08-17 05:37:20,989 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:20,989 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35373. Reason: nanny-close
2023-08-17 05:37:20,989 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34181'. Reason: nanny-close
2023-08-17 05:37:20,989 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:20,990 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:20,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34305'. Reason: nanny-close
2023-08-17 05:37:20,990 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36639. Reason: nanny-close
2023-08-17 05:37:20,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52956; closing.
2023-08-17 05:37:20,990 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:20,990 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250640.9906535')
2023-08-17 05:37:20,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34113'. Reason: nanny-close
2023-08-17 05:37:20,990 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37803. Reason: nanny-close
2023-08-17 05:37:20,990 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:20,990 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:20,991 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:20,991 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52974; closing.
2023-08-17 05:37:20,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:20,991 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45853. Reason: nanny-close
2023-08-17 05:37:20,991 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250640.991815')
2023-08-17 05:37:20,992 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:20,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:20,992 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52966; closing.
2023-08-17 05:37:20,992 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37983. Reason: nanny-close
2023-08-17 05:37:20,992 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:20,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:20,992 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:20,992 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:20,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:20,993 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53006; closing.
2023-08-17 05:37:20,993 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33643', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250640.9934082')
2023-08-17 05:37:20,994 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:20,994 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:20,994 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:20,994 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:20,994 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52974>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-17 05:37:20,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35373', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250640.9957113')
2023-08-17 05:37:20,996 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:20,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52990; closing.
2023-08-17 05:37:20,996 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36639', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250640.996906')
2023-08-17 05:37:20,997 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53024; closing.
2023-08-17 05:37:20,997 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52954; closing.
2023-08-17 05:37:20,998 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37803', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250640.99816')
2023-08-17 05:37:20,998 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45853', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250640.998569')
2023-08-17 05:37:20,998 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53012; closing.
2023-08-17 05:37:20,999 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37983', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250640.9994853')
2023-08-17 05:37:20,999 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:37:20,999 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53012>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-17 05:37:22,553 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:37:22,553 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:37:22,554 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:37:22,555 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-17 05:37:22,555 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-08-17 05:37:24,682 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:24,686 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42561 instead
  warnings.warn(
2023-08-17 05:37:24,689 - distributed.scheduler - INFO - State start
2023-08-17 05:37:25,423 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:25,424 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-17 05:37:25,426 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42561/status
2023-08-17 05:37:25,685 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38061'
2023-08-17 05:37:25,708 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40509'
2023-08-17 05:37:25,710 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40695'
2023-08-17 05:37:25,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38969'
2023-08-17 05:37:25,726 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37265'
2023-08-17 05:37:25,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43703'
2023-08-17 05:37:25,742 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39559'
2023-08-17 05:37:25,751 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33191'
2023-08-17 05:37:25,801 - distributed.scheduler - INFO - Receive client connection: Client-23f5e179-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:25,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37222
2023-08-17 05:37:27,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:27,531 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:27,536 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:27,684 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:27,684 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:27,688 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:27,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:27,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:27,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:27,694 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:27,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:27,699 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:27,710 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:27,710 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:27,714 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:27,790 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:27,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:27,795 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:27,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:27,795 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:27,800 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:27,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:27,814 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:27,819 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:30,698 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32969
2023-08-17 05:37:30,699 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32969
2023-08-17 05:37:30,699 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40109
2023-08-17 05:37:30,699 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,699 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,699 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:30,699 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:30,700 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q2ykfghm
2023-08-17 05:37:30,700 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7b05fb4d-89d1-4390-b0a7-7a45a81534c8
2023-08-17 05:37:30,701 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2325e9e-502e-41d0-8ca1-adfbc6f0d169
2023-08-17 05:37:30,705 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35247
2023-08-17 05:37:30,706 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35247
2023-08-17 05:37:30,706 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45995
2023-08-17 05:37:30,706 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,706 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,706 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:30,706 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:30,706 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v2z6i0sz
2023-08-17 05:37:30,707 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af0cd46d-7d98-4ea3-93d9-10622be9932e
2023-08-17 05:37:30,706 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39457
2023-08-17 05:37:30,707 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39457
2023-08-17 05:37:30,707 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33163
2023-08-17 05:37:30,707 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,707 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,707 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:30,708 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:30,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bhenhb13
2023-08-17 05:37:30,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-afaa57fb-691d-449d-84cf-0401f870da47
2023-08-17 05:37:30,708 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c7061ee6-5575-4c26-95ec-6c7b7b7803ac
2023-08-17 05:37:30,708 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42057
2023-08-17 05:37:30,709 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42057
2023-08-17 05:37:30,709 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36083
2023-08-17 05:37:30,709 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,709 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,709 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:30,709 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:30,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-djt4fw05
2023-08-17 05:37:30,710 - distributed.worker - INFO - Starting Worker plugin PreImport-fa0deb01-ba0f-41aa-94e8-203bb4a303b7
2023-08-17 05:37:30,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65c66566-1a5d-4efc-ba37-1096e7119ff9
2023-08-17 05:37:30,866 - distributed.worker - INFO - Starting Worker plugin PreImport-1e2a52e0-5808-4227-8e9b-2105f2ba505d
2023-08-17 05:37:30,866 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,879 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7821dbb1-5ef1-4d81-98f3-2101f2367b06
2023-08-17 05:37:30,880 - distributed.worker - INFO - Starting Worker plugin PreImport-9003a93e-bff7-44cb-adb1-df2fd711c745
2023-08-17 05:37:30,880 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,881 - distributed.worker - INFO - Starting Worker plugin PreImport-18ef6b49-9cb7-41f6-80e9-b9e1ee45cfb9
2023-08-17 05:37:30,881 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,888 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37885
2023-08-17 05:37:30,888 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37885
2023-08-17 05:37:30,888 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42899
2023-08-17 05:37:30,889 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,889 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,889 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:30,889 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:30,889 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o5rz7sn2
2023-08-17 05:37:30,889 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-15cd8ea8-d0ed-416b-a873-6bb949c702b6
2023-08-17 05:37:30,890 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d26963f7-4909-4fef-ac34-a539164613c2
2023-08-17 05:37:30,889 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3dc5e179-8396-4a23-a893-cdf6ea34ab86
2023-08-17 05:37:30,890 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,895 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43333
2023-08-17 05:37:30,896 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43333
2023-08-17 05:37:30,896 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34377
2023-08-17 05:37:30,896 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,896 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,896 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:30,896 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:30,896 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u72o0qs5
2023-08-17 05:37:30,896 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5f3946dc-35df-4bbb-9419-2fc87e081848
2023-08-17 05:37:30,904 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38789
2023-08-17 05:37:30,905 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38789
2023-08-17 05:37:30,905 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35157
2023-08-17 05:37:30,905 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,905 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,905 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:30,905 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:30,905 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zigf6pn7
2023-08-17 05:37:30,905 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42321
2023-08-17 05:37:30,905 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42321
2023-08-17 05:37:30,905 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45889
2023-08-17 05:37:30,906 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,906 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02808b4f-9ac8-4414-a74b-3a75eb909202
2023-08-17 05:37:30,906 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,906 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:30,906 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:37:30,906 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q7hm3a_u
2023-08-17 05:37:30,906 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39457', status: init, memory: 0, processing: 0>
2023-08-17 05:37:30,906 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12807567-4191-43b4-8351-7a64c5cb1b1f
2023-08-17 05:37:30,907 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39457
2023-08-17 05:37:30,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37232
2023-08-17 05:37:30,908 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:30,909 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,909 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35247', status: init, memory: 0, processing: 0>
2023-08-17 05:37:30,910 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35247
2023-08-17 05:37:30,910 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37242
2023-08-17 05:37:30,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:30,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:30,912 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,912 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:30,915 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32969', status: init, memory: 0, processing: 0>
2023-08-17 05:37:30,916 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32969
2023-08-17 05:37:30,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37250
2023-08-17 05:37:30,917 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:30,918 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,918 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:30,923 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42057', status: init, memory: 0, processing: 0>
2023-08-17 05:37:30,924 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42057
2023-08-17 05:37:30,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37252
2023-08-17 05:37:30,925 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:30,925 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:30,926 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:30,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:31,030 - distributed.worker - INFO - Starting Worker plugin PreImport-448dd91f-0081-4942-b7c7-2fce470e645e
2023-08-17 05:37:31,031 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:31,033 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ade50a57-549e-4142-88db-554c689fcc9f
2023-08-17 05:37:31,033 - distributed.worker - INFO - Starting Worker plugin PreImport-04ff61ea-dbee-4036-8876-13b7dca7ed3a
2023-08-17 05:37:31,033 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:31,036 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0967f3fd-28b4-48ac-b7ea-a647091f230b
2023-08-17 05:37:31,036 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-10d36cb6-ef35-40d8-aac2-dbe75e0b32e0
2023-08-17 05:37:31,036 - distributed.worker - INFO - Starting Worker plugin PreImport-4b1c7b06-2fb7-4a0e-9a7e-fd256b285544
2023-08-17 05:37:31,036 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:31,038 - distributed.worker - INFO - Starting Worker plugin PreImport-300bb858-6eb9-41c6-84f0-00d9456eb73c
2023-08-17 05:37:31,039 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:31,057 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37885', status: init, memory: 0, processing: 0>
2023-08-17 05:37:31,058 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37885
2023-08-17 05:37:31,058 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37254
2023-08-17 05:37:31,059 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:31,059 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:31,059 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:31,060 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43333', status: init, memory: 0, processing: 0>
2023-08-17 05:37:31,060 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43333
2023-08-17 05:37:31,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37270
2023-08-17 05:37:31,061 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:31,061 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:31,062 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:31,062 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:31,063 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:31,076 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42321', status: init, memory: 0, processing: 0>
2023-08-17 05:37:31,077 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42321
2023-08-17 05:37:31,077 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37276
2023-08-17 05:37:31,078 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:31,080 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:31,080 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:31,080 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38789', status: init, memory: 0, processing: 0>
2023-08-17 05:37:31,081 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38789
2023-08-17 05:37:31,081 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37290
2023-08-17 05:37:31,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:31,082 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:31,083 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:31,084 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:31,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:31,097 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:31,097 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:31,097 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:31,097 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:31,097 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:31,098 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:31,098 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:31,098 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:37:31,102 - distributed.scheduler - INFO - Remove client Client-23f5e179-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:31,102 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37222; closing.
2023-08-17 05:37:31,103 - distributed.scheduler - INFO - Remove client Client-23f5e179-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:31,103 - distributed.scheduler - INFO - Close client connection: Client-23f5e179-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:31,104 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38969'. Reason: nanny-close
2023-08-17 05:37:31,105 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:31,105 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38061'. Reason: nanny-close
2023-08-17 05:37:31,106 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:31,106 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42057. Reason: nanny-close
2023-08-17 05:37:31,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40509'. Reason: nanny-close
2023-08-17 05:37:31,107 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:31,107 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42321. Reason: nanny-close
2023-08-17 05:37:31,107 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40695'. Reason: nanny-close
2023-08-17 05:37:31,107 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:31,108 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37265'. Reason: nanny-close
2023-08-17 05:37:31,108 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32969. Reason: nanny-close
2023-08-17 05:37:31,108 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:31,108 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43333. Reason: nanny-close
2023-08-17 05:37:31,108 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43703'. Reason: nanny-close
2023-08-17 05:37:31,108 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:31,108 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37252; closing.
2023-08-17 05:37:31,108 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:31,109 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42057', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250651.1090398')
2023-08-17 05:37:31,109 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39559'. Reason: nanny-close
2023-08-17 05:37:31,109 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:31,109 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38789. Reason: nanny-close
2023-08-17 05:37:31,109 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:31,109 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33191'. Reason: nanny-close
2023-08-17 05:37:31,109 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39457. Reason: nanny-close
2023-08-17 05:37:31,109 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:31,110 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:31,110 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:31,110 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37885. Reason: nanny-close
2023-08-17 05:37:31,110 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:31,110 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37250; closing.
2023-08-17 05:37:31,111 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37276; closing.
2023-08-17 05:37:31,111 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35247. Reason: nanny-close
2023-08-17 05:37:31,111 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:31,112 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250651.1119223')
2023-08-17 05:37:31,112 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:31,112 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:31,112 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:31,112 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:31,112 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42321', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250651.112336')
2023-08-17 05:37:31,112 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:31,112 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37270; closing.
2023-08-17 05:37:31,112 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:31,113 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43333', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250651.1132026')
2023-08-17 05:37:31,113 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:31,114 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37290; closing.
2023-08-17 05:37:31,114 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37232; closing.
2023-08-17 05:37:31,114 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:31,114 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:31,114 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37254; closing.
2023-08-17 05:37:31,115 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38789', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250651.1149764')
2023-08-17 05:37:31,115 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:31,115 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39457', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250651.1153913')
2023-08-17 05:37:31,115 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37885', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250651.1157935')
2023-08-17 05:37:31,116 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37242; closing.
2023-08-17 05:37:31,116 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35247', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250651.1166134')
2023-08-17 05:37:31,116 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:37:32,622 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:37:32,622 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:37:32,623 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:37:32,624 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-17 05:37:32,625 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-08-17 05:37:34,728 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:34,732 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35623 instead
  warnings.warn(
2023-08-17 05:37:34,736 - distributed.scheduler - INFO - State start
2023-08-17 05:37:34,757 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:34,758 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-17 05:37:34,759 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35623/status
2023-08-17 05:37:34,862 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34649'
2023-08-17 05:37:35,441 - distributed.scheduler - INFO - Receive client connection: Client-29f081ba-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:35,454 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58232
2023-08-17 05:37:37,152 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:37,152 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:37,700 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:38,916 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33893
2023-08-17 05:37:38,917 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33893
2023-08-17 05:37:38,917 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-08-17 05:37:38,917 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:38,917 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:38,917 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:38,917 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-17 05:37:38,917 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kg_tjdyo
2023-08-17 05:37:38,917 - distributed.worker - INFO - Starting Worker plugin PreImport-c86898a8-9840-4462-bdae-9af3ec78f1b8
2023-08-17 05:37:38,918 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90b2d6df-ae58-469e-a851-5cedeef66255
2023-08-17 05:37:38,918 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9dbf0ba0-0b1b-4f56-84cd-a0ac3582c163
2023-08-17 05:37:38,918 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:38,940 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33893', status: init, memory: 0, processing: 0>
2023-08-17 05:37:38,941 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33893
2023-08-17 05:37:38,942 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58246
2023-08-17 05:37:38,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:38,943 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:38,943 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:38,944 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:38,997 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:37:39,000 - distributed.scheduler - INFO - Remove client Client-29f081ba-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:39,000 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58232; closing.
2023-08-17 05:37:39,000 - distributed.scheduler - INFO - Remove client Client-29f081ba-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:39,001 - distributed.scheduler - INFO - Close client connection: Client-29f081ba-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:39,001 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34649'. Reason: nanny-close
2023-08-17 05:37:39,002 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:39,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33893. Reason: nanny-close
2023-08-17 05:37:39,005 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58246; closing.
2023-08-17 05:37:39,005 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:39,005 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33893', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250659.005571')
2023-08-17 05:37:39,005 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:37:39,007 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:40,018 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:37:40,018 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:37:40,019 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:37:40,020 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-17 05:37:40,020 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-08-17 05:37:44,007 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:44,011 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45955 instead
  warnings.warn(
2023-08-17 05:37:44,015 - distributed.scheduler - INFO - State start
2023-08-17 05:37:44,037 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:44,038 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-17 05:37:44,038 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45955/status
2023-08-17 05:37:44,126 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40827'
2023-08-17 05:37:44,201 - distributed.scheduler - INFO - Receive client connection: Client-2f837c63-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:44,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58344
2023-08-17 05:37:45,724 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:45,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:46,261 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:47,215 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38755
2023-08-17 05:37:47,216 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38755
2023-08-17 05:37:47,216 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35991
2023-08-17 05:37:47,216 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:37:47,216 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:47,216 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:47,216 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-17 05:37:47,216 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ra06ugs
2023-08-17 05:37:47,217 - distributed.worker - INFO - Starting Worker plugin PreImport-8bb7be3d-fc9a-4612-af20-50cffee515fa
2023-08-17 05:37:47,218 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b31e5fd-597a-4982-960a-387cc2e7de6c
2023-08-17 05:37:47,218 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b352515c-d79a-4fa4-b8d2-b05cc583b06c
2023-08-17 05:37:47,221 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:47,250 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38755', status: init, memory: 0, processing: 0>
2023-08-17 05:37:47,251 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38755
2023-08-17 05:37:47,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57486
2023-08-17 05:37:47,252 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:47,253 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:37:47,253 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:47,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:37:47,300 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:37:47,303 - distributed.scheduler - INFO - Remove client Client-2f837c63-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:47,303 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58344; closing.
2023-08-17 05:37:47,303 - distributed.scheduler - INFO - Remove client Client-2f837c63-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:47,304 - distributed.scheduler - INFO - Close client connection: Client-2f837c63-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:47,305 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40827'. Reason: nanny-close
2023-08-17 05:37:47,305 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:47,306 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38755. Reason: nanny-close
2023-08-17 05:37:47,308 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57486; closing.
2023-08-17 05:37:47,308 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:37:47,308 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38755', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250667.3086562')
2023-08-17 05:37:47,308 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:37:47,310 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:48,321 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:37:48,321 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:37:48,321 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:37:48,322 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-17 05:37:48,323 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-08-17 05:37:50,354 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:50,357 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38243 instead
  warnings.warn(
2023-08-17 05:37:50,362 - distributed.scheduler - INFO - State start
2023-08-17 05:37:50,383 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:50,384 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-17 05:37:50,385 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38243/status
2023-08-17 05:37:54,163 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:37:54,164 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:37:54,164 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:37:54,165 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-17 05:37:54,165 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-08-17 05:37:56,191 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:56,196 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39509 instead
  warnings.warn(
2023-08-17 05:37:56,199 - distributed.scheduler - INFO - State start
2023-08-17 05:37:56,221 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:37:56,222 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-08-17 05:37:56,223 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39509/status
2023-08-17 05:37:56,386 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44535'
2023-08-17 05:37:57,922 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:37:57,922 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:37:57,926 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:37:58,155 - distributed.scheduler - INFO - Receive client connection: Client-36c5d6b4-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:58,167 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32952
2023-08-17 05:37:58,801 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35773
2023-08-17 05:37:58,801 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35773
2023-08-17 05:37:58,801 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37783
2023-08-17 05:37:58,802 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-17 05:37:58,802 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:58,802 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:37:58,802 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-17 05:37:58,802 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-sr5wv13g
2023-08-17 05:37:58,802 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-346fbe25-9bb1-40bf-8a24-5756722a27eb
2023-08-17 05:37:58,802 - distributed.worker - INFO - Starting Worker plugin PreImport-ed386088-f89a-4429-9de9-8b0019a8c952
2023-08-17 05:37:58,803 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57aed0a2-32ae-4299-97a6-27bc8b016580
2023-08-17 05:37:58,803 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:58,833 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35773', status: init, memory: 0, processing: 0>
2023-08-17 05:37:58,833 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35773
2023-08-17 05:37:58,834 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32968
2023-08-17 05:37:58,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:37:58,835 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-17 05:37:58,835 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:37:58,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-17 05:37:58,884 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:37:58,887 - distributed.scheduler - INFO - Remove client Client-36c5d6b4-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:58,887 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32952; closing.
2023-08-17 05:37:58,887 - distributed.scheduler - INFO - Remove client Client-36c5d6b4-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:58,888 - distributed.scheduler - INFO - Close client connection: Client-36c5d6b4-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:37:58,888 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44535'. Reason: nanny-close
2023-08-17 05:37:58,889 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:37:58,890 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35773. Reason: nanny-close
2023-08-17 05:37:58,892 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32968; closing.
2023-08-17 05:37:58,892 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-17 05:37:58,892 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35773', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250678.8923929')
2023-08-17 05:37:58,892 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:37:58,893 - distributed.nanny - INFO - Worker closed
2023-08-17 05:37:59,804 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:37:59,805 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:37:59,805 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:37:59,806 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-08-17 05:37:59,806 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-08-17 05:38:01,829 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:38:01,833 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45569 instead
  warnings.warn(
2023-08-17 05:38:01,836 - distributed.scheduler - INFO - State start
2023-08-17 05:38:01,858 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:38:01,859 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-17 05:38:01,860 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45569/status
2023-08-17 05:38:01,967 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37479'
2023-08-17 05:38:01,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43493'
2023-08-17 05:38:01,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42511'
2023-08-17 05:38:01,994 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36767'
2023-08-17 05:38:02,002 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45713'
2023-08-17 05:38:02,011 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37259'
2023-08-17 05:38:02,020 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45685'
2023-08-17 05:38:02,029 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41711'
2023-08-17 05:38:02,675 - distributed.scheduler - INFO - Receive client connection: Client-3a1bc1c1-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:02,692 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58214
2023-08-17 05:38:03,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:38:03,821 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:38:03,822 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:38:03,823 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:38:03,825 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:38:03,826 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:38:03,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:38:03,827 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:38:03,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:38:03,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:38:03,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:38:03,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:38:03,831 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:38:03,833 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:38:03,834 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:38:03,835 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:38:03,835 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:38:03,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:38:03,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:38:03,839 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:38:03,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:38:03,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:38:03,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:38:03,847 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:38:06,687 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34905
2023-08-17 05:38:06,688 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34905
2023-08-17 05:38:06,688 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44317
2023-08-17 05:38:06,688 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,688 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,688 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:38:06,688 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:38:06,689 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5o5vo9h5
2023-08-17 05:38:06,689 - distributed.worker - INFO - Starting Worker plugin PreImport-75b25dd5-d716-4106-9265-12df590142ae
2023-08-17 05:38:06,689 - distributed.worker - INFO - Starting Worker plugin RMMSetup-22c27fbc-63ac-4ace-8db0-e9001d7bb25d
2023-08-17 05:38:06,699 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39015
2023-08-17 05:38:06,699 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39015
2023-08-17 05:38:06,699 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38071
2023-08-17 05:38:06,699 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,700 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,700 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:38:06,700 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:38:06,700 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-myzwlgow
2023-08-17 05:38:06,700 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81ec6b9d-dc95-4fc9-ad28-45d6e56fdba1
2023-08-17 05:38:06,708 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39783
2023-08-17 05:38:06,709 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39783
2023-08-17 05:38:06,709 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33915
2023-08-17 05:38:06,709 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,709 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,709 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:38:06,710 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:38:06,710 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ogcyflka
2023-08-17 05:38:06,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-822a02cd-5881-4108-a378-46501b8094e4
2023-08-17 05:38:06,743 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35219
2023-08-17 05:38:06,744 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35219
2023-08-17 05:38:06,744 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42803
2023-08-17 05:38:06,744 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,744 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,744 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:38:06,744 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:38:06,744 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nlcp60eq
2023-08-17 05:38:06,745 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02df0f65-f682-4ee2-8dda-4724d9205f95
2023-08-17 05:38:06,746 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41055
2023-08-17 05:38:06,747 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41055
2023-08-17 05:38:06,747 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42777
2023-08-17 05:38:06,747 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,747 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,747 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:38:06,747 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:38:06,748 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4nyp_1i6
2023-08-17 05:38:06,748 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c8d3fdd-4d0c-450c-a6db-d9c57aabd323
2023-08-17 05:38:06,748 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6552ef2f-28ce-405d-8eb6-f26e8ade8c4e
2023-08-17 05:38:06,754 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43183
2023-08-17 05:38:06,755 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43183
2023-08-17 05:38:06,755 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44625
2023-08-17 05:38:06,755 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,755 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,756 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:38:06,756 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:38:06,756 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pn2ewfjk
2023-08-17 05:38:06,756 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2756e371-15cd-4456-8c8b-b75ae9475912
2023-08-17 05:38:06,755 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39011
2023-08-17 05:38:06,757 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39011
2023-08-17 05:38:06,757 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36483
2023-08-17 05:38:06,757 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,757 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,757 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:38:06,757 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:38:06,757 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i1uj32ek
2023-08-17 05:38:06,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f65e661a-08f3-49bb-ba6c-16f55be202a2
2023-08-17 05:38:06,758 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1bc7ce2-3501-4187-81e0-2049b3a55bd0
2023-08-17 05:38:06,760 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35205
2023-08-17 05:38:06,760 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35205
2023-08-17 05:38:06,761 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45151
2023-08-17 05:38:06,761 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,761 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,761 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:38:06,761 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-17 05:38:06,761 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-meq1_5vv
2023-08-17 05:38:06,762 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0503fcc6-d3cb-432a-981a-c25130cd833e
2023-08-17 05:38:06,762 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df82c685-bd3e-48ca-8890-fa15922bc040
2023-08-17 05:38:06,893 - distributed.worker - INFO - Starting Worker plugin PreImport-a00d2dcf-92fa-4489-8427-63cad4954159
2023-08-17 05:38:06,894 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,904 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42da085e-37be-442e-a502-6f8c1208094f
2023-08-17 05:38:06,904 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3122d05-b684-47a9-9c6f-a0fbea649b47
2023-08-17 05:38:06,904 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-763e57a9-6415-4d13-8f82-2680e39e47c0
2023-08-17 05:38:06,904 - distributed.worker - INFO - Starting Worker plugin PreImport-3a18a381-27e9-4e30-948a-52466959dca3
2023-08-17 05:38:06,905 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,905 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,906 - distributed.worker - INFO - Starting Worker plugin PreImport-11378240-631a-45f0-b387-1f5df4ef4504
2023-08-17 05:38:06,907 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,914 - distributed.worker - INFO - Starting Worker plugin PreImport-81e5ae4e-6168-4681-bb73-d19e809c4fc2
2023-08-17 05:38:06,914 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-84aa331e-5547-4041-abbd-5bab79b307b2
2023-08-17 05:38:06,914 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c48cd2ab-a758-43a7-b4fd-e9cdcc2e47ab
2023-08-17 05:38:06,914 - distributed.worker - INFO - Starting Worker plugin PreImport-30ee0b50-8a10-4ef8-b72d-7da2fc47a536
2023-08-17 05:38:06,914 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,914 - distributed.worker - INFO - Starting Worker plugin PreImport-b1dc2cc5-0941-4720-913b-c66ad10daae0
2023-08-17 05:38:06,914 - distributed.worker - INFO - Starting Worker plugin PreImport-4b8bb51c-865f-477c-b25c-cf20819649d9
2023-08-17 05:38:06,915 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,915 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,915 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,933 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39011', status: init, memory: 0, processing: 0>
2023-08-17 05:38:06,935 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39011
2023-08-17 05:38:06,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34150
2023-08-17 05:38:06,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:38:06,937 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,937 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:38:06,945 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35219', status: init, memory: 0, processing: 0>
2023-08-17 05:38:06,946 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35219
2023-08-17 05:38:06,946 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34192
2023-08-17 05:38:06,947 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:38:06,947 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41055', status: init, memory: 0, processing: 0>
2023-08-17 05:38:06,948 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41055
2023-08-17 05:38:06,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34202
2023-08-17 05:38:06,948 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,948 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,948 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35205', status: init, memory: 0, processing: 0>
2023-08-17 05:38:06,949 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35205
2023-08-17 05:38:06,949 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34222
2023-08-17 05:38:06,949 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:38:06,949 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43183', status: init, memory: 0, processing: 0>
2023-08-17 05:38:06,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:38:06,950 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43183
2023-08-17 05:38:06,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34208
2023-08-17 05:38:06,950 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,950 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,950 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:38:06,951 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39783', status: init, memory: 0, processing: 0>
2023-08-17 05:38:06,951 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:38:06,951 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,952 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39783
2023-08-17 05:38:06,952 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34170
2023-08-17 05:38:06,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:38:06,952 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,952 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,952 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39015', status: init, memory: 0, processing: 0>
2023-08-17 05:38:06,953 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39015
2023-08-17 05:38:06,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34158
2023-08-17 05:38:06,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:38:06,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:38:06,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:38:06,954 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,954 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:38:06,955 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,955 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:38:06,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:38:06,961 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34905', status: init, memory: 0, processing: 0>
2023-08-17 05:38:06,962 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34905
2023-08-17 05:38:06,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34184
2023-08-17 05:38:06,963 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:38:06,964 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:38:06,964 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:06,966 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:38:07,036 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:38:07,036 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:38:07,036 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:38:07,037 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:38:07,037 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:38:07,037 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:38:07,037 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:38:07,037 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-17 05:38:07,050 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:07,050 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:07,050 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:07,051 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:07,051 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:07,051 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:07,051 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:07,051 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:07,055 - distributed.scheduler - INFO - Remove client Client-3a1bc1c1-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:07,055 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58214; closing.
2023-08-17 05:38:07,055 - distributed.scheduler - INFO - Remove client Client-3a1bc1c1-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:07,056 - distributed.scheduler - INFO - Close client connection: Client-3a1bc1c1-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:07,057 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42511'. Reason: nanny-close
2023-08-17 05:38:07,057 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:38:07,058 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37259'. Reason: nanny-close
2023-08-17 05:38:07,058 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:38:07,058 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34905. Reason: nanny-close
2023-08-17 05:38:07,058 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37479'. Reason: nanny-close
2023-08-17 05:38:07,059 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:38:07,059 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43493'. Reason: nanny-close
2023-08-17 05:38:07,059 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39015. Reason: nanny-close
2023-08-17 05:38:07,060 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:38:07,060 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36767'. Reason: nanny-close
2023-08-17 05:38:07,060 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35205. Reason: nanny-close
2023-08-17 05:38:07,060 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:38:07,061 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35219. Reason: nanny-close
2023-08-17 05:38:07,061 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45713'. Reason: nanny-close
2023-08-17 05:38:07,061 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:38:07,061 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:38:07,061 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34184; closing.
2023-08-17 05:38:07,061 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34905', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250687.0617635')
2023-08-17 05:38:07,061 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45685'. Reason: nanny-close
2023-08-17 05:38:07,061 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:38:07,061 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39783. Reason: nanny-close
2023-08-17 05:38:07,062 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:38:07,062 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41711'. Reason: nanny-close
2023-08-17 05:38:07,062 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39011. Reason: nanny-close
2023-08-17 05:38:07,062 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:38:07,062 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:38:07,062 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34158; closing.
2023-08-17 05:38:07,062 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:38:07,063 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41055. Reason: nanny-close
2023-08-17 05:38:07,063 - distributed.nanny - INFO - Worker closed
2023-08-17 05:38:07,063 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43183. Reason: nanny-close
2023-08-17 05:38:07,063 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39015', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250687.0638733')
2023-08-17 05:38:07,063 - distributed.nanny - INFO - Worker closed
2023-08-17 05:38:07,064 - distributed.nanny - INFO - Worker closed
2023-08-17 05:38:07,064 - distributed.nanny - INFO - Worker closed
2023-08-17 05:38:07,064 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:38:07,065 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34222; closing.
2023-08-17 05:38:07,065 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:38:07,065 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34192; closing.
2023-08-17 05:38:07,065 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:38:07,065 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35205', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250687.0656343')
2023-08-17 05:38:07,065 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:38:07,066 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35219', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250687.0660625')
2023-08-17 05:38:07,066 - distributed.nanny - INFO - Worker closed
2023-08-17 05:38:07,066 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34170; closing.
2023-08-17 05:38:07,067 - distributed.nanny - INFO - Worker closed
2023-08-17 05:38:07,067 - distributed.nanny - INFO - Worker closed
2023-08-17 05:38:07,067 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39783', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250687.067141')
2023-08-17 05:38:07,067 - distributed.nanny - INFO - Worker closed
2023-08-17 05:38:07,067 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34150; closing.
2023-08-17 05:38:07,067 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34202; closing.
2023-08-17 05:38:07,068 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39011', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250687.0683415')
2023-08-17 05:38:07,068 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41055', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250687.0688052')
2023-08-17 05:38:07,069 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34208; closing.
2023-08-17 05:38:07,069 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43183', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250687.069653')
2023-08-17 05:38:07,069 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:38:08,474 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:38:08,475 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:38:08,475 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:38:08,476 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-17 05:38:08,476 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-08-17 05:38:10,446 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:38:10,450 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43069 instead
  warnings.warn(
2023-08-17 05:38:10,453 - distributed.scheduler - INFO - State start
2023-08-17 05:38:10,478 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:38:10,479 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-17 05:38:10,480 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43069/status
2023-08-17 05:38:10,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42401'
2023-08-17 05:38:12,111 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:38:12,111 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:38:12,114 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:38:12,189 - distributed.scheduler - INFO - Receive client connection: Client-3f4f3dcf-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:12,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34342
2023-08-17 05:38:13,005 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35931
2023-08-17 05:38:13,005 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35931
2023-08-17 05:38:13,006 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36969
2023-08-17 05:38:13,006 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:38:13,006 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:13,006 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:38:13,006 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-17 05:38:13,006 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e0frhuwn
2023-08-17 05:38:13,006 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a1dd5ec-2359-4cf2-bbb3-63e1ced15e4b
2023-08-17 05:38:13,006 - distributed.worker - INFO - Starting Worker plugin PreImport-40953a5e-6480-48a0-b854-53cc5403e438
2023-08-17 05:38:13,007 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3cdda8e6-3b22-4285-b2f3-003f98334b6f
2023-08-17 05:38:13,111 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:13,139 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35931', status: init, memory: 0, processing: 0>
2023-08-17 05:38:13,140 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35931
2023-08-17 05:38:13,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34372
2023-08-17 05:38:13,141 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:38:13,141 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:38:13,141 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:13,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:38:13,222 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-17 05:38:13,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:13,227 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:13,230 - distributed.scheduler - INFO - Remove client Client-3f4f3dcf-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:13,230 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34342; closing.
2023-08-17 05:38:13,230 - distributed.scheduler - INFO - Remove client Client-3f4f3dcf-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:13,231 - distributed.scheduler - INFO - Close client connection: Client-3f4f3dcf-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:13,231 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42401'. Reason: nanny-close
2023-08-17 05:38:13,232 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:38:13,233 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35931. Reason: nanny-close
2023-08-17 05:38:13,235 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34372; closing.
2023-08-17 05:38:13,235 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:38:13,235 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35931', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250693.2355042')
2023-08-17 05:38:13,235 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:38:13,237 - distributed.nanny - INFO - Worker closed
2023-08-17 05:38:14,147 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:38:14,148 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:38:14,148 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:38:14,149 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-17 05:38:14,149 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-08-17 05:38:16,158 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:38:16,162 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32971 instead
  warnings.warn(
2023-08-17 05:38:16,166 - distributed.scheduler - INFO - State start
2023-08-17 05:38:16,185 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-17 05:38:16,186 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-17 05:38:16,187 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32971/status
2023-08-17 05:38:16,316 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45933'
2023-08-17 05:38:17,406 - distributed.scheduler - INFO - Receive client connection: Client-42b0aa98-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:17,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33790
2023-08-17 05:38:17,838 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-17 05:38:17,838 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-17 05:38:17,842 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-17 05:38:18,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40919
2023-08-17 05:38:18,754 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40919
2023-08-17 05:38:18,754 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37473
2023-08-17 05:38:18,754 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-17 05:38:18,754 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:18,754 - distributed.worker - INFO -               Threads:                          1
2023-08-17 05:38:18,754 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-17 05:38:18,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kl5517d9
2023-08-17 05:38:18,755 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c56979af-fa1b-4bd5-9d9d-c120463a50c9
2023-08-17 05:38:18,755 - distributed.worker - INFO - Starting Worker plugin PreImport-81fe3d86-3f39-4065-8b7f-9761095b1309
2023-08-17 05:38:18,755 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8a0fdb7-3e59-431d-9c1d-a9b84c804214
2023-08-17 05:38:18,866 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:18,899 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40919', status: init, memory: 0, processing: 0>
2023-08-17 05:38:18,900 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40919
2023-08-17 05:38:18,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33800
2023-08-17 05:38:18,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-08-17 05:38:18,901 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-17 05:38:18,901 - distributed.worker - INFO - -------------------------------------------------
2023-08-17 05:38:18,903 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-17 05:38:18,948 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-08-17 05:38:18,952 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-17 05:38:18,956 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:18,957 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-17 05:38:18,960 - distributed.scheduler - INFO - Remove client Client-42b0aa98-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:18,960 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33790; closing.
2023-08-17 05:38:18,960 - distributed.scheduler - INFO - Remove client Client-42b0aa98-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:18,960 - distributed.scheduler - INFO - Close client connection: Client-42b0aa98-3cc0-11ee-b15d-d8c49764f6bb
2023-08-17 05:38:18,961 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45933'. Reason: nanny-close
2023-08-17 05:38:18,962 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-17 05:38:18,963 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40919. Reason: nanny-close
2023-08-17 05:38:18,965 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-17 05:38:18,965 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33800; closing.
2023-08-17 05:38:18,965 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40919', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1692250698.9654012')
2023-08-17 05:38:18,965 - distributed.scheduler - INFO - Lost all workers
2023-08-17 05:38:18,967 - distributed.nanny - INFO - Worker closed
2023-08-17 05:38:19,977 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-17 05:38:19,978 - distributed.scheduler - INFO - Scheduler closing...
2023-08-17 05:38:19,978 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-17 05:38:19,979 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-17 05:38:19,979 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41099 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37085 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44369 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42055 instead
  warnings.warn(
2023-08-17 05:39:01,311 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 355, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1927, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1255, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1562, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1141' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
2023-08-17 05:39:01,342 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 355, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1927, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1255, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1562, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1140' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45623 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45835 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35217 instead
  warnings.warn(
2023-08-17 05:39:33,853 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1539, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 355, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1927, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1255, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1374, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1620, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1562, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39151 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34159 instead
  warnings.warn(
[1692250796.355934] [dgx13:67039:0]            sock.c:481  UCX  ERROR bind(fd=120 addr=0.0.0.0:54932) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42145 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40777 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41687 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37101 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33529 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41003 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45791 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43731 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42941 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46329 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44759 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35365 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44597 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40009 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46451 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35501 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42411 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46175 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41439 instead
  warnings.warn(
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-17 05:44:57,606 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-17 05:44:57,611 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-17 05:44:57,615 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f63cbe982e0>>, <Task finished name='Task-7' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:204> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 207, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-7' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:204> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 207, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-17 05:44:57,620 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f65041c92e0>>, <Task finished name='Task-7' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:204> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 207, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-7' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:204> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 207, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-17 05:44:57,629 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-17 05:44:57,636 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-17 05:44:57,642 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fabb1eaa2e0>>, <Task finished name='Task-7' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:204> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 207, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-17 05:44:57,643 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f90cf9a52e0>>, <Task finished name='Task-7' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:204> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 207, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-7' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:204> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 207, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-7' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:204> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 207, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 983, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 505, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 432, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-17 05:44:59,618 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-08-17 05:44:59,623 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-08-17 05:44:59,646 - distributed.nanny - ERROR - Worker process died unexpectedly
