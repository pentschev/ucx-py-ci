2023-01-27 22:49:30,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:30,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:30,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:30,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:30,301 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:30,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:30,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:30,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:30,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:30,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:30,405 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:30,406 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:30,438 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:30,438 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:30,463 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:30,463 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:44506:0:44506] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  44506) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f3b481326b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f3b4813288f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f3b48132bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f3bbd7a4980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f3b483ae1b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f3b483d7638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f3b31de50ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f3b31de5674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f3b31de7a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f3b4813c8a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f3b31de7b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f3b483aaf5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f3b48656c79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x141eca) [0x559b0cd04eca]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x559b0ccf2549]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x559b0cced7d7]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x559b0ccffbb9]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x559b0cceee22]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x559b0ccffec3]
19  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bc75) [0x559b0cd0ec75]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x264071) [0x559b0ce27071]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xf7187) [0x559b0ccba187]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x133747) [0x559b0ccf6747]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f0a) [0x559b0ccf46ba]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x559b0ccffec3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x559b0cceee22]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x559b0ccffec3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x559b0cceee22]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x559b0ccffec3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x559b0cceee22]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x559b0ccffec3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x559b0cceee22]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x559b0cced7d7]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x559b0ccffbb9]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4cbc) [0x559b0ccf346c]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x559b0cced7d7]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14baf8) [0x559b0cd0eaf8]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x559b0cd0f254]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x21b8b8) [0x559b0cdde8b8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x559b0ccf7a57]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x559b0ccf2549]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x559b0ccffec3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bb91) [0x559b0cd0eb91]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x559b0ccf2549]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x559b0ccffec3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x559b0cceee22]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x559b0cced7d7]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x559b0ccffbb9]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x559b0cceee22]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x559b0ccffec3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3c3) [0x559b0cceeb73]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x559b0cced7d7]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x559b0ccffbb9]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11df) [0x559b0ccef98f]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x559b0cced7d7]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x559b0cced497]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x559b0cced449]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x559b0cda7ddb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x213409) [0x559b0cdd6409]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20f5a4) [0x559b0cdd25a4]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x559b0cdca27d]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x559b0cdca14d]
=================================
[dgx13:44509:0:44509] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  44509) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7fe59c1096b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7fe59c10988f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7fe59c109bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7fe6117cd980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7fe59c3851b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7fe59c3ae638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7fe589de50ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7fe589de5674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7fe589de7a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fe59c1138a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7fe589de7b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7fe59c381f5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7fe59c62dc79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x141eca) [0x55c1062d1eca]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55c1062bf549]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c1062ba7d7]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55c1062ccbb9]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c1062bbe22]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x153244) [0x55c1062e3244]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8301) [0x7fe604020301]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x55c1062c4a57]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xf7187) [0x55c106287187]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x133747) [0x55c1062c3747]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f0a) [0x55c1062c16ba]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c1062ccec3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c1062bbe22]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c1062ccec3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c1062bbe22]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c1062ccec3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c1062bbe22]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c1062ccec3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c1062bbe22]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c1062ba7d7]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55c1062ccbb9]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4cbc) [0x55c1062c046c]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c1062ba7d7]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14baf8) [0x55c1062dbaf8]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55c1062dc254]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x21b8b8) [0x55c1063ab8b8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x55c1062c4a57]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55c1062bf549]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c1062ccec3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bb91) [0x55c1062dbb91]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55c1062bf549]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c1062ccec3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c1062bbe22]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c1062ba7d7]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55c1062ccbb9]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c1062bbe22]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c1062ccec3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3c3) [0x55c1062bbb73]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c1062ba7d7]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55c1062ccbb9]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11df) [0x55c1062bc98f]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c1062ba7d7]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55c1062ba497]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55c1062ba449]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55c106374ddb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x213409) [0x55c1063a3409]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20f5a4) [0x55c10639f5a4]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55c10639727d]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55c10639714d]
=================================
[dgx13:44499:0:44499] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
[dgx13:44512:0:44512] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  44499) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f28e0e236b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f28e0e2388f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f28e0e23bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f2964591980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f28e109f1b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f28e10c8638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f28e0bdb0ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f28e0bdb674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f28e0bdda78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f28e0e2d8a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f28e0bddb1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f28e109bf5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f28e1347c79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x141eca) [0x55c5d4e04eca]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55c5d4df2549]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c5d4ded7d7]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55c5d4dffbb9]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c5d4deee22]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x153244) [0x55c5d4e16244]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8301) [0x7f28fec0e301]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x55c5d4df7a57]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xf7187) [0x55c5d4dba187]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x133747) [0x55c5d4df6747]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f0a) [0x55c5d4df46ba]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c5d4dffec3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c5d4deee22]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c5d4dffec3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c5d4deee22]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c5d4dffec3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c5d4deee22]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c5d4dffec3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c5d4deee22]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c5d4ded7d7]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55c5d4dffbb9]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4cbc) [0x55c5d4df346c]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c5d4ded7d7]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14baf8) [0x55c5d4e0eaf8]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55c5d4e0f254]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x21b8b8) [0x55c5d4ede8b8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x55c5d4df7a57]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55c5d4df2549]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c5d4dffec3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bb91) [0x55c5d4e0eb91]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x55c5d4df2549]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c5d4dffec3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c5d4deee22]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c5d4ded7d7]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55c5d4dffbb9]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x55c5d4deee22]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x55c5d4dffec3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3c3) [0x55c5d4deeb73]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c5d4ded7d7]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x55c5d4dffbb9]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11df) [0x55c5d4def98f]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x55c5d4ded7d7]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55c5d4ded497]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55c5d4ded449]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55c5d4ea7ddb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x213409) [0x55c5d4ed6409]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20f5a4) [0x55c5d4ed25a4]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55c5d4eca27d]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55c5d4eca14d]
=================================
==== backtrace (tid:  44512) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f710cb986b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f710cb9888f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f710cb98bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f71821fc980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f710ce141b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f710ce3d638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f710c9500ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f710c950674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f710c952a78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f710cba28a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f710c952b1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f710ce10f5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f710d0bcc79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x141eca) [0x555b26b4ceca]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x555b26b3a549]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x555b26b357d7]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x555b26b47bb9]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x555b26b36e22]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x153244) [0x555b26b5e244]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8301) [0x7f711c878301]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x555b26b3fa57]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xf7187) [0x555b26b02187]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x133747) [0x555b26b3e747]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f0a) [0x555b26b3c6ba]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x555b26b47ec3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x555b26b36e22]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x555b26b47ec3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x555b26b36e22]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x555b26b47ec3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x555b26b36e22]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x555b26b47ec3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x555b26b36e22]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x555b26b357d7]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x555b26b47bb9]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4cbc) [0x555b26b3b46c]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x555b26b357d7]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14baf8) [0x555b26b56af8]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x555b26b57254]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x21b8b8) [0x555b26c268b8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x555b26b3fa57]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x555b26b3a549]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x555b26b47ec3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bb91) [0x555b26b56b91]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x555b26b3a549]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x555b26b47ec3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x555b26b36e22]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x555b26b357d7]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x555b26b47bb9]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x555b26b36e22]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x555b26b47ec3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3c3) [0x555b26b36b73]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x555b26b357d7]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x555b26b47bb9]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11df) [0x555b26b3798f]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x555b26b357d7]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x555b26b35497]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x555b26b35449]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x555b26befddb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x213409) [0x555b26c1e409]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20f5a4) [0x555b26c1a5a4]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x555b26c1227d]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x555b26c1214d]
=================================
[dgx13:44515:0:44515] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  44515) ====
 0  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2d4) [0x7f6a082d56b4]
 1  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2d88f) [0x7f6a082d588f]
 2  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(+0x2dbb4) [0x7f6a082d5bb4]
 3  /lib/x86_64-linux-gnu/libpthread.so.0(+0x12980) [0x7f6a7d945980]
 4  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f6a085511b7]
 5  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x508) [0x7f6a0857a638]
 6  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f0ff) [0x7f6a0808d0ff]
 7  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x1f674) [0x7f6a0808d674]
 8  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(+0x21a78) [0x7f6a0808fa78]
 9  /datasets/pentschev/miniconda3/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f6a082df8a9]
10  /datasets/pentschev/miniconda3/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7b) [0x7f6a0808fb1b]
11  /datasets/pentschev/miniconda3/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x6a) [0x7f6a0854df5a]
12  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x26c79) [0x7f6a087f9c79]
13  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x141eca) [0x556199fa0eca]
14  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x556199f8e549]
15  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x556199f897d7]
16  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x556199f9bbb9]
17  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x556199f8ae22]
18  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x153244) [0x556199fb2244]
19  /datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8301) [0x7f6a7001b301]
20  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x556199f93a57]
21  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0xf7187) [0x556199f56187]
22  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x133747) [0x556199f92747]
23  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f0a) [0x556199f906ba]
24  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x556199f9bec3]
25  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x556199f8ae22]
26  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x556199f9bec3]
27  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x556199f8ae22]
28  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x556199f9bec3]
29  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x556199f8ae22]
30  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x556199f9bec3]
31  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x556199f8ae22]
32  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x556199f897d7]
33  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x556199f9bbb9]
34  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4cbc) [0x556199f8f46c]
35  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x556199f897d7]
36  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14baf8) [0x556199faaaf8]
37  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyObject_Call+0xb4) [0x556199fab254]
38  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x21b8b8) [0x55619a07a8b8]
39  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyObject_MakeTpCall+0x347) [0x556199f93a57]
40  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x556199f8e549]
41  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x556199f9bec3]
42  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x14bb91) [0x556199faab91]
43  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d99) [0x556199f8e549]
44  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x556199f9bec3]
45  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x556199f8ae22]
46  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x556199f897d7]
47  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x556199f9bbb9]
48  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x672) [0x556199f8ae22]
49  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x13cec3) [0x556199f9bec3]
50  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3c3) [0x556199f8ab73]
51  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x556199f897d7]
52  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyFunction_Vectorcall+0xb9) [0x556199f9bbb9]
53  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11df) [0x556199f8b98f]
54  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x12a7d7) [0x556199f897d7]
55  /datasets/pentschev/miniconda3/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x556199f89497]
56  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x556199f89449]
57  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55619a043ddb]
58  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x213409) [0x55619a072409]
59  /datasets/pentschev/miniconda3/envs/gdf/bin/python(+0x20f5a4) [0x55619a06e5a4]
60  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55619a06627d]
61  /datasets/pentschev/miniconda3/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55619a06614d]
=================================
2023-01-27 22:49:38,995 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33943
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f9834018180, tag: 0x24e9ecd12c8dad4, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f9834018180, tag: 0x24e9ecd12c8dad4, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-27 22:49:38,996 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33943
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f9b300b71c0, tag: 0x4cd6cd2261cbecec, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f9b300b71c0, tag: 0x4cd6cd2261cbecec, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-27 22:49:39,026 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:58415
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f9834018240, tag: 0xa491daefe9221287, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f9834018240, tag: 0xa491daefe9221287, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-27 22:49:39,058 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:36965
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f9834018280, tag: 0x7a16437a473dc15f, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f9834018280, tag: 0x7a16437a473dc15f, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-27 22:49:39,059 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47563 -> ucx://127.0.0.1:36965
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f9834018400, tag: 0x375578c4a845ebaf, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-01-27 22:49:39,051 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:58415
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7ff7140a5180, tag: 0x10c63fec14a7b0ae, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7ff7140a5180, tag: 0x10c63fec14a7b0ae, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-27 22:49:39,082 - distributed.nanny - WARNING - Restarting worker
2023-01-27 22:49:39,124 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:55619 -> ucx://127.0.0.1:36965
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f9b300b73c0, tag: 0x5f8b95260c1c5222, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-01-27 22:49:39,125 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:55619 -> ucx://127.0.0.1:48813
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f9b300b7380, tag: 0x8ac9199331e44fe6, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-01-27 22:49:39,125 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:36965
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f9b300b72c0, tag: 0x2c2221191e525d5f, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f9b300b72c0, tag: 0x2c2221191e525d5f, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-27 22:49:39,125 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:48813
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f9b300b7140, tag: 0x517da408f26cf844, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f9b300b7140, tag: 0x517da408f26cf844, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-27 22:49:39,126 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:58415
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7f9b300b7180, tag: 0xe6af5c0e37aeebda, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7f9b300b7180, tag: 0xe6af5c0e37aeebda, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-01-27 22:49:39,127 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44505
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f9b300b7200, tag: 0xbf0da0c15d300570, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f9b300b7200, tag: 0xbf0da0c15d300570, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-27 22:49:39,177 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47563 -> ucx://127.0.0.1:48813
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f9834018340, tag: 0x8c528f26019c66da, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1766, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-01-27 22:49:39,178 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44505
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7f98340181c0, tag: 0xfb521f422e49dae3, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7f98340181c0, tag: 0xfb521f422e49dae3, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-01-27 22:49:39,179 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:48813
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f9834018100, tag: 0xd13c0a2b49f9b32e, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f9834018100, tag: 0xd13c0a2b49f9b32e, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-01-27 22:49:39,190 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:36965
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7ff7140a5200, tag: 0xae01c3747df29ed0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7ff7140a5200, tag: 0xae01c3747df29ed0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-01-27 22:49:39,190 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44505
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #010] ep: 0x7ff7140a5100, tag: 0x61f53578348df149, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #010] ep: 0x7ff7140a5100, tag: 0x61f53578348df149, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-01-27 22:49:39,191 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:48813
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #010] ep: 0x7ff7140a5240, tag: 0x9dfbc86db039df3c, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #010] ep: 0x7ff7140a5240, tag: 0x9dfbc86db039df3c, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-01-27 22:49:39,200 - distributed.nanny - WARNING - Restarting worker
2023-01-27 22:49:39,358 - distributed.nanny - WARNING - Restarting worker
2023-01-27 22:49:39,360 - distributed.nanny - WARNING - Restarting worker
2023-01-27 22:49:39,392 - distributed.nanny - WARNING - Restarting worker
2023-01-27 22:49:41,003 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 5)
Function:  <dask.layers.CallableLazyImport object at 0x7ff6f5
args:      (                 key   payload
0         1318863339  99051695
1         1352208751  78953234
2          664778035  81293845
3         1353618300  74974697
4         1302090160  62919800
...              ...       ...
99999995    62713713  72068972
99999996  1349206659  33160153
99999997  1359655348  40095643
99999998  1352540937  33956372
99999999  1312812172  90324512

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-27 22:49:41,050 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:41,050 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:41,068 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-a4f9b755aad5bd022ceeabb70498dd6f', 4)
Function:  <dask.layers.CallableLazyImport object at 0x7f9820
args:      (               key   payload
shuffle                     
4            32449  64831972
4            32467  50102445
4            32468  73546964
4            32474  53766307
4            52452  54856704
...            ...       ...
4        799984257  71111498
4        799984258  11391079
4        799984259  26700133
4        799984270  42726757
4        799984277   6161444

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-27 22:49:41,137 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:41,138 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:41,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:41,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:41,248 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-a4f9b755aad5bd022ceeabb70498dd6f', 2)
Function:  <dask.layers.CallableLazyImport object at 0x7ff6f5
args:      (               key   payload
shuffle                     
2            32452  49638085
2            32454  74263049
2            32458  28634935
2            32463  85786927
2            52450  64541170
...            ...       ...
2        799984377  97294454
2        799984256  60063683
2        799984273  87651165
2        799984276  73479219
2        799984285  43890014

[100000000 rows x 2 columns], ['key'], 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-27 22:49:41,257 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:41,257 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:41,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:41,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
terminate called after throwing an instance of 'rmm::out_of_memory'
  what():  std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2023-01-27 22:49:42,775 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33817
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #097] ep: 0x7f9834018200, tag: 0x9fcd1891089c194d, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #097] ep: 0x7f9834018200, tag: 0x9fcd1891089c194d, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-01-27 22:49:42,775 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33817
Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #128] ep: 0x7f9b300b7240, tag: 0xb738e719153e5c19, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2058, in gather_dep
    response = await get_data_from_worker(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2872, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 419, in retry_operation
    return await retry(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 404, in retry
    return await coro()
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in _get_data
    response = await send_recv(
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 986, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 741, in wrapper
    return await func(*args, **kwargs)
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #128] ep: 0x7f9b300b7240, tag: 0xb738e719153e5c19, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-01-27 22:49:42,877 - distributed.nanny - WARNING - Restarting worker
2023-01-27 22:49:43,125 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-3393696d3757f92da87d0dddd0c6ae01', 4)
Function:  generate_chunk
args:      (4, 100000000, 8, 'other', 0.3, True)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-27 22:49:43,307 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-5366301a596b755bb8c33997d3ce7726', 0)
Function:  generate_chunk
args:      (0, 100000000, 8, 'build', 0.3, True)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded')"

2023-01-27 22:49:44,935 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:44,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
terminate called after throwing an instance of 'rmm::out_of_memory'
  what():  std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2023-01-27 22:49:47,218 - distributed.nanny - WARNING - Restarting worker
Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 742, in cupy.cuda.memory.alloc
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 230, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
Exception ignored in: 'cupy.cuda.thrust.cupy_malloc'
Traceback (most recent call last):
  File "cupy/cuda/memory.pyx", line 742, in cupy.cuda.memory.alloc
  File "/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 230, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:193: Maximum pool size exceeded
2023-01-27 22:49:47,555 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-3393696d3757f92da87d0dddd0c6ae01', 5)
Function:  generate_chunk
args:      (5, 100000000, 8, 'other', 0.3, True)
kwargs:    {}
Exception: "RuntimeError('transform: failed to synchronize: cudaErrorIllegalAddress: an illegal memory access was encountered')"

2023-01-27 22:49:49,120 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-01-27 22:49:49,120 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-01-27 22:49:50,077 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-5366301a596b755bb8c33997d3ce7726', 3)
Function:  generate_chunk
args:      (3, 100000000, 8, 'build', 0.3, True)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-01-27 22:49:52,749 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-5366301a596b755bb8c33997d3ce7726', 4)
Function:  generate_chunk
args:      (4, 100000000, 8, 'build', 0.3, True)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-01-27 22:49:52,765 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-5366301a596b755bb8c33997d3ce7726', 1)
Function:  generate_chunk
args:      (1, 100000000, 8, 'build', 0.3, True)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-01-27 22:49:52,787 - distributed.worker - WARNING - Compute Failed
Key:       ('generate-data-5366301a596b755bb8c33997d3ce7726', 2)
Function:  generate_chunk
args:      (2, 100000000, 8, 'build', 0.3, True)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /datasets/pentschev/miniconda3/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

/datasets/pentschev/miniconda3/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
