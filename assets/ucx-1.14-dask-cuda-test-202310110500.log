============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-10-11 05:41:35,596 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:41:35,601 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37263 instead
  warnings.warn(
2023-10-11 05:41:35,604 - distributed.scheduler - INFO - State start
2023-10-11 05:41:35,626 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:41:35,627 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-11 05:41:35,627 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37263/status
2023-10-11 05:41:35,628 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:41:36,013 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37033'
2023-10-11 05:41:36,033 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36005'
2023-10-11 05:41:36,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34623'
2023-10-11 05:41:36,047 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46813'
2023-10-11 05:41:37,707 - distributed.scheduler - INFO - Receive client connection: Client-d638a687-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:41:37,719 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35264
2023-10-11 05:41:37,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:37,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:37,918 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:37,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:37,929 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:37,934 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:37,968 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:37,968 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:37,974 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:37,975 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:37,975 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:37,982 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-10-11 05:41:38,136 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45479
2023-10-11 05:41:38,136 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45479
2023-10-11 05:41:38,136 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36793
2023-10-11 05:41:38,136 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-11 05:41:38,136 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:38,136 - distributed.worker - INFO -               Threads:                          4
2023-10-11 05:41:38,136 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-11 05:41:38,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-czd45u4z
2023-10-11 05:41:38,137 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0314ed89-ffd4-4e10-9ccd-fcd574f71f21
2023-10-11 05:41:38,137 - distributed.worker - INFO - Starting Worker plugin PreImport-c12bf26d-d30f-4cd2-8f32-d6674227a13a
2023-10-11 05:41:38,137 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b86df4ca-5d20-405b-90a1-37fc0c99b701
2023-10-11 05:41:38,138 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:39,597 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45479', status: init, memory: 0, processing: 0>
2023-10-11 05:41:39,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45479
2023-10-11 05:41:39,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35278
2023-10-11 05:41:39,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:39,602 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-11 05:41:39,602 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:39,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-11 05:41:41,899 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39703
2023-10-11 05:41:41,900 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39703
2023-10-11 05:41:41,900 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35519
2023-10-11 05:41:41,901 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-11 05:41:41,901 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:41,901 - distributed.worker - INFO -               Threads:                          4
2023-10-11 05:41:41,901 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-11 05:41:41,901 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-jg0sxvoo
2023-10-11 05:41:41,901 - distributed.worker - INFO - Starting Worker plugin PreImport-e1fd76e0-215f-48fd-b194-649abb47eece
2023-10-11 05:41:41,901 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f459ea8e-8954-4f12-a320-92bfa435673e
2023-10-11 05:41:41,902 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75205e5b-0aa7-445c-95da-4144fb204227
2023-10-11 05:41:41,902 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:42,221 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45271
2023-10-11 05:41:42,222 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45271
2023-10-11 05:41:42,222 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46541
2023-10-11 05:41:42,221 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44871
2023-10-11 05:41:42,222 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-11 05:41:42,222 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44871
2023-10-11 05:41:42,222 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:42,222 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33107
2023-10-11 05:41:42,222 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-11 05:41:42,222 - distributed.worker - INFO -               Threads:                          4
2023-10-11 05:41:42,223 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:42,223 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-11 05:41:42,223 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-87u0u32y
2023-10-11 05:41:42,223 - distributed.worker - INFO -               Threads:                          4
2023-10-11 05:41:42,223 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-11 05:41:42,223 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-lwgsf_l4
2023-10-11 05:41:42,223 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e213db6-4458-4dc8-ba13-9d6d91b6cee0
2023-10-11 05:41:42,224 - distributed.worker - INFO - Starting Worker plugin PreImport-dd3aeae0-387b-4584-a7f8-00d945694c8d
2023-10-11 05:41:42,224 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d4d7f68-1347-4f43-a814-483f7722894a
2023-10-11 05:41:42,224 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de945045-9e19-4930-9022-ff0b2debe219
2023-10-11 05:41:42,224 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:42,225 - distributed.worker - INFO - Starting Worker plugin PreImport-b74fd551-0077-4de4-a224-17d2ac043f5c
2023-10-11 05:41:42,225 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19b9d67a-5742-43fa-b29c-e7390b2610e0
2023-10-11 05:41:42,226 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:42,375 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39703', status: init, memory: 0, processing: 0>
2023-10-11 05:41:42,376 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39703
2023-10-11 05:41:42,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50532
2023-10-11 05:41:42,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:42,377 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-11 05:41:42,377 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:42,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-11 05:41:42,403 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45271', status: init, memory: 0, processing: 0>
2023-10-11 05:41:42,404 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45271
2023-10-11 05:41:42,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50558
2023-10-11 05:41:42,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:42,406 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-11 05:41:42,406 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:42,408 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-11 05:41:42,412 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44871', status: init, memory: 0, processing: 0>
2023-10-11 05:41:42,414 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44871
2023-10-11 05:41:42,414 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50546
2023-10-11 05:41:42,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:42,415 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-11 05:41:42,415 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:42,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-11 05:41:42,436 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-11 05:41:42,436 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-11 05:41:42,436 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-11 05:41:43,074 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-11 05:41:43,080 - distributed.scheduler - INFO - Remove client Client-d638a687-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:41:43,081 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35264; closing.
2023-10-11 05:41:43,081 - distributed.scheduler - INFO - Remove client Client-d638a687-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:41:43,081 - distributed.scheduler - INFO - Close client connection: Client-d638a687-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:41:43,082 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37033'. Reason: nanny-close
2023-10-11 05:41:43,083 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:43,084 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36005'. Reason: nanny-close
2023-10-11 05:41:43,084 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:43,084 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44871. Reason: nanny-close
2023-10-11 05:41:43,084 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34623'. Reason: nanny-close
2023-10-11 05:41:43,084 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:43,085 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46813'. Reason: nanny-close
2023-10-11 05:41:43,085 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45271. Reason: nanny-close
2023-10-11 05:41:43,085 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:43,085 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39703. Reason: nanny-close
2023-10-11 05:41:43,086 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50546; closing.
2023-10-11 05:41:43,086 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-11 05:41:43,086 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45479. Reason: nanny-close
2023-10-11 05:41:43,086 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002903.0864987')
2023-10-11 05:41:43,087 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-11 05:41:43,087 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-11 05:41:43,087 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:43,088 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50558; closing.
2023-10-11 05:41:43,088 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50532; closing.
2023-10-11 05:41:43,088 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45271', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002903.0886567')
2023-10-11 05:41:43,088 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:43,089 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39703', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002903.0889497')
2023-10-11 05:41:43,089 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:43,090 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35278; closing.
2023-10-11 05:41:43,090 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45479', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002903.090245')
2023-10-11 05:41:43,090 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-11 05:41:43,090 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:41:43,092 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:44,349 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:41:44,350 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:41:44,350 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:41:44,351 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-11 05:41:44,352 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-10-11 05:41:46,524 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:41:46,529 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38781 instead
  warnings.warn(
2023-10-11 05:41:46,533 - distributed.scheduler - INFO - State start
2023-10-11 05:41:46,556 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:41:46,557 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:41:46,558 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38781/status
2023-10-11 05:41:46,558 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:41:46,814 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46815'
2023-10-11 05:41:46,830 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32949'
2023-10-11 05:41:46,841 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37067'
2023-10-11 05:41:46,856 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36357'
2023-10-11 05:41:46,858 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40185'
2023-10-11 05:41:46,867 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33727'
2023-10-11 05:41:46,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40385'
2023-10-11 05:41:46,888 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42351'
2023-10-11 05:41:47,760 - distributed.scheduler - INFO - Receive client connection: Client-dcb8a8bd-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:41:47,782 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44416
2023-10-11 05:41:48,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:48,761 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:48,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:48,765 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:48,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:48,769 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:48,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:48,810 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:48,814 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:48,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:48,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:48,838 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:48,845 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:48,845 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:48,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:48,872 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:48,872 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:48,877 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:48,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:48,879 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:48,883 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:48,936 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:41:48,936 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:41:48,940 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:41:53,598 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36349
2023-10-11 05:41:53,599 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36349
2023-10-11 05:41:53,599 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43087
2023-10-11 05:41:53,599 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:41:53,599 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:53,599 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:41:53,599 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:41:53,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vzun7y7m
2023-10-11 05:41:53,600 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04d568dd-985b-478f-9ae0-824fd3c4fb50
2023-10-11 05:41:53,822 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43771
2023-10-11 05:41:53,823 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43771
2023-10-11 05:41:53,823 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32997
2023-10-11 05:41:53,823 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:41:53,823 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:53,823 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:41:53,823 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:41:53,823 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wzuo45_7
2023-10-11 05:41:53,824 - distributed.worker - INFO - Starting Worker plugin PreImport-126732d4-c17e-4ac1-953d-8220a9b6c16a
2023-10-11 05:41:53,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c946981-a875-413d-94ee-a9ce78725489
2023-10-11 05:41:53,824 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9441d3a7-ea9e-4cb0-b140-f0bd279e2dc4
2023-10-11 05:41:53,825 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38965
2023-10-11 05:41:53,825 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38965
2023-10-11 05:41:53,826 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40945
2023-10-11 05:41:53,826 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:41:53,826 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:53,826 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:41:53,826 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:41:53,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v68tz3ms
2023-10-11 05:41:53,826 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5ad387e9-cf86-4658-9588-74de13d968ca
2023-10-11 05:41:53,829 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38463
2023-10-11 05:41:53,830 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38463
2023-10-11 05:41:53,830 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33365
2023-10-11 05:41:53,830 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:41:53,831 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:53,831 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:41:53,831 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:41:53,831 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-33gxjsai
2023-10-11 05:41:53,831 - distributed.worker - INFO - Starting Worker plugin RMMSetup-483c535c-aa1c-4a2b-bbf4-afdb6578f40a
2023-10-11 05:41:53,886 - distributed.worker - INFO - Starting Worker plugin PreImport-607ac203-e2cc-4005-984f-e1cb9b865041
2023-10-11 05:41:53,886 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1bc131ac-8e8e-438c-8cf3-2c34219bfca0
2023-10-11 05:41:53,886 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:53,920 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36349', status: init, memory: 0, processing: 0>
2023-10-11 05:41:53,922 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36349
2023-10-11 05:41:53,922 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54184
2023-10-11 05:41:53,923 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:53,924 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:41:53,924 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:53,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:41:53,999 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1e2ef59d-c1b8-4651-bc48-31222b1edb4b
2023-10-11 05:41:54,002 - distributed.worker - INFO - Starting Worker plugin PreImport-4c784c97-a56a-4046-92ae-117cf4be6bc3
2023-10-11 05:41:54,002 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,004 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-546d74d5-6e4c-4ae9-920c-f72d98452521
2023-10-11 05:41:54,014 - distributed.worker - INFO - Starting Worker plugin PreImport-49cfcd6d-6a65-4703-b4fa-d0de69296b9f
2023-10-11 05:41:54,015 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,041 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34729
2023-10-11 05:41:54,042 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34729
2023-10-11 05:41:54,042 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36691
2023-10-11 05:41:54,042 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,042 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,042 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:41:54,042 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:41:54,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ifo75a66
2023-10-11 05:41:54,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09c53a63-a6f0-4611-9e0e-44b51d0ea5ba
2023-10-11 05:41:54,046 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38965', status: init, memory: 0, processing: 0>
2023-10-11 05:41:54,047 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38965
2023-10-11 05:41:54,047 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54194
2023-10-11 05:41:54,047 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43771', status: init, memory: 0, processing: 0>
2023-10-11 05:41:54,048 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43771
2023-10-11 05:41:54,048 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54208
2023-10-11 05:41:54,048 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:54,049 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,049 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,049 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:54,050 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,051 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,051 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:41:54,052 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:41:54,054 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39899
2023-10-11 05:41:54,054 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39899
2023-10-11 05:41:54,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38879
2023-10-11 05:41:54,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,055 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,055 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:41:54,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:41:54,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xfkafufq
2023-10-11 05:41:54,056 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a61f803e-0272-42f8-b528-57fd9b486ee7
2023-10-11 05:41:54,057 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38463', status: init, memory: 0, processing: 0>
2023-10-11 05:41:54,057 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38463
2023-10-11 05:41:54,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54210
2023-10-11 05:41:54,059 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:54,058 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36705
2023-10-11 05:41:54,059 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36705
2023-10-11 05:41:54,059 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42535
2023-10-11 05:41:54,059 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,059 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,059 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:41:54,059 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:41:54,059 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3trds8pq
2023-10-11 05:41:54,060 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,060 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,060 - distributed.worker - INFO - Starting Worker plugin RMMSetup-864169ed-84c8-4dbd-8f80-be86f157472b
2023-10-11 05:41:54,062 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:41:54,180 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45537
2023-10-11 05:41:54,181 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45537
2023-10-11 05:41:54,181 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44245
2023-10-11 05:41:54,181 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,181 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,181 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:41:54,181 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:41:54,181 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-66ijugxj
2023-10-11 05:41:54,182 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d21bb4c-f8d6-4c03-9a2e-f6f042b29615
2023-10-11 05:41:54,223 - distributed.worker - INFO - Starting Worker plugin PreImport-6bdacafe-c647-47b1-8288-2a7008be87a7
2023-10-11 05:41:54,224 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a1b1ca7-063c-4ff5-8224-86a1af1378c1
2023-10-11 05:41:54,224 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,225 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f8283ce-3d24-4b2e-adad-610614a615d0
2023-10-11 05:41:54,225 - distributed.worker - INFO - Starting Worker plugin PreImport-b91e8657-9ed9-4798-8c70-5e3d8249db2f
2023-10-11 05:41:54,225 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,226 - distributed.worker - INFO - Starting Worker plugin PreImport-e63e4530-9d3b-4737-a7fc-a97204c3725b
2023-10-11 05:41:54,226 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5124032e-651c-428c-a783-180f1542fa67
2023-10-11 05:41:54,227 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,259 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34729', status: init, memory: 0, processing: 0>
2023-10-11 05:41:54,260 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34729
2023-10-11 05:41:54,260 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54220
2023-10-11 05:41:54,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:54,262 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,262 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,262 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39899', status: init, memory: 0, processing: 0>
2023-10-11 05:41:54,262 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39899
2023-10-11 05:41:54,262 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54238
2023-10-11 05:41:54,263 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36705', status: init, memory: 0, processing: 0>
2023-10-11 05:41:54,263 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:41:54,263 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:54,264 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36705
2023-10-11 05:41:54,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54236
2023-10-11 05:41:54,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,264 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:54,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:41:54,266 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,266 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,268 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:41:54,492 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f882ae3-a67b-4455-8726-01bfbd960cb0
2023-10-11 05:41:54,492 - distributed.worker - INFO - Starting Worker plugin PreImport-8988fc44-54d8-4598-b3a1-04d62b464fac
2023-10-11 05:41:54,492 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,513 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45537', status: init, memory: 0, processing: 0>
2023-10-11 05:41:54,513 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45537
2023-10-11 05:41:54,513 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54244
2023-10-11 05:41:54,514 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:41:54,515 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:41:54,515 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:41:54,517 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:41:54,576 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:41:54,576 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:41:54,577 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:41:54,577 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:41:54,577 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:41:54,577 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:41:54,577 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:41:54,577 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:41:54,582 - distributed.scheduler - INFO - Remove client Client-dcb8a8bd-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:41:54,582 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44416; closing.
2023-10-11 05:41:54,582 - distributed.scheduler - INFO - Remove client Client-dcb8a8bd-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:41:54,582 - distributed.scheduler - INFO - Close client connection: Client-dcb8a8bd-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:41:54,584 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46815'. Reason: nanny-close
2023-10-11 05:41:54,584 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:54,585 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32949'. Reason: nanny-close
2023-10-11 05:41:54,585 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:54,585 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39899. Reason: nanny-close
2023-10-11 05:41:54,585 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37067'. Reason: nanny-close
2023-10-11 05:41:54,586 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:54,586 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36349. Reason: nanny-close
2023-10-11 05:41:54,586 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36357'. Reason: nanny-close
2023-10-11 05:41:54,586 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:54,586 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38965. Reason: nanny-close
2023-10-11 05:41:54,586 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40185'. Reason: nanny-close
2023-10-11 05:41:54,587 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:54,587 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:41:54,587 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54238; closing.
2023-10-11 05:41:54,587 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43771. Reason: nanny-close
2023-10-11 05:41:54,587 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33727'. Reason: nanny-close
2023-10-11 05:41:54,587 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39899', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002914.5875044')
2023-10-11 05:41:54,587 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:54,587 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45537. Reason: nanny-close
2023-10-11 05:41:54,587 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:41:54,587 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40385'. Reason: nanny-close
2023-10-11 05:41:54,588 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:54,588 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34729. Reason: nanny-close
2023-10-11 05:41:54,588 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42351'. Reason: nanny-close
2023-10-11 05:41:54,588 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:54,588 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:41:54,589 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54184; closing.
2023-10-11 05:41:54,589 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:41:54,589 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38463. Reason: nanny-close
2023-10-11 05:41:54,589 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:54,589 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002914.5897982')
2023-10-11 05:41:54,589 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36705. Reason: nanny-close
2023-10-11 05:41:54,589 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:41:54,590 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54194; closing.
2023-10-11 05:41:54,590 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:41:54,590 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:41:54,590 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38965', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002914.5906496')
2023-10-11 05:41:54,590 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:54,591 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54208; closing.
2023-10-11 05:41:54,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:41:54,591 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43771', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002914.5916123')
2023-10-11 05:41:54,591 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:54,591 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:54,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:41:54,591 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:54,592 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54244; closing.
2023-10-11 05:41:54,592 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54220; closing.
2023-10-11 05:41:54,592 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45537', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002914.5926917')
2023-10-11 05:41:54,593 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002914.5930524')
2023-10-11 05:41:54,593 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54210; closing.
2023-10-11 05:41:54,593 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:54,593 - distributed.nanny - INFO - Worker closed
2023-10-11 05:41:54,593 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54236; closing.
2023-10-11 05:41:54,594 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38463', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002914.594031')
2023-10-11 05:41:54,594 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36705', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002914.5945005')
2023-10-11 05:41:54,594 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:41:56,452 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:41:56,453 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:41:56,453 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:41:56,455 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:41:56,456 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-10-11 05:41:58,596 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:41:58,601 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46649 instead
  warnings.warn(
2023-10-11 05:41:58,604 - distributed.scheduler - INFO - State start
2023-10-11 05:41:58,716 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:41:58,716 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:41:58,717 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46649/status
2023-10-11 05:41:58,717 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:41:59,035 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45253'
2023-10-11 05:41:59,055 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44599'
2023-10-11 05:41:59,065 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41665'
2023-10-11 05:41:59,081 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34035'
2023-10-11 05:41:59,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44417'
2023-10-11 05:41:59,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42201'
2023-10-11 05:41:59,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39065'
2023-10-11 05:41:59,110 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38457'
2023-10-11 05:42:00,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:00,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:00,919 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:00,920 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:00,921 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:00,924 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:00,926 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:00,926 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:00,931 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:00,984 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:00,984 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:00,985 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:00,985 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:00,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:00,986 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:00,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:00,986 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:00,989 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:00,990 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:00,990 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:00,990 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:01,026 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:01,026 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:01,031 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:03,015 - distributed.scheduler - INFO - Receive client connection: Client-e3f9c91d-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:03,027 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40786
2023-10-11 05:42:04,286 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33855
2023-10-11 05:42:04,287 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33855
2023-10-11 05:42:04,287 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46763
2023-10-11 05:42:04,287 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,287 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,288 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:04,288 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:04,288 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vvdxnkym
2023-10-11 05:42:04,288 - distributed.worker - INFO - Starting Worker plugin PreImport-7ad1fa5f-09bc-4825-8e7f-b4d4e50861e5
2023-10-11 05:42:04,288 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af3eee0f-9e1a-40f8-a146-cacbef2b9d2a
2023-10-11 05:42:04,288 - distributed.worker - INFO - Starting Worker plugin RMMSetup-418b5581-8425-4e69-a788-2cfc4044958d
2023-10-11 05:42:04,308 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35035
2023-10-11 05:42:04,309 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35035
2023-10-11 05:42:04,309 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42877
2023-10-11 05:42:04,309 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,309 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,309 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:04,310 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:04,310 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qv_k2xfp
2023-10-11 05:42:04,310 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f555e7ec-3819-4eb2-940a-c1764229bbbd
2023-10-11 05:42:04,310 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44883
2023-10-11 05:42:04,310 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44883
2023-10-11 05:42:04,311 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44711
2023-10-11 05:42:04,311 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,311 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,311 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:04,311 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:04,311 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-meb8tjy_
2023-10-11 05:42:04,311 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be05a2a6-6c6c-4900-b93f-02ebf1321827
2023-10-11 05:42:04,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45521
2023-10-11 05:42:04,324 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45521
2023-10-11 05:42:04,324 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33939
2023-10-11 05:42:04,324 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,324 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,324 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:04,324 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:04,324 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bgibmzh3
2023-10-11 05:42:04,325 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0cd460a7-4ce5-424a-ad7c-5160ffdf599e
2023-10-11 05:42:04,340 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34011
2023-10-11 05:42:04,341 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34011
2023-10-11 05:42:04,341 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43119
2023-10-11 05:42:04,341 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,341 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,341 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:04,341 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:04,341 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qcamx0rl
2023-10-11 05:42:04,342 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13562b92-480e-43e2-b32e-d4911350a326
2023-10-11 05:42:04,347 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43497
2023-10-11 05:42:04,348 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43497
2023-10-11 05:42:04,348 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43437
2023-10-11 05:42:04,348 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,348 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,348 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:04,349 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:04,349 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-phpbod55
2023-10-11 05:42:04,349 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08e983c0-36c4-4087-888b-ba84e7ee92eb
2023-10-11 05:42:04,348 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33987
2023-10-11 05:42:04,349 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33987
2023-10-11 05:42:04,349 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44765
2023-10-11 05:42:04,349 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,349 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,349 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:04,350 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:04,350 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0j6e_1wp
2023-10-11 05:42:04,350 - distributed.worker - INFO - Starting Worker plugin PreImport-403c5307-0338-44e1-95ca-97c61e451247
2023-10-11 05:42:04,350 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a55be674-57f9-45a9-bdea-e4041b8cd731
2023-10-11 05:42:04,351 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50a028b1-01d1-4b18-9e30-030ccf9c0659
2023-10-11 05:42:04,369 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45217
2023-10-11 05:42:04,371 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45217
2023-10-11 05:42:04,371 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40297
2023-10-11 05:42:04,371 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,371 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,371 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:04,372 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:04,372 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u4xnd17r
2023-10-11 05:42:04,373 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3c2ae31-18c9-4556-a26c-498ee6b31978
2023-10-11 05:42:04,375 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2224198f-9800-4c21-9f71-4ab36fb08036
2023-10-11 05:42:04,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3bb8cfd-3803-4bfb-9922-632b80170275
2023-10-11 05:42:04,379 - distributed.worker - INFO - Starting Worker plugin PreImport-2028d799-3d60-4bc2-8dbb-8f38ddb3eb33
2023-10-11 05:42:04,379 - distributed.worker - INFO - Starting Worker plugin PreImport-1934979b-79f2-4a66-85fa-a3a70962ecef
2023-10-11 05:42:04,379 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,380 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,383 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c4608b0-0053-43e6-a6b9-8fce1924d261
2023-10-11 05:42:04,383 - distributed.worker - INFO - Starting Worker plugin PreImport-07aae126-af0b-40eb-a712-dfaf20d7819f
2023-10-11 05:42:04,383 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,384 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-203fa25e-5e07-4b86-9996-fc215ab4407f
2023-10-11 05:42:04,384 - distributed.worker - INFO - Starting Worker plugin PreImport-f32008e1-0623-4d2f-b620-600f5cdad88d
2023-10-11 05:42:04,385 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,385 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,385 - distributed.worker - INFO - Starting Worker plugin PreImport-4823b71a-00a0-4a9d-a7dd-d15fadd41c4c
2023-10-11 05:42:04,386 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-78f508ff-6aa9-49db-a523-d54a8d6eeb50
2023-10-11 05:42:04,386 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,387 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b55fb468-f41b-44fe-8ba6-1d743f909704
2023-10-11 05:42:04,387 - distributed.worker - INFO - Starting Worker plugin PreImport-4ef43726-9fc6-498f-9de3-ac5ac3dd67b4
2023-10-11 05:42:04,387 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,397 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33855', status: init, memory: 0, processing: 0>
2023-10-11 05:42:04,399 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33855
2023-10-11 05:42:04,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40802
2023-10-11 05:42:04,400 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:04,401 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,401 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:04,403 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44883', status: init, memory: 0, processing: 0>
2023-10-11 05:42:04,404 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44883
2023-10-11 05:42:04,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40804
2023-10-11 05:42:04,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:04,405 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34011', status: init, memory: 0, processing: 0>
2023-10-11 05:42:04,406 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,406 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34011
2023-10-11 05:42:04,406 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,406 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40820
2023-10-11 05:42:04,407 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43497', status: init, memory: 0, processing: 0>
2023-10-11 05:42:04,407 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:04,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:04,407 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43497
2023-10-11 05:42:04,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40830
2023-10-11 05:42:04,407 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,407 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,408 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:04,409 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:04,409 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,409 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,411 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:04,412 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35035', status: init, memory: 0, processing: 0>
2023-10-11 05:42:04,413 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35035
2023-10-11 05:42:04,413 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40806
2023-10-11 05:42:04,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:04,414 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33987', status: init, memory: 0, processing: 0>
2023-10-11 05:42:04,415 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33987
2023-10-11 05:42:04,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40834
2023-10-11 05:42:04,415 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,415 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,416 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45217', status: init, memory: 0, processing: 0>
2023-10-11 05:42:04,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:04,417 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45217
2023-10-11 05:42:04,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40858
2023-10-11 05:42:04,417 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,417 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:04,418 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45521', status: init, memory: 0, processing: 0>
2023-10-11 05:42:04,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:04,419 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45521
2023-10-11 05:42:04,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40844
2023-10-11 05:42:04,419 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,419 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:04,420 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:04,421 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:04,421 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:04,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:04,423 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:04,469 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:04,469 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:04,470 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:04,470 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:04,470 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:04,470 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:04,470 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:04,470 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:04,475 - distributed.scheduler - INFO - Remove client Client-e3f9c91d-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:04,475 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40786; closing.
2023-10-11 05:42:04,476 - distributed.scheduler - INFO - Remove client Client-e3f9c91d-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:04,476 - distributed.scheduler - INFO - Close client connection: Client-e3f9c91d-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:04,477 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45253'. Reason: nanny-close
2023-10-11 05:42:04,478 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:04,479 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44599'. Reason: nanny-close
2023-10-11 05:42:04,479 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:04,479 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45521. Reason: nanny-close
2023-10-11 05:42:04,479 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41665'. Reason: nanny-close
2023-10-11 05:42:04,480 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:04,480 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35035. Reason: nanny-close
2023-10-11 05:42:04,480 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34035'. Reason: nanny-close
2023-10-11 05:42:04,480 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:04,480 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43497. Reason: nanny-close
2023-10-11 05:42:04,481 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44417'. Reason: nanny-close
2023-10-11 05:42:04,481 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:04,481 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44883. Reason: nanny-close
2023-10-11 05:42:04,481 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42201'. Reason: nanny-close
2023-10-11 05:42:04,481 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:04,481 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:04,482 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45217. Reason: nanny-close
2023-10-11 05:42:04,482 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40844; closing.
2023-10-11 05:42:04,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39065'. Reason: nanny-close
2023-10-11 05:42:04,482 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45521', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002924.4822853')
2023-10-11 05:42:04,482 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:04,482 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:04,482 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:04,482 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33987. Reason: nanny-close
2023-10-11 05:42:04,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38457'. Reason: nanny-close
2023-10-11 05:42:04,482 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:04,482 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:04,483 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33855. Reason: nanny-close
2023-10-11 05:42:04,483 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34011. Reason: nanny-close
2023-10-11 05:42:04,483 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:04,483 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:04,484 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40804; closing.
2023-10-11 05:42:04,484 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40830; closing.
2023-10-11 05:42:04,484 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:04,484 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:04,484 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40806; closing.
2023-10-11 05:42:04,484 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:04,484 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44883', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002924.4849315')
2023-10-11 05:42:04,485 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:04,485 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:04,485 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43497', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002924.4852936')
2023-10-11 05:42:04,485 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:04,485 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35035', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002924.4856334')
2023-10-11 05:42:04,486 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:04,486 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40858; closing.
2023-10-11 05:42:04,486 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:04,486 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:04,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45217', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002924.4870036')
2023-10-11 05:42:04,487 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:04,487 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40834; closing.
2023-10-11 05:42:04,487 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40802; closing.
2023-10-11 05:42:04,487 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40820; closing.
2023-10-11 05:42:04,488 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33987', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002924.4880884')
2023-10-11 05:42:04,488 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33855', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002924.4884527')
2023-10-11 05:42:04,488 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34011', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002924.4888046')
2023-10-11 05:42:04,488 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:42:05,945 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:42:05,945 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:42:05,946 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:42:05,947 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:42:05,947 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-10-11 05:42:08,348 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:08,353 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45817 instead
  warnings.warn(
2023-10-11 05:42:08,357 - distributed.scheduler - INFO - State start
2023-10-11 05:42:08,384 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:08,385 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:42:08,386 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45817/status
2023-10-11 05:42:08,386 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:42:08,485 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37041'
2023-10-11 05:42:08,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32965'
2023-10-11 05:42:08,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37805'
2023-10-11 05:42:08,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44699'
2023-10-11 05:42:08,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40271'
2023-10-11 05:42:08,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38817'
2023-10-11 05:42:08,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46193'
2023-10-11 05:42:08,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33429'
2023-10-11 05:42:09,720 - distributed.scheduler - INFO - Receive client connection: Client-e9b04496-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:09,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40996
2023-10-11 05:42:10,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:10,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:10,452 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:10,455 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:10,455 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:10,459 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:10,494 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:10,494 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:10,498 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:10,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:10,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:10,513 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:10,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:10,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:10,528 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:10,528 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:10,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:10,533 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:10,568 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:10,568 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:10,573 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:10,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:10,651 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:10,658 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:13,698 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37653
2023-10-11 05:42:13,699 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37653
2023-10-11 05:42:13,699 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45819
2023-10-11 05:42:13,699 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:13,699 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:13,699 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:13,699 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:13,699 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2pj90890
2023-10-11 05:42:13,700 - distributed.worker - INFO - Starting Worker plugin PreImport-58809436-7205-444d-8f33-75b1a9d7f2a9
2023-10-11 05:42:13,700 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3d7ae68-c704-4498-9eee-23231105d855
2023-10-11 05:42:13,700 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64a48ac4-f953-4a4e-b089-f58c0be11fb3
2023-10-11 05:42:13,718 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40227
2023-10-11 05:42:13,719 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40227
2023-10-11 05:42:13,718 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45107
2023-10-11 05:42:13,719 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33117
2023-10-11 05:42:13,719 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45107
2023-10-11 05:42:13,719 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:13,719 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39353
2023-10-11 05:42:13,719 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:13,719 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:13,719 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:13,719 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:13,719 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:13,719 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bonr957n
2023-10-11 05:42:13,719 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:13,719 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:13,719 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g308_cfz
2023-10-11 05:42:13,719 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38335
2023-10-11 05:42:13,719 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38335
2023-10-11 05:42:13,719 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42825
2023-10-11 05:42:13,719 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:13,720 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:13,720 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d38c4fcb-a114-4511-aed3-3e25b2e06c5b
2023-10-11 05:42:13,720 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:13,720 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed3b693f-10d7-46ab-90fb-8f061aadfbc2
2023-10-11 05:42:13,720 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:13,720 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vcqk6wvh
2023-10-11 05:42:13,720 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cb880b15-850d-4d53-ab12-31e378081eca
2023-10-11 05:42:13,724 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44587
2023-10-11 05:42:13,725 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44587
2023-10-11 05:42:13,725 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34347
2023-10-11 05:42:13,725 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:13,725 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:13,725 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:13,725 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:13,725 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ghfoiu1
2023-10-11 05:42:13,726 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f437640f-027d-46e8-9058-339bb14ab9bd
2023-10-11 05:42:13,741 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43039
2023-10-11 05:42:13,742 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43039
2023-10-11 05:42:13,742 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34079
2023-10-11 05:42:13,742 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:13,742 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:13,741 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35885
2023-10-11 05:42:13,743 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:13,743 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35885
2023-10-11 05:42:13,743 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:13,743 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z43ofxxo
2023-10-11 05:42:13,743 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46539
2023-10-11 05:42:13,743 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:13,743 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:13,743 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:13,743 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:13,743 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c3fin_z1
2023-10-11 05:42:13,744 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fe51219-3bb0-4606-ba62-9ff81ff4b555
2023-10-11 05:42:13,744 - distributed.worker - INFO - Starting Worker plugin RMMSetup-126d9a97-ced7-4aee-bb23-72ea1563763e
2023-10-11 05:42:13,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42695
2023-10-11 05:42:13,752 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42695
2023-10-11 05:42:13,752 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41105
2023-10-11 05:42:13,752 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:13,752 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:13,752 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:13,752 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:13,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5xyvgarm
2023-10-11 05:42:13,753 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ddd7e39c-69f5-42ba-b0ca-df9e33c8635e
2023-10-11 05:42:15,150 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29808c40-d140-4efa-a630-14780a1b6e1b
2023-10-11 05:42:15,151 - distributed.worker - INFO - Starting Worker plugin PreImport-ad0287ed-a122-4b50-af4a-e5c5513ddd0e
2023-10-11 05:42:15,151 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,156 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fbb20816-11f1-41ad-8a7a-b1fec26887eb
2023-10-11 05:42:15,157 - distributed.worker - INFO - Starting Worker plugin PreImport-c16f6ed6-49e9-49e1-8018-de92cfc6705a
2023-10-11 05:42:15,157 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,163 - distributed.worker - INFO - Starting Worker plugin PreImport-643b0ba4-c158-4657-a1c2-59e5352db025
2023-10-11 05:42:15,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57304279-196e-443b-a366-0d2a96487cef
2023-10-11 05:42:15,165 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,168 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,179 - distributed.worker - INFO - Starting Worker plugin PreImport-2b69087a-62b8-4548-9b51-265bfcd2abfa
2023-10-11 05:42:15,180 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d813c8f1-2359-4eb4-a5e9-3cca0eda469d
2023-10-11 05:42:15,180 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,190 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35885', status: init, memory: 0, processing: 0>
2023-10-11 05:42:15,191 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35885
2023-10-11 05:42:15,192 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47328
2023-10-11 05:42:15,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:15,194 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:15,194 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,195 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:15,196 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efb84ebc-cd25-4cad-8b57-6a01d1230b75
2023-10-11 05:42:15,196 - distributed.worker - INFO - Starting Worker plugin PreImport-451c2e6f-42e9-4475-a58d-29620871589e
2023-10-11 05:42:15,196 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,197 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37653', status: init, memory: 0, processing: 0>
2023-10-11 05:42:15,198 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d9e5b631-90aa-4e99-bdc8-ff07c24900e7
2023-10-11 05:42:15,198 - distributed.worker - INFO - Starting Worker plugin PreImport-b7d56fff-9cf8-4554-b489-c9c2da60a2c9
2023-10-11 05:42:15,198 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37653
2023-10-11 05:42:15,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47354
2023-10-11 05:42:15,199 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,199 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:15,200 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40227', status: init, memory: 0, processing: 0>
2023-10-11 05:42:15,200 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40227
2023-10-11 05:42:15,200 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47342
2023-10-11 05:42:15,200 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:15,200 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,201 - distributed.worker - INFO - Starting Worker plugin PreImport-a2786a60-c834-4b07-a9a7-3961d958c37c
2023-10-11 05:42:15,202 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c721c300-b3ef-485f-aa0c-c67befe66f25
2023-10-11 05:42:15,202 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:15,202 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:15,202 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,203 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:15,203 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,205 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:15,209 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45107', status: init, memory: 0, processing: 0>
2023-10-11 05:42:15,209 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45107
2023-10-11 05:42:15,210 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47348
2023-10-11 05:42:15,211 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:15,212 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:15,212 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:15,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38335', status: init, memory: 0, processing: 0>
2023-10-11 05:42:15,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38335
2023-10-11 05:42:15,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47366
2023-10-11 05:42:15,219 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:15,220 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:15,220 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:15,229 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44587', status: init, memory: 0, processing: 0>
2023-10-11 05:42:15,229 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44587
2023-10-11 05:42:15,229 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47382
2023-10-11 05:42:15,230 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:15,231 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:15,231 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:15,243 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43039', status: init, memory: 0, processing: 0>
2023-10-11 05:42:15,243 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43039
2023-10-11 05:42:15,243 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47392
2023-10-11 05:42:15,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:15,246 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:15,246 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,247 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42695', status: init, memory: 0, processing: 0>
2023-10-11 05:42:15,247 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42695
2023-10-11 05:42:15,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47394
2023-10-11 05:42:15,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:15,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:15,249 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:15,250 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:15,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:15,277 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:15,277 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:15,278 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:15,278 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:15,278 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:15,278 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:15,278 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:15,279 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:15,289 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:15,289 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:15,289 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:15,289 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:15,290 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:15,290 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:15,290 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:15,290 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:15,296 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:15,298 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:15,300 - distributed.scheduler - INFO - Remove client Client-e9b04496-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:15,300 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40996; closing.
2023-10-11 05:42:15,301 - distributed.scheduler - INFO - Remove client Client-e9b04496-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:15,301 - distributed.scheduler - INFO - Close client connection: Client-e9b04496-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:15,302 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37041'. Reason: nanny-close
2023-10-11 05:42:15,302 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:15,303 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32965'. Reason: nanny-close
2023-10-11 05:42:15,303 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:15,304 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38335. Reason: nanny-close
2023-10-11 05:42:15,304 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37805'. Reason: nanny-close
2023-10-11 05:42:15,304 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:15,304 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45107. Reason: nanny-close
2023-10-11 05:42:15,304 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44699'. Reason: nanny-close
2023-10-11 05:42:15,305 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:15,305 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44587. Reason: nanny-close
2023-10-11 05:42:15,305 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40271'. Reason: nanny-close
2023-10-11 05:42:15,305 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:15,305 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37653. Reason: nanny-close
2023-10-11 05:42:15,306 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38817'. Reason: nanny-close
2023-10-11 05:42:15,306 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:15,306 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:15,306 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47366; closing.
2023-10-11 05:42:15,306 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40227. Reason: nanny-close
2023-10-11 05:42:15,306 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46193'. Reason: nanny-close
2023-10-11 05:42:15,307 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38335', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002935.3069525')
2023-10-11 05:42:15,307 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:15,307 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:15,307 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43039. Reason: nanny-close
2023-10-11 05:42:15,307 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33429'. Reason: nanny-close
2023-10-11 05:42:15,307 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:15,307 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:15,308 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35885. Reason: nanny-close
2023-10-11 05:42:15,308 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:15,308 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:15,308 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:15,308 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47354; closing.
2023-10-11 05:42:15,308 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47382; closing.
2023-10-11 05:42:15,309 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42695. Reason: nanny-close
2023-10-11 05:42:15,309 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37653', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002935.3094857')
2023-10-11 05:42:15,309 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:15,309 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:15,309 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002935.309844')
2023-10-11 05:42:15,310 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:15,310 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:15,310 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47348; closing.
2023-10-11 05:42:15,310 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:15,310 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45107', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002935.3107438')
2023-10-11 05:42:15,311 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47342; closing.
2023-10-11 05:42:15,311 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:15,311 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:15,311 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:15,311 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40227', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002935.3117876')
2023-10-11 05:42:15,312 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47392; closing.
2023-10-11 05:42:15,312 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:15,312 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47328; closing.
2023-10-11 05:42:15,312 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:15,312 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43039', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002935.3127506')
2023-10-11 05:42:15,313 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35885', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002935.3131056')
2023-10-11 05:42:15,313 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47394; closing.
2023-10-11 05:42:15,313 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42695', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002935.3137946')
2023-10-11 05:42:15,313 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:42:17,020 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:42:17,020 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:42:17,020 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:42:17,022 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:42:17,022 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-10-11 05:42:19,198 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:19,203 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41429 instead
  warnings.warn(
2023-10-11 05:42:19,207 - distributed.scheduler - INFO - State start
2023-10-11 05:42:19,229 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:19,230 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:42:19,231 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41429/status
2023-10-11 05:42:19,231 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:42:19,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44929'
2023-10-11 05:42:19,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40295'
2023-10-11 05:42:19,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37245'
2023-10-11 05:42:19,438 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36281'
2023-10-11 05:42:19,440 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40021'
2023-10-11 05:42:19,449 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38791'
2023-10-11 05:42:19,458 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38361'
2023-10-11 05:42:19,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39919'
2023-10-11 05:42:21,613 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:21,613 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:21,617 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:21,625 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:21,625 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:21,629 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:21,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:21,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:21,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:21,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:21,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:21,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:21,678 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:21,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:21,681 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:21,681 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:21,683 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:21,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:21,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:21,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:21,690 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:21,690 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:21,693 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:21,695 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:23,345 - distributed.scheduler - INFO - Receive client connection: Client-f030ba69-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:23,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49466
2023-10-11 05:42:26,032 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40445
2023-10-11 05:42:26,033 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40445
2023-10-11 05:42:26,033 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37285
2023-10-11 05:42:26,033 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,034 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,034 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:26,034 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:26,034 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bk8vcrvg
2023-10-11 05:42:26,034 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2aa1f6e-50f1-4840-8119-3190e2cd938c
2023-10-11 05:42:26,043 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41957
2023-10-11 05:42:26,044 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41957
2023-10-11 05:42:26,044 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40557
2023-10-11 05:42:26,044 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,044 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,044 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:26,044 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:26,044 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1qx7fmow
2023-10-11 05:42:26,045 - distributed.worker - INFO - Starting Worker plugin RMMSetup-730cf9c8-1f92-4253-ad73-5ccb751bbfb4
2023-10-11 05:42:26,045 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46189
2023-10-11 05:42:26,045 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46189
2023-10-11 05:42:26,045 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38497
2023-10-11 05:42:26,045 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,046 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,046 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:26,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:26,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7dfwtbgu
2023-10-11 05:42:26,046 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-23fe0ae6-fbc8-499f-84f6-abe99e4b5cc1
2023-10-11 05:42:26,047 - distributed.worker - INFO - Starting Worker plugin PreImport-01a39c82-2e9f-41f6-9b8a-50633c17879f
2023-10-11 05:42:26,047 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70a4e83f-4d2f-41da-8d1a-f40c5a0c3279
2023-10-11 05:42:26,085 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33499
2023-10-11 05:42:26,086 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33499
2023-10-11 05:42:26,086 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42831
2023-10-11 05:42:26,086 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,086 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,086 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:26,086 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:26,086 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v4xc69cs
2023-10-11 05:42:26,087 - distributed.worker - INFO - Starting Worker plugin PreImport-b5fd27ea-1fc0-4d6b-a32f-956892e1cced
2023-10-11 05:42:26,087 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8f0df440-e2e5-49b3-b54f-31f5faa7f6b1
2023-10-11 05:42:26,087 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ffb1876a-f519-4749-b4cb-d57ad77ae1fc
2023-10-11 05:42:26,090 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46555
2023-10-11 05:42:26,091 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46555
2023-10-11 05:42:26,091 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46101
2023-10-11 05:42:26,091 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,091 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,091 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:26,091 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:26,091 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qdigguse
2023-10-11 05:42:26,092 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b065f87-77e5-4929-9b7b-79d0b86187ab
2023-10-11 05:42:26,094 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36067
2023-10-11 05:42:26,095 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36067
2023-10-11 05:42:26,095 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41245
2023-10-11 05:42:26,095 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34765
2023-10-11 05:42:26,095 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,096 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,096 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34765
2023-10-11 05:42:26,096 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43319
2023-10-11 05:42:26,096 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,096 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:26,096 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,096 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:26,096 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3sq82bxm
2023-10-11 05:42:26,096 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:26,096 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:26,096 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8z65jhhi
2023-10-11 05:42:26,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e75dc209-cd60-4f0c-9e2a-ab166810812a
2023-10-11 05:42:26,097 - distributed.worker - INFO - Starting Worker plugin PreImport-c7938e28-2d38-4347-9ee4-d4719d90240a
2023-10-11 05:42:26,097 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96e6c95c-8cee-4a75-bdee-07a689dd93a1
2023-10-11 05:42:26,098 - distributed.worker - INFO - Starting Worker plugin RMMSetup-06633e08-edff-485b-bd71-4cf1492c0bb5
2023-10-11 05:42:26,097 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42343
2023-10-11 05:42:26,098 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42343
2023-10-11 05:42:26,098 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44359
2023-10-11 05:42:26,098 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,098 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,098 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:26,098 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:26,099 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-58sip913
2023-10-11 05:42:26,099 - distributed.worker - INFO - Starting Worker plugin RMMSetup-322eba25-ec35-4f90-a4c6-ef5da7ee4094
2023-10-11 05:42:26,312 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-327d450f-c90d-4570-b2e0-4e522b9d9d27
2023-10-11 05:42:26,314 - distributed.worker - INFO - Starting Worker plugin PreImport-597cd8c2-d7e4-4352-9070-f3d172b95cef
2023-10-11 05:42:26,315 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,316 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-50c75dd3-b69f-477a-ac87-1975d3bc0f4f
2023-10-11 05:42:26,317 - distributed.worker - INFO - Starting Worker plugin PreImport-12aa51fd-c684-4df6-a1b5-1c866f2f9edb
2023-10-11 05:42:26,317 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,323 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,325 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65b1a3f6-3c8f-493f-b2dc-6b4091126ec4
2023-10-11 05:42:26,326 - distributed.worker - INFO - Starting Worker plugin PreImport-94cf9846-822f-4c0d-8103-d049e4f1610b
2023-10-11 05:42:26,326 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,327 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,332 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a03129ce-a0e5-443c-88e2-1ff0384ea89c
2023-10-11 05:42:26,333 - distributed.worker - INFO - Starting Worker plugin PreImport-23cb723a-1df5-4afa-9bbd-e904f4c511d5
2023-10-11 05:42:26,333 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,338 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,339 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a0e7696-49d2-4203-a23f-66a79d21ec67
2023-10-11 05:42:26,340 - distributed.worker - INFO - Starting Worker plugin PreImport-bc8bcba8-09bc-42f4-b0f8-4e9e058f33f8
2023-10-11 05:42:26,341 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,342 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40445', status: init, memory: 0, processing: 0>
2023-10-11 05:42:26,344 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40445
2023-10-11 05:42:26,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49482
2023-10-11 05:42:26,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:26,345 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,345 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:26,348 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41957', status: init, memory: 0, processing: 0>
2023-10-11 05:42:26,348 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41957
2023-10-11 05:42:26,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49480
2023-10-11 05:42:26,349 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:26,350 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,350 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,351 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46555', status: init, memory: 0, processing: 0>
2023-10-11 05:42:26,352 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46555
2023-10-11 05:42:26,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49496
2023-10-11 05:42:26,353 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33499', status: init, memory: 0, processing: 0>
2023-10-11 05:42:26,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:26,353 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:26,353 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33499
2023-10-11 05:42:26,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49488
2023-10-11 05:42:26,354 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,354 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,354 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:26,355 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,355 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,355 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:26,355 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42343', status: init, memory: 0, processing: 0>
2023-10-11 05:42:26,356 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42343
2023-10-11 05:42:26,356 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49518
2023-10-11 05:42:26,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:26,357 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:26,358 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,358 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,360 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:26,363 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46189', status: init, memory: 0, processing: 0>
2023-10-11 05:42:26,364 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46189
2023-10-11 05:42:26,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49502
2023-10-11 05:42:26,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:26,367 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,367 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,369 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:26,372 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34765', status: init, memory: 0, processing: 0>
2023-10-11 05:42:26,372 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34765
2023-10-11 05:42:26,373 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49534
2023-10-11 05:42:26,374 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:26,375 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,375 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,375 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36067', status: init, memory: 0, processing: 0>
2023-10-11 05:42:26,376 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36067
2023-10-11 05:42:26,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49538
2023-10-11 05:42:26,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:26,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:26,378 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:26,379 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:26,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:26,453 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:26,453 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:26,453 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:26,453 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:26,453 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:26,454 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:26,454 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:26,454 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:26,466 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:26,466 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:26,466 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:26,466 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:26,466 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:26,467 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:26,467 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:26,467 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:42:26,475 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:26,477 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:26,480 - distributed.scheduler - INFO - Remove client Client-f030ba69-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:26,480 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49466; closing.
2023-10-11 05:42:26,480 - distributed.scheduler - INFO - Remove client Client-f030ba69-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:26,480 - distributed.scheduler - INFO - Close client connection: Client-f030ba69-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:26,482 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44929'. Reason: nanny-close
2023-10-11 05:42:26,482 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:26,484 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40295'. Reason: nanny-close
2023-10-11 05:42:26,484 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:26,484 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46189. Reason: nanny-close
2023-10-11 05:42:26,484 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37245'. Reason: nanny-close
2023-10-11 05:42:26,485 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:26,485 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41957. Reason: nanny-close
2023-10-11 05:42:26,485 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36281'. Reason: nanny-close
2023-10-11 05:42:26,485 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:26,486 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46555. Reason: nanny-close
2023-10-11 05:42:26,486 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40021'. Reason: nanny-close
2023-10-11 05:42:26,486 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:26,486 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42343. Reason: nanny-close
2023-10-11 05:42:26,486 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49502; closing.
2023-10-11 05:42:26,486 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38791'. Reason: nanny-close
2023-10-11 05:42:26,487 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:26,487 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:26,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46189', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002946.4872158')
2023-10-11 05:42:26,487 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36067. Reason: nanny-close
2023-10-11 05:42:26,487 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:26,487 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38361'. Reason: nanny-close
2023-10-11 05:42:26,487 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:26,487 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:26,488 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34765. Reason: nanny-close
2023-10-11 05:42:26,488 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49496; closing.
2023-10-11 05:42:26,488 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39919'. Reason: nanny-close
2023-10-11 05:42:26,488 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:26,488 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:26,488 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33499. Reason: nanny-close
2023-10-11 05:42:26,488 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:26,489 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46555', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002946.4891605')
2023-10-11 05:42:26,489 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:26,489 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:26,489 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49480; closing.
2023-10-11 05:42:26,489 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:26,489 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40445. Reason: nanny-close
2023-10-11 05:42:26,490 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:26,490 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:26,491 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:26,490 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49496>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49496>: Stream is closed
2023-10-11 05:42:26,491 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49518; closing.
2023-10-11 05:42:26,491 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:26,491 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41957', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002946.4919195')
2023-10-11 05:42:26,492 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:26,492 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:26,492 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42343', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002946.4924953')
2023-10-11 05:42:26,492 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:26,493 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49538; closing.
2023-10-11 05:42:26,493 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49534; closing.
2023-10-11 05:42:26,493 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:26,493 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49488; closing.
2023-10-11 05:42:26,494 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36067', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002946.4943137')
2023-10-11 05:42:26,494 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34765', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002946.4946945')
2023-10-11 05:42:26,495 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33499', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002946.4950695')
2023-10-11 05:42:26,495 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49482; closing.
2023-10-11 05:42:26,495 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40445', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002946.4958904')
2023-10-11 05:42:26,496 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:42:26,496 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49482>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-11 05:42:27,999 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:42:27,999 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:42:28,000 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:42:28,001 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:42:28,002 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-10-11 05:42:30,055 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:30,059 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42177 instead
  warnings.warn(
2023-10-11 05:42:30,063 - distributed.scheduler - INFO - State start
2023-10-11 05:42:30,277 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:30,278 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:42:30,278 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42177/status
2023-10-11 05:42:30,279 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:42:31,116 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44833'
2023-10-11 05:42:31,130 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41009'
2023-10-11 05:42:31,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43135'
2023-10-11 05:42:31,155 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40291'
2023-10-11 05:42:31,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44589'
2023-10-11 05:42:31,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43597'
2023-10-11 05:42:31,167 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39107'
2023-10-11 05:42:31,183 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42387'
2023-10-11 05:42:31,287 - distributed.scheduler - INFO - Receive client connection: Client-f6b8e2d0-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:31,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57658
2023-10-11 05:42:33,810 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:33,811 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:33,815 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:33,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:33,821 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:33,826 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:33,861 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:33,861 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:33,866 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:33,867 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:33,867 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:33,871 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:33,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:33,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:33,878 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:33,887 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:33,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:33,892 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:33,909 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:33,909 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:33,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:33,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:33,914 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:33,914 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:37,976 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44073
2023-10-11 05:42:37,977 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44073
2023-10-11 05:42:37,977 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45439
2023-10-11 05:42:37,977 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:37,977 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:37,977 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:37,977 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:37,977 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w8hluhcf
2023-10-11 05:42:37,978 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9aab3895-bf81-4f32-b60d-dad67abd0480
2023-10-11 05:42:37,978 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b244c94-733a-405c-a6b4-38568f236beb
2023-10-11 05:42:37,986 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33219
2023-10-11 05:42:37,987 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33219
2023-10-11 05:42:37,987 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44027
2023-10-11 05:42:37,987 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:37,987 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:37,987 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:37,987 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:37,987 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dpm9_6lm
2023-10-11 05:42:37,988 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9db74644-0562-41c9-847f-52b06dfc93b8
2023-10-11 05:42:37,992 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46667
2023-10-11 05:42:37,992 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46667
2023-10-11 05:42:37,992 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33193
2023-10-11 05:42:37,992 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46595
2023-10-11 05:42:37,992 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:37,993 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46595
2023-10-11 05:42:37,993 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:37,993 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42873
2023-10-11 05:42:37,993 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:37,993 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:37,993 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:37,993 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:37,993 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hzkyjsga
2023-10-11 05:42:37,993 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:37,993 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:37,993 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-okfl1471
2023-10-11 05:42:37,993 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c44f5b52-41cb-4df0-8f1b-2e7b97ed6370
2023-10-11 05:42:37,994 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d51bbc7c-0f13-42b9-b28d-52fffd44a97f
2023-10-11 05:42:38,824 - distributed.worker - INFO - Starting Worker plugin PreImport-a8fdddeb-3497-4ee9-a56d-ec3ae9fb9610
2023-10-11 05:42:38,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bdae9b3a-ca37-4b13-8c5e-1396300ed740
2023-10-11 05:42:38,824 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:38,849 - distributed.worker - INFO - Starting Worker plugin PreImport-23c7d244-ad1b-42a9-b6dc-fd36fb4dff06
2023-10-11 05:42:38,849 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,002 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae28ac4f-802e-4c04-87f0-8b43bd802fb2
2023-10-11 05:42:39,002 - distributed.worker - INFO - Starting Worker plugin PreImport-30917435-77b9-478a-a3bf-561463ad0d7e
2023-10-11 05:42:39,002 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,009 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a888712b-e608-4635-b1f3-a8d4b5f20736
2023-10-11 05:42:39,009 - distributed.worker - INFO - Starting Worker plugin PreImport-5b030409-8280-467e-bba4-e40332e1d479
2023-10-11 05:42:39,010 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,016 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33219', status: init, memory: 0, processing: 0>
2023-10-11 05:42:39,020 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33219
2023-10-11 05:42:39,020 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57692
2023-10-11 05:42:39,021 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44073', status: init, memory: 0, processing: 0>
2023-10-11 05:42:39,021 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:39,021 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44073
2023-10-11 05:42:39,021 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57706
2023-10-11 05:42:39,021 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,022 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,023 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:39,023 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:39,023 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,024 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:39,194 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46667', status: init, memory: 0, processing: 0>
2023-10-11 05:42:39,194 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46667
2023-10-11 05:42:39,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57714
2023-10-11 05:42:39,195 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46595', status: init, memory: 0, processing: 0>
2023-10-11 05:42:39,196 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46595
2023-10-11 05:42:39,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57708
2023-10-11 05:42:39,196 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:39,197 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:39,197 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,197 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,198 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,198 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:39,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:39,203 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45377
2023-10-11 05:42:39,204 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45377
2023-10-11 05:42:39,204 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43691
2023-10-11 05:42:39,204 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,204 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,204 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:39,204 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:39,204 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kod4gesx
2023-10-11 05:42:39,204 - distributed.worker - INFO - Starting Worker plugin PreImport-afd2bd8e-efe9-40c8-af73-d5e20199ebee
2023-10-11 05:42:39,205 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c167f1c-cb0a-4347-ae71-30630296f85d
2023-10-11 05:42:39,205 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea67436f-ca05-41ac-85a9-890988aa9e86
2023-10-11 05:42:39,212 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41625
2023-10-11 05:42:39,213 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41625
2023-10-11 05:42:39,213 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43865
2023-10-11 05:42:39,213 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,213 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,213 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:39,213 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:39,213 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ykmibm3l
2023-10-11 05:42:39,214 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f546a6dd-807b-48a7-9f5c-2299811fc973
2023-10-11 05:42:39,221 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45413
2023-10-11 05:42:39,222 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45413
2023-10-11 05:42:39,222 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39197
2023-10-11 05:42:39,222 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,222 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,222 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:39,222 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:39,222 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2z4fscbf
2023-10-11 05:42:39,222 - distributed.worker - INFO - Starting Worker plugin RMMSetup-401fe408-cd7e-4156-9ece-b6985c739967
2023-10-11 05:42:39,399 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39995
2023-10-11 05:42:39,400 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39995
2023-10-11 05:42:39,400 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38671
2023-10-11 05:42:39,400 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,400 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,400 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:39,401 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:42:39,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qzaxm6ur
2023-10-11 05:42:39,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85d9ba72-af70-47c6-9a36-3115b6264dd9
2023-10-11 05:42:39,611 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f54a0d97-b7b9-42d3-b1d3-2b008b5b939c
2023-10-11 05:42:39,612 - distributed.worker - INFO - Starting Worker plugin PreImport-d0ac6d99-340e-485b-9c61-6caae9250a68
2023-10-11 05:42:39,612 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,645 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a50bc6d7-efb1-4c97-b4a5-5941b87e1a03
2023-10-11 05:42:39,645 - distributed.worker - INFO - Starting Worker plugin PreImport-7a180d9b-be9c-4206-be32-df298bd57e72
2023-10-11 05:42:39,645 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,654 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,658 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41625', status: init, memory: 0, processing: 0>
2023-10-11 05:42:39,659 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41625
2023-10-11 05:42:39,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57718
2023-10-11 05:42:39,661 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:39,663 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,663 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:39,677 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45413', status: init, memory: 0, processing: 0>
2023-10-11 05:42:39,677 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45413
2023-10-11 05:42:39,678 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57734
2023-10-11 05:42:39,678 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:39,679 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,679 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:39,695 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45377', status: init, memory: 0, processing: 0>
2023-10-11 05:42:39,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45377
2023-10-11 05:42:39,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57738
2023-10-11 05:42:39,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:39,698 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,698 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:39,704 - distributed.worker - INFO - Starting Worker plugin PreImport-d811e443-06ff-47b0-a2ba-5b11646998c5
2023-10-11 05:42:39,704 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0fd5927a-d3da-4c00-aeba-08a5c2c5c0c7
2023-10-11 05:42:39,704 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,751 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39995', status: init, memory: 0, processing: 0>
2023-10-11 05:42:39,751 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39995
2023-10-11 05:42:39,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57742
2023-10-11 05:42:39,753 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:39,754 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:39,754 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:39,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:39,816 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:39,817 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:39,817 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:39,817 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:39,817 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:39,817 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:39,817 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:39,818 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:42:39,822 - distributed.scheduler - INFO - Remove client Client-f6b8e2d0-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:39,823 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57658; closing.
2023-10-11 05:42:39,823 - distributed.scheduler - INFO - Remove client Client-f6b8e2d0-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:39,823 - distributed.scheduler - INFO - Close client connection: Client-f6b8e2d0-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:39,824 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44833'. Reason: nanny-close
2023-10-11 05:42:39,825 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:39,825 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41009'. Reason: nanny-close
2023-10-11 05:42:39,825 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:39,826 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43135'. Reason: nanny-close
2023-10-11 05:42:39,826 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39995. Reason: nanny-close
2023-10-11 05:42:39,826 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:39,826 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33219. Reason: nanny-close
2023-10-11 05:42:39,826 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40291'. Reason: nanny-close
2023-10-11 05:42:39,827 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:39,827 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46667. Reason: nanny-close
2023-10-11 05:42:39,827 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44589'. Reason: nanny-close
2023-10-11 05:42:39,827 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:39,827 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43597'. Reason: nanny-close
2023-10-11 05:42:39,827 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45377. Reason: nanny-close
2023-10-11 05:42:39,828 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:39,828 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45413. Reason: nanny-close
2023-10-11 05:42:39,828 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:39,828 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39107'. Reason: nanny-close
2023-10-11 05:42:39,828 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57692; closing.
2023-10-11 05:42:39,828 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:39,828 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:39,828 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46595. Reason: nanny-close
2023-10-11 05:42:39,828 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33219', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002959.8287783')
2023-10-11 05:42:39,828 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42387'. Reason: nanny-close
2023-10-11 05:42:39,829 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:39,829 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41625. Reason: nanny-close
2023-10-11 05:42:39,829 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:39,830 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44073. Reason: nanny-close
2023-10-11 05:42:39,830 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:39,830 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:39,830 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:39,830 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:39,830 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57742; closing.
2023-10-11 05:42:39,830 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:39,831 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:39,832 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:39,832 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57714; closing.
2023-10-11 05:42:39,832 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39995', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002959.8324056')
2023-10-11 05:42:39,832 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:39,832 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:39,833 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:39,833 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:39,834 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46667', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002959.8341877')
2023-10-11 05:42:39,834 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:39,834 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57738; closing.
2023-10-11 05:42:39,835 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57734; closing.
2023-10-11 05:42:39,835 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57708; closing.
2023-10-11 05:42:39,835 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:39,835 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45377', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002959.8358104')
2023-10-11 05:42:39,836 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45413', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002959.836175')
2023-10-11 05:42:39,836 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46595', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002959.8365228')
2023-10-11 05:42:39,836 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57706; closing.
2023-10-11 05:42:39,837 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57718; closing.
2023-10-11 05:42:39,837 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44073', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002959.8374794')
2023-10-11 05:42:39,837 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002959.8379354')
2023-10-11 05:42:39,838 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:42:41,592 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:42:41,592 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:42:41,593 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:42:41,595 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:42:41,595 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-10-11 05:42:43,984 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:43,988 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43603 instead
  warnings.warn(
2023-10-11 05:42:43,991 - distributed.scheduler - INFO - State start
2023-10-11 05:42:44,014 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:44,015 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:42:44,016 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43603/status
2023-10-11 05:42:44,016 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:42:44,057 - distributed.scheduler - INFO - Receive client connection: Client-feea7a76-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:44,069 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44502
2023-10-11 05:42:44,221 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41341'
2023-10-11 05:42:45,943 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:45,943 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:46,479 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:47,411 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42531
2023-10-11 05:42:47,412 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42531
2023-10-11 05:42:47,412 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-10-11 05:42:47,412 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:47,412 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:47,412 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:47,412 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-11 05:42:47,412 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-witwwg92
2023-10-11 05:42:47,413 - distributed.worker - INFO - Starting Worker plugin PreImport-090f3f06-15c9-4e59-b1db-ec5b2a367665
2023-10-11 05:42:47,413 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-834df5ee-a8f7-488b-8049-8d003ee3196f
2023-10-11 05:42:47,413 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf64475b-c3f6-4406-adb0-0d69d1470ad8
2023-10-11 05:42:47,414 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:47,443 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42531', status: init, memory: 0, processing: 0>
2023-10-11 05:42:47,444 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42531
2023-10-11 05:42:47,444 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44526
2023-10-11 05:42:47,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:47,446 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:47,446 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:47,447 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:47,625 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:47,628 - distributed.scheduler - INFO - Remove client Client-feea7a76-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:47,628 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44502; closing.
2023-10-11 05:42:47,628 - distributed.scheduler - INFO - Remove client Client-feea7a76-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:47,629 - distributed.scheduler - INFO - Close client connection: Client-feea7a76-67f8-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:47,629 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41341'. Reason: nanny-close
2023-10-11 05:42:47,630 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:47,631 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42531. Reason: nanny-close
2023-10-11 05:42:47,633 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44526; closing.
2023-10-11 05:42:47,633 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:47,633 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42531', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002967.633657')
2023-10-11 05:42:47,634 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:42:47,634 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:48,746 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:42:48,746 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:42:48,747 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:42:48,748 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:42:48,748 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-10-11 05:42:52,833 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:52,838 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37609 instead
  warnings.warn(
2023-10-11 05:42:52,842 - distributed.scheduler - INFO - State start
2023-10-11 05:42:52,864 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:52,865 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:42:52,866 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37609/status
2023-10-11 05:42:52,866 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:42:53,002 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36025'
2023-10-11 05:42:54,665 - distributed.scheduler - INFO - Receive client connection: Client-04309565-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:54,677 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47340
2023-10-11 05:42:54,684 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:42:54,684 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:42:55,235 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:42:56,065 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46513
2023-10-11 05:42:56,066 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46513
2023-10-11 05:42:56,066 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40051
2023-10-11 05:42:56,066 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:42:56,066 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:56,067 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:42:56,067 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-11 05:42:56,067 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a3_uuuud
2023-10-11 05:42:56,067 - distributed.worker - INFO - Starting Worker plugin PreImport-ea077a45-aa16-48a2-b55a-1806d0b3103f
2023-10-11 05:42:56,068 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b7461812-aba1-4fff-9e65-e96b2c09a814
2023-10-11 05:42:56,069 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f265a9be-114d-4e32-a43e-98be62ba8b2d
2023-10-11 05:42:56,069 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:56,097 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46513', status: init, memory: 0, processing: 0>
2023-10-11 05:42:56,098 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46513
2023-10-11 05:42:56,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47372
2023-10-11 05:42:56,099 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:42:56,100 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:42:56,100 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:42:56,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:42:56,134 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:42:56,137 - distributed.scheduler - INFO - Remove client Client-04309565-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:56,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47340; closing.
2023-10-11 05:42:56,138 - distributed.scheduler - INFO - Remove client Client-04309565-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:56,139 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36025'. Reason: nanny-close
2023-10-11 05:42:56,139 - distributed.scheduler - INFO - Close client connection: Client-04309565-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:42:56,139 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:42:56,140 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46513. Reason: nanny-close
2023-10-11 05:42:56,142 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:42:56,142 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47372; closing.
2023-10-11 05:42:56,143 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002976.1431212')
2023-10-11 05:42:56,143 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:42:56,144 - distributed.nanny - INFO - Worker closed
2023-10-11 05:42:57,556 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:42:57,556 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:42:57,557 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:42:57,558 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:42:57,558 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-10-11 05:42:59,795 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:42:59,801 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40583 instead
  warnings.warn(
2023-10-11 05:42:59,807 - distributed.scheduler - INFO - State start
2023-10-11 05:43:00,005 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:43:00,006 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:43:00,006 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40583/status
2023-10-11 05:43:00,007 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:43:04,426 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:37432'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37432>: Stream is closed
2023-10-11 05:43:04,774 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:43:04,774 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:43:04,774 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:43:04,775 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:43:04,776 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-10-11 05:43:07,015 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:43:07,019 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42501 instead
  warnings.warn(
2023-10-11 05:43:07,023 - distributed.scheduler - INFO - State start
2023-10-11 05:43:07,046 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:43:07,047 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-11 05:43:07,048 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42501/status
2023-10-11 05:43:07,048 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:43:07,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36523'
2023-10-11 05:43:07,891 - distributed.scheduler - INFO - Receive client connection: Client-0ca2f84d-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:07,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40130
2023-10-11 05:43:09,058 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:09,059 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:09,063 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:10,303 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39743
2023-10-11 05:43:10,303 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39743
2023-10-11 05:43:10,303 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40461
2023-10-11 05:43:10,304 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-11 05:43:10,304 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:10,304 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:10,304 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-11 05:43:10,304 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-j8klt4bg
2023-10-11 05:43:10,304 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3a14510b-2fe9-4c52-b362-8b784c7bd64d
2023-10-11 05:43:10,304 - distributed.worker - INFO - Starting Worker plugin PreImport-69855d97-5b77-4461-96b3-6c2edb86a753
2023-10-11 05:43:10,304 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c74dba86-d72a-4384-bfed-3361a8400c5b
2023-10-11 05:43:10,304 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:10,474 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39743', status: init, memory: 0, processing: 0>
2023-10-11 05:43:10,475 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39743
2023-10-11 05:43:10,475 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54166
2023-10-11 05:43:10,476 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:43:10,477 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-11 05:43:10,477 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:10,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-11 05:43:10,549 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:10,709 - distributed.scheduler - INFO - Remove client Client-0ca2f84d-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:10,709 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40130; closing.
2023-10-11 05:43:10,710 - distributed.scheduler - INFO - Remove client Client-0ca2f84d-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:10,710 - distributed.scheduler - INFO - Close client connection: Client-0ca2f84d-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:10,711 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36523'. Reason: nanny-close
2023-10-11 05:43:10,711 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:43:10,712 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39743. Reason: nanny-close
2023-10-11 05:43:10,714 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54166; closing.
2023-10-11 05:43:10,714 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-11 05:43:10,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39743', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002990.714975')
2023-10-11 05:43:10,715 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:43:10,716 - distributed.nanny - INFO - Worker closed
2023-10-11 05:43:11,727 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:43:11,727 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:43:11,728 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:43:11,729 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-11 05:43:11,729 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-10-11 05:43:14,052 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:43:14,056 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40905 instead
  warnings.warn(
2023-10-11 05:43:14,061 - distributed.scheduler - INFO - State start
2023-10-11 05:43:14,086 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:43:14,087 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:43:14,088 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40905/status
2023-10-11 05:43:14,088 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:43:14,197 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37897'
2023-10-11 05:43:14,211 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36813'
2023-10-11 05:43:14,226 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37135'
2023-10-11 05:43:14,238 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35975'
2023-10-11 05:43:14,240 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36437'
2023-10-11 05:43:14,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36811'
2023-10-11 05:43:14,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40449'
2023-10-11 05:43:14,274 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38429'
2023-10-11 05:43:15,195 - distributed.scheduler - INFO - Receive client connection: Client-10c8f59f-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:15,209 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57754
2023-10-11 05:43:16,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:16,098 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:16,099 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:16,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:16,100 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:16,101 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:16,102 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:16,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:16,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:16,104 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:16,105 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:16,107 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:16,148 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:16,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:16,153 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:16,190 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:16,191 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:16,195 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:16,205 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:16,205 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:16,210 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:16,242 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:16,242 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:16,248 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:19,042 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37519
2023-10-11 05:43:19,042 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37519
2023-10-11 05:43:19,043 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38499
2023-10-11 05:43:19,043 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,043 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,043 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:19,043 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:43:19,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_keyq04n
2023-10-11 05:43:19,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6711196f-b3ef-4a49-ba29-1c92473ef544
2023-10-11 05:43:19,044 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43765
2023-10-11 05:43:19,045 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43765
2023-10-11 05:43:19,045 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40233
2023-10-11 05:43:19,045 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,045 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,045 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:19,045 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:43:19,045 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hiad6s9g
2023-10-11 05:43:19,046 - distributed.worker - INFO - Starting Worker plugin PreImport-c14a5cf7-6996-4aec-80d8-41842b4c9bee
2023-10-11 05:43:19,046 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-24af516d-a843-46db-bccf-29c9e97e4031
2023-10-11 05:43:19,046 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f65be76d-4e04-4532-adc8-484c971d9311
2023-10-11 05:43:19,054 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45369
2023-10-11 05:43:19,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45369
2023-10-11 05:43:19,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41693
2023-10-11 05:43:19,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,056 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,056 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:19,056 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:43:19,056 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0em6plmo
2023-10-11 05:43:19,056 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6263a32f-45ac-47b2-9d15-62e937295301
2023-10-11 05:43:19,057 - distributed.worker - INFO - Starting Worker plugin PreImport-8a5ea001-9d37-49e1-97cd-ddfc50c66d16
2023-10-11 05:43:19,057 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fedaf3b-ff48-4ec1-ac76-499e174faf77
2023-10-11 05:43:19,195 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,218 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,219 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb296ed4-a61d-482d-99b8-2f3d719a5d69
2023-10-11 05:43:19,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39629
2023-10-11 05:43:19,220 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39629
2023-10-11 05:43:19,220 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34171
2023-10-11 05:43:19,220 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,220 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,220 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:19,220 - distributed.worker - INFO - Starting Worker plugin PreImport-45086c35-75c3-463b-9811-9bb618edc029
2023-10-11 05:43:19,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:43:19,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j_eb6t81
2023-10-11 05:43:19,220 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,221 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7c37a44-ff0d-4937-9308-5fa099f2a23b
2023-10-11 05:43:19,221 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0df67c2-2789-4f18-9d57-9e4ce5cd31ad
2023-10-11 05:43:19,233 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43765', status: init, memory: 0, processing: 0>
2023-10-11 05:43:19,234 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43765
2023-10-11 05:43:19,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57772
2023-10-11 05:43:19,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:43:19,236 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36193
2023-10-11 05:43:19,237 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36193
2023-10-11 05:43:19,237 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38959
2023-10-11 05:43:19,237 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,237 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,237 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:19,237 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:43:19,237 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-eqnkvi21
2023-10-11 05:43:19,237 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,238 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,238 - distributed.worker - INFO - Starting Worker plugin RMMSetup-78fcf323-40f6-49cf-a3d4-af399ecd9730
2023-10-11 05:43:19,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:43:19,251 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45369', status: init, memory: 0, processing: 0>
2023-10-11 05:43:19,252 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45369
2023-10-11 05:43:19,252 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57784
2023-10-11 05:43:19,253 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:43:19,254 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,254 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,255 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:43:19,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46835
2023-10-11 05:43:19,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46835
2023-10-11 05:43:19,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45565
2023-10-11 05:43:19,257 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,257 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,258 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37519', status: init, memory: 0, processing: 0>
2023-10-11 05:43:19,258 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:19,258 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:43:19,258 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nuw81t8g
2023-10-11 05:43:19,258 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37519
2023-10-11 05:43:19,258 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57790
2023-10-11 05:43:19,258 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c8d4dc2-fe53-4e58-baa4-8cb20a9fd784
2023-10-11 05:43:19,258 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45529
2023-10-11 05:43:19,259 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45529
2023-10-11 05:43:19,259 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40265
2023-10-11 05:43:19,259 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,259 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,259 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:19,259 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:43:19,260 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z0ket6vy
2023-10-11 05:43:19,260 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:43:19,260 - distributed.worker - INFO - Starting Worker plugin RMMSetup-314628ff-8819-4184-99f0-b04f54aa12a9
2023-10-11 05:43:19,261 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,261 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,263 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:43:19,271 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41097
2023-10-11 05:43:19,272 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41097
2023-10-11 05:43:19,272 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45213
2023-10-11 05:43:19,272 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,272 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,272 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:19,272 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-11 05:43:19,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ertesbzu
2023-10-11 05:43:19,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de3e95cb-b0a6-4f46-ac4b-d31483195d7b
2023-10-11 05:43:19,390 - distributed.worker - INFO - Starting Worker plugin PreImport-5031aec1-c4dc-4d8f-b3ef-b5d489ed01ee
2023-10-11 05:43:19,390 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,406 - distributed.worker - INFO - Starting Worker plugin PreImport-5350d1d7-dc63-4e2c-9120-43fcb447eb5c
2023-10-11 05:43:19,406 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-458ac7c7-ce96-4b89-85e9-369abc520ea6
2023-10-11 05:43:19,406 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e23c92d-6ee4-49fc-82bc-fad963cf48cb
2023-10-11 05:43:19,406 - distributed.worker - INFO - Starting Worker plugin PreImport-d1d6cb79-1bd2-4664-8019-3689d8022d97
2023-10-11 05:43:19,407 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3cff416f-02e9-49ae-b4df-d9eeeccbc13f
2023-10-11 05:43:19,407 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,407 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,407 - distributed.worker - INFO - Starting Worker plugin PreImport-5e7dfde8-d577-4234-8fd1-9b4a1a670b6f
2023-10-11 05:43:19,408 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,408 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b82b0867-6827-489f-bdb3-80278cda789b
2023-10-11 05:43:19,409 - distributed.worker - INFO - Starting Worker plugin PreImport-ff5c04e6-874b-420e-82f2-602dce07aad6
2023-10-11 05:43:19,409 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,417 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39629', status: init, memory: 0, processing: 0>
2023-10-11 05:43:19,418 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39629
2023-10-11 05:43:19,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57802
2023-10-11 05:43:19,419 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:43:19,420 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,420 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:43:19,437 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41097', status: init, memory: 0, processing: 0>
2023-10-11 05:43:19,437 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41097
2023-10-11 05:43:19,437 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57812
2023-10-11 05:43:19,438 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46835', status: init, memory: 0, processing: 0>
2023-10-11 05:43:19,438 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:43:19,439 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46835
2023-10-11 05:43:19,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57826
2023-10-11 05:43:19,439 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,439 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,439 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36193', status: init, memory: 0, processing: 0>
2023-10-11 05:43:19,440 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:43:19,440 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36193
2023-10-11 05:43:19,440 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57836
2023-10-11 05:43:19,440 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,440 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:43:19,441 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:43:19,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:43:19,443 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,443 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,443 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45529', status: init, memory: 0, processing: 0>
2023-10-11 05:43:19,443 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45529
2023-10-11 05:43:19,443 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57814
2023-10-11 05:43:19,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:43:19,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:43:19,446 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:43:19,446 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:19,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:43:19,499 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:43:19,499 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:43:19,499 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:43:19,499 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:43:19,499 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:43:19,499 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:43:19,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:43:19,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-11 05:43:19,514 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:19,514 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:19,514 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:19,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:19,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:19,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:19,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:19,515 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:19,520 - distributed.scheduler - INFO - Remove client Client-10c8f59f-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:19,520 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57754; closing.
2023-10-11 05:43:19,520 - distributed.scheduler - INFO - Remove client Client-10c8f59f-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:19,520 - distributed.scheduler - INFO - Close client connection: Client-10c8f59f-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:19,522 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37897'. Reason: nanny-close
2023-10-11 05:43:19,522 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:43:19,523 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36813'. Reason: nanny-close
2023-10-11 05:43:19,523 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:43:19,523 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41097. Reason: nanny-close
2023-10-11 05:43:19,524 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37135'. Reason: nanny-close
2023-10-11 05:43:19,524 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:43:19,524 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39629. Reason: nanny-close
2023-10-11 05:43:19,524 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35975'. Reason: nanny-close
2023-10-11 05:43:19,525 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:43:19,525 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37519. Reason: nanny-close
2023-10-11 05:43:19,525 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36437'. Reason: nanny-close
2023-10-11 05:43:19,525 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:43:19,525 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:43:19,525 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57812; closing.
2023-10-11 05:43:19,525 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43765. Reason: nanny-close
2023-10-11 05:43:19,525 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36811'. Reason: nanny-close
2023-10-11 05:43:19,526 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:43:19,526 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41097', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002999.5261767')
2023-10-11 05:43:19,526 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:43:19,526 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45369. Reason: nanny-close
2023-10-11 05:43:19,526 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40449'. Reason: nanny-close
2023-10-11 05:43:19,526 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:43:19,526 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46835. Reason: nanny-close
2023-10-11 05:43:19,527 - distributed.nanny - INFO - Worker closed
2023-10-11 05:43:19,527 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38429'. Reason: nanny-close
2023-10-11 05:43:19,527 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:43:19,527 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57802; closing.
2023-10-11 05:43:19,527 - distributed.nanny - INFO - Worker closed
2023-10-11 05:43:19,528 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36193. Reason: nanny-close
2023-10-11 05:43:19,528 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:43:19,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39629', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002999.5285718')
2023-10-11 05:43:19,528 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:43:19,528 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45529. Reason: nanny-close
2023-10-11 05:43:19,528 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57790; closing.
2023-10-11 05:43:19,529 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:43:19,529 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:43:19,529 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37519', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002999.5294726')
2023-10-11 05:43:19,529 - distributed.nanny - INFO - Worker closed
2023-10-11 05:43:19,529 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57772; closing.
2023-10-11 05:43:19,530 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:43:19,530 - distributed.nanny - INFO - Worker closed
2023-10-11 05:43:19,530 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43765', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002999.5305285')
2023-10-11 05:43:19,530 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:43:19,530 - distributed.nanny - INFO - Worker closed
2023-10-11 05:43:19,530 - distributed.nanny - INFO - Worker closed
2023-10-11 05:43:19,530 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57784; closing.
2023-10-11 05:43:19,531 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57826; closing.
2023-10-11 05:43:19,531 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45369', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002999.5316308')
2023-10-11 05:43:19,532 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46835', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002999.53203')
2023-10-11 05:43:19,532 - distributed.nanny - INFO - Worker closed
2023-10-11 05:43:19,532 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57836; closing.
2023-10-11 05:43:19,532 - distributed.nanny - INFO - Worker closed
2023-10-11 05:43:19,532 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36193', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002999.532911')
2023-10-11 05:43:19,533 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57814; closing.
2023-10-11 05:43:19,533 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45529', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697002999.5336728')
2023-10-11 05:43:19,533 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:43:21,140 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:43:21,140 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:43:21,141 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:43:21,142 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:43:21,143 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-10-11 05:43:23,450 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:43:23,454 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40365 instead
  warnings.warn(
2023-10-11 05:43:23,458 - distributed.scheduler - INFO - State start
2023-10-11 05:43:23,483 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:43:23,484 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:43:23,485 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40365/status
2023-10-11 05:43:23,486 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:43:23,536 - distributed.scheduler - INFO - Receive client connection: Client-16630a33-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:23,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37023'
2023-10-11 05:43:23,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42650
2023-10-11 05:43:25,236 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:25,236 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:25,240 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:26,347 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33611
2023-10-11 05:43:26,348 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33611
2023-10-11 05:43:26,348 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38259
2023-10-11 05:43:26,348 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:43:26,348 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:26,348 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:26,348 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-11 05:43:26,348 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xb05ziyu
2023-10-11 05:43:26,349 - distributed.worker - INFO - Starting Worker plugin RMMSetup-83638f1c-da9a-4311-8024-1074b34513c7
2023-10-11 05:43:26,446 - distributed.worker - INFO - Starting Worker plugin PreImport-fb11efdf-c7e1-4435-919b-6f096edbe642
2023-10-11 05:43:26,446 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-728534c7-2919-41a9-9f03-04b857ab4222
2023-10-11 05:43:26,447 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:26,470 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33611', status: init, memory: 0, processing: 0>
2023-10-11 05:43:26,471 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33611
2023-10-11 05:43:26,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42672
2023-10-11 05:43:26,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-11 05:43:26,473 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-11 05:43:26,473 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:26,475 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-11 05:43:26,515 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-11 05:43:26,519 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:26,520 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-11 05:43:26,523 - distributed.scheduler - INFO - Remove client Client-16630a33-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:26,523 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42650; closing.
2023-10-11 05:43:26,523 - distributed.scheduler - INFO - Remove client Client-16630a33-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:26,524 - distributed.scheduler - INFO - Close client connection: Client-16630a33-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:26,525 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37023'. Reason: nanny-close
2023-10-11 05:43:26,525 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-11 05:43:26,526 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33611. Reason: nanny-close
2023-10-11 05:43:26,528 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-11 05:43:26,528 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42672; closing.
2023-10-11 05:43:26,528 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33611', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697003006.5289023')
2023-10-11 05:43:26,529 - distributed.scheduler - INFO - Lost all workers
2023-10-11 05:43:26,530 - distributed.nanny - INFO - Worker closed
2023-10-11 05:43:27,541 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:43:27,542 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:43:27,542 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:43:27,543 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:43:27,544 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-10-11 05:43:29,803 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:43:29,808 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43767 instead
  warnings.warn(
2023-10-11 05:43:29,812 - distributed.scheduler - INFO - State start
2023-10-11 05:43:29,833 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-11 05:43:29,834 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-11 05:43:29,835 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43767/status
2023-10-11 05:43:29,835 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-11 05:43:29,855 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33767'
2023-10-11 05:43:31,142 - distributed.scheduler - INFO - Receive client connection: Client-1a3c92ad-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:31,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58458
2023-10-11 05:43:31,615 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-11 05:43:31,615 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-11 05:43:31,619 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-11 05:43:32,500 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35607
2023-10-11 05:43:32,500 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35607
2023-10-11 05:43:32,501 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32987
2023-10-11 05:43:32,501 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-11 05:43:32,501 - distributed.worker - INFO - -------------------------------------------------
2023-10-11 05:43:32,501 - distributed.worker - INFO -               Threads:                          1
2023-10-11 05:43:32,501 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-11 05:43:32,501 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dhbeq7bn
2023-10-11 05:43:32,501 - distributed.worker - INFO - Starting Worker plugin RMMSetup-591e11a1-5851-4408-ac1b-d70937c98b06
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-10-11 05:43:33,084 - distributed.worker - INFO - Starting Worker plugin PreImport-8c5338b6-1b9e-425c-aff4-17c3528baea4
2023-10-11 05:43:33,084 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87915b57-4841-44d2-8a89-60f9cd53a059
2023-10-11 05:43:33,084 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35607. Reason: failure-to-start-<class 'MemoryError'>
2023-10-11 05:43:33,084 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-10-11 05:43:33,086 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:43:33,097 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:43:33,100 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33767'. Reason: nanny-instantiate-failed
2023-10-11 05:43:33,101 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-10-11 05:43:33,477 - distributed.nanny - INFO - Worker process 55629 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 121, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-10-11 05:43:33,478 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:42784'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42784>: Stream is closed
2023-10-11 05:43:41,175 - distributed.scheduler - INFO - Remove client Client-1a3c92ad-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:41,175 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58458; closing.
2023-10-11 05:43:41,175 - distributed.scheduler - INFO - Remove client Client-1a3c92ad-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:41,176 - distributed.scheduler - INFO - Close client connection: Client-1a3c92ad-67f9-11ee-8c3a-d8c49764f6bb
2023-10-11 05:43:41,176 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-11 05:43:41,177 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-11 05:43:41,177 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-11 05:43:41,178 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-11 05:43:41,178 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36781 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44849 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45677 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34591 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38007 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40825 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38435 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43087 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36991 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46513 instead
  warnings.warn(
[1697003124.845498] [dgx13:57757:0]            sock.c:470  UCX  ERROR bind(fd=127 addr=0.0.0.0:51116) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35059 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38341 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39667 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35257 instead
  warnings.warn(
2023-10-11 05:46:21,985 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-11 05:46:21,990 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:37879', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45371 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33123 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33437 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37211 instead
  warnings.warn(
[1697003251.306496] [dgx13:60386:0]            sock.c:470  UCX  ERROR bind(fd=127 addr=0.0.0.0:41598) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43007 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37951 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34105 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40879 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39679 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45783 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44009 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44887 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37061 instead
  warnings.warn(
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 113, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39491 instead
  warnings.warn(
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 113, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43479 instead
  warnings.warn(
Process SpawnProcess-29:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 113, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38099 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44603 instead
  warnings.warn(
2023-10-11 05:51:06,528 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-f5d864bf-69c6-4179-974c-8055b2b30f24
Function:  _run_coroutine_on_worker
args:      (164720885810100551926084854696029830229, <function shuffle_task at 0x7faf54299820>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set()}, {0: {0}, 1: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-11 05:51:06,548 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-876f4515-4205-44ca-b916-ab1854546985
Function:  _run_coroutine_on_worker
args:      (164720885810100551926084854696029830229, <function shuffle_task at 0x7ffa7de82310>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set()}, {0: {0}, 1: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

Process SpawnProcess-31:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 146, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 342, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 372, in shuffle_task
    no_comm_postprocess = get_no_comm_postprocess(stage, num_rounds, batchsize, proxify)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 76, in get_no_comm_postprocess
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39157 instead
  warnings.warn(
2023-10-11 05:51:17,537 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-37cc24a5-8411-4345-b3e0-73754c4a1249
Function:  _run_coroutine_on_worker
args:      (88549205582343964693993171283931145793, <function shuffle_task at 0x7f96515330d0>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set(), 2: set()}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-11 05:51:17,545 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-33753618-b0cd-407a-bce0-518421a305ed
Function:  _run_coroutine_on_worker
args:      (88549205582343964693993171283931145793, <function shuffle_task at 0x7f6ccc356160>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set(), 2: set()}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-11 05:51:17,552 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-b902305a-7e85-4771-92e2-ac32cbfdca31
Function:  _run_coroutine_on_worker
args:      (88549205582343964693993171283931145793, <function shuffle_task at 0x7f2e48b06700>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set(), 2: set()}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

Process SpawnProcess-32:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 146, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 342, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 372, in shuffle_task
    no_comm_postprocess = get_no_comm_postprocess(stage, num_rounds, batchsize, proxify)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 76, in get_no_comm_postprocess
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46021 instead
  warnings.warn(
Process SpawnProcess-33:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 113, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34953 instead
  warnings.warn(
[1697003491.200872] [dgx13:64944:0]            sock.c:470  UCX  ERROR bind(fd=164 addr=0.0.0.0:58378) failed: Address already in use
Process SpawnProcess-34:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 113, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42305 instead
  warnings.warn(
Process SpawnProcess-35:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 113, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41545 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35535 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39627 instead
  warnings.warn(
2023-10-11 05:52:06,915 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-bd285234-e4ae-426a-aaa1-56813c9baf06
Function:  _run_coroutine_on_worker
args:      (316099969623185827367765468096212512505, <function shuffle_task at 0x7f922449daf0>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 4), ('from_pandas-eaba71b388f24e387b93842f216e4045', 0)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-11 05:52:06,976 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-ccc8001a-f2f9-4c96-9d1f-ce77887dbc22
Function:  _run_coroutine_on_worker
args:      (316099969623185827367765468096212512505, <function shuffle_task at 0x7fed2f7c1700>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 4), ('from_pandas-eaba71b388f24e387b93842f216e4045', 0)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-11 05:52:06,980 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-472687af-c8f3-48be-99b6-675b1264f169
Function:  _run_coroutine_on_worker
args:      (316099969623185827367765468096212512505, <function shuffle_task at 0x7fed5667dee0>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 4), ('from_pandas-eaba71b388f24e387b93842f216e4045', 0)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-11 05:52:06,984 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-d8b8acc4-ec08-4687-9faa-19139ccbd01f
Function:  _run_coroutine_on_worker
args:      (316099969623185827367765468096212512505, <function shuffle_task at 0x7fa8d205c820>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 4), ('from_pandas-eaba71b388f24e387b93842f216e4045', 0)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

Process SpawnProcess-40:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 247, in _test_dataframe_shuffle_merge
    got = ddf1.merge(ddf2, on="key").set_index("key").compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 5287, in set_index
    return set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 237, in set_index
    divisions, mins, maxes, presorted = _calculate_divisions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 50, in _calculate_divisions
    divisions, sizes, mins, maxes = compute(divisions, sizes, mins, maxes)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 372, in shuffle_task
    no_comm_postprocess = get_no_comm_postprocess(stage, num_rounds, batchsize, proxify)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 76, in get_no_comm_postprocess
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38807 instead
  warnings.warn(
Process SpawnProcess-41:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 239, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34553 instead
  warnings.warn(
Process SpawnProcess-42:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 239, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46691 instead
  warnings.warn(
Process SpawnProcess-43:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 239, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43629 instead
  warnings.warn(
[1697003547.389853] [dgx13:65755:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:59133) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39105 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46375 instead
  warnings.warn(
2023-10-11 05:52:46,470 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-a39b3fe3-9c07-43a6-8e03-667200398361
Function:  _run_coroutine_on_worker
args:      (290922628004614055690051773685866795075, <function shuffle_task at 0x7f328d5f11f0>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 4), ('from_pandas-eaba71b388f24e387b93842f216e4045', 0)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-11 05:52:46,525 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-06dd40df-17a8-4468-bf4e-38f2c0e92a15
Function:  _run_coroutine_on_worker
args:      (290922628004614055690051773685866795075, <function shuffle_task at 0x7f88341241f0>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 0), ('from_pandas-eaba71b388f24e387b93842f216e4045', 4)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-11 05:52:46,530 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-f8f8cdc6-82be-43b7-b08d-5dd5e5bdddd1
Function:  _run_coroutine_on_worker
args:      (290922628004614055690051773685866795075, <function shuffle_task at 0x7fc86d2344c0>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 4), ('from_pandas-eaba71b388f24e387b93842f216e4045', 0)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-11 05:52:46,540 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-e7ef36f3-fb0a-44d0-9ca2-7b91e902d040
Function:  _run_coroutine_on_worker
args:      (290922628004614055690051773685866795075, <function shuffle_task at 0x7fe273421820>, ('explicit-comms-shuffle-8a9c181b2d24bc345fc4f6aca80171aa', {0: {('from_pandas-eaba71b388f24e387b93842f216e4045', 0), ('from_pandas-eaba71b388f24e387b93842f216e4045', 4)}, 1: {('from_pandas-eaba71b388f24e387b93842f216e4045', 2)}, 2: {('from_pandas-eaba71b388f24e387b93842f216e4045', 1)}, 3: {('from_pandas-eaba71b388f24e387b93842f216e4045', 3)}}, {0: {0, 4}, 1: {1}, 2: {2}, 3: {3}}, ['key'], 5, False, 2, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

Process SpawnProcess-46:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 247, in _test_dataframe_shuffle_merge
    got = ddf1.merge(ddf2, on="key").set_index("key").compute()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 5287, in set_index
    return set_index(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 237, in set_index
    divisions, mins, maxes, presorted = _calculate_divisions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/shuffle.py", line 50, in _calculate_divisions
    divisions, sizes, mins, maxes = compute(divisions, sizes, mins, maxes)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 372, in shuffle_task
    no_comm_postprocess = get_no_comm_postprocess(stage, num_rounds, batchsize, proxify)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 76, in get_no_comm_postprocess
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43041 instead
  warnings.warn(
Process SpawnProcess-47:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 239, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41633 instead
  warnings.warn(
Process SpawnProcess-48:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 239, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37313 instead
  warnings.warn(
Process SpawnProcess-49:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 239, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45069 instead
  warnings.warn(
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Process SpawnProcess-50:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 278, in _test_jit_unspill
    df = cudf.DataFrame.from_pandas(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 115, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5133, in from_pandas
    data[col_name] = column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 2256, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1995, in as_column
    col = ColumnBase.from_arrow(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 379, in from_arrow
    result = libcudf.interop.from_arrow(data)[0]
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "interop.pyx", line 182, in cudf._lib.interop.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38137 instead
  warnings.warn(
2023-10-11 05:53:24,854 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-10-11 05:53:24,859 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:53:24,903 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-33' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Nanny failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-10-11 05:53:24,941 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f4732e86be0>>, <Task finished name='Task-32' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Worker failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 387, in _correct_state_internal
    await w  # for tornado gen.coroutine support
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 605, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:53:27,835 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-10-11 05:53:27,840 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:53:27,854 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-263' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Nanny failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Process SpawnProcess-51:
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 268, in _test_jit_unspill
    with dask_cuda.LocalCUDACluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/local_cuda_cluster.py", line 375, in __init__
    self.sync(self._correct_state)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 387, in _correct_state_internal
    await w  # for tornado gen.coroutine support
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 605, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 362, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 448, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 748, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 889, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers 2023-10-11 05:53:41,595 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-11 05:53:41,597 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:33133', name: 0, status: closed, stored: 0, running: 0/5, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] 2023-10-11 05:53:48,254 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-10-11 05:53:48,258 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:53:54,704 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 3
2023-10-11 05:53:54,709 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 3', 'time': 1697003634.7037504}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:53:55,203 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1540, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 723, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2023-10-11 05:53:55,207 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1391, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
[1697003635.496239] [dgx13:66632:0]            sock.c:470  UCX  ERROR bind(fd=164 addr=0.0.0.0:40806) failed: Address already in use
2023-10-11 05:53:55,710 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 2
2023-10-11 05:53:55,714 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 2', 'time': 1697003635.7101088}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:53:55,780 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 5
2023-10-11 05:53:55,784 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 5', 'time': 1697003635.7791526}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:53:55,787 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 4
2023-10-11 05:53:55,791 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 4', 'time': 1697003635.7868476}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:53:55,818 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 1
2023-10-11 05:53:55,821 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 1', 'time': 1697003635.816166}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:53:55,862 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 7
2023-10-11 05:53:55,864 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 6
2023-10-11 05:53:55,868 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 7', 'time': 1697003635.8612983}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-10-11 05:53:55,870 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1480, in start_unsafe
    await self._register_with_scheduler()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1226, in _register_with_scheduler
    raise ValueError(f"Unexpected response from register: {response!r}")
ValueError: Unexpected response from register: {'status': 'error', 'message': 'name taken, 6', 'time': 1697003635.8622024}

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 953, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 630, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41179 instead
  warnings.warn(
2023-10-11 05:54:06,534 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-95:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 26, in _test_initialize_ucx_tcp
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46003 instead
  warnings.warn(
2023-10-11 05:54:09,291 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-96:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 61, in _test_initialize_ucx_nvlink
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41917 instead
  warnings.warn(
2023-10-11 05:54:12,050 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-97:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 531, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 97, in _test_initialize_ucx_infiniband
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 359, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 426, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 399, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33741 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster [1697003670.192738] [dgx13:66983:0]            sock.c:470  UCX  ERROR bind(fd=164 addr=0.0.0.0:50404) failed: Address already in use
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol [1697003726.077161] [dgx13:67596:0]            sock.c:470  UCX  ERROR bind(fd=103 addr=0.0.0.0:55744) failed: Address already in use
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 69 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
