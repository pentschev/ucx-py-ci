[dgx13:84231:0:84231] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  84231) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f024c97ef1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7f024c97f114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7f024c97f2da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f02edad9420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f024c9f85d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f024ca1d859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7f024c93a42f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7f024c93d798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f024c987989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f024c93c62d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f024c9f5c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f024caa706a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x561a3b0986fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561a3b094094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561a3b0a5519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561a3b0955c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561a3b0a57c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x561a3b0b2e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x561a3b1bdb2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x561a3b04fd05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x561a3b09c7f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x561a3b09a929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561a3b0a57c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561a3b0955c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561a3b0a57c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561a3b0955c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561a3b0a57c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561a3b0955c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561a3b0a57c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561a3b0955c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561a3b094094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561a3b0a5519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x561a3b096128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561a3b094094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x561a3b0b2ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x561a3b0b344c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x561a3b17610e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x561a3b09d77c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x561a3b0986fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561a3b0a57c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x561a3b0b2dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x561a3b0986fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561a3b0a57c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561a3b0955c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561a3b094094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561a3b0a5519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561a3b0955c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x561a3b0a57c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x561a3b095312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561a3b094094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561a3b0a5519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x561a3b096128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x561a3b094094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x561a3b093d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x561a3b093d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x561a3b14107b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x561a3b16dfca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x561a3b16a353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x561a3b16216a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x561a3b16205c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x561a3b161297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x561a3b134f07]
=================================
[dgx13:84226:0:84226] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  84226) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fe571e5df1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7fe571e5e114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7fe571e5e2da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fe618fc2420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fe571ed75d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fe571efc859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7fe571e1942f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7fe571e1c798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fe571e66989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fe571e1b62d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fe571ed4c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fe571f8606a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5599023056fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559902301094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x559902312519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5599023025c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x5599023b5162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fe59914b1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55990230a77c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x5599022bcd05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x5599023097f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x559902307929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5599023127c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5599023025c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5599023127c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5599023025c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5599023127c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5599023025c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5599023127c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5599023025c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559902301094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x559902312519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x559902303128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559902301094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x55990231fccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55990232044c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x5599023e310e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55990230a77c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5599023056fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5599023127c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x55990231fdac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5599023056fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5599023127c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5599023025c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559902301094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x559902312519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5599023025c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5599023127c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x559902302312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559902301094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x559902312519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x559902303128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x559902301094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x559902300d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x559902300d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5599023ae07b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x5599023dafca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x5599023d7353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x5599023cf16a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x5599023cf05c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x5599023ce297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5599023a1f07]
=================================
[dgx13:84228:0:84228] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  84228) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f44552eff1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7f44552f0114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7f44552f02da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f44fa453420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f44553695d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f445538e859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7f44552ab42f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7f44552ae798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f44552f8989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f44552ad62d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f4455366c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f445541806a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5560e6ac96fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560e6ac5094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5560e6ad6519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560e6ac65c6]
16  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560e6ad67c2]
17  /opt/conda/envs/gdf/bin/python(+0x14de83) [0x5560e6ae3e83]
18  /opt/conda/envs/gdf/bin/python(+0x258b2c) [0x5560e6beeb2c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x5560e6a80d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x5560e6acd7f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x5560e6acb929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560e6ad67c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560e6ac65c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560e6ad67c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560e6ac65c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560e6ad67c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560e6ac65c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560e6ad67c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560e6ac65c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560e6ac5094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5560e6ad6519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x5560e6ac7128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560e6ac5094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x5560e6ae3ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x5560e6ae444c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x5560e6ba710e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x5560e6ace77c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5560e6ac96fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560e6ad67c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x5560e6ae3dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x5560e6ac96fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560e6ad67c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560e6ac65c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560e6ac5094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5560e6ad6519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x5560e6ac65c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x5560e6ad67c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x5560e6ac6312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560e6ac5094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x5560e6ad6519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x5560e6ac7128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x5560e6ac5094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x5560e6ac4d68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5560e6ac4d19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5560e6b7207b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x5560e6b9efca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x5560e6b9b353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x5560e6b9316a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x5560e6b9305c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x5560e6b92297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x5560e6b65f07]
=================================
[dgx13:84234:0:84234] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  84234) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fed1c08cf1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7fed1c08d114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7fed1c08d2da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fedbd1ca420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fed1c1065d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fed1c12b859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7fed1c04842f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7fed1c04b798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fed1c095989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fed1c04a62d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fed1c103c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fed1c1b506a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x563e1ce8f6fb]
13  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x563e1ce8b094]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x563e1ce9c519]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563e1ce8c5c6]
16  /opt/conda/envs/gdf/bin/python(+0x1e3162) [0x563e1cf3f162]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fed3d3521e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x563e1ce9477c]
19  /opt/conda/envs/gdf/bin/python(+0xead05) [0x563e1ce46d05]
20  /opt/conda/envs/gdf/bin/python(+0x1377f3) [0x563e1ce937f3]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x59f9) [0x563e1ce91929]
22  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x563e1ce9c7c2]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563e1ce8c5c6]
24  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x563e1ce9c7c2]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563e1ce8c5c6]
26  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x563e1ce9c7c2]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563e1ce8c5c6]
28  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x563e1ce9c7c2]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563e1ce8c5c6]
30  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x563e1ce8b094]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x563e1ce9c519]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x563e1ce8d128]
33  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x563e1ce8b094]
34  /opt/conda/envs/gdf/bin/python(+0x14dccb) [0x563e1cea9ccb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x563e1ceaa44c]
36  /opt/conda/envs/gdf/bin/python(+0x21110e) [0x563e1cf6d10e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x563e1ce9477c]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x563e1ce8f6fb]
39  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x563e1ce9c7c2]
40  /opt/conda/envs/gdf/bin/python(+0x14ddac) [0x563e1cea9dac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x37cb) [0x563e1ce8f6fb]
42  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x563e1ce9c7c2]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563e1ce8c5c6]
44  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x563e1ce8b094]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x563e1ce9c519]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x563e1ce8c5c6]
47  /opt/conda/envs/gdf/bin/python(+0x1407c2) [0x563e1ce9c7c2]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x563e1ce8c312]
49  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x563e1ce8b094]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x563e1ce9c519]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f8) [0x563e1ce8d128]
52  /opt/conda/envs/gdf/bin/python(+0x12f094) [0x563e1ce8b094]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x563e1ce8ad68]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x563e1ce8ad19]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x563e1cf3807b]
56  /opt/conda/envs/gdf/bin/python(+0x208fca) [0x563e1cf64fca]
57  /opt/conda/envs/gdf/bin/python(+0x205353) [0x563e1cf61353]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x563e1cf5916a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x563e1cf5905c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x563e1cf58297]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x563e1cf2bf07]
=================================
2023-11-04 06:16:04,456 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:35167 -> ucx://127.0.0.1:54659
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f14f47f42c0, tag: 0x89c1348ccfb66bc4, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-11-04 06:16:04,456 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:43417 -> ucx://127.0.0.1:54659
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fb969d422c0, tag: 0x953f66d42b1e4235, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-11-04 06:16:04,456 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54659
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f5c51ba6240, tag: 0x6e45436ca6cdbf3e, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f5c51ba6240, tag: 0x6e45436ca6cdbf3e, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-11-04 06:16:04,458 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54659
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f14f47f4240, tag: 0x7a020cd1be1cce5e, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f14f47f4240, tag: 0x7a020cd1be1cce5e, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-11-04 06:16:04,458 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54659
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fb969d42200, tag: 0x9bda6f07cc9e3b80, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fb969d42200, tag: 0x9bda6f07cc9e3b80, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-11-04 06:16:04,456 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54659
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f3b25490280, tag: 0x7224284dbe915fd0, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 377, in connect
    handshake = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f3b25490280, tag: 0x7224284dbe915fd0, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
Task exception was never retrieved
future: <Task finished name='Task-842' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
2023-11-04 06:16:04,512 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:36899 -> ucx://127.0.0.1:39647
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f3b25490100, tag: 0x457c9a752de32218, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-11-04 06:16:04,512 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39647
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f5c51ba6100, tag: 0xffad46a37069ff55, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f5c51ba6100, tag: 0xffad46a37069ff55, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-11-04 06:16:04,514 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:43417 -> ucx://127.0.0.1:42783
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fb969d42400, tag: 0x4ceb8692035de029, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
Task exception was never retrieved
future: <Task finished name='Task-835' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-11-04 06:16:04,517 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39647
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 471, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 473, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
Task exception was never retrieved
future: <Task finished name='Task-841' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
2023-11-04 06:16:04,518 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39647
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 471, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 473, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 366, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2849, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-11-04 06:16:04,563 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:39647
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f14f47f4100, tag: 0x5ef32488a0f103a7, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f14f47f4100, tag: 0x5ef32488a0f103a7, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-11-04 06:16:04,939 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:38049
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f14f47f4180, tag: 0x3bcc267c161fade3, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f14f47f4180, tag: 0x3bcc267c161fade3, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-11-04 06:16:04,939 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:38049
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7fb969d42140, tag: 0x69d44c9a89936ad5, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7fb969d42140, tag: 0x69d44c9a89936ad5, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-11-04 06:16:04,939 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:38049
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 396, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f5c51ba6180, tag: 0x8971163d7c36c3d0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f5c51ba6180, tag: 0x8971163d7c36c3d0, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-11-04 06:16:04,939 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:38049
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 354, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f3b25490340, tag: 0x71eca051d1e09cf0, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f3b25490340, tag: 0x71eca051d1e09cf0, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-11-04 06:16:04,940 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:46881 -> ucx://127.0.0.1:38049
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f5c51ba63c0, tag: 0x1dc3fedae27a23d3, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1782, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 342, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-11-04 06:16:06,801 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-11-04 06:16:06,801 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-11-04 06:16:06,900 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-11-04 06:16:06,901 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2852, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 377, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 378, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded
2023-11-04 06:16:06,911 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 2)
Function:  _concat
args:      ([                key   payload
19078     838159554  36732955
19098     863977263  27663463
19100     851416388  67762005
9302      106286966  74155071
9304      846160393  73490626
...             ...       ...
99978131  859039162  81464486
99978132  606766337   1604004
99978134  849040629  41150391
99978139  829171899   8250104
99978143  853836151  60392632

[12503879 rows x 2 columns],                 key   payload
739       936096034  97800947
81537     901693371  51852251
747       322254608  47072271
81558     215137427   3607071
121540    908512685  19573303
...             ...       ...
99969975  119013748  38276681
99969983  905750387  50368159
99980142  928991420  35773263
99980154  515369200  95475384
99980159  914965552  56161365

[12499576 rows x 2 columns],                  key   payload
11904     1055350019  65860337
11906     1001295647  59219396
11908     1037813457  48510175
11916      127655448  36725164
40568     1067741223  37398008
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-11-04 06:16:07,017 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 0)
Function:  _concat
args:      ([               key   payload
shuffle                     
0           365506  84518815
0           415121  38499406
0           627772  61582982
0           403416  39379336
0           360876  43594591
...            ...       ...
0        799940126  36639810
0        799937759   2770496
0        799872545  46501054
0        799911074  43848371
0        799993401  93986618

[12505522 rows x 2 columns],                key   payload
shuffle                     
1           140984  72989944
1           123598  69197300
1            44677   3687523
1           263668  96015185
1           130715  63220449
...            ...       ...
1        799879376   8447711
1        799909370  80544775
1        799718583  85754754
1        799790826  53198171
1        799858655  19976749

[12497568 rows x 2 columns],                key   payload
shuffle                     
2           730306  42504741
2           699103  63395575
2           732207  58124390
2           589301   3291434
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-11-04 06:16:07,079 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6)
Function:  _concat
args:      ([                key   payload
19073     815909673   2767089
19084     868281172  15996870
19085     831354309  25775194
19090     849934736   6802354
19101     858825089   5463391
...             ...       ...
99978201  866034196  80630174
99978205  864098317   2330175
99985261  856157025  85026241
99978207  809073857  14622991
99978142  825285085  55530840

[12497796 rows x 2 columns],                 key   payload
742       946552337  99908514
81544     930677399  17836449
744       967194688  67515048
81546     955662371  25070808
121543    904977453  40955870
...             ...       ...
99969976  223898458  78456998
99980137  955917888  70905800
99980138  927821692  86140065
99980140  912994732  57231597
99980149  964937881  44484012

[12497151 rows x 2 columns],                  key   payload
11905      736087777  38331461
11910     1042833530  92570282
11922      325760582  59879210
11925      326327682  26465148
40544     1030516096  57809737
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-11-04 06:16:07,220 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 4)
Function:  _concat
args:      ([               key   payload
shuffle                     
0           593056  61822160
0           422753  89266711
0           601278  38324566
0           639612  69757945
0           407528  17628949
...            ...       ...
0        799944339  45279242
0        799937277  47989166
0        799956022  26642638
0        799911502  61547698
0        799970411  97257383

[12503392 rows x 2 columns],                key   payload
shuffle                     
1           181327  31153470
1           186368  70923224
1           120320  70095407
1           310087  29722542
1           191465  17648241
...            ...       ...
1        799850587  92168753
1        799935366  50103917
1        799888671  53050047
1        799895304  61662281
1        799755850  56173402

[12500389 rows x 2 columns],                key   payload
shuffle                     
2           658276  59649323
2           706622  78858509
2           348021  20087576
2           629196  29011594
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-11-04 06:16:07,282 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 3)
Function:  _concat
args:      ([               key   payload
shuffle                     
0           631385  94032339
0           598039  89166290
0           603726   6498929
0            99918  44759549
0           729824  43011434
...            ...       ...
0        799940127  61752343
0        799917755  47563931
0        799969437  26092015
0        799874591  85709096
0        799935530  40960626

[12497076 rows x 2 columns],                key   payload
shuffle                     
1           242880  76926411
1           130709   5771650
1           236570  83597001
1           183717  54881748
1            77053  99462111
...            ...       ...
1        799898322  58243133
1        799782979  84369190
1        799912786  23636467
1        799814129  69992464
1        799790833  79572383

[12501362 rows x 2 columns],                key   payload
shuffle                     
2           627543  20010879
2           588765  12613749
2           667101  99070599
2           733775  85647388
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-11-04 06:16:07,393 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 4)
Function:  _concat
args:      ([                key   payload
19076     818035417  79397563
19082     711951042  51925905
19086     849876042  16133410
19089     817794762  48525310
19096     830566920  21854447
...             ...       ...
99978015  837933148  29072754
99978114  510242410   5665486
99978116  103845877  68015888
99978130  865939823  10922478
99978135  851832062  50468430

[12500589 rows x 2 columns],                 key   payload
754       315097529  23447541
81536     920613129  53954454
757       900537337  99381089
81540     954052808  33295040
121541    905586158  11395275
...             ...       ...
99980127  901742915  54720204
99980132  952472697    163311
99980139  930272035  90106413
99980144  222563777  86486209
99980150  927386118  91310698

[12501715 rows x 2 columns],                  key   payload
11917     1065614086  48749189
11923      136696397  99442068
11928      531953793  76354052
11933      535495744  20544406
40557     1011404228   1901610
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-11-04 06:16:07,502 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b0648aa7f9e27a40d5f4d39e31c5b5a2', 7)
Function:  _concat
args:      ([               key   payload
shuffle                     
0           516942  90578371
0           363976  82450528
0           424276  48836057
0           408375  23560771
0           452880  97366438
...            ...       ...
0        799891065  48935099
0        799976699  89950179
0        799935529  69857676
0        799888983  69185018
0        799953472  18582267

[12498151 rows x 2 columns],                key   payload
shuffle                     
1            44170  83919721
1           192695  16119461
1           219304  23976603
1           171321  63328345
1           123607  35809588
...            ...       ...
1        799726165  67911254
1        799752210  31520156
1        799918443  57219308
1        799860730  49564648
1        799887787   7161194

[12498591 rows x 2 columns],                key   payload
shuffle                     
2           620402  17120731
2           558997  27745078
2           730653  13759982
2           296539  33704938
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:201: Maximum pool size exceeded')"

2023-11-04 06:16:09,299 - distributed.worker - ERROR - 'int' object is not subscriptable
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2059, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2861, in get_data_from_worker
    status = response["status"]
TypeError: 'int' object is not subscriptable
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 24 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
