============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-25 06:30:39,509 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:30:39,513 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41185 instead
  warnings.warn(
2024-01-25 06:30:39,517 - distributed.scheduler - INFO - State start
2024-01-25 06:30:39,538 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:30:39,539 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-25 06:30:39,539 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41185/status
2024-01-25 06:30:39,540 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:30:39,629 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40231'
2024-01-25 06:30:39,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38873'
2024-01-25 06:30:39,649 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34375'
2024-01-25 06:30:39,656 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40379'
2024-01-25 06:30:40,430 - distributed.scheduler - INFO - Receive client connection: Client-40ba4903-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:40,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57556
2024-01-25 06:30:41,345 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:41,345 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:41,346 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:41,346 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:41,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:41,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:41,349 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:41,350 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:41,350 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40793
2024-01-25 06:30:41,350 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40793
2024-01-25 06:30:41,350 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44705
2024-01-25 06:30:41,350 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-25 06:30:41,351 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,351 - distributed.worker - INFO -               Threads:                          4
2024-01-25 06:30:41,351 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-25 06:30:41,351 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-pnwozfy_
2024-01-25 06:30:41,351 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36461
2024-01-25 06:30:41,351 - distributed.worker - INFO - Starting Worker plugin RMMSetup-01a17b2b-37b9-4199-9ae9-7e6f0b85e8b3
2024-01-25 06:30:41,351 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36461
2024-01-25 06:30:41,351 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d805b707-9923-4ea1-ac88-8e65b066119e
2024-01-25 06:30:41,351 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40445
2024-01-25 06:30:41,351 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-25 06:30:41,351 - distributed.worker - INFO - Starting Worker plugin PreImport-541197c3-b003-40e0-bd14-e49cec4da297
2024-01-25 06:30:41,351 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,351 - distributed.worker - INFO -               Threads:                          4
2024-01-25 06:30:41,351 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,351 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-25 06:30:41,351 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-oupw0p08
2024-01-25 06:30:41,351 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:41,351 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5f3a0c4d-810b-4f3e-aedc-0c2dcf64f14c
2024-01-25 06:30:41,352 - distributed.worker - INFO - Starting Worker plugin PreImport-3c961274-31a9-4a63-b522-41f4c357adc1
2024-01-25 06:30:41,352 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95aa2bf7-2123-4be2-9fdd-494c607dd917
2024-01-25 06:30:41,352 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,352 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45451
2024-01-25 06:30:41,352 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45451
2024-01-25 06:30:41,352 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43537
2024-01-25 06:30:41,352 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-25 06:30:41,352 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,352 - distributed.worker - INFO -               Threads:                          4
2024-01-25 06:30:41,353 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-25 06:30:41,353 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-xpfanxqe
2024-01-25 06:30:41,353 - distributed.worker - INFO - Starting Worker plugin PreImport-b3d20fd1-8d15-4719-a30c-098ec4f0f67c
2024-01-25 06:30:41,353 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf689282-529a-4bdb-9fe6-7206b9aeba64
2024-01-25 06:30:41,353 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7dd99134-5019-4251-8312-490c6397b623
2024-01-25 06:30:41,353 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:41,361 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:41,365 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:41,366 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42641
2024-01-25 06:30:41,366 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42641
2024-01-25 06:30:41,366 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37949
2024-01-25 06:30:41,366 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-25 06:30:41,366 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,366 - distributed.worker - INFO -               Threads:                          4
2024-01-25 06:30:41,366 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-25 06:30:41,366 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-y6afdyt7
2024-01-25 06:30:41,366 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c46854d1-6014-4504-95eb-fa9a569458fa
2024-01-25 06:30:41,366 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f11f1e14-632a-408b-bd40-a95688d0c66c
2024-01-25 06:30:41,367 - distributed.worker - INFO - Starting Worker plugin PreImport-cb7e671e-38b8-464b-85e3-d7101f7ee5aa
2024-01-25 06:30:41,368 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,706 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36461', status: init, memory: 0, processing: 0>
2024-01-25 06:30:41,707 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36461
2024-01-25 06:30:41,708 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57588
2024-01-25 06:30:41,708 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:41,709 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-25 06:30:41,709 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,710 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-25 06:30:41,715 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40793', status: init, memory: 0, processing: 0>
2024-01-25 06:30:41,715 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40793
2024-01-25 06:30:41,715 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57582
2024-01-25 06:30:41,716 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:41,717 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-25 06:30:41,717 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,718 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-25 06:30:41,747 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42641', status: init, memory: 0, processing: 0>
2024-01-25 06:30:41,748 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42641
2024-01-25 06:30:41,748 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57608
2024-01-25 06:30:41,749 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45451', status: init, memory: 0, processing: 0>
2024-01-25 06:30:41,749 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:41,750 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45451
2024-01-25 06:30:41,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57600
2024-01-25 06:30:41,750 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-25 06:30:41,750 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:41,751 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-25 06:30:41,751 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-25 06:30:41,751 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:41,753 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-25 06:30:41,793 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-25 06:30:41,793 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-25 06:30:41,794 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-25 06:30:41,794 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-25 06:30:41,799 - distributed.scheduler - INFO - Remove client Client-40ba4903-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:41,799 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57556; closing.
2024-01-25 06:30:41,799 - distributed.scheduler - INFO - Remove client Client-40ba4903-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:41,800 - distributed.scheduler - INFO - Close client connection: Client-40ba4903-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:41,801 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40231'. Reason: nanny-close
2024-01-25 06:30:41,801 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:41,801 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38873'. Reason: nanny-close
2024-01-25 06:30:41,802 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:41,802 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34375'. Reason: nanny-close
2024-01-25 06:30:41,802 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45451. Reason: nanny-close
2024-01-25 06:30:41,802 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:41,803 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40379'. Reason: nanny-close
2024-01-25 06:30:41,803 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36461. Reason: nanny-close
2024-01-25 06:30:41,803 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:41,803 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42641. Reason: nanny-close
2024-01-25 06:30:41,803 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40793. Reason: nanny-close
2024-01-25 06:30:41,804 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-25 06:30:41,804 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-25 06:30:41,804 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57600; closing.
2024-01-25 06:30:41,805 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57588; closing.
2024-01-25 06:30:41,805 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-25 06:30:41,805 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45451', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164241.8052914')
2024-01-25 06:30:41,805 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-25 06:30:41,805 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36461', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164241.8058739')
2024-01-25 06:30:41,806 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:41,806 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:41,806 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:41,806 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57582; closing.
2024-01-25 06:30:41,806 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57608; closing.
2024-01-25 06:30:41,807 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:41,807 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40793', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164241.8073676')
2024-01-25 06:30:41,807 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42641', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164241.80778')
2024-01-25 06:30:41,807 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:30:41,808 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:57582>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:57582>: Stream is closed
2024-01-25 06:30:42,617 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:30:42,617 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:30:42,617 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:30:42,619 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-25 06:30:42,619 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-25 06:30:44,671 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:30:44,676 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38949 instead
  warnings.warn(
2024-01-25 06:30:44,679 - distributed.scheduler - INFO - State start
2024-01-25 06:30:44,702 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:30:44,703 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-25 06:30:44,704 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38949/status
2024-01-25 06:30:44,704 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:30:44,838 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33175'
2024-01-25 06:30:44,849 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32795'
2024-01-25 06:30:44,862 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33347'
2024-01-25 06:30:44,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36035'
2024-01-25 06:30:44,874 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33309'
2024-01-25 06:30:44,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38821'
2024-01-25 06:30:44,890 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34125'
2024-01-25 06:30:44,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43567'
2024-01-25 06:30:46,725 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:46,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:46,725 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:46,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:46,725 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:46,725 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:46,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:46,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:46,729 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:46,729 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:46,729 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:46,729 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:46,730 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45391
2024-01-25 06:30:46,730 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45391
2024-01-25 06:30:46,730 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44459
2024-01-25 06:30:46,730 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43853
2024-01-25 06:30:46,730 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44459
2024-01-25 06:30:46,730 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:46,730 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37231
2024-01-25 06:30:46,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:46,730 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41329
2024-01-25 06:30:46,730 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37231
2024-01-25 06:30:46,730 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:46,730 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:46,730 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46717
2024-01-25 06:30:46,730 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33759
2024-01-25 06:30:46,730 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:46,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:46,730 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:46,730 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33759
2024-01-25 06:30:46,730 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3k5fagvg
2024-01-25 06:30:46,730 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:46,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:46,730 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46557
2024-01-25 06:30:46,730 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:46,730 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:46,730 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:46,730 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hw3cn3z8
2024-01-25 06:30:46,730 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:46,731 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:46,731 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bxglm3md
2024-01-25 06:30:46,731 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bc360550-ada1-4f21-b52f-f7ff7b3afa70
2024-01-25 06:30:46,731 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:46,731 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:46,731 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kw3m7q_x
2024-01-25 06:30:46,731 - distributed.worker - INFO - Starting Worker plugin PreImport-81479739-110c-439c-bf7b-db45766b3434
2024-01-25 06:30:46,731 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb0c6ce4-63c0-4ad4-a51c-56e7b575ae39
2024-01-25 06:30:46,731 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c8e56277-e475-4f1c-8a1a-0d303c0a7581
2024-01-25 06:30:46,731 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d773408-0105-497d-bc0c-b49f284f6746
2024-01-25 06:30:46,731 - distributed.worker - INFO - Starting Worker plugin PreImport-3553c8db-9f4b-47e3-ae20-243c0ecf7955
2024-01-25 06:30:46,731 - distributed.worker - INFO - Starting Worker plugin PreImport-61ee6560-29f0-4722-a778-f5fac37eea0b
2024-01-25 06:30:46,731 - distributed.worker - INFO - Starting Worker plugin RMMSetup-302c403d-47b9-456f-9704-9dc3fe94cbf1
2024-01-25 06:30:46,731 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8bd2035b-1724-4a1e-a31a-d77c07233327
2024-01-25 06:30:46,731 - distributed.worker - INFO - Starting Worker plugin PreImport-165e8edd-8569-4b5b-9576-b26d02da11f8
2024-01-25 06:30:46,732 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad38cda8-22e1-470a-b056-849c5b6a2755
2024-01-25 06:30:46,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:46,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:46,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:46,742 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:46,746 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:46,746 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:46,747 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46331
2024-01-25 06:30:46,747 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37327
2024-01-25 06:30:46,747 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46331
2024-01-25 06:30:46,747 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37327
2024-01-25 06:30:46,747 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36111
2024-01-25 06:30:46,747 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46199
2024-01-25 06:30:46,747 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:46,747 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:46,747 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:46,747 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:46,747 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:46,747 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:46,747 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:46,747 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:46,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-455zc0z5
2024-01-25 06:30:46,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a_2fsm4t
2024-01-25 06:30:46,747 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c5fc7b6-7ec9-4273-b8d3-0d557097b104
2024-01-25 06:30:46,747 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6491e8cc-c093-43ca-9b0f-c59f514f8cc4
2024-01-25 06:30:46,750 - distributed.worker - INFO - Starting Worker plugin PreImport-8849d2da-668f-4b62-87a2-461c509338ac
2024-01-25 06:30:46,750 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6bd75e84-3775-43c3-9d69-d1253a9682ae
2024-01-25 06:30:46,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:46,806 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:46,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:46,811 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46353
2024-01-25 06:30:46,811 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46353
2024-01-25 06:30:46,811 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33947
2024-01-25 06:30:46,811 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:46,811 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:46,811 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:46,811 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:46,811 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1g2pi9b7
2024-01-25 06:30:46,811 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d784e99f-52f8-40ae-8648-55b65b7a1463
2024-01-25 06:30:46,812 - distributed.worker - INFO - Starting Worker plugin PreImport-688fb76b-d0bc-47db-a6d7-71d8c12cdb2a
2024-01-25 06:30:46,812 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9409b325-d2cd-4b17-89b6-7e14d761ccf7
2024-01-25 06:30:46,823 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:46,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:46,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:46,829 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39407
2024-01-25 06:30:46,829 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39407
2024-01-25 06:30:46,829 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33975
2024-01-25 06:30:46,829 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:46,829 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:46,829 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:46,829 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:46,829 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0z4_3a9e
2024-01-25 06:30:46,830 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c11b2765-6455-4f81-82b3-c6437f34d2b3
2024-01-25 06:30:46,830 - distributed.worker - INFO - Starting Worker plugin PreImport-2d65630d-6000-4d86-b92d-ceba2518ac22
2024-01-25 06:30:46,830 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ce2eee8-6b47-40da-8e72-91d6b6006ff6
2024-01-25 06:30:47,165 - distributed.scheduler - INFO - Receive client connection: Client-43d2b42b-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:47,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35138
2024-01-25 06:30:49,030 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,054 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44459', status: init, memory: 0, processing: 0>
2024-01-25 06:30:49,057 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44459
2024-01-25 06:30:49,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35154
2024-01-25 06:30:49,057 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:49,058 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:49,058 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:49,081 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-07493ca7-52fb-4ae8-8609-097199267880
2024-01-25 06:30:49,082 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,114 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33759', status: init, memory: 0, processing: 0>
2024-01-25 06:30:49,115 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33759
2024-01-25 06:30:49,115 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35162
2024-01-25 06:30:49,117 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:49,118 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:49,118 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,120 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:49,141 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,149 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,170 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45391', status: init, memory: 0, processing: 0>
2024-01-25 06:30:49,171 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45391
2024-01-25 06:30:49,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35182
2024-01-25 06:30:49,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:49,173 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37327', status: init, memory: 0, processing: 0>
2024-01-25 06:30:49,173 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:49,173 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,174 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37327
2024-01-25 06:30:49,174 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35178
2024-01-25 06:30:49,175 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:49,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:49,176 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:49,176 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:49,197 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,205 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,208 - distributed.worker - INFO - Starting Worker plugin PreImport-dd1621df-7b8c-40d2-90a4-06f6ca934fbd
2024-01-25 06:30:49,209 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca0c2fce-d626-4067-ac02-2a4863a16a22
2024-01-25 06:30:49,210 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,213 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,227 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37231', status: init, memory: 0, processing: 0>
2024-01-25 06:30:49,228 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37231
2024-01-25 06:30:49,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35196
2024-01-25 06:30:49,229 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:49,230 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:49,230 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,231 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46331', status: init, memory: 0, processing: 0>
2024-01-25 06:30:49,232 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46331
2024-01-25 06:30:49,232 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35214
2024-01-25 06:30:49,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:49,233 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:49,234 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:49,234 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:49,237 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39407', status: init, memory: 0, processing: 0>
2024-01-25 06:30:49,238 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39407
2024-01-25 06:30:49,238 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35222
2024-01-25 06:30:49,239 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:49,239 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:49,240 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,240 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46353', status: init, memory: 0, processing: 0>
2024-01-25 06:30:49,240 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46353
2024-01-25 06:30:49,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35206
2024-01-25 06:30:49,241 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:49,242 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:49,243 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:49,243 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:49,245 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:49,329 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:49,329 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:49,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:49,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:49,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:49,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:49,330 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:49,331 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:49,335 - distributed.scheduler - INFO - Remove client Client-43d2b42b-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:49,335 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35138; closing.
2024-01-25 06:30:49,336 - distributed.scheduler - INFO - Remove client Client-43d2b42b-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:49,336 - distributed.scheduler - INFO - Close client connection: Client-43d2b42b-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:49,337 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33175'. Reason: nanny-close
2024-01-25 06:30:49,338 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:49,338 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32795'. Reason: nanny-close
2024-01-25 06:30:49,339 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:49,339 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33347'. Reason: nanny-close
2024-01-25 06:30:49,339 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33759. Reason: nanny-close
2024-01-25 06:30:49,339 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:49,339 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36035'. Reason: nanny-close
2024-01-25 06:30:49,340 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37231. Reason: nanny-close
2024-01-25 06:30:49,340 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:49,340 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33309'. Reason: nanny-close
2024-01-25 06:30:49,340 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44459. Reason: nanny-close
2024-01-25 06:30:49,340 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:49,340 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38821'. Reason: nanny-close
2024-01-25 06:30:49,341 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45391. Reason: nanny-close
2024-01-25 06:30:49,341 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:49,341 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34125'. Reason: nanny-close
2024-01-25 06:30:49,341 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46353. Reason: nanny-close
2024-01-25 06:30:49,341 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:49,341 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43567'. Reason: nanny-close
2024-01-25 06:30:49,341 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:49,341 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37327. Reason: nanny-close
2024-01-25 06:30:49,342 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:49,342 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35162; closing.
2024-01-25 06:30:49,342 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:49,342 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46331. Reason: nanny-close
2024-01-25 06:30:49,342 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:49,342 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33759', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164249.342376')
2024-01-25 06:30:49,342 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:49,343 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39407. Reason: nanny-close
2024-01-25 06:30:49,343 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35196; closing.
2024-01-25 06:30:49,343 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35154; closing.
2024-01-25 06:30:49,343 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:49,343 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:49,343 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:49,344 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:49,344 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:49,344 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:49,344 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:49,344 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37231', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164249.34462')
2024-01-25 06:30:49,345 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44459', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164249.3449802')
2024-01-25 06:30:49,345 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:49,345 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35182; closing.
2024-01-25 06:30:49,345 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:49,345 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:49,346 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35206; closing.
2024-01-25 06:30:49,346 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:49,346 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45391', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164249.3464525')
2024-01-25 06:30:49,346 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:49,347 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46353', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164249.3472898')
2024-01-25 06:30:49,347 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35178; closing.
2024-01-25 06:30:49,347 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35214; closing.
2024-01-25 06:30:49,348 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35222; closing.
2024-01-25 06:30:49,348 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164249.3484237')
2024-01-25 06:30:49,348 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46331', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164249.3488724')
2024-01-25 06:30:49,349 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39407', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164249.349275')
2024-01-25 06:30:49,349 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:30:50,403 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:30:50,404 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:30:50,404 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:30:50,405 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-25 06:30:50,406 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-25 06:30:52,526 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:30:52,530 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45483 instead
  warnings.warn(
2024-01-25 06:30:52,534 - distributed.scheduler - INFO - State start
2024-01-25 06:30:53,080 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:30:53,082 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-25 06:30:53,083 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45483/status
2024-01-25 06:30:53,083 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:30:53,322 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43123'
2024-01-25 06:30:53,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36285'
2024-01-25 06:30:53,344 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43221'
2024-01-25 06:30:53,353 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39789'
2024-01-25 06:30:53,358 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39937'
2024-01-25 06:30:53,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44645'
2024-01-25 06:30:53,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36885'
2024-01-25 06:30:53,386 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36271'
2024-01-25 06:30:55,019 - distributed.scheduler - INFO - Receive client connection: Client-4880d856-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:55,032 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46264
2024-01-25 06:30:55,191 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:55,191 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:55,195 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:55,196 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38381
2024-01-25 06:30:55,196 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38381
2024-01-25 06:30:55,196 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37547
2024-01-25 06:30:55,196 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:55,196 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:55,196 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:55,196 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:55,196 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wccspu23
2024-01-25 06:30:55,196 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87db06d9-bbca-45de-8690-f78af853d452
2024-01-25 06:30:55,197 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea0cb73c-e444-4a72-85db-5f9d5a642dcd
2024-01-25 06:30:55,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:55,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:55,226 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:55,227 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35017
2024-01-25 06:30:55,227 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35017
2024-01-25 06:30:55,227 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38025
2024-01-25 06:30:55,227 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:55,227 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:55,227 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:55,227 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:55,227 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0rom4qk7
2024-01-25 06:30:55,228 - distributed.worker - INFO - Starting Worker plugin PreImport-81885aba-980c-4ec6-936d-b9f464cf5b2a
2024-01-25 06:30:55,228 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b67633c-f444-4221-9b9d-2a0aeef187e9
2024-01-25 06:30:55,243 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:55,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:55,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:55,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41015
2024-01-25 06:30:55,248 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41015
2024-01-25 06:30:55,248 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32799
2024-01-25 06:30:55,248 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:55,248 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:55,248 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:55,248 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:55,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x7kh5ikl
2024-01-25 06:30:55,248 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cfc32813-767c-4326-a0d1-83a34e0a5cb7
2024-01-25 06:30:55,249 - distributed.worker - INFO - Starting Worker plugin PreImport-dcec8384-b5c4-4910-a1ac-59675fe6dc89
2024-01-25 06:30:55,249 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07588d95-df9f-4355-ae3a-a0e59e50d6bf
2024-01-25 06:30:55,253 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:55,253 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:55,253 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:55,253 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:55,257 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:55,257 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:55,258 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41009
2024-01-25 06:30:55,258 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41009
2024-01-25 06:30:55,258 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37803
2024-01-25 06:30:55,258 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43189
2024-01-25 06:30:55,258 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37803
2024-01-25 06:30:55,258 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:55,258 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44035
2024-01-25 06:30:55,258 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:55,258 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:55,258 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:55,258 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:55,258 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:55,258 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:55,258 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kn_iqln8
2024-01-25 06:30:55,258 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:55,258 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fq4dv7d0
2024-01-25 06:30:55,258 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e1ddf22-c445-4134-9102-49fa8d15fb31
2024-01-25 06:30:55,258 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a948cdb8-6b04-4d15-9f0e-f96f1034768d
2024-01-25 06:30:55,259 - distributed.worker - INFO - Starting Worker plugin PreImport-8dd1e8ec-32df-4e0c-8506-6e347a811050
2024-01-25 06:30:55,260 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f78312d-631f-40e0-893b-9fcf170cad15
2024-01-25 06:30:55,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:55,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:55,311 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:55,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:55,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:55,312 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45639
2024-01-25 06:30:55,312 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45639
2024-01-25 06:30:55,312 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43337
2024-01-25 06:30:55,312 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:55,312 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:55,312 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:55,312 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:55,312 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j78gk32u
2024-01-25 06:30:55,312 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce95cf83-1e30-46f0-a59e-05ef07031810
2024-01-25 06:30:55,313 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e52fedd-c993-4689-9a96-341411e32276
2024-01-25 06:30:55,315 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:55,316 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36913
2024-01-25 06:30:55,316 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36913
2024-01-25 06:30:55,316 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41797
2024-01-25 06:30:55,316 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:55,316 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:55,317 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:55,317 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:55,317 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bz58koay
2024-01-25 06:30:55,317 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1aa2c1c-b71e-4083-b4e1-3b38f6c07a34
2024-01-25 06:30:55,488 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:30:55,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:30:55,503 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:30:55,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40789
2024-01-25 06:30:55,506 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40789
2024-01-25 06:30:55,506 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40511
2024-01-25 06:30:55,506 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:30:55,506 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:55,506 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:30:55,506 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:30:55,506 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hz0ws222
2024-01-25 06:30:55,507 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f93c801-19f5-453c-b848-b0995da7cb9f
2024-01-25 06:30:55,508 - distributed.worker - INFO - Starting Worker plugin PreImport-ec63215e-5475-4ae6-80e8-2224cb190659
2024-01-25 06:30:55,508 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b3f9e60-823a-49c0-9235-5b682c2d0006
2024-01-25 06:30:57,318 - distributed.worker - INFO - Starting Worker plugin PreImport-5d721392-2a51-4c74-b905-be0384331cfb
2024-01-25 06:30:57,320 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,353 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38381', status: init, memory: 0, processing: 0>
2024-01-25 06:30:57,354 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38381
2024-01-25 06:30:57,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46284
2024-01-25 06:30:57,355 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:57,356 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:57,356 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:57,486 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,518 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41015', status: init, memory: 0, processing: 0>
2024-01-25 06:30:57,519 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41015
2024-01-25 06:30:57,519 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46290
2024-01-25 06:30:57,519 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-18450ee6-af73-4931-a88d-f20267bbe0d1
2024-01-25 06:30:57,520 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:57,520 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,521 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:57,521 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,524 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:57,554 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35017', status: init, memory: 0, processing: 0>
2024-01-25 06:30:57,555 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35017
2024-01-25 06:30:57,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46304
2024-01-25 06:30:57,556 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:57,558 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:57,558 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,560 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:57,564 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,595 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37803', status: init, memory: 0, processing: 0>
2024-01-25 06:30:57,596 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37803
2024-01-25 06:30:57,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46318
2024-01-25 06:30:57,597 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:57,598 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:57,598 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,599 - distributed.worker - INFO - Starting Worker plugin PreImport-01538fd6-c55a-4b92-8713-2cf3f3ff0381
2024-01-25 06:30:57,600 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f9aad38-d8d4-4c02-b711-675908164c1f
2024-01-25 06:30:57,600 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:57,620 - distributed.worker - INFO - Starting Worker plugin PreImport-3670316f-cb82-4604-91dc-1ae2b0fc9b94
2024-01-25 06:30:57,621 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ffcd1188-fe1c-4ee6-bceb-43ca5b051bd0
2024-01-25 06:30:57,621 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,624 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41009', status: init, memory: 0, processing: 0>
2024-01-25 06:30:57,625 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41009
2024-01-25 06:30:57,625 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46326
2024-01-25 06:30:57,626 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:57,627 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:57,627 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,629 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:57,643 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36913', status: init, memory: 0, processing: 0>
2024-01-25 06:30:57,644 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36913
2024-01-25 06:30:57,644 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,644 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46332
2024-01-25 06:30:57,645 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:57,646 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:57,646 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,646 - distributed.worker - INFO - Starting Worker plugin PreImport-2208d68f-6974-4028-bab5-61f481db54df
2024-01-25 06:30:57,646 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,648 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:57,668 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40789', status: init, memory: 0, processing: 0>
2024-01-25 06:30:57,669 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40789
2024-01-25 06:30:57,669 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46340
2024-01-25 06:30:57,670 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45639', status: init, memory: 0, processing: 0>
2024-01-25 06:30:57,670 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:57,671 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45639
2024-01-25 06:30:57,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46344
2024-01-25 06:30:57,671 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:57,671 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,672 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:30:57,672 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:30:57,672 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:30:57,673 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:57,674 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:30:57,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:57,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:57,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:57,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:57,691 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:57,692 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:57,692 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:57,692 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:30:57,696 - distributed.scheduler - INFO - Remove client Client-4880d856-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:57,696 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46264; closing.
2024-01-25 06:30:57,696 - distributed.scheduler - INFO - Remove client Client-4880d856-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:57,697 - distributed.scheduler - INFO - Close client connection: Client-4880d856-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:30:57,698 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43123'. Reason: nanny-close
2024-01-25 06:30:57,698 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:57,698 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36285'. Reason: nanny-close
2024-01-25 06:30:57,699 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:57,699 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43221'. Reason: nanny-close
2024-01-25 06:30:57,699 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35017. Reason: nanny-close
2024-01-25 06:30:57,699 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:57,700 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39789'. Reason: nanny-close
2024-01-25 06:30:57,700 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38381. Reason: nanny-close
2024-01-25 06:30:57,700 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:57,700 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39937'. Reason: nanny-close
2024-01-25 06:30:57,700 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36913. Reason: nanny-close
2024-01-25 06:30:57,700 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:57,701 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44645'. Reason: nanny-close
2024-01-25 06:30:57,701 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41009. Reason: nanny-close
2024-01-25 06:30:57,701 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:57,701 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36885'. Reason: nanny-close
2024-01-25 06:30:57,701 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37803. Reason: nanny-close
2024-01-25 06:30:57,701 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36271'. Reason: nanny-close
2024-01-25 06:30:57,702 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41015. Reason: nanny-close
2024-01-25 06:30:57,702 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:57,702 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46304; closing.
2024-01-25 06:30:57,702 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:57,702 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:57,702 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35017', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164257.7024872')
2024-01-25 06:30:57,702 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:57,703 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46284; closing.
2024-01-25 06:30:57,703 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46332; closing.
2024-01-25 06:30:57,703 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:57,703 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:57,704 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:57,704 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:57,704 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:57,704 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:57,704 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38381', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164257.7047284')
2024-01-25 06:30:57,705 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36913', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164257.705083')
2024-01-25 06:30:57,705 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:57,705 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46326; closing.
2024-01-25 06:30:57,706 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:57,706 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46318; closing.
2024-01-25 06:30:57,706 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41009', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164257.7065084')
2024-01-25 06:30:57,706 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46290; closing.
2024-01-25 06:30:57,707 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37803', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164257.7072458')
2024-01-25 06:30:57,707 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41015', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164257.707596')
2024-01-25 06:30:57,724 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:57,724 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:30:57,725 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40789. Reason: nanny-close
2024-01-25 06:30:57,725 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45639. Reason: nanny-close
2024-01-25 06:30:57,727 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46344; closing.
2024-01-25 06:30:57,727 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:57,727 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:30:57,727 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45639', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164257.7276127')
2024-01-25 06:30:57,728 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46340; closing.
2024-01-25 06:30:57,728 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40789', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164257.7283354')
2024-01-25 06:30:57,728 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:30:57,728 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:57,728 - distributed.nanny - INFO - Worker closed
2024-01-25 06:30:57,728 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46340>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-25 06:30:58,814 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:30:58,814 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:30:58,815 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:30:58,816 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-25 06:30:58,816 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-25 06:31:00,908 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:00,912 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39897 instead
  warnings.warn(
2024-01-25 06:31:00,916 - distributed.scheduler - INFO - State start
2024-01-25 06:31:00,937 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:00,938 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-25 06:31:00,939 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39897/status
2024-01-25 06:31:00,939 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:31:01,133 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43637'
2024-01-25 06:31:01,146 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39669'
2024-01-25 06:31:01,159 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42297'
2024-01-25 06:31:01,172 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37027'
2024-01-25 06:31:01,176 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42187'
2024-01-25 06:31:01,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41353'
2024-01-25 06:31:01,199 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37839'
2024-01-25 06:31:01,211 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45939'
2024-01-25 06:31:02,501 - distributed.scheduler - INFO - Receive client connection: Client-4d7bc7fb-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:02,517 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55462
2024-01-25 06:31:03,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:03,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:03,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:03,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:03,061 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:03,061 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:03,062 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35223
2024-01-25 06:31:03,062 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42255
2024-01-25 06:31:03,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35223
2024-01-25 06:31:03,062 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42255
2024-01-25 06:31:03,062 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32777
2024-01-25 06:31:03,062 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35655
2024-01-25 06:31:03,062 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:03,062 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:03,062 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:03,062 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:03,062 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:03,062 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:03,062 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:03,062 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:03,062 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4alhdut4
2024-01-25 06:31:03,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-635oxsui
2024-01-25 06:31:03,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-270e2040-b8f3-47ce-9c5d-4177b8aff105
2024-01-25 06:31:03,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-52fb1053-27fd-4cc8-b000-604f3ee3decd
2024-01-25 06:31:03,063 - distributed.worker - INFO - Starting Worker plugin PreImport-f49b5e68-c21d-4fac-9df1-8990e3a0f28d
2024-01-25 06:31:03,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cef657d9-8119-47ea-83c0-0a3bf10877c7
2024-01-25 06:31:03,063 - distributed.worker - INFO - Starting Worker plugin PreImport-8b37aea0-83bb-4978-a443-92bd7f4404b9
2024-01-25 06:31:03,064 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62c08716-c81b-4914-84d9-da7b67945bcc
2024-01-25 06:31:03,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:03,069 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:03,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:03,069 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:03,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:03,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:03,074 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36083
2024-01-25 06:31:03,074 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36083
2024-01-25 06:31:03,074 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36101
2024-01-25 06:31:03,074 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44245
2024-01-25 06:31:03,074 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:03,074 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44245
2024-01-25 06:31:03,074 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:03,074 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40609
2024-01-25 06:31:03,074 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:03,074 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:03,074 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:03,074 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:03,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zv1gcvh2
2024-01-25 06:31:03,074 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:03,074 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:03,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r8iw6tau
2024-01-25 06:31:03,074 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dc5859dd-7443-4c52-a87b-a689046d0f6c
2024-01-25 06:31:03,074 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e2d3bd5-d798-4a71-9d32-608aaf5aeeb5
2024-01-25 06:31:03,075 - distributed.worker - INFO - Starting Worker plugin PreImport-dc7a7945-6613-4fdf-92ae-7717c6fcc334
2024-01-25 06:31:03,076 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ed6b6c5-2e19-46f6-8611-a4b3af73975c
2024-01-25 06:31:03,087 - distributed.worker - INFO - Starting Worker plugin PreImport-e4683021-4df7-46ab-ad85-bd9dd4039edc
2024-01-25 06:31:03,088 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b018211c-7397-4bb4-a220-fe7d860422ec
2024-01-25 06:31:03,136 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:03,136 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:03,144 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:03,146 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39899
2024-01-25 06:31:03,146 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39899
2024-01-25 06:31:03,146 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45529
2024-01-25 06:31:03,146 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:03,146 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:03,146 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:03,147 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:03,147 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bolmyhtw
2024-01-25 06:31:03,147 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9760b29-044f-4b6c-82c0-a8db34377fed
2024-01-25 06:31:03,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:03,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:03,336 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:03,336 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:03,336 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:03,336 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43655
2024-01-25 06:31:03,337 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43655
2024-01-25 06:31:03,337 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40325
2024-01-25 06:31:03,337 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:03,337 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:03,337 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:03,337 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:03,337 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s7a_3u30
2024-01-25 06:31:03,337 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c93a7498-b60d-4e02-a14b-f7c548340e09
2024-01-25 06:31:03,337 - distributed.worker - INFO - Starting Worker plugin PreImport-5e37818a-45ed-4fd4-9be9-70ce8820d9f8
2024-01-25 06:31:03,337 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e880aa90-561f-4594-9383-f06b52931493
2024-01-25 06:31:03,341 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:03,342 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33889
2024-01-25 06:31:03,342 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33889
2024-01-25 06:31:03,342 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43251
2024-01-25 06:31:03,342 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:03,342 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:03,342 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:03,342 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:03,342 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nn9wci9o
2024-01-25 06:31:03,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d5c9d26-b6b3-4fcc-8c3c-19a67a00d1d0
2024-01-25 06:31:03,342 - distributed.worker - INFO - Starting Worker plugin PreImport-dbd18c56-b4f8-4741-9c5f-6ef0160c3450
2024-01-25 06:31:03,343 - distributed.worker - INFO - Starting Worker plugin RMMSetup-781f47a2-fe62-414d-9fcc-2b7844244bf7
2024-01-25 06:31:03,376 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:03,377 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:03,383 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:03,384 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46301
2024-01-25 06:31:03,384 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46301
2024-01-25 06:31:03,384 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43629
2024-01-25 06:31:03,384 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:03,384 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:03,384 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:03,385 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:03,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bk4s8gsb
2024-01-25 06:31:03,385 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e406ab45-0fab-4bb4-b723-afd3391a55a4
2024-01-25 06:31:03,385 - distributed.worker - INFO - Starting Worker plugin PreImport-ce6e1a42-a81e-4ad7-bb79-d8245156d1ff
2024-01-25 06:31:03,385 - distributed.worker - INFO - Starting Worker plugin RMMSetup-421908d7-0ff6-460f-a178-e2c1b92b979b
2024-01-25 06:31:04,878 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:04,904 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42255', status: init, memory: 0, processing: 0>
2024-01-25 06:31:04,908 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42255
2024-01-25 06:31:04,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55486
2024-01-25 06:31:04,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:04,909 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:04,910 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:04,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:06,025 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,062 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44245', status: init, memory: 0, processing: 0>
2024-01-25 06:31:06,063 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44245
2024-01-25 06:31:06,063 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55506
2024-01-25 06:31:06,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:06,065 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:06,065 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,067 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:06,207 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,245 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35223', status: init, memory: 0, processing: 0>
2024-01-25 06:31:06,245 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35223
2024-01-25 06:31:06,245 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55510
2024-01-25 06:31:06,247 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:06,248 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:06,248 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:06,780 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,805 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43655', status: init, memory: 0, processing: 0>
2024-01-25 06:31:06,806 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43655
2024-01-25 06:31:06,806 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55524
2024-01-25 06:31:06,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:06,808 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:06,808 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:06,811 - distributed.worker - INFO - Starting Worker plugin PreImport-d76c3167-f382-42a1-8b0c-cc94b4b7d053
2024-01-25 06:31:06,812 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa9c9e6b-ba10-43fc-b0e6-f0ac9eaee5b6
2024-01-25 06:31:06,812 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,813 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,834 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39899', status: init, memory: 0, processing: 0>
2024-01-25 06:31:06,834 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39899
2024-01-25 06:31:06,834 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55538
2024-01-25 06:31:06,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:06,836 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:06,836 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:06,845 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36083', status: init, memory: 0, processing: 0>
2024-01-25 06:31:06,846 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36083
2024-01-25 06:31:06,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55552
2024-01-25 06:31:06,848 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:06,848 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:06,848 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,851 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:06,870 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,884 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,903 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33889', status: init, memory: 0, processing: 0>
2024-01-25 06:31:06,904 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33889
2024-01-25 06:31:06,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55556
2024-01-25 06:31:06,905 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46301', status: init, memory: 0, processing: 0>
2024-01-25 06:31:06,905 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46301
2024-01-25 06:31:06,905 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55568
2024-01-25 06:31:06,905 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:06,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:06,906 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:06,906 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,907 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:06,907 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:06,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:06,909 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:06,938 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:06,938 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:06,938 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:06,938 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:06,939 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:06,939 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:06,939 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:06,939 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:06,959 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:06,959 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:06,959 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:06,959 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:06,959 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:06,959 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:06,959 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:06,959 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:06,970 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:06,972 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:06,975 - distributed.scheduler - INFO - Remove client Client-4d7bc7fb-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:06,975 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55462; closing.
2024-01-25 06:31:06,975 - distributed.scheduler - INFO - Remove client Client-4d7bc7fb-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:06,976 - distributed.scheduler - INFO - Close client connection: Client-4d7bc7fb-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:06,976 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43637'. Reason: nanny-close
2024-01-25 06:31:06,977 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:06,977 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39669'. Reason: nanny-close
2024-01-25 06:31:06,978 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:06,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42297'. Reason: nanny-close
2024-01-25 06:31:06,978 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35223. Reason: nanny-close
2024-01-25 06:31:06,978 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:06,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37027'. Reason: nanny-close
2024-01-25 06:31:06,979 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36083. Reason: nanny-close
2024-01-25 06:31:06,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:06,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42187'. Reason: nanny-close
2024-01-25 06:31:06,979 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42255. Reason: nanny-close
2024-01-25 06:31:06,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:06,980 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41353'. Reason: nanny-close
2024-01-25 06:31:06,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43655. Reason: nanny-close
2024-01-25 06:31:06,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:06,980 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37839'. Reason: nanny-close
2024-01-25 06:31:06,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44245. Reason: nanny-close
2024-01-25 06:31:06,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:06,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45939'. Reason: nanny-close
2024-01-25 06:31:06,981 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:06,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33889. Reason: nanny-close
2024-01-25 06:31:06,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:06,981 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55510; closing.
2024-01-25 06:31:06,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39899. Reason: nanny-close
2024-01-25 06:31:06,981 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:06,981 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35223', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164266.9817085')
2024-01-25 06:31:06,981 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:06,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:06,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46301. Reason: nanny-close
2024-01-25 06:31:06,982 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55552; closing.
2024-01-25 06:31:06,983 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55486; closing.
2024-01-25 06:31:06,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:06,983 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:06,983 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:06,983 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:06,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:06,983 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:06,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:06,984 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36083', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164266.9841108')
2024-01-25 06:31:06,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:06,984 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42255', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164266.984462')
2024-01-25 06:31:06,984 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:06,984 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:06,985 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55524; closing.
2024-01-25 06:31:06,985 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:06,985 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:06,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55506; closing.
2024-01-25 06:31:06,986 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43655', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164266.9864666')
2024-01-25 06:31:06,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55556; closing.
2024-01-25 06:31:06,987 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55538; closing.
2024-01-25 06:31:06,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44245', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164266.9874046')
2024-01-25 06:31:06,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164266.9877398')
2024-01-25 06:31:06,988 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39899', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164266.9881296')
2024-01-25 06:31:06,988 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55568; closing.
2024-01-25 06:31:06,988 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46301', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164266.988827')
2024-01-25 06:31:06,989 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:31:07,842 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:31:07,843 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:31:07,843 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:31:07,845 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-25 06:31:07,845 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-25 06:31:10,114 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:10,118 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37253 instead
  warnings.warn(
2024-01-25 06:31:10,122 - distributed.scheduler - INFO - State start
2024-01-25 06:31:10,144 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:10,145 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-25 06:31:10,146 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37253/status
2024-01-25 06:31:10,146 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:31:10,426 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35683'
2024-01-25 06:31:10,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45277'
2024-01-25 06:31:10,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35905'
2024-01-25 06:31:10,462 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34623'
2024-01-25 06:31:10,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46321'
2024-01-25 06:31:10,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44485'
2024-01-25 06:31:10,487 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40725'
2024-01-25 06:31:10,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42825'
2024-01-25 06:31:10,879 - distributed.scheduler - INFO - Receive client connection: Client-52e81686-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:10,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59738
2024-01-25 06:31:12,296 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:12,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:12,300 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:12,301 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42129
2024-01-25 06:31:12,301 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42129
2024-01-25 06:31:12,301 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35323
2024-01-25 06:31:12,301 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:12,301 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:12,301 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:12,301 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:12,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3kkdvvf8
2024-01-25 06:31:12,302 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5647071d-03d3-4d4d-a5e2-1b536f414ef3
2024-01-25 06:31:12,302 - distributed.worker - INFO - Starting Worker plugin PreImport-529ca6ef-783f-4cc6-812b-2358df0507e0
2024-01-25 06:31:12,302 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59ecd665-954a-46b4-9486-412a4d512c9f
2024-01-25 06:31:12,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:12,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:12,316 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:12,316 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:12,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:12,319 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:12,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:12,320 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33009
2024-01-25 06:31:12,320 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33009
2024-01-25 06:31:12,320 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45657
2024-01-25 06:31:12,320 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:12,321 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:12,321 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:12,321 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:12,321 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xst8bao0
2024-01-25 06:31:12,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:12,321 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-933e868c-9353-4ce7-882d-977a75b3e8e7
2024-01-25 06:31:12,321 - distributed.worker - INFO - Starting Worker plugin PreImport-96493cae-d3f3-43ed-aa62-4c1932dface9
2024-01-25 06:31:12,321 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef73f648-5b26-4917-a56f-73ed5b710d71
2024-01-25 06:31:12,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36883
2024-01-25 06:31:12,322 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36883
2024-01-25 06:31:12,322 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35807
2024-01-25 06:31:12,322 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:12,322 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:12,322 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:12,322 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:12,322 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s70nz34j
2024-01-25 06:31:12,322 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7df5d4bf-2e65-4e17-a642-8b6290354564
2024-01-25 06:31:12,323 - distributed.worker - INFO - Starting Worker plugin PreImport-ca17da3f-d4bd-4525-b393-a1d03db4bf0c
2024-01-25 06:31:12,324 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c3dac85-b85c-4dcd-9b9f-5f55c1e9d605
2024-01-25 06:31:12,324 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:12,325 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33289
2024-01-25 06:31:12,325 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33289
2024-01-25 06:31:12,325 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40119
2024-01-25 06:31:12,325 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:12,325 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:12,325 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:12,325 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:12,325 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mdv1eymg
2024-01-25 06:31:12,326 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e5755a36-097d-465c-a1fa-80c69104fe5d
2024-01-25 06:31:12,327 - distributed.worker - INFO - Starting Worker plugin PreImport-341e9426-0f56-4c83-8603-0efc82e8e6b7
2024-01-25 06:31:12,328 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97cfcc19-72a8-4c00-9474-cdd7c6e77d0d
2024-01-25 06:31:12,351 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:12,351 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:12,355 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:12,356 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35291
2024-01-25 06:31:12,356 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35291
2024-01-25 06:31:12,356 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41155
2024-01-25 06:31:12,356 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:12,356 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:12,356 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:12,356 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:12,356 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ns3hw09l
2024-01-25 06:31:12,357 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c5a254f-8f34-42d9-a3c3-791c09e884a8
2024-01-25 06:31:12,357 - distributed.worker - INFO - Starting Worker plugin PreImport-124adc9d-de6b-4ec9-a92a-9dccf5ed01d8
2024-01-25 06:31:12,357 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5db31edf-74eb-4e4f-adbd-166d93fe723f
2024-01-25 06:31:12,444 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:12,444 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:12,449 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:12,449 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40529
2024-01-25 06:31:12,449 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40529
2024-01-25 06:31:12,450 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44669
2024-01-25 06:31:12,450 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:12,450 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:12,450 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:12,450 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:12,450 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3r4fdl6m
2024-01-25 06:31:12,450 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7aa5c2ab-4342-4187-a9da-3d61b793006e
2024-01-25 06:31:12,472 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:12,473 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:12,476 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:12,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:12,477 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:12,478 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39793
2024-01-25 06:31:12,478 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39793
2024-01-25 06:31:12,478 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39275
2024-01-25 06:31:12,478 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:12,478 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:12,478 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:12,478 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:12,478 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_mli8evl
2024-01-25 06:31:12,478 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc6d1153-c162-49e8-b5d6-1855e61c42af
2024-01-25 06:31:12,479 - distributed.worker - INFO - Starting Worker plugin PreImport-dfa1aa25-8cb7-4f08-a206-d87b25206551
2024-01-25 06:31:12,479 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17e6064b-12ac-4c61-a6ce-bba56a404979
2024-01-25 06:31:12,481 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:12,481 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35431
2024-01-25 06:31:12,482 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35431
2024-01-25 06:31:12,482 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46125
2024-01-25 06:31:12,482 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:12,482 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:12,482 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:12,482 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:12,482 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_0ie_wr1
2024-01-25 06:31:12,482 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd335222-0a7b-4741-894b-fe0f7996fbae
2024-01-25 06:31:12,482 - distributed.worker - INFO - Starting Worker plugin PreImport-789fbb90-6034-4465-93a6-8ca110a09b35
2024-01-25 06:31:12,482 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b2959ff-fb90-46d1-b3c8-63f16ad65721
2024-01-25 06:31:14,598 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,619 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,632 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,633 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42129', status: init, memory: 0, processing: 0>
2024-01-25 06:31:14,635 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42129
2024-01-25 06:31:14,635 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59750
2024-01-25 06:31:14,637 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:14,637 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:14,638 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:14,642 - distributed.worker - INFO - Starting Worker plugin PreImport-d0f4c21e-33f2-46f3-9109-4ee5855feba0
2024-01-25 06:31:14,642 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a49f2d22-2097-4950-9e0a-a0e94f9ad406
2024-01-25 06:31:14,643 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,652 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36883', status: init, memory: 0, processing: 0>
2024-01-25 06:31:14,653 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36883
2024-01-25 06:31:14,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59760
2024-01-25 06:31:14,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:14,655 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:14,656 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,658 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:14,658 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35291', status: init, memory: 0, processing: 0>
2024-01-25 06:31:14,659 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35291
2024-01-25 06:31:14,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59776
2024-01-25 06:31:14,660 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:14,660 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:14,660 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,662 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:14,662 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,664 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,668 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40529', status: init, memory: 0, processing: 0>
2024-01-25 06:31:14,668 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40529
2024-01-25 06:31:14,668 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59784
2024-01-25 06:31:14,669 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:14,670 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:14,670 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:14,680 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,680 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,697 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33289', status: init, memory: 0, processing: 0>
2024-01-25 06:31:14,698 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33289
2024-01-25 06:31:14,698 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59786
2024-01-25 06:31:14,700 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:14,700 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33009', status: init, memory: 0, processing: 0>
2024-01-25 06:31:14,701 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33009
2024-01-25 06:31:14,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59798
2024-01-25 06:31:14,701 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:14,701 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,702 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:14,703 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39793', status: init, memory: 0, processing: 0>
2024-01-25 06:31:14,703 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:14,703 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,703 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:14,704 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39793
2024-01-25 06:31:14,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59800
2024-01-25 06:31:14,705 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:14,705 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35431', status: init, memory: 0, processing: 0>
2024-01-25 06:31:14,705 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:14,705 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,706 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35431
2024-01-25 06:31:14,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59802
2024-01-25 06:31:14,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:14,707 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:14,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:14,707 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:14,707 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:14,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:14,801 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:14,801 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:14,802 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:14,802 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:14,802 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:14,802 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:14,802 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:14,803 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:14,814 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:14,814 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:14,814 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:14,815 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:14,815 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:14,815 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:14,815 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:14,815 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:31:14,823 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:14,825 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:14,827 - distributed.scheduler - INFO - Remove client Client-52e81686-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:14,827 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59738; closing.
2024-01-25 06:31:14,827 - distributed.scheduler - INFO - Remove client Client-52e81686-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:14,828 - distributed.scheduler - INFO - Close client connection: Client-52e81686-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:14,829 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35683'. Reason: nanny-close
2024-01-25 06:31:14,829 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:14,830 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45277'. Reason: nanny-close
2024-01-25 06:31:14,830 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:14,830 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35905'. Reason: nanny-close
2024-01-25 06:31:14,831 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:14,831 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33289. Reason: nanny-close
2024-01-25 06:31:14,831 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34623'. Reason: nanny-close
2024-01-25 06:31:14,831 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:14,831 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46321'. Reason: nanny-close
2024-01-25 06:31:14,831 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33009. Reason: nanny-close
2024-01-25 06:31:14,832 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35291. Reason: nanny-close
2024-01-25 06:31:14,832 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:14,832 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44485'. Reason: nanny-close
2024-01-25 06:31:14,832 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35431. Reason: nanny-close
2024-01-25 06:31:14,832 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:14,832 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40725'. Reason: nanny-close
2024-01-25 06:31:14,833 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:14,833 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42825'. Reason: nanny-close
2024-01-25 06:31:14,833 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36883. Reason: nanny-close
2024-01-25 06:31:14,833 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:14,833 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42129. Reason: nanny-close
2024-01-25 06:31:14,833 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40529. Reason: nanny-close
2024-01-25 06:31:14,833 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:14,833 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59802; closing.
2024-01-25 06:31:14,834 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:14,834 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:14,834 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39793. Reason: nanny-close
2024-01-25 06:31:14,834 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35431', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164274.834284')
2024-01-25 06:31:14,834 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:14,834 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59776; closing.
2024-01-25 06:31:14,835 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:14,835 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:14,835 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:14,835 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35291', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164274.8355513')
2024-01-25 06:31:14,835 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59786; closing.
2024-01-25 06:31:14,836 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:14,836 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:14,836 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:14,836 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:14,836 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:14,837 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33289', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164274.837038')
2024-01-25 06:31:14,837 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:14,837 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59798; closing.
2024-01-25 06:31:14,837 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:14,837 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:14,839 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:14,838 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:59776>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-25 06:31:14,840 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33009', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164274.8403494')
2024-01-25 06:31:14,840 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59784; closing.
2024-01-25 06:31:14,841 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59750; closing.
2024-01-25 06:31:14,841 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59760; closing.
2024-01-25 06:31:14,841 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59800; closing.
2024-01-25 06:31:14,841 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40529', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164274.8418386')
2024-01-25 06:31:14,842 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42129', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164274.8423183')
2024-01-25 06:31:14,842 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36883', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164274.8426297')
2024-01-25 06:31:14,843 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39793', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164274.8429658')
2024-01-25 06:31:14,843 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:31:15,945 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:31:15,946 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:31:15,947 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:31:15,949 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-25 06:31:15,949 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-25 06:31:18,046 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:18,050 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40455 instead
  warnings.warn(
2024-01-25 06:31:18,054 - distributed.scheduler - INFO - State start
2024-01-25 06:31:18,082 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:18,083 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-25 06:31:18,084 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40455/status
2024-01-25 06:31:18,084 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:31:18,404 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41381'
2024-01-25 06:31:18,416 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45477'
2024-01-25 06:31:18,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35973'
2024-01-25 06:31:18,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40031'
2024-01-25 06:31:18,442 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38987'
2024-01-25 06:31:18,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45699'
2024-01-25 06:31:18,459 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36243'
2024-01-25 06:31:18,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33611'
2024-01-25 06:31:18,741 - distributed.scheduler - INFO - Receive client connection: Client-57b873c9-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:18,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59956
2024-01-25 06:31:20,299 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:20,299 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:20,299 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:20,299 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:20,303 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:20,303 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:20,304 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33433
2024-01-25 06:31:20,304 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33433
2024-01-25 06:31:20,304 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32825
2024-01-25 06:31:20,304 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43373
2024-01-25 06:31:20,304 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32825
2024-01-25 06:31:20,304 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:20,304 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:20,304 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43815
2024-01-25 06:31:20,304 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:20,304 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:20,305 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:20,305 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:20,305 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:20,305 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7h3vqhty
2024-01-25 06:31:20,305 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:20,305 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_w53uhn8
2024-01-25 06:31:20,305 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-43f5afb9-194f-4779-a543-316a03ada250
2024-01-25 06:31:20,305 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab361d2e-100a-4ad6-b1c8-3fe8244017fe
2024-01-25 06:31:20,305 - distributed.worker - INFO - Starting Worker plugin PreImport-5f2a2cd1-325e-4266-90f8-3a416cecde51
2024-01-25 06:31:20,305 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e494e62f-ebf1-45e3-bf4b-6cd2d27df49b
2024-01-25 06:31:20,306 - distributed.worker - INFO - Starting Worker plugin PreImport-0ca88fb1-e6e1-4555-8404-063bf6c97a6a
2024-01-25 06:31:20,307 - distributed.worker - INFO - Starting Worker plugin RMMSetup-75a147c1-9b57-4147-8b5e-4ade0fbaa3c7
2024-01-25 06:31:20,307 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:20,307 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:20,307 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:20,307 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:20,311 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:20,311 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:20,312 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36441
2024-01-25 06:31:20,312 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36441
2024-01-25 06:31:20,312 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41231
2024-01-25 06:31:20,312 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:20,312 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39233
2024-01-25 06:31:20,312 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:20,312 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39233
2024-01-25 06:31:20,312 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:20,312 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42207
2024-01-25 06:31:20,312 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:20,312 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:20,312 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ij9m0a0t
2024-01-25 06:31:20,312 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:20,312 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:20,313 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:20,313 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a5knqzyo
2024-01-25 06:31:20,313 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8de5052b-9d83-4e98-8d1e-cbc8cc88c239
2024-01-25 06:31:20,313 - distributed.worker - INFO - Starting Worker plugin PreImport-4cbe18b0-618e-42b4-8daf-d0e7c7a5fdd5
2024-01-25 06:31:20,313 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac897991-928c-44ea-9ae9-472953e317cf
2024-01-25 06:31:20,313 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc9807e6-c065-44c0-9b1c-c536bc2913ff
2024-01-25 06:31:20,314 - distributed.worker - INFO - Starting Worker plugin PreImport-14a82a48-4625-456d-8858-0cae6403658b
2024-01-25 06:31:20,314 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0a948fa-4ff6-4ee8-93cc-88b441840ea7
2024-01-25 06:31:20,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:20,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:20,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:20,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:20,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:20,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:20,366 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:20,367 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41697
2024-01-25 06:31:20,367 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41697
2024-01-25 06:31:20,367 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46315
2024-01-25 06:31:20,367 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:20,367 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:20,367 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:20,367 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:20,367 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qg3u0jlv
2024-01-25 06:31:20,367 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb3c97f1-a8da-4400-9143-25599efc572f
2024-01-25 06:31:20,368 - distributed.worker - INFO - Starting Worker plugin PreImport-a5543898-f313-4dfe-b658-111d3135773f
2024-01-25 06:31:20,368 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5574161-9d5b-4b3f-aaf2-76171b579dbc
2024-01-25 06:31:20,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:20,369 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:20,369 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36015
2024-01-25 06:31:20,369 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36015
2024-01-25 06:31:20,369 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43311
2024-01-25 06:31:20,369 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:20,369 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:20,369 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:20,369 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:20,369 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rsggrrp_
2024-01-25 06:31:20,369 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0437fe30-31e4-40a5-8d37-02a741a7f88e
2024-01-25 06:31:20,370 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33103
2024-01-25 06:31:20,370 - distributed.worker - INFO - Starting Worker plugin PreImport-e9b2e666-4d42-4de2-868d-db4abc318b16
2024-01-25 06:31:20,370 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33103
2024-01-25 06:31:20,370 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42487
2024-01-25 06:31:20,370 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8263e31-1d77-4939-8dca-cfcca780539e
2024-01-25 06:31:20,370 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:20,370 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:20,370 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:20,370 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:20,370 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mltfsab0
2024-01-25 06:31:20,370 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c2e0e0a-5fc0-4000-bfda-3131d1416f8b
2024-01-25 06:31:20,435 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:20,436 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:20,440 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:20,441 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46165
2024-01-25 06:31:20,441 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46165
2024-01-25 06:31:20,441 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34805
2024-01-25 06:31:20,441 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:20,441 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:20,441 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:20,441 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:20,441 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ny3872qn
2024-01-25 06:31:20,441 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dd510155-f714-4f8d-a8ab-da35db7b04f9
2024-01-25 06:31:20,442 - distributed.worker - INFO - Starting Worker plugin PreImport-62414295-fb1c-4320-a4fb-d552200bb892
2024-01-25 06:31:20,442 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fae11452-0d5c-460b-b4ff-69d4b96096ea
2024-01-25 06:31:22,475 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:22,498 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36441', status: init, memory: 0, processing: 0>
2024-01-25 06:31:22,500 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36441
2024-01-25 06:31:22,500 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53666
2024-01-25 06:31:22,501 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:22,502 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:22,502 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:22,503 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:22,579 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:22,584 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:22,584 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:22,613 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39233', status: init, memory: 0, processing: 0>
2024-01-25 06:31:22,614 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39233
2024-01-25 06:31:22,614 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53676
2024-01-25 06:31:22,616 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:22,617 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:22,617 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:22,617 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33433', status: init, memory: 0, processing: 0>
2024-01-25 06:31:22,618 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33433
2024-01-25 06:31:22,618 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53692
2024-01-25 06:31:22,619 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32825', status: init, memory: 0, processing: 0>
2024-01-25 06:31:22,619 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:22,619 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:22,619 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32825
2024-01-25 06:31:22,619 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53708
2024-01-25 06:31:22,620 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:22,620 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:22,621 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:22,622 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:22,622 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:22,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:22,625 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:23,209 - distributed.worker - INFO - Starting Worker plugin PreImport-a26a02ee-7dde-46a4-a612-7c73fa764c80
2024-01-25 06:31:23,210 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb04a89e-e4e7-4e7d-85a7-7ee0dbdf03d7
2024-01-25 06:31:23,211 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:23,233 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33103', status: init, memory: 0, processing: 0>
2024-01-25 06:31:23,234 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33103
2024-01-25 06:31:23,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53718
2024-01-25 06:31:23,235 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:23,236 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:23,236 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:23,238 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:23,366 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:23,386 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:23,389 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41697', status: init, memory: 0, processing: 0>
2024-01-25 06:31:23,390 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41697
2024-01-25 06:31:23,390 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53726
2024-01-25 06:31:23,391 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:23,392 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:23,392 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:23,394 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:23,409 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36015', status: init, memory: 0, processing: 0>
2024-01-25 06:31:23,410 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36015
2024-01-25 06:31:23,410 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53734
2024-01-25 06:31:23,411 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:23,412 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:23,412 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:23,413 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:23,934 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:23,969 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46165', status: init, memory: 0, processing: 0>
2024-01-25 06:31:23,970 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46165
2024-01-25 06:31:23,970 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53776
2024-01-25 06:31:23,972 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:23,973 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:23,973 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:23,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:24,045 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:24,046 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:24,046 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:24,046 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:24,046 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:24,046 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:24,046 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:24,047 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:24,051 - distributed.scheduler - INFO - Remove client Client-57b873c9-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:24,051 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59956; closing.
2024-01-25 06:31:24,051 - distributed.scheduler - INFO - Remove client Client-57b873c9-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:24,052 - distributed.scheduler - INFO - Close client connection: Client-57b873c9-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:24,053 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41381'. Reason: nanny-close
2024-01-25 06:31:24,053 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:24,054 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45477'. Reason: nanny-close
2024-01-25 06:31:24,054 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:24,055 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35973'. Reason: nanny-close
2024-01-25 06:31:24,055 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33433. Reason: nanny-close
2024-01-25 06:31:24,055 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:24,056 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40031'. Reason: nanny-close
2024-01-25 06:31:24,056 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32825. Reason: nanny-close
2024-01-25 06:31:24,056 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:24,056 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38987'. Reason: nanny-close
2024-01-25 06:31:24,056 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41697. Reason: nanny-close
2024-01-25 06:31:24,056 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:24,057 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45699'. Reason: nanny-close
2024-01-25 06:31:24,057 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36441. Reason: nanny-close
2024-01-25 06:31:24,057 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:24,057 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36243'. Reason: nanny-close
2024-01-25 06:31:24,057 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39233. Reason: nanny-close
2024-01-25 06:31:24,057 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:24,058 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33611'. Reason: nanny-close
2024-01-25 06:31:24,058 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:24,058 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46165. Reason: nanny-close
2024-01-25 06:31:24,058 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:24,058 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:24,058 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53692; closing.
2024-01-25 06:31:24,058 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:24,058 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33103. Reason: nanny-close
2024-01-25 06:31:24,058 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33433', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164284.0587447')
2024-01-25 06:31:24,058 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:24,059 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36015. Reason: nanny-close
2024-01-25 06:31:24,059 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53708; closing.
2024-01-25 06:31:24,059 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53726; closing.
2024-01-25 06:31:24,059 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:24,059 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:24,059 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:24,059 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:24,060 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32825', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164284.0600352')
2024-01-25 06:31:24,060 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:24,060 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:24,060 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41697', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164284.060416')
2024-01-25 06:31:24,060 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:24,060 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:24,061 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:24,061 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:24,061 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53676; closing.
2024-01-25 06:31:24,061 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53666; closing.
2024-01-25 06:31:24,062 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:24,062 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:24,062 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53708>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-25 06:31:24,064 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53726>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-25 06:31:24,064 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39233', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164284.0644393')
2024-01-25 06:31:24,064 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36441', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164284.0648139')
2024-01-25 06:31:24,065 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53718; closing.
2024-01-25 06:31:24,065 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53776; closing.
2024-01-25 06:31:24,066 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33103', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164284.0660183')
2024-01-25 06:31:24,066 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46165', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164284.066392')
2024-01-25 06:31:24,066 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53734; closing.
2024-01-25 06:31:24,067 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36015', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164284.0671163')
2024-01-25 06:31:24,067 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:31:24,918 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:31:24,919 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:31:24,919 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:31:24,920 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-25 06:31:24,921 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-25 06:31:27,177 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:27,185 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35879 instead
  warnings.warn(
2024-01-25 06:31:27,191 - distributed.scheduler - INFO - State start
2024-01-25 06:31:27,215 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:27,217 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-25 06:31:27,218 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35879/status
2024-01-25 06:31:27,218 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:31:27,287 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46579'
2024-01-25 06:31:28,860 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:28,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:29,358 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:29,359 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43745
2024-01-25 06:31:29,359 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43745
2024-01-25 06:31:29,359 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-25 06:31:29,359 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:29,359 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:29,359 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:29,360 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-25 06:31:29,360 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rkhsb8si
2024-01-25 06:31:29,360 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5b09f82-a4d9-4bbf-b316-620db8783365
2024-01-25 06:31:29,360 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e509635-2c52-4a86-83cc-07711e7c4fe4
2024-01-25 06:31:29,361 - distributed.worker - INFO - Starting Worker plugin PreImport-2291e004-8a9e-4e13-8099-b7aad9980faf
2024-01-25 06:31:29,361 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:29,420 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43745', status: init, memory: 0, processing: 0>
2024-01-25 06:31:29,432 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43745
2024-01-25 06:31:29,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53864
2024-01-25 06:31:29,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:29,434 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:29,435 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:29,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:30,428 - distributed.scheduler - INFO - Receive client connection: Client-5d03d3a8-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:30,429 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52962
2024-01-25 06:31:30,435 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:30,440 - distributed.scheduler - INFO - Remove client Client-5d03d3a8-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:30,440 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52962; closing.
2024-01-25 06:31:30,441 - distributed.scheduler - INFO - Remove client Client-5d03d3a8-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:30,442 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46579'. Reason: nanny-close
2024-01-25 06:31:30,442 - distributed.scheduler - INFO - Close client connection: Client-5d03d3a8-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:30,442 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:30,443 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43745. Reason: nanny-close
2024-01-25 06:31:30,445 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:30,445 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53864; closing.
2024-01-25 06:31:30,446 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43745', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164290.4463162')
2024-01-25 06:31:30,446 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:31:30,447 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:31,157 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:31:31,158 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:31:31,158 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:31:31,160 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-25 06:31:31,160 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-25 06:31:35,364 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:35,369 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38577 instead
  warnings.warn(
2024-01-25 06:31:35,373 - distributed.scheduler - INFO - State start
2024-01-25 06:31:35,555 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:35,556 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-25 06:31:35,557 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38577/status
2024-01-25 06:31:35,557 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:31:35,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36071'
2024-01-25 06:31:37,356 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:37,356 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:37,671 - distributed.scheduler - INFO - Receive client connection: Client-61fc4da5-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:37,683 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53090
2024-01-25 06:31:37,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:37,850 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32889
2024-01-25 06:31:37,850 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32889
2024-01-25 06:31:37,850 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38951
2024-01-25 06:31:37,850 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:37,850 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:37,850 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:37,850 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-25 06:31:37,850 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tgqses7q
2024-01-25 06:31:37,851 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2bc4717c-5848-4c1c-85ef-91d65506287e
2024-01-25 06:31:37,851 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe87f2a5-3258-4565-8bf3-0c9347688ce6
2024-01-25 06:31:37,851 - distributed.worker - INFO - Starting Worker plugin PreImport-35407417-1208-49b4-9b3a-82c5373bf9f2
2024-01-25 06:31:37,853 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:37,912 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32889', status: init, memory: 0, processing: 0>
2024-01-25 06:31:37,913 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32889
2024-01-25 06:31:37,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53116
2024-01-25 06:31:37,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:37,916 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:37,916 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:37,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:37,995 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:37,999 - distributed.scheduler - INFO - Remove client Client-61fc4da5-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:37,999 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53090; closing.
2024-01-25 06:31:37,999 - distributed.scheduler - INFO - Remove client Client-61fc4da5-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:38,000 - distributed.scheduler - INFO - Close client connection: Client-61fc4da5-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:38,000 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36071'. Reason: nanny-close
2024-01-25 06:31:38,001 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:38,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32889. Reason: nanny-close
2024-01-25 06:31:38,006 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53116; closing.
2024-01-25 06:31:38,006 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:38,006 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164298.0062706')
2024-01-25 06:31:38,006 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:31:38,009 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:38,665 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:31:38,666 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:31:38,666 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:31:38,667 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-25 06:31:38,668 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-25 06:31:40,703 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:40,708 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36095 instead
  warnings.warn(
2024-01-25 06:31:40,712 - distributed.scheduler - INFO - State start
2024-01-25 06:31:40,748 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:40,749 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-25 06:31:40,750 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36095/status
2024-01-25 06:31:40,750 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:31:43,038 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:48384'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48384>: Stream is closed
2024-01-25 06:31:43,473 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:31:43,474 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:31:43,474 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:31:43,475 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-25 06:31:43,475 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-25 06:31:45,560 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:45,565 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33557 instead
  warnings.warn(
2024-01-25 06:31:45,568 - distributed.scheduler - INFO - State start
2024-01-25 06:31:45,591 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:45,592 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-25 06:31:45,593 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33557/status
2024-01-25 06:31:45,593 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:31:45,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39189'
2024-01-25 06:31:46,770 - distributed.scheduler - INFO - Receive client connection: Client-6815c815-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:46,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59474
2024-01-25 06:31:47,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:47,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:47,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:47,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41707
2024-01-25 06:31:47,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41707
2024-01-25 06:31:47,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43671
2024-01-25 06:31:47,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-25 06:31:47,536 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:47,536 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:47,536 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-25 06:31:47,536 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-vmiwg2sr
2024-01-25 06:31:47,536 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-015f372e-f244-490a-a3f1-d52e6d808126
2024-01-25 06:31:47,536 - distributed.worker - INFO - Starting Worker plugin PreImport-a4cecfdb-6de9-4083-8b70-1edef83ffaad
2024-01-25 06:31:47,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37d93506-4d0d-4787-84d4-67b04e3e1b4d
2024-01-25 06:31:47,537 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:47,589 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41707', status: init, memory: 0, processing: 0>
2024-01-25 06:31:47,590 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41707
2024-01-25 06:31:47,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59502
2024-01-25 06:31:47,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:47,592 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-25 06:31:47,592 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:47,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-25 06:31:47,604 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:47,607 - distributed.scheduler - INFO - Remove client Client-6815c815-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:47,607 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59474; closing.
2024-01-25 06:31:47,607 - distributed.scheduler - INFO - Remove client Client-6815c815-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:47,608 - distributed.scheduler - INFO - Close client connection: Client-6815c815-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:47,609 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39189'. Reason: nanny-close
2024-01-25 06:31:47,637 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:47,638 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41707. Reason: nanny-close
2024-01-25 06:31:47,640 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-25 06:31:47,640 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59502; closing.
2024-01-25 06:31:47,641 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41707', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164307.6410336')
2024-01-25 06:31:47,641 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:31:47,641 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:48,274 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:31:48,274 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:31:48,275 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:31:48,276 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-25 06:31:48,276 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-25 06:31:50,481 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:50,486 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33841 instead
  warnings.warn(
2024-01-25 06:31:50,489 - distributed.scheduler - INFO - State start
2024-01-25 06:31:50,512 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:50,513 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-25 06:31:50,514 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33841/status
2024-01-25 06:31:50,514 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:31:50,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36157'
2024-01-25 06:31:50,765 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38869'
2024-01-25 06:31:50,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35949'
2024-01-25 06:31:50,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46027'
2024-01-25 06:31:50,794 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39829'
2024-01-25 06:31:50,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43125'
2024-01-25 06:31:50,815 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38829'
2024-01-25 06:31:50,827 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40917'
2024-01-25 06:31:51,422 - distributed.scheduler - INFO - Receive client connection: Client-6b048ff6-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:51,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46608
2024-01-25 06:31:52,593 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:52,593 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:52,593 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:52,594 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:52,597 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:52,598 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35015
2024-01-25 06:31:52,598 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35015
2024-01-25 06:31:52,598 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:52,598 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40639
2024-01-25 06:31:52,599 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:52,599 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:52,599 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:52,599 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:52,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4p4_v81u
2024-01-25 06:31:52,599 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b60ed33e-e4d4-422b-83bd-92a690928eac
2024-01-25 06:31:52,599 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33551
2024-01-25 06:31:52,599 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33551
2024-01-25 06:31:52,599 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39311
2024-01-25 06:31:52,599 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56bf6df3-ede8-4fe9-84c7-6c31f2d56e0e
2024-01-25 06:31:52,599 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:52,600 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:52,600 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:52,600 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:52,600 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-te8a50c5
2024-01-25 06:31:52,600 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cc766df-fbc3-4ec7-8cb2-59274f1a3b62
2024-01-25 06:31:52,628 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:52,628 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:52,633 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:52,634 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40841
2024-01-25 06:31:52,634 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40841
2024-01-25 06:31:52,634 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45907
2024-01-25 06:31:52,634 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:52,634 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:52,634 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:52,634 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:52,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pjnadxpl
2024-01-25 06:31:52,635 - distributed.worker - INFO - Starting Worker plugin PreImport-455bb085-f9d1-4116-9ff5-cb72761ca7f2
2024-01-25 06:31:52,635 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-16cbcb2b-deb2-4e01-9f78-8c8168623e38
2024-01-25 06:31:52,635 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0389f0c9-d0f1-40ac-8dc1-3ebce2fc039f
2024-01-25 06:31:52,659 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:52,659 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:52,661 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:52,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:52,664 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:52,665 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45337
2024-01-25 06:31:52,665 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45337
2024-01-25 06:31:52,665 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39241
2024-01-25 06:31:52,665 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:52,665 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:52,665 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:52,665 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:52,665 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:52,665 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7ael0suz
2024-01-25 06:31:52,666 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f237f960-7e54-4a6b-a3f5-32d4832231c5
2024-01-25 06:31:52,666 - distributed.worker - INFO - Starting Worker plugin PreImport-427c7fc8-384e-4016-8761-ff91a7056a26
2024-01-25 06:31:52,666 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e4274ee-35ca-4dca-b573-4c0255ed4d50
2024-01-25 06:31:52,666 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35101
2024-01-25 06:31:52,666 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35101
2024-01-25 06:31:52,666 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32851
2024-01-25 06:31:52,666 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:52,666 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:52,666 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:52,666 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:52,666 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dobl4qur
2024-01-25 06:31:52,667 - distributed.worker - INFO - Starting Worker plugin RMMSetup-56223662-8b91-4170-9040-ccb192d0d8cc
2024-01-25 06:31:52,673 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:52,673 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:52,677 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:52,678 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35653
2024-01-25 06:31:52,678 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35653
2024-01-25 06:31:52,679 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46297
2024-01-25 06:31:52,679 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:52,679 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:52,679 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:52,679 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:52,679 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q4zm6blc
2024-01-25 06:31:52,679 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e294e227-e61b-4fda-af8c-8432bdd416ad
2024-01-25 06:31:52,679 - distributed.worker - INFO - Starting Worker plugin PreImport-ac684062-48a8-40d5-93ce-d84bb90a9dd3
2024-01-25 06:31:52,679 - distributed.worker - INFO - Starting Worker plugin RMMSetup-82f952ae-737b-4ff6-a106-527ec9581c38
2024-01-25 06:31:52,893 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:52,894 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:52,899 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:52,900 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38831
2024-01-25 06:31:52,900 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38831
2024-01-25 06:31:52,900 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40683
2024-01-25 06:31:52,900 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:52,900 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:52,900 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:52,900 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:52,900 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lxm3wyfq
2024-01-25 06:31:52,901 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f12fa78-f564-43b9-ab14-a35dfb5cda6c
2024-01-25 06:31:52,901 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fce62bf5-b2cd-4073-aa66-271283c9ffe8
2024-01-25 06:31:52,929 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:52,930 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:52,940 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:52,942 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38871
2024-01-25 06:31:52,942 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38871
2024-01-25 06:31:52,942 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46635
2024-01-25 06:31:52,942 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:52,942 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:52,942 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:52,942 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-25 06:31:52,942 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l_lprzhj
2024-01-25 06:31:52,943 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-786a16db-d8ae-4505-824d-1ee4ed96b3aa
2024-01-25 06:31:52,943 - distributed.worker - INFO - Starting Worker plugin PreImport-4acfd179-ddd0-4e49-9e11-49363fcde99e
2024-01-25 06:31:52,944 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1871063-7ff2-48e6-ae56-3808bf4ff55b
2024-01-25 06:31:54,259 - distributed.worker - INFO - Starting Worker plugin PreImport-54d2459c-5e94-4b18-a896-b41a4b95fc4a
2024-01-25 06:31:54,260 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a8191c9-2ac3-4aa8-9c8f-409b42d1102e
2024-01-25 06:31:54,260 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,283 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33551', status: init, memory: 0, processing: 0>
2024-01-25 06:31:54,285 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33551
2024-01-25 06:31:54,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46638
2024-01-25 06:31:54,286 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:54,287 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:54,287 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,289 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:54,591 - distributed.worker - INFO - Starting Worker plugin PreImport-7f980d0b-bc20-400d-9d8e-9b044535d210
2024-01-25 06:31:54,593 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,626 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35015', status: init, memory: 0, processing: 0>
2024-01-25 06:31:54,626 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35015
2024-01-25 06:31:54,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46652
2024-01-25 06:31:54,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:54,629 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:54,629 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:54,656 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,678 - distributed.worker - INFO - Starting Worker plugin PreImport-515bd80e-d294-44a6-8ab5-c96bdf79f12d
2024-01-25 06:31:54,678 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-317f2bb9-1f18-42f9-ada4-557fc2850f1d
2024-01-25 06:31:54,679 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,690 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40841', status: init, memory: 0, processing: 0>
2024-01-25 06:31:54,691 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40841
2024-01-25 06:31:54,691 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46660
2024-01-25 06:31:54,692 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:54,693 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:54,693 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:54,703 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35101', status: init, memory: 0, processing: 0>
2024-01-25 06:31:54,704 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35101
2024-01-25 06:31:54,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46668
2024-01-25 06:31:54,705 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:54,706 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:54,706 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:54,731 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,744 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,753 - distributed.worker - INFO - Starting Worker plugin PreImport-feb33539-3a9c-43e1-9d49-f2f960b5befa
2024-01-25 06:31:54,754 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,754 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35653', status: init, memory: 0, processing: 0>
2024-01-25 06:31:54,754 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35653
2024-01-25 06:31:54,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46674
2024-01-25 06:31:54,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:54,756 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:54,756 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,758 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:54,774 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,774 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38831', status: init, memory: 0, processing: 0>
2024-01-25 06:31:54,775 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38831
2024-01-25 06:31:54,775 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46686
2024-01-25 06:31:54,776 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:54,777 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:54,777 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:54,779 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45337', status: init, memory: 0, processing: 0>
2024-01-25 06:31:54,779 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45337
2024-01-25 06:31:54,779 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46678
2024-01-25 06:31:54,781 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:54,782 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:54,782 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:54,797 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38871', status: init, memory: 0, processing: 0>
2024-01-25 06:31:54,798 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38871
2024-01-25 06:31:54,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46698
2024-01-25 06:31:54,799 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:31:54,800 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:31:54,800 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:54,801 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:31:54,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:54,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:54,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:54,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:54,858 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:54,859 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:54,859 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:54,859 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-25 06:31:54,871 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:54,872 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:54,872 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:54,872 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:54,872 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:54,872 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:54,872 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:54,873 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:31:54,876 - distributed.scheduler - INFO - Remove client Client-6b048ff6-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:54,877 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46608; closing.
2024-01-25 06:31:54,877 - distributed.scheduler - INFO - Remove client Client-6b048ff6-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:54,877 - distributed.scheduler - INFO - Close client connection: Client-6b048ff6-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:54,878 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36157'. Reason: nanny-close
2024-01-25 06:31:54,879 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:54,879 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38869'. Reason: nanny-close
2024-01-25 06:31:54,880 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:54,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35949'. Reason: nanny-close
2024-01-25 06:31:54,881 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40841. Reason: nanny-close
2024-01-25 06:31:54,881 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:54,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46027'. Reason: nanny-close
2024-01-25 06:31:54,881 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35015. Reason: nanny-close
2024-01-25 06:31:54,881 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:54,882 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33551. Reason: nanny-close
2024-01-25 06:31:54,882 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39829'. Reason: nanny-close
2024-01-25 06:31:54,882 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:54,882 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43125'. Reason: nanny-close
2024-01-25 06:31:54,882 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35101. Reason: nanny-close
2024-01-25 06:31:54,883 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:54,883 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38829'. Reason: nanny-close
2024-01-25 06:31:54,883 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:54,883 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45337. Reason: nanny-close
2024-01-25 06:31:54,883 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46660; closing.
2024-01-25 06:31:54,883 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:54,883 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40917'. Reason: nanny-close
2024-01-25 06:31:54,883 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40841', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164314.8837574')
2024-01-25 06:31:54,883 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38871. Reason: nanny-close
2024-01-25 06:31:54,883 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:54,884 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:31:54,884 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35653. Reason: nanny-close
2024-01-25 06:31:54,884 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:54,884 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:54,884 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38831. Reason: nanny-close
2024-01-25 06:31:54,885 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:54,885 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:54,885 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46668; closing.
2024-01-25 06:31:54,885 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46638; closing.
2024-01-25 06:31:54,885 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:54,885 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46652; closing.
2024-01-25 06:31:54,885 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:54,886 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:54,886 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:54,886 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:54,886 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35101', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164314.8865077')
2024-01-25 06:31:54,886 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:31:54,886 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33551', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164314.8868723')
2024-01-25 06:31:54,887 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:54,887 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35015', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164314.8872108')
2024-01-25 06:31:54,887 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:54,887 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46698; closing.
2024-01-25 06:31:54,888 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:54,888 - distributed.nanny - INFO - Worker closed
2024-01-25 06:31:54,888 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164314.8887177')
2024-01-25 06:31:54,889 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46674; closing.
2024-01-25 06:31:54,889 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46678; closing.
2024-01-25 06:31:54,889 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46686; closing.
2024-01-25 06:31:54,889 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35653', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164314.8897114')
2024-01-25 06:31:54,890 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45337', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164314.8900719')
2024-01-25 06:31:54,890 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38831', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164314.890421')
2024-01-25 06:31:54,890 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:31:55,844 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:31:55,844 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:31:55,845 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:31:55,846 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-25 06:31:55,846 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-25 06:31:57,852 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:57,856 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44573 instead
  warnings.warn(
2024-01-25 06:31:57,860 - distributed.scheduler - INFO - State start
2024-01-25 06:31:57,880 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:31:57,881 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-25 06:31:57,882 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44573/status
2024-01-25 06:31:57,882 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-25 06:31:57,911 - distributed.scheduler - INFO - Receive client connection: Client-6f6e99cf-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:31:57,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46810
2024-01-25 06:31:57,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36243'
2024-01-25 06:31:59,683 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:31:59,683 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:31:59,687 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:31:59,688 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46199
2024-01-25 06:31:59,688 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46199
2024-01-25 06:31:59,688 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44173
2024-01-25 06:31:59,688 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:31:59,688 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:31:59,688 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:31:59,689 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-25 06:31:59,689 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-851p8xkq
2024-01-25 06:31:59,689 - distributed.worker - INFO - Starting Worker plugin PreImport-186386b8-da65-4069-b182-1ac8f8f55921
2024-01-25 06:31:59,689 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d67ac510-0117-44fb-b05a-2033a96b55f7
2024-01-25 06:31:59,825 - distributed.scheduler - INFO - Receive client connection: Client-719e4f28-bb4b-11ee-baa7-d8c49764f6bb
2024-01-25 06:31:59,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46820
2024-01-25 06:31:59,977 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98df169a-0e42-4f29-9685-57e7533dd190
2024-01-25 06:31:59,978 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:32:00,035 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46199', status: init, memory: 0, processing: 0>
2024-01-25 06:32:00,036 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46199
2024-01-25 06:32:00,036 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51190
2024-01-25 06:32:00,036 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:32:00,037 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:32:00,037 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:32:00,038 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:32:00,066 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:32:00,069 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:32:00,071 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-25 06:32:00,073 - distributed.scheduler - INFO - Remove client Client-6f6e99cf-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:32:00,073 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46810; closing.
2024-01-25 06:32:00,073 - distributed.scheduler - INFO - Remove client Client-6f6e99cf-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:32:00,074 - distributed.scheduler - INFO - Close client connection: Client-6f6e99cf-bb4b-11ee-b64a-d8c49764f6bb
2024-01-25 06:32:00,075 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36243'. Reason: nanny-close
2024-01-25 06:32:00,075 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:32:00,076 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46199. Reason: nanny-close
2024-01-25 06:32:00,077 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:32:00,077 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51190; closing.
2024-01-25 06:32:00,078 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46199', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1706164320.0782726')
2024-01-25 06:32:00,078 - distributed.scheduler - INFO - Lost all workers
2024-01-25 06:32:00,079 - distributed.nanny - INFO - Worker closed
2024-01-25 06:32:00,740 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-25 06:32:00,740 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-25 06:32:00,741 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:32:00,742 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-25 06:32:00,743 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-25 06:32:02,971 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46853'
2024-01-25 06:32:03,026 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:32:03,030 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38737 instead
  warnings.warn(
2024-01-25 06:32:03,034 - distributed.scheduler - INFO - State start
2024-01-25 06:32:03,057 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-25 06:32:03,057 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-25 06:32:03,058 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-25 06:32:03,059 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-25 06:32:04,932 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-25 06:32:04,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-25 06:32:04,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-25 06:32:04,937 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33147
2024-01-25 06:32:04,937 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33147
2024-01-25 06:32:04,937 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36457
2024-01-25 06:32:04,937 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-25 06:32:04,937 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:32:04,937 - distributed.worker - INFO -               Threads:                          1
2024-01-25 06:32:04,937 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-25 06:32:04,937 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z4on48kb
2024-01-25 06:32:04,938 - distributed.worker - INFO - Starting Worker plugin PreImport-ab06ae53-2c5e-4777-933e-71d54d97e296
2024-01-25 06:32:04,938 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3e5b14a5-ab23-42a6-9823-8818aba93883
2024-01-25 06:32:05,959 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-577f65ed-9649-45ad-93e9-df9e4105a163
2024-01-25 06:32:05,960 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:32:06,063 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-25 06:32:06,064 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-25 06:32:06,064 - distributed.worker - INFO - -------------------------------------------------
2024-01-25 06:32:06,066 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-25 06:32:06,120 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-25 06:32:06,125 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-25 06:32:06,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46853'. Reason: nanny-close
2024-01-25 06:32:06,135 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-25 06:32:06,136 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33147. Reason: nanny-close
2024-01-25 06:32:06,138 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-25 06:32:06,140 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45329 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39887 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32875 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34571 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39317 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39051 instead
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 51 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
