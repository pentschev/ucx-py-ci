============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/subprocess.py", line 73, in <module>
    import msvcrt
ModuleNotFoundError: No module named 'msvcrt'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 6, in <module>
    from dask.__main__ import main
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__init__.py", line 1, in <module>
    from dask import config, datasets
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/datasets.py", line 3, in <module>
    from dask.utils import import_required
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 11, in <module>
    import uuid
  File "/opt/conda/envs/gdf/lib/python3.9/uuid.py", line 59, in <module>
    import platform
  File "/opt/conda/envs/gdf/lib/python3.9/platform.py", line 119, in <module>
    import subprocess
  File "/opt/conda/envs/gdf/lib/python3.9/subprocess.py", line 80, in <module>
    import selectors
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 978, in get_code
  File "<frozen importlib._bootstrap_external>", line 647, in _compile_bytecode
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 6, in <module>
    from dask.__main__ import main
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__init__.py", line 3, in <module>
    from dask.base import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 33, in <module>
    from dask.system import CPU_COUNT
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/system.py", line 6, in <module>
    import psutil
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/psutil/__init__.py", line 102, in <module>
    from . import _pslinux as _psplatform
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/psutil/_pslinux.py", line 1667, in <module>
    class Process(object):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/psutil/_pslinux.py", line 2262, in Process
    def gids(self, _gids_re=re.compile(br'Gid:\t(\d+)\t(\d+)\t(\d+)')):
  File "/opt/conda/envs/gdf/lib/python3.9/re.py", line 252, in compile
    return _compile(pattern, flags)
  File "/opt/conda/envs/gdf/lib/python3.9/re.py", line 304, in _compile
    p = sre_compile.compile(pattern, flags)
  File "/opt/conda/envs/gdf/lib/python3.9/sre_compile.py", line 792, in compile
    code = _code(p, flags)
  File "/opt/conda/envs/gdf/lib/python3.9/sre_compile.py", line 631, in _code
    _compile(code, p.data, flags)
  File "/opt/conda/envs/gdf/lib/python3.9/sre_compile.py", line 184, in _compile
    _compile(code, p, _combine_flags(flags, add_flags, del_flags))
  File "/opt/conda/envs/gdf/lib/python3.9/sre_compile.py", line 164, in _compile
    _compile(code, av[2], flags)
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-05-03 05:40:17,720 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:17,725 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46879 instead
  warnings.warn(
2023-05-03 05:40:17,729 - distributed.scheduler - INFO - State start
2023-05-03 05:40:17,751 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:17,752 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:40:17,752 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:40:17,753 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-03 05:40:17,920 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40559'
2023-05-03 05:40:17,947 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42527'
2023-05-03 05:40:17,949 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37263'
2023-05-03 05:40:17,960 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45621'
2023-05-03 05:40:17,971 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35497'
2023-05-03 05:40:17,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34063'
2023-05-03 05:40:17,996 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34091'
2023-05-03 05:40:18,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36363'
2023-05-03 05:40:19,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,676 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,676 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,701 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,730 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,730 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,768 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,772 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,791 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,805 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:19,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:19,830 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,833 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,890 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,890 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:19,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:20,025 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:24,182 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43213
2023-05-03 05:40:24,182 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43213
2023-05-03 05:40:24,182 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46737
2023-05-03 05:40:24,182 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,182 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,182 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:24,183 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:24,183 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2vw9bgay
2023-05-03 05:40:24,183 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60f41c9b-c8fb-4dce-8448-0a5a1f59bd30
2023-05-03 05:40:24,184 - distributed.worker - INFO - Starting Worker plugin PreImport-7b2e7596-e473-489a-a340-f750a21cd650
2023-05-03 05:40:24,184 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4dd71212-2145-43f0-840c-aaa9a205501a
2023-05-03 05:40:24,186 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35647
2023-05-03 05:40:24,187 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35647
2023-05-03 05:40:24,187 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43223
2023-05-03 05:40:24,187 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,187 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,187 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:24,187 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:24,187 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eu67xfpn
2023-05-03 05:40:24,188 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-05b1d122-318f-4ab4-a8c5-825d2fdba518
2023-05-03 05:40:24,188 - distributed.worker - INFO - Starting Worker plugin PreImport-16aeb3d8-53c6-4c44-99bf-edb6e38a1fab
2023-05-03 05:40:24,188 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7925a1cd-c7de-4fc1-9bb3-1c54b726ff26
2023-05-03 05:40:24,213 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45745
2023-05-03 05:40:24,213 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45745
2023-05-03 05:40:24,213 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43729
2023-05-03 05:40:24,213 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,214 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,214 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:24,214 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:24,214 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-eloqu9qt
2023-05-03 05:40:24,214 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ecccddae-dd7d-467c-9563-2be2c9c589bf
2023-05-03 05:40:24,214 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a475a62d-d50a-41e8-874f-c82ba4ec53f8
2023-05-03 05:40:24,386 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45331
2023-05-03 05:40:24,387 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45331
2023-05-03 05:40:24,387 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36783
2023-05-03 05:40:24,387 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,387 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,387 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:24,387 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:24,387 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qmyl1jgy
2023-05-03 05:40:24,387 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-05e2d5e1-a254-42a9-97ea-a4f055a50587
2023-05-03 05:40:24,388 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca0a95ff-1ddd-4cca-a02a-7e734325fe24
2023-05-03 05:40:24,389 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41311
2023-05-03 05:40:24,389 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41311
2023-05-03 05:40:24,389 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34031
2023-05-03 05:40:24,389 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,389 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,389 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:24,389 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:24,389 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1umg221i
2023-05-03 05:40:24,390 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a70b950-d63b-46d5-b418-00a4776dedb2
2023-05-03 05:40:24,446 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,492 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,494 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,494 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,497 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:24,534 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46263
2023-05-03 05:40:24,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46263
2023-05-03 05:40:24,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34273
2023-05-03 05:40:24,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,535 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,535 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:24,535 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:24,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x_0nm8n_
2023-05-03 05:40:24,536 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a09fecf-48e7-466b-8548-1bfe21f982ce
2023-05-03 05:40:24,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7396841d-f248-4572-b0cd-12e1bf4833b8
2023-05-03 05:40:24,537 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,538 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,540 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:24,578 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44077
2023-05-03 05:40:24,578 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44077
2023-05-03 05:40:24,578 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37323
2023-05-03 05:40:24,579 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,579 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,579 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:24,579 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:24,579 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qu91ap86
2023-05-03 05:40:24,579 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10f0c05d-5f02-4dd4-b629-64ab668e5b1b
2023-05-03 05:40:24,592 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42745
2023-05-03 05:40:24,592 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42745
2023-05-03 05:40:24,593 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41549
2023-05-03 05:40:24,593 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,593 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,593 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:24,593 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:24,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bvgd4qby
2023-05-03 05:40:24,593 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2263b0df-89ad-4a92-b479-daf5daa695f2
2023-05-03 05:40:24,593 - distributed.worker - INFO - Starting Worker plugin PreImport-cff70c6f-188d-4b23-9ea0-8ba39aca0c35
2023-05-03 05:40:24,594 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14f56bef-7488-4480-953d-57201022e1e6
2023-05-03 05:40:24,626 - distributed.worker - INFO - Starting Worker plugin PreImport-d582db50-27b0-4594-aaf5-1c8749bf0737
2023-05-03 05:40:24,626 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,658 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,658 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:24,671 - distributed.worker - INFO - Starting Worker plugin PreImport-60a591e0-d366-45e2-9b6a-09425784624a
2023-05-03 05:40:24,672 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed99880b-492d-4971-88e0-801f14980a59
2023-05-03 05:40:24,672 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,685 - distributed.worker - INFO - Starting Worker plugin PreImport-c460e223-9484-4b1a-92c5-8e9badea6576
2023-05-03 05:40:24,686 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,706 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,707 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:24,716 - distributed.worker - INFO - Starting Worker plugin PreImport-06a87189-4530-459c-8b46-66d10affef1b
2023-05-03 05:40:24,716 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61f3a5c8-09c0-4ccf-88b0-ec14d3c19020
2023-05-03 05:40:24,716 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,716 - distributed.worker - INFO - Starting Worker plugin PreImport-9a032859-9787-4ac6-8531-8566da8590b1
2023-05-03 05:40:24,717 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,726 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,726 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,726 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,729 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:24,749 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,749 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:24,756 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,756 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,758 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:24,758 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:24,759 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:24,760 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:33,853 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,853 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,853 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,854 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,854 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,854 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,854 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,854 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33037
2023-05-03 05:40:33,862 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34865
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37269
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34865
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34865
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44929
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34865
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41849
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37269
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37269
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33505
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34865
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37269
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44929
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34865
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44929
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46325
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44929
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37269
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41849
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41849
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37933
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37269
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34865
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34865
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41849
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33505
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44929
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33505
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44929
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33505
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46325
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37269
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46325
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41849
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41849
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37269
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46325
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37933
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37933
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33505
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33505
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44929
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44929
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37933
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46325
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46325
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41849
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41849
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37933
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37933
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33505
2023-05-03 05:40:33,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33505
2023-05-03 05:40:33,864 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46325
2023-05-03 05:40:33,864 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46325
2023-05-03 05:40:33,864 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37933
2023-05-03 05:40:33,864 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37933
2023-05-03 05:40:33,952 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:33,952 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:33,953 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:33,953 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:33,953 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:33,953 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:33,953 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:33,953 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:33,960 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40559'. Reason: nanny-close
2023-05-03 05:40:33,961 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,961 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42527'. Reason: nanny-close
2023-05-03 05:40:33,962 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,962 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44077. Reason: nanny-close
2023-05-03 05:40:33,962 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37263'. Reason: nanny-close
2023-05-03 05:40:33,963 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,963 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45621'. Reason: nanny-close
2023-05-03 05:40:33,963 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41311. Reason: nanny-close
2023-05-03 05:40:33,964 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35647. Reason: nanny-close
2023-05-03 05:40:33,964 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,965 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,963 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,965 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,965 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35497'. Reason: nanny-close
2023-05-03 05:40:33,966 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44077
2023-05-03 05:40:33,966 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,966 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44077
2023-05-03 05:40:33,966 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44077
2023-05-03 05:40:33,966 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,966 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44077
2023-05-03 05:40:33,966 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44077
2023-05-03 05:40:33,966 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34063'. Reason: nanny-close
2023-05-03 05:40:33,966 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,966 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46263. Reason: nanny-close
2023-05-03 05:40:33,966 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,967 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45745. Reason: nanny-close
2023-05-03 05:40:33,967 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34091'. Reason: nanny-close
2023-05-03 05:40:33,967 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,967 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42745. Reason: nanny-close
2023-05-03 05:40:33,967 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36363'. Reason: nanny-close
2023-05-03 05:40:33,967 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,967 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:33,968 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44077
2023-05-03 05:40:33,968 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45331. Reason: nanny-close
2023-05-03 05:40:33,968 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,968 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,969 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,969 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43213. Reason: nanny-close
2023-05-03 05:40:33,970 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,970 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,970 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,970 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,971 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:33,971 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:33,972 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-05-03 05:40:37,726 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:37,731 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38081 instead
  warnings.warn(
2023-05-03 05:40:37,735 - distributed.scheduler - INFO - State start
2023-05-03 05:40:37,757 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:37,758 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-03 05:40:37,759 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38081/status
2023-05-03 05:40:38,071 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38715'
2023-05-03 05:40:38,096 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40709'
2023-05-03 05:40:38,113 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41149'
2023-05-03 05:40:38,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41771'
2023-05-03 05:40:38,144 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36083'
2023-05-03 05:40:38,149 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45789'
2023-05-03 05:40:38,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44733'
2023-05-03 05:40:38,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40929'
2023-05-03 05:40:38,897 - distributed.scheduler - INFO - Receive client connection: Client-077127c3-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:40:38,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46490
2023-05-03 05:40:39,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:39,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:39,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:39,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:39,895 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:39,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:39,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:39,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:39,915 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:39,915 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:39,916 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:39,916 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:39,918 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:39,918 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:39,921 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:39,954 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:39,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:39,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:39,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:39,961 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:39,970 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:39,970 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:40,005 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:40,009 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:40,242 - distributed.scheduler - INFO - Receive client connection: Client-072807cd-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:40:40,243 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46512
2023-05-03 05:40:42,684 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44769
2023-05-03 05:40:42,685 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44769
2023-05-03 05:40:42,685 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34819
2023-05-03 05:40:42,685 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:42,685 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:42,685 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:42,685 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:42,685 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6sqcte0v
2023-05-03 05:40:42,686 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c586080-e91f-4faf-9c05-e7f8ecb74a18
2023-05-03 05:40:42,743 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45663
2023-05-03 05:40:42,743 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45663
2023-05-03 05:40:42,743 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36203
2023-05-03 05:40:42,744 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:42,744 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:42,744 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:42,744 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:42,744 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-itpiikmu
2023-05-03 05:40:42,744 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fb56c4e4-3b57-4b0f-a686-2d982e938bd3
2023-05-03 05:40:42,745 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d60a800-4687-4d90-83fc-5cc1c97cff8b
2023-05-03 05:40:42,840 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45501
2023-05-03 05:40:42,840 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45501
2023-05-03 05:40:42,840 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44421
2023-05-03 05:40:42,840 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:42,840 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:42,840 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:42,840 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:42,840 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0py7jgfy
2023-05-03 05:40:42,841 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d0d052dd-b962-419f-9f76-42d09bb3af8d
2023-05-03 05:40:42,841 - distributed.worker - INFO - Starting Worker plugin PreImport-d6cbc5d6-3b1e-47aa-9468-40db5a701726
2023-05-03 05:40:42,841 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea99016c-54ec-4430-bdf2-d7dd38a98e2a
2023-05-03 05:40:42,856 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41971
2023-05-03 05:40:42,857 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41971
2023-05-03 05:40:42,857 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41033
2023-05-03 05:40:42,857 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:42,857 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:42,857 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:42,857 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:42,857 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rm90u84o
2023-05-03 05:40:42,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-847823b1-d254-4e5d-8a70-ca27d264b4f8
2023-05-03 05:40:42,858 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72e19498-af4f-4ad4-9243-bf3b9c9a0b9b
2023-05-03 05:40:42,887 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:42,918 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45501', status: init, memory: 0, processing: 0>
2023-05-03 05:40:42,919 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45501
2023-05-03 05:40:42,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46530
2023-05-03 05:40:42,920 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:42,920 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:42,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:42,958 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44731
2023-05-03 05:40:42,959 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44731
2023-05-03 05:40:42,959 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43767
2023-05-03 05:40:42,959 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:42,959 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:42,959 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:42,959 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:42,959 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-m517eyr6
2023-05-03 05:40:42,960 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b73c696e-526b-4611-84f8-23eaa5a20909
2023-05-03 05:40:42,960 - distributed.worker - INFO - Starting Worker plugin RMMSetup-129b915b-b7fd-4ebb-8191-37441e703255
2023-05-03 05:40:43,003 - distributed.worker - INFO - Starting Worker plugin PreImport-a0512b15-5913-4759-99a7-76a637640df9
2023-05-03 05:40:43,003 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4b07a3e-cc1b-4aeb-b278-321683762bbe
2023-05-03 05:40:43,003 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,010 - distributed.worker - INFO - Starting Worker plugin PreImport-1bf1e018-e69c-47f4-8cd2-1972a6e9f972
2023-05-03 05:40:43,011 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,017 - distributed.worker - INFO - Starting Worker plugin PreImport-d651d187-5999-4907-b173-f3c0fd0fbe8f
2023-05-03 05:40:43,017 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,041 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44769', status: init, memory: 0, processing: 0>
2023-05-03 05:40:43,042 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44769
2023-05-03 05:40:43,042 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46540
2023-05-03 05:40:43,043 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,043 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,044 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41971', status: init, memory: 0, processing: 0>
2023-05-03 05:40:43,045 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41971
2023-05-03 05:40:43,045 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46548
2023-05-03 05:40:43,045 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,045 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:43,046 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,048 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:43,048 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45663', status: init, memory: 0, processing: 0>
2023-05-03 05:40:43,049 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45663
2023-05-03 05:40:43,049 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46560
2023-05-03 05:40:43,049 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,050 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,052 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:43,153 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43481
2023-05-03 05:40:43,154 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43481
2023-05-03 05:40:43,154 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37377
2023-05-03 05:40:43,154 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,154 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,154 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:43,154 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:43,154 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-srix4ctq
2023-05-03 05:40:43,154 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40e06186-dcc7-4c57-8ad8-c7f37a2ebfa6
2023-05-03 05:40:43,155 - distributed.worker - INFO - Starting Worker plugin PreImport-b5e6da95-3f45-4342-94b8-329c2bb5fdb7
2023-05-03 05:40:43,155 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9d73c5c1-4cb1-4846-8c9e-1292c54af4ce
2023-05-03 05:40:43,237 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,242 - distributed.worker - INFO - Starting Worker plugin PreImport-99e9ddba-2d39-4e01-a9ff-4cb42e0c4466
2023-05-03 05:40:43,243 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,268 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43481', status: init, memory: 0, processing: 0>
2023-05-03 05:40:43,269 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43481
2023-05-03 05:40:43,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46576
2023-05-03 05:40:43,270 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,270 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,272 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:43,280 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44731', status: init, memory: 0, processing: 0>
2023-05-03 05:40:43,281 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44731
2023-05-03 05:40:43,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46582
2023-05-03 05:40:43,281 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,282 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,284 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:43,558 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46621
2023-05-03 05:40:43,559 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46621
2023-05-03 05:40:43,559 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40141
2023-05-03 05:40:43,559 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,559 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,559 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:43,559 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:43,559 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xif0hxzy
2023-05-03 05:40:43,559 - distributed.worker - INFO - Starting Worker plugin RMMSetup-963f6bb1-e256-4b3a-8451-169be12895aa
2023-05-03 05:40:43,619 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39845
2023-05-03 05:40:43,619 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39845
2023-05-03 05:40:43,620 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46139
2023-05-03 05:40:43,620 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,620 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,620 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:43,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:43,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nvez2_be
2023-05-03 05:40:43,620 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e5f075ab-41b9-4735-9293-8b53440d6f03
2023-05-03 05:40:43,621 - distributed.worker - INFO - Starting Worker plugin PreImport-a1214aab-d87d-4992-a8b5-ce3166812255
2023-05-03 05:40:43,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c7d1430-2135-406a-a4b8-9b4a09993a30
2023-05-03 05:40:43,648 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,682 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39845', status: init, memory: 0, processing: 0>
2023-05-03 05:40:43,683 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39845
2023-05-03 05:40:43,683 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46584
2023-05-03 05:40:43,683 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,683 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,685 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:43,688 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-35c40f01-c927-491f-a1d0-d23f2b7263f6
2023-05-03 05:40:43,689 - distributed.worker - INFO - Starting Worker plugin PreImport-2b40f496-a924-4a0b-b124-07fa27d6d3ce
2023-05-03 05:40:43,689 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,733 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46621', status: init, memory: 0, processing: 0>
2023-05-03 05:40:43,734 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46621
2023-05-03 05:40:43,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46600
2023-05-03 05:40:43,734 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:43,734 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:43,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:43,847 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,848 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,848 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,848 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,848 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,848 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,848 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,848 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,848 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,849 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,850 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:40:43,856 - distributed.scheduler - INFO - Remove client Client-072807cd-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:40:43,856 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46512; closing.
2023-05-03 05:40:43,857 - distributed.scheduler - INFO - Remove client Client-072807cd-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:40:43,857 - distributed.scheduler - INFO - Remove client Client-077127c3-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:40:43,857 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46490; closing.
2023-05-03 05:40:43,857 - distributed.scheduler - INFO - Remove client Client-077127c3-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:40:43,858 - distributed.scheduler - INFO - Close client connection: Client-072807cd-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:40:43,858 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38715'. Reason: nanny-close
2023-05-03 05:40:43,858 - distributed.scheduler - INFO - Close client connection: Client-077127c3-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:40:43,858 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:43,859 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40709'. Reason: nanny-close
2023-05-03 05:40:43,859 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46621. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41149'. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41771'. Reason: nanny-close
2023-05-03 05:40:43,860 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44769. Reason: nanny-close
2023-05-03 05:40:43,861 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:43,861 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36083'. Reason: nanny-close
2023-05-03 05:40:43,861 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45501. Reason: nanny-close
2023-05-03 05:40:43,861 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:43,861 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45789'. Reason: nanny-close
2023-05-03 05:40:43,861 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45663. Reason: nanny-close
2023-05-03 05:40:43,862 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:43,862 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:43,862 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44733'. Reason: nanny-close
2023-05-03 05:40:43,862 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:43,862 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41971. Reason: nanny-close
2023-05-03 05:40:43,862 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46600; closing.
2023-05-03 05:40:43,863 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40929'. Reason: nanny-close
2023-05-03 05:40:43,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:43,863 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43481. Reason: nanny-close
2023-05-03 05:40:43,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:43,863 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:40:43,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:43,863 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44731. Reason: nanny-close
2023-05-03 05:40:43,863 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46621', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:43,863 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46621
2023-05-03 05:40:43,863 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:43,864 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:43,864 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:43,864 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39845. Reason: nanny-close
2023-05-03 05:40:43,865 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:43,865 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:43,865 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:43,865 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:43,866 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46530; closing.
2023-05-03 05:40:43,866 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:43,866 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:40:43,866 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:43,866 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46540; closing.
2023-05-03 05:40:43,867 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:43,867 - distributed.nanny - INFO - Worker closed
2023-05-03 05:40:43,868 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45501', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:43,869 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45501
2023-05-03 05:40:43,870 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44769', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:43,870 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44769
2023-05-03 05:40:43,871 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46560; closing.
2023-05-03 05:40:43,871 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46548; closing.
2023-05-03 05:40:43,871 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46582; closing.
2023-05-03 05:40:43,872 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45663', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:43,872 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45663
2023-05-03 05:40:43,873 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41971', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:43,873 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41971
2023-05-03 05:40:43,873 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44731', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:43,873 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44731
2023-05-03 05:40:43,873 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46576; closing.
2023-05-03 05:40:43,874 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46584; closing.
2023-05-03 05:40:43,874 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43481', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:43,874 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43481
2023-05-03 05:40:43,874 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39845', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:43,875 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39845
2023-05-03 05:40:43,875 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:40:43,991 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45061', status: init, memory: 0, processing: 0>
2023-05-03 05:40:43,991 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45061
2023-05-03 05:40:43,991 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46616
2023-05-03 05:40:44,009 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34177', status: init, memory: 0, processing: 0>
2023-05-03 05:40:44,010 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34177
2023-05-03 05:40:44,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46610
2023-05-03 05:40:44,048 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46616; closing.
2023-05-03 05:40:44,048 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45061', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:44,048 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45061
2023-05-03 05:40:44,049 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46610; closing.
2023-05-03 05:40:44,050 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34177', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:44,050 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34177
2023-05-03 05:40:44,050 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:40:44,560 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38801', status: init, memory: 0, processing: 0>
2023-05-03 05:40:44,561 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38801
2023-05-03 05:40:44,561 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60670
2023-05-03 05:40:44,580 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41677', status: init, memory: 0, processing: 0>
2023-05-03 05:40:44,581 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41677
2023-05-03 05:40:44,581 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60672
2023-05-03 05:40:44,584 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33277', status: init, memory: 0, processing: 0>
2023-05-03 05:40:44,585 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33277
2023-05-03 05:40:44,585 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60686
2023-05-03 05:40:44,614 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60670; closing.
2023-05-03 05:40:44,615 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38801', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:44,615 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38801
2023-05-03 05:40:44,616 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60672; closing.
2023-05-03 05:40:44,616 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41677', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:44,616 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41677
2023-05-03 05:40:44,617 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60686; closing.
2023-05-03 05:40:44,617 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33277', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:44,617 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33277
2023-05-03 05:40:44,618 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:40:44,645 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40789', status: init, memory: 0, processing: 0>
2023-05-03 05:40:44,646 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40789
2023-05-03 05:40:44,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60688
2023-05-03 05:40:44,652 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45369', status: init, memory: 0, processing: 0>
2023-05-03 05:40:44,652 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45369
2023-05-03 05:40:44,652 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60704
2023-05-03 05:40:44,653 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33661', status: init, memory: 0, processing: 0>
2023-05-03 05:40:44,654 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33661
2023-05-03 05:40:44,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60692
2023-05-03 05:40:44,666 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60688; closing.
2023-05-03 05:40:44,666 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40789', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:44,667 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40789
2023-05-03 05:40:44,668 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60692; closing.
2023-05-03 05:40:44,668 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60704; closing.
2023-05-03 05:40:44,669 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33661', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:44,669 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33661
2023-05-03 05:40:44,669 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45369', status: closing, memory: 0, processing: 0>
2023-05-03 05:40:44,669 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45369
2023-05-03 05:40:44,670 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:40:45,676 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:40:45,677 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:40:45,677 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:40:45,679 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-03 05:40:45,680 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-05-03 05:40:48,013 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:48,017 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40185 instead
  warnings.warn(
2023-05-03 05:40:48,022 - distributed.scheduler - INFO - State start
2023-05-03 05:40:48,043 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:40:48,044 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-03 05:40:48,044 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40185/status
2023-05-03 05:40:48,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40837'
2023-05-03 05:40:48,382 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43787'
2023-05-03 05:40:48,385 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46791'
2023-05-03 05:40:48,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38183'
2023-05-03 05:40:48,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32985'
2023-05-03 05:40:48,423 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45391'
2023-05-03 05:40:48,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43603'
2023-05-03 05:40:48,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39961'
2023-05-03 05:40:49,653 - distributed.scheduler - INFO - Receive client connection: Client-0d6fe25d-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:40:49,669 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32814
2023-05-03 05:40:49,717 - distributed.scheduler - INFO - Receive client connection: Client-0d562a4d-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:40:49,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32828
2023-05-03 05:40:50,109 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,109 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,109 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,109 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,111 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,111 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,114 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,114 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,135 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,138 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,138 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,139 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,142 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,142 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,199 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:40:50,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:40:50,225 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:50,260 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:40:52,966 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45173
2023-05-03 05:40:52,966 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45173
2023-05-03 05:40:52,967 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33667
2023-05-03 05:40:52,967 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:52,967 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:52,967 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:52,967 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:52,967 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qwf75vsx
2023-05-03 05:40:52,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4366bcf-5548-4d2c-befa-d3e7bd117d58
2023-05-03 05:40:52,968 - distributed.worker - INFO - Starting Worker plugin PreImport-82d90788-8804-415c-9362-d5541cf462f7
2023-05-03 05:40:52,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1f659d6-5f11-4f4e-a791-6ac5420d899c
2023-05-03 05:40:53,339 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,548 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37473
2023-05-03 05:40:53,549 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37473
2023-05-03 05:40:53,549 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38051
2023-05-03 05:40:53,549 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:53,549 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,549 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:53,549 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:53,549 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xdhoj03t
2023-05-03 05:40:53,549 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d17d961-b16e-42e7-9b78-b5577ea534ce
2023-05-03 05:40:53,550 - distributed.worker - INFO - Starting Worker plugin PreImport-50671417-7661-4ec1-97cf-808d9663fc01
2023-05-03 05:40:53,550 - distributed.worker - INFO - Starting Worker plugin RMMSetup-653b4f19-e982-4274-9bdc-737bb15a4e57
2023-05-03 05:40:53,559 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36359
2023-05-03 05:40:53,560 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36359
2023-05-03 05:40:53,560 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42783
2023-05-03 05:40:53,560 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:53,560 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,560 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:53,560 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:53,560 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yg0k1kq9
2023-05-03 05:40:53,561 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11fd8d6e-9406-4c57-8a17-3459a2f2ad98
2023-05-03 05:40:53,562 - distributed.worker - INFO - Starting Worker plugin RMMSetup-03b2fc6d-aba0-4500-ae85-06647d36d2bc
2023-05-03 05:40:53,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44691', status: init, memory: 0, processing: 0>
2023-05-03 05:40:53,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44691
2023-05-03 05:40:53,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32834
2023-05-03 05:40:53,598 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45173', status: init, memory: 0, processing: 0>
2023-05-03 05:40:53,599 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45173
2023-05-03 05:40:53,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32844
2023-05-03 05:40:53,600 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:53,600 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:53,890 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39653
2023-05-03 05:40:53,890 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39653
2023-05-03 05:40:53,890 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38413
2023-05-03 05:40:53,890 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:53,890 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:53,890 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:53,890 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:53,890 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4bz5ujtw
2023-05-03 05:40:53,891 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55063877-4c7b-4fd4-af81-b507e5c45895
2023-05-03 05:40:53,891 - distributed.worker - INFO - Starting Worker plugin PreImport-50a50de8-bb41-4db5-ba49-287924112ecc
2023-05-03 05:40:53,892 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3df13a76-44fd-4ad1-8e54-a22c7f7e1e6e
2023-05-03 05:40:54,045 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35943
2023-05-03 05:40:54,045 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35943
2023-05-03 05:40:54,046 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33981
2023-05-03 05:40:54,046 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,046 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,046 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:54,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:54,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ci9rf44a
2023-05-03 05:40:54,047 - distributed.worker - INFO - Starting Worker plugin PreImport-d525318d-faa8-496b-a692-8c9e87de0f55
2023-05-03 05:40:54,047 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1f56722-1597-4d66-84b3-649f8c30c8fb
2023-05-03 05:40:54,143 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37723
2023-05-03 05:40:54,143 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37723
2023-05-03 05:40:54,143 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45711
2023-05-03 05:40:54,143 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,143 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,143 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:54,143 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:54,144 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s5r6khe0
2023-05-03 05:40:54,144 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ddc029b0-06f6-4873-b211-b36d2775113f
2023-05-03 05:40:54,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38449
2023-05-03 05:40:54,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38449
2023-05-03 05:40:54,163 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34477
2023-05-03 05:40:54,163 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,163 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,163 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:54,163 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:54,163 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tac9lgls
2023-05-03 05:40:54,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a20dd6c4-aad3-4da0-b6b4-73b0d2553bae
2023-05-03 05:40:54,237 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43087
2023-05-03 05:40:54,238 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43087
2023-05-03 05:40:54,238 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44977
2023-05-03 05:40:54,238 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,238 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,238 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:40:54,238 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:40:54,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9ec_nkhe
2023-05-03 05:40:54,239 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e7867fa-e65e-4e16-9d8e-b1dc332dc2e7
2023-05-03 05:40:54,240 - distributed.worker - INFO - Starting Worker plugin PreImport-5ffdb0ac-aeec-4d76-83ae-c57080bfdf37
2023-05-03 05:40:54,240 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d68bd2fe-9aa9-4dd6-9b3f-6574ae5245cb
2023-05-03 05:40:54,363 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38359', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,364 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38359
2023-05-03 05:40:54,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55616
2023-05-03 05:40:54,379 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,397 - distributed.worker - INFO - Starting Worker plugin PreImport-071f9274-30c3-4ad3-97bf-938cbc2b5c42
2023-05-03 05:40:54,397 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,407 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44617', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,408 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44617
2023-05-03 05:40:54,408 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55632
2023-05-03 05:40:54,413 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37473', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,414 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37473
2023-05-03 05:40:54,414 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55634
2023-05-03 05:40:54,414 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,415 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,418 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43193', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,418 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43193
2023-05-03 05:40:54,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55650
2023-05-03 05:40:54,427 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36359', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,427 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36359
2023-05-03 05:40:54,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55664
2023-05-03 05:40:54,428 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,428 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,430 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,455 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44549', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,456 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44549
2023-05-03 05:40:54,456 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55674
2023-05-03 05:40:54,456 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34695', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,458 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34695
2023-05-03 05:40:54,458 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55680
2023-05-03 05:40:54,486 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,495 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,496 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a3b0cfac-6a8d-4045-9a23-57209d80f536
2023-05-03 05:40:54,496 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,548 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43087', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,549 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43087
2023-05-03 05:40:54,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55708
2023-05-03 05:40:54,550 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35943', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,550 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,550 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35943
2023-05-03 05:40:54,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55712
2023-05-03 05:40:54,551 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39653', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,551 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,551 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,552 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39653
2023-05-03 05:40:54,552 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55694
2023-05-03 05:40:54,552 - distributed.worker - INFO - Starting Worker plugin PreImport-35182862-d62a-40db-8d95-bf8c13d26b1a
2023-05-03 05:40:54,552 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2951e33c-86fc-41ab-a9ae-7f52a2d4f130
2023-05-03 05:40:54,552 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,552 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,552 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,553 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,553 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46661', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,553 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46661
2023-05-03 05:40:54,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55690
2023-05-03 05:40:54,554 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,581 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38449', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,582 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38449
2023-05-03 05:40:54,582 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55728
2023-05-03 05:40:54,582 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,583 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,585 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:40:54,586 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41489', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,587 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41489
2023-05-03 05:40:54,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55738
2023-05-03 05:40:54,637 - distributed.worker - INFO - Starting Worker plugin PreImport-1f4cf9fc-daa9-418b-9045-dd4c800d0fae
2023-05-03 05:40:54,637 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11841df3-ff9d-43b0-b492-e990b72bb233
2023-05-03 05:40:54,638 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,670 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37723', status: init, memory: 0, processing: 0>
2023-05-03 05:40:54,671 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37723
2023-05-03 05:40:54,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55778
2023-05-03 05:40:54,672 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:40:54,672 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:40:54,675 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:05,696 - distributed.scheduler - INFO - Remove client Client-0d6fe25d-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:05,697 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32814; closing.
2023-05-03 05:41:05,697 - distributed.scheduler - INFO - Remove client Client-0d6fe25d-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:05,698 - distributed.scheduler - INFO - Close client connection: Client-0d6fe25d-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:05,703 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55674; closing.
2023-05-03 05:41:05,704 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44549', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,704 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,705 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32834; closing.
2023-05-03 05:41:05,705 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44691', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,705 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44691
2023-05-03 05:41:05,707 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,707 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,707 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,707 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,707 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,707 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55738; closing.
2023-05-03 05:41:05,707 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,707 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,707 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55616; closing.
2023-05-03 05:41:05,707 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44549
2023-05-03 05:41:05,709 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:32834>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-05-03 05:41:05,710 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41489', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,711 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41489
2023-05-03 05:41:05,711 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38359', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,711 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38359
2023-05-03 05:41:05,711 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55690; closing.
2023-05-03 05:41:05,712 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46661', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,712 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46661
2023-05-03 05:41:05,713 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55632; closing.
2023-05-03 05:41:05,713 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55680; closing.
2023-05-03 05:41:05,713 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55650; closing.
2023-05-03 05:41:05,714 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44617', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,714 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44617
2023-05-03 05:41:05,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34695', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,715 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34695
2023-05-03 05:41:05,715 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43193', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,715 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43193
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44691
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44691
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41489
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44691
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41489
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38359
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38359
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41489
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44691
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46661
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46661
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38359
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44691
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44617
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46661
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44617
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41489
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34695
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44691
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44617
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34695
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41489
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44691
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38359
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43193
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34695
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43193
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38359
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46661
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41489
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43193
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44691
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41489
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46661
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44617
2023-05-03 05:41:05,717 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38359
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44617
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34695
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38359
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46661
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41489
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34695
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43193
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44617
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43193
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46661
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38359
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34695
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46661
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44617
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43193
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44617
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34695
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34695
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43193
2023-05-03 05:41:05,718 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43193
2023-05-03 05:41:05,809 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:05,810 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:05,810 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:05,810 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:05,810 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:05,810 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:05,810 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:05,810 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:05,822 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:05,822 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:05,822 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:05,822 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:05,822 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:05,823 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:05,823 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:05,823 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:05,830 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:05,831 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:05,834 - distributed.scheduler - INFO - Remove client Client-0d562a4d-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:05,834 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32828; closing.
2023-05-03 05:41:05,834 - distributed.scheduler - INFO - Remove client Client-0d562a4d-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:05,835 - distributed.scheduler - INFO - Close client connection: Client-0d562a4d-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:05,836 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40837'. Reason: nanny-close
2023-05-03 05:41:05,836 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,837 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43787'. Reason: nanny-close
2023-05-03 05:41:05,838 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,838 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35943. Reason: nanny-close
2023-05-03 05:41:05,838 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46791'. Reason: nanny-close
2023-05-03 05:41:05,839 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,839 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38183'. Reason: nanny-close
2023-05-03 05:41:05,839 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39653. Reason: nanny-close
2023-05-03 05:41:05,839 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,839 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37723. Reason: nanny-close
2023-05-03 05:41:05,839 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32985'. Reason: nanny-close
2023-05-03 05:41:05,840 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,840 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55712; closing.
2023-05-03 05:41:05,840 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,840 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45391'. Reason: nanny-close
2023-05-03 05:41:05,840 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37473. Reason: nanny-close
2023-05-03 05:41:05,840 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35943', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,840 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,840 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35943
2023-05-03 05:41:05,841 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43603'. Reason: nanny-close
2023-05-03 05:41:05,841 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,841 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45173. Reason: nanny-close
2023-05-03 05:41:05,841 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,841 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,841 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35943
2023-05-03 05:41:05,841 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,842 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39961'. Reason: nanny-close
2023-05-03 05:41:05,842 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38449. Reason: nanny-close
2023-05-03 05:41:05,842 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55694; closing.
2023-05-03 05:41:05,842 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:05,842 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35943
2023-05-03 05:41:05,842 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35943
2023-05-03 05:41:05,842 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,842 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39653', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,842 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,842 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43087. Reason: nanny-close
2023-05-03 05:41:05,842 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39653
2023-05-03 05:41:05,843 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35943
2023-05-03 05:41:05,843 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55778; closing.
2023-05-03 05:41:05,843 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,843 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35943
2023-05-03 05:41:05,843 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36359. Reason: nanny-close
2023-05-03 05:41:05,843 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,843 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37723', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,843 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37723
2023-05-03 05:41:05,844 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,844 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55634; closing.
2023-05-03 05:41:05,844 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,844 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,844 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:05,844 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32844; closing.
2023-05-03 05:41:05,845 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,845 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,845 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37473', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,845 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37473
2023-05-03 05:41:05,845 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45173', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,845 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45173
2023-05-03 05:41:05,846 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,846 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55728; closing.
2023-05-03 05:41:05,846 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55708; closing.
2023-05-03 05:41:05,846 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:05,847 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38449', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,847 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38449
2023-05-03 05:41:05,847 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43087', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,847 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43087
2023-05-03 05:41:05,848 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55664; closing.
2023-05-03 05:41:05,848 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36359', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:05,849 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36359
2023-05-03 05:41:05,849 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:07,805 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:41:07,806 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:07,807 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:07,809 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-03 05:41:07,810 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-05-03 05:41:10,092 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:10,096 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34567 instead
  warnings.warn(
2023-05-03 05:41:10,100 - distributed.scheduler - INFO - State start
2023-05-03 05:41:10,122 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:10,123 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:10,123 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:10,124 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3844, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 580, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 249, in main
    asyncio.run(run())
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/runners.py", line 44, in run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 245, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 225, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-05-03 05:41:10,454 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42103'
2023-05-03 05:41:10,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44059'
2023-05-03 05:41:10,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42329'
2023-05-03 05:41:10,496 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46297'
2023-05-03 05:41:10,508 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42185'
2023-05-03 05:41:10,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36083'
2023-05-03 05:41:10,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32787'
2023-05-03 05:41:10,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36627'
2023-05-03 05:41:12,226 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:12,226 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:12,235 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:12,235 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:12,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:12,256 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:12,256 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:12,257 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:12,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:12,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:12,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:12,273 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:12,276 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:12,279 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:12,279 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:12,279 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:12,289 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:12,289 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:12,293 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:12,297 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:12,297 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:12,329 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:12,342 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:12,356 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:14,451 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34127
2023-05-03 05:41:14,451 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34127
2023-05-03 05:41:14,451 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35727
2023-05-03 05:41:14,452 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,452 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,452 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:14,452 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:14,452 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3sh29ixx
2023-05-03 05:41:14,452 - distributed.worker - INFO - Starting Worker plugin PreImport-f2cc6a5d-661a-4731-8dd7-8addb0e7d71d
2023-05-03 05:41:14,452 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b614b13-2843-4008-a86c-d616a5ac2976
2023-05-03 05:41:14,456 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46527
2023-05-03 05:41:14,457 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46527
2023-05-03 05:41:14,457 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36573
2023-05-03 05:41:14,457 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,457 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,457 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:14,457 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:14,457 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-znsvmnw7
2023-05-03 05:41:14,458 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-487050a0-2c81-404b-846b-6e8b9d4a6a70
2023-05-03 05:41:14,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e82579fe-d599-49ce-a37b-2980268cdb36
2023-05-03 05:41:14,491 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45763
2023-05-03 05:41:14,491 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45763
2023-05-03 05:41:14,491 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46521
2023-05-03 05:41:14,491 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,491 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,491 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:14,491 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:14,491 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wljzuy4p
2023-05-03 05:41:14,492 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4382b08a-69d3-424c-bde0-4581b2b5d5b3
2023-05-03 05:41:14,623 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38651
2023-05-03 05:41:14,623 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38651
2023-05-03 05:41:14,623 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42797
2023-05-03 05:41:14,623 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,623 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,623 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:14,623 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:14,624 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-abyuv37k
2023-05-03 05:41:14,624 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2479ca9a-4cc2-4c7b-9488-65e9f07cf4af
2023-05-03 05:41:14,647 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45287
2023-05-03 05:41:14,647 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45287
2023-05-03 05:41:14,648 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39217
2023-05-03 05:41:14,648 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,648 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,648 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:14,648 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:14,648 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3j2f3ipl
2023-05-03 05:41:14,649 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dc1c9398-9120-4dda-a7f3-f701494add2b
2023-05-03 05:41:14,649 - distributed.worker - INFO - Starting Worker plugin PreImport-2965ec58-600d-424b-8187-96bb36a372b1
2023-05-03 05:41:14,650 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c37b926-9223-4ec9-99ec-e0509d226197
2023-05-03 05:41:14,663 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39307
2023-05-03 05:41:14,664 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39307
2023-05-03 05:41:14,664 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40425
2023-05-03 05:41:14,664 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,664 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,664 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:14,664 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:14,664 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2cyyrkt8
2023-05-03 05:41:14,665 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f52a86a-400a-4964-a3c0-19390e4a662b
2023-05-03 05:41:14,665 - distributed.worker - INFO - Starting Worker plugin PreImport-91a96e99-c83e-4745-b782-d4f0c9f1f416
2023-05-03 05:41:14,666 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bfc93e42-0aa7-4e95-8629-c1d435c30908
2023-05-03 05:41:14,688 - distributed.worker - INFO - Starting Worker plugin PreImport-5a4a8872-3226-4aca-b635-0fd8a1bb6996
2023-05-03 05:41:14,688 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,732 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,732 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,734 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38681
2023-05-03 05:41:14,734 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38681
2023-05-03 05:41:14,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:14,734 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40357
2023-05-03 05:41:14,734 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,735 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,735 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:14,735 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:14,735 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i7zeem5g
2023-05-03 05:41:14,736 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d580bfa-f03f-4846-a2ea-59550d74efae
2023-05-03 05:41:14,736 - distributed.worker - INFO - Starting Worker plugin PreImport-45a2a09d-f961-4003-8800-228fb4c836e3
2023-05-03 05:41:14,736 - distributed.worker - INFO - Starting Worker plugin RMMSetup-684e485a-cb6c-4e85-9ebb-3550aa33348a
2023-05-03 05:41:14,740 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-09766e5b-0d3d-4148-8f6d-0f6a9290c11c
2023-05-03 05:41:14,740 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,766 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36767
2023-05-03 05:41:14,767 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36767
2023-05-03 05:41:14,767 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41989
2023-05-03 05:41:14,767 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,767 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,767 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:14,767 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-05-03 05:41:14,767 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3u4td6w1
2023-05-03 05:41:14,768 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-74dfd7de-ebb2-46f2-9f04-1a2c0a801862
2023-05-03 05:41:14,772 - distributed.worker - INFO - Starting Worker plugin PreImport-0de542e2-2a76-4f6e-98d9-c14f86091332
2023-05-03 05:41:14,772 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04e67402-9a4d-4c04-a2dc-989cb69bb406
2023-05-03 05:41:14,805 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,805 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:14,835 - distributed.worker - INFO - Starting Worker plugin PreImport-96d0cb4d-3a94-4e05-9764-e6f50dafeffb
2023-05-03 05:41:14,835 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f701a75-8c97-448c-92c5-00cda8a2ec32
2023-05-03 05:41:14,836 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,857 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,877 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,878 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,881 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:14,882 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,888 - distributed.worker - INFO - Starting Worker plugin PreImport-58979d67-e47c-4b6d-a435-86fdc468110d
2023-05-03 05:41:14,889 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-071a7535-7a18-4965-94a0-47386804d153
2023-05-03 05:41:14,889 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,892 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,895 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,895 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:14,924 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,925 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,926 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,927 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:14,930 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:14,932 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,932 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,933 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:14,964 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:14,964 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:14,966 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:14,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:14,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:14,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:14,975 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:14,976 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:14,976 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:14,976 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:14,976 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-05-03 05:41:14,984 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46297'. Reason: nanny-close
2023-05-03 05:41:14,984 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:14,985 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42185'. Reason: nanny-close
2023-05-03 05:41:14,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:14,986 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34127. Reason: nanny-close
2023-05-03 05:41:14,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42103'. Reason: nanny-close
2023-05-03 05:41:14,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:14,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44059'. Reason: nanny-close
2023-05-03 05:41:14,986 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45287. Reason: nanny-close
2023-05-03 05:41:14,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:14,987 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42329'. Reason: nanny-close
2023-05-03 05:41:14,987 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38651. Reason: nanny-close
2023-05-03 05:41:14,987 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:14,988 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:14,988 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36083'. Reason: nanny-close
2023-05-03 05:41:14,988 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38681. Reason: nanny-close
2023-05-03 05:41:14,988 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32787'. Reason: nanny-close
2023-05-03 05:41:14,988 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:14,988 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:14,988 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39307. Reason: nanny-close
2023-05-03 05:41:14,988 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36627'. Reason: nanny-close
2023-05-03 05:41:14,989 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:14,989 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:14,989 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:14,989 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45763. Reason: nanny-close
2023-05-03 05:41:14,990 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:14,990 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34127
2023-05-03 05:41:14,990 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46527. Reason: nanny-close
2023-05-03 05:41:14,990 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34127
2023-05-03 05:41:14,990 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:14,990 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:14,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:14,991 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34127
2023-05-03 05:41:14,991 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36767. Reason: nanny-close
2023-05-03 05:41:14,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:14,992 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34127
2023-05-03 05:41:14,992 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34127
2023-05-03 05:41:14,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:14,992 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:14,992 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:14,993 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:14,993 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:14,993 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:14,994 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:14,994 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-05-03 05:41:18,688 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:18,693 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40211 instead
  warnings.warn(
2023-05-03 05:41:18,697 - distributed.scheduler - INFO - State start
2023-05-03 05:41:18,718 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:18,720 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-03 05:41:18,720 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40211/status
2023-05-03 05:41:18,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44577'
2023-05-03 05:41:19,023 - distributed.scheduler - INFO - Receive client connection: Client-1f8eb281-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:19,040 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53060
2023-05-03 05:41:19,246 - distributed.scheduler - INFO - Receive client connection: Client-20ff1d25-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:19,247 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53086
2023-05-03 05:41:20,613 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:20,613 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:20,933 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:21,796 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36331
2023-05-03 05:41:21,796 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36331
2023-05-03 05:41:21,796 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-05-03 05:41:21,796 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:21,796 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:21,796 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:21,796 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-03 05:41:21,796 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zabm_jrq
2023-05-03 05:41:21,797 - distributed.worker - INFO - Starting Worker plugin PreImport-6f0cd847-98fd-4c65-81e6-cacea52bb58d
2023-05-03 05:41:21,797 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6bb32c63-4ee3-4b73-8de0-158715d6d466
2023-05-03 05:41:21,797 - distributed.worker - INFO - Starting Worker plugin RMMSetup-db1594ab-5d4f-4852-b895-e3b2286a4d92
2023-05-03 05:41:21,798 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:21,840 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36331', status: init, memory: 0, processing: 0>
2023-05-03 05:41:21,844 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36331
2023-05-03 05:41:21,844 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53114
2023-05-03 05:41:21,845 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:21,845 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:21,847 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:21,910 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:21,911 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:21,913 - distributed.scheduler - INFO - Remove client Client-1f8eb281-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:21,914 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53060; closing.
2023-05-03 05:41:21,914 - distributed.scheduler - INFO - Remove client Client-1f8eb281-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:21,914 - distributed.scheduler - INFO - Remove client Client-20ff1d25-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:21,914 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53086; closing.
2023-05-03 05:41:21,915 - distributed.scheduler - INFO - Remove client Client-20ff1d25-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:21,915 - distributed.scheduler - INFO - Close client connection: Client-1f8eb281-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:21,915 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44577'. Reason: nanny-close
2023-05-03 05:41:21,916 - distributed.scheduler - INFO - Close client connection: Client-20ff1d25-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:21,916 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:21,917 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36331. Reason: nanny-close
2023-05-03 05:41:21,919 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:21,919 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53114; closing.
2023-05-03 05:41:21,919 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36331', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:21,919 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36331
2023-05-03 05:41:21,920 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:21,920 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:23,082 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:41:23,083 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:23,083 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:23,084 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-03 05:41:23,085 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-05-03 05:41:27,471 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:27,476 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44955 instead
  warnings.warn(
2023-05-03 05:41:27,480 - distributed.scheduler - INFO - State start
2023-05-03 05:41:27,501 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:27,502 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-03 05:41:27,502 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44955/status
2023-05-03 05:41:27,641 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46043'
2023-05-03 05:41:27,830 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36109', status: init, memory: 0, processing: 0>
2023-05-03 05:41:27,848 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36109
2023-05-03 05:41:27,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52036
2023-05-03 05:41:27,892 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52036; closing.
2023-05-03 05:41:27,893 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36109', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:27,893 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36109
2023-05-03 05:41:27,893 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:28,595 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:52042 closed before handshake completed
2023-05-03 05:41:29,355 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:29,356 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:29,674 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:30,495 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40125
2023-05-03 05:41:30,495 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40125
2023-05-03 05:41:30,495 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37285
2023-05-03 05:41:30,495 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:30,495 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:30,495 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:30,495 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-03 05:41:30,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ba9e9ikb
2023-05-03 05:41:30,496 - distributed.worker - INFO - Starting Worker plugin PreImport-b27648ad-745a-482d-8f65-54ba37c815cc
2023-05-03 05:41:30,497 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3daf8984-91b2-4a5c-8878-601388798c4d
2023-05-03 05:41:30,498 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4487d85d-94cc-4c01-81ac-8aff2bcfbd91
2023-05-03 05:41:30,498 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:30,537 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40125', status: init, memory: 0, processing: 0>
2023-05-03 05:41:30,538 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40125
2023-05-03 05:41:30,538 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52048
2023-05-03 05:41:30,539 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:30,539 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:30,541 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:32,046 - distributed.scheduler - INFO - Receive client connection: Client-24da308d-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:32,048 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52064
2023-05-03 05:41:32,056 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:32,060 - distributed.scheduler - INFO - Remove client Client-24da308d-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:32,060 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52064; closing.
2023-05-03 05:41:32,060 - distributed.scheduler - INFO - Remove client Client-24da308d-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:32,061 - distributed.scheduler - INFO - Close client connection: Client-24da308d-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:32,061 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46043'. Reason: nanny-close
2023-05-03 05:41:32,062 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:32,063 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40125. Reason: nanny-close
2023-05-03 05:41:32,066 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:32,066 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52048; closing.
2023-05-03 05:41:32,066 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40125', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:32,066 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40125
2023-05-03 05:41:32,066 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:32,067 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:33,430 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:41:33,430 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:33,431 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:33,432 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-03 05:41:33,432 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-05-03 05:41:35,914 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:35,919 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45593 instead
  warnings.warn(
2023-05-03 05:41:35,923 - distributed.scheduler - INFO - State start
2023-05-03 05:41:35,944 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:35,945 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-03 05:41:35,946 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45593/status
2023-05-03 05:41:39,796 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:41:39,796 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:39,797 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:39,797 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-03 05:41:39,798 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-05-03 05:41:42,143 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:42,147 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38381 instead
  warnings.warn(
2023-05-03 05:41:42,151 - distributed.scheduler - INFO - State start
2023-05-03 05:41:42,173 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:42,174 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-05-03 05:41:42,175 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38381/status
2023-05-03 05:41:42,237 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40925'
2023-05-03 05:41:42,849 - distributed.scheduler - INFO - Receive client connection: Client-2d87d10b-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:42,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45860
2023-05-03 05:41:43,924 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:43,924 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:43,932 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:44,731 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42687
2023-05-03 05:41:44,731 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42687
2023-05-03 05:41:44,731 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35231
2023-05-03 05:41:44,732 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-05-03 05:41:44,732 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:44,732 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:44,732 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-03 05:41:44,732 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-o3pcl635
2023-05-03 05:41:44,732 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c63261dc-bb10-42a1-8650-553c6a142b56
2023-05-03 05:41:44,732 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91b5627b-d509-494e-ab03-899e70eed03e
2023-05-03 05:41:44,732 - distributed.worker - INFO - Starting Worker plugin PreImport-8b5393ba-aaa2-44f3-89d6-dfe6bd8e7a3e
2023-05-03 05:41:44,732 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:44,755 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42687', status: init, memory: 0, processing: 0>
2023-05-03 05:41:44,759 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42687
2023-05-03 05:41:44,759 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55766
2023-05-03 05:41:44,760 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-05-03 05:41:44,760 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:44,762 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-05-03 05:41:44,791 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:44,794 - distributed.scheduler - INFO - Remove client Client-2d87d10b-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:44,794 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45860; closing.
2023-05-03 05:41:44,795 - distributed.scheduler - INFO - Remove client Client-2d87d10b-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:44,795 - distributed.scheduler - INFO - Close client connection: Client-2d87d10b-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:44,797 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40925'. Reason: nanny-close
2023-05-03 05:41:44,798 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:44,800 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42687. Reason: nanny-close
2023-05-03 05:41:44,801 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-05-03 05:41:44,802 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55766; closing.
2023-05-03 05:41:44,802 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42687', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:44,802 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42687
2023-05-03 05:41:44,802 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:44,803 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:45,963 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:41:45,963 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:45,964 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:45,964 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-05-03 05:41:45,965 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 79, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 56, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 210, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 4, in <module>
    from dask.dataframe import backends, dispatch, rolling
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/backends.py", line 23, in <module>
    from dask.dataframe.core import DataFrame, Index, Scalar, Series, _Frame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/core.py", line 37, in <module>
    from dask.dataframe import methods
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/methods.py", line 27, in <module>
    from dask.dataframe.utils import is_dataframe_like, is_index_like, is_series_like
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/utils.py", line 20, in <module>
    from dask.dataframe import (  # noqa: F401 register pandas extension types
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/_dtypes.py", line 4, in <module>
    from dask.dataframe.extensions import make_array_nonempty, make_scalar
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/extensions.py", line 6, in <module>
    from dask.dataframe.accessor import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/accessor.py", line 121, in <module>
    class DatetimeAccessor(Accessor):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/accessor.py", line 80, in __init_subclass__
    _bind_property(cls, pd_cls, attr, min_version)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/accessor.py", line 34, in _bind_property
    setattr(cls, attr, property(derived_from(pd_cls, version=min_version)(func)))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 851, in wrapper
    method.__doc__ = _derived_from(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 814, in _derived_from
    doc = skip_doctest(doc)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 681, in skip_doctest
    return "\n".join([_skip_doctest(line) for line in doc.split("\n")])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 681, in <listcomp>
    return "\n".join([_skip_doctest(line) for line in doc.split("\n")])
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 79, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 56, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 210, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 15, in <module>
    from .cuda_worker import CUDAWorker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 12, in <module>
    from distributed import Nanny
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/__init__.py", line 22, in <module>
    from distributed.actor import Actor, ActorFuture, BaseActorFuture
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/actor.py", line 14, in <module>
    from distributed.client import Future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 49, in <module>
    from distributed.core import ErrorMessage
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 27, in <module>
    from distributed.comm import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/__init__.py", line 48, in <module>
    _register_transports()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/__init__.py", line 22, in _register_transports
    from distributed.comm import inproc, ws
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ws.py", line 12, in <module>
    from tornado import web
  File "<frozen importlib._bootstrap>", line 1055, in _handle_fromlist
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/__init__.py", line 66, in __getattr__
    return importlib.import_module("." + name, __name__)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/web.py", line 91, in <module>
    from tornado.httpserver import HTTPServer
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/httpserver.py", line 32, in <module>
    from tornado.http1connection import HTTP1ServerConnection, HTTP1ConnectionParameters
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/http1connection.py", line 34, in <module>
    from tornado import iostream
  File "<frozen importlib._bootstrap>", line 1055, in _handle_fromlist
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/__init__.py", line 66, in __getattr__
    return importlib.import_module("." + name, __name__)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/iostream.py", line 40, in <module>
    from tornado.netutil import ssl_wrap_socket, _client_ssl_defaults, _server_ssl_defaults
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 35, in <module>
    _client_ssl_defaults = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)
  File "/opt/conda/envs/gdf/lib/python3.9/ssl.py", line 751, in create_default_context
    context.load_default_certs(purpose)
  File "/opt/conda/envs/gdf/lib/python3.9/ssl.py", line 576, in load_default_certs
    self.set_default_verify_paths()
KeyboardInterrupt
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-05-03 05:41:50,300 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:50,305 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39857 instead
  warnings.warn(
2023-05-03 05:41:50,309 - distributed.scheduler - INFO - State start
2023-05-03 05:41:50,330 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:50,331 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-03 05:41:50,331 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39857/status
2023-05-03 05:41:50,347 - distributed.scheduler - INFO - Receive client connection: Client-3388e72b-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:50,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42164
2023-05-03 05:41:50,493 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36455'
2023-05-03 05:41:50,622 - distributed.scheduler - INFO - Receive client connection: Client-326de0ee-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:50,622 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42208
2023-05-03 05:41:52,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:52,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:52,320 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:53,156 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38915
2023-05-03 05:41:53,156 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38915
2023-05-03 05:41:53,156 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35825
2023-05-03 05:41:53,156 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:53,156 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:53,157 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:53,157 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-03 05:41:53,157 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ps0jnlpi
2023-05-03 05:41:53,157 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-50daa591-f6be-499a-b2bf-62c9fd591c7d
2023-05-03 05:41:53,157 - distributed.worker - INFO - Starting Worker plugin PreImport-b81f2244-d3eb-40f9-b092-1aec427f6566
2023-05-03 05:41:53,157 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc721f48-0b26-4616-95b0-43cc499b282d
2023-05-03 05:41:53,268 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:53,302 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38915', status: init, memory: 0, processing: 0>
2023-05-03 05:41:53,304 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38915
2023-05-03 05:41:53,304 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42222
2023-05-03 05:41:53,305 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:53,305 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:53,307 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:53,316 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:53,320 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:53,322 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:53,325 - distributed.scheduler - INFO - Remove client Client-326de0ee-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:53,325 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42208; closing.
2023-05-03 05:41:53,325 - distributed.scheduler - INFO - Remove client Client-326de0ee-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:53,326 - distributed.scheduler - INFO - Close client connection: Client-326de0ee-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:53,327 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36455'. Reason: nanny-close
2023-05-03 05:41:53,331 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:53,336 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:53,337 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:53,338 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:53,339 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38915. Reason: nanny-close
2023-05-03 05:41:53,340 - distributed.scheduler - INFO - Remove client Client-3388e72b-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:53,341 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42164; closing.
2023-05-03 05:41:53,341 - distributed.scheduler - INFO - Remove client Client-3388e72b-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:53,341 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:53,341 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42222; closing.
2023-05-03 05:41:53,342 - distributed.scheduler - INFO - Close client connection: Client-3388e72b-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:53,342 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38915', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:53,343 - distributed.nanny - INFO - Worker closed
2023-05-03 05:41:53,343 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38915
2023-05-03 05:41:53,343 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:54,494 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:41:54,495 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:41:54,495 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:41:54,496 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-03 05:41:54,497 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-05-03 05:41:56,690 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:56,694 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46569 instead
  warnings.warn(
2023-05-03 05:41:56,698 - distributed.scheduler - INFO - State start
2023-05-03 05:41:56,719 - distributed.scheduler - INFO - -----------------------------------------------
2023-05-03 05:41:56,720 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-05-03 05:41:56,721 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46569/status
2023-05-03 05:41:56,875 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46233'
2023-05-03 05:41:57,112 - distributed.scheduler - INFO - Receive client connection: Client-3650a3c6-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:57,128 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41948
2023-05-03 05:41:58,024 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36681', status: init, memory: 0, processing: 0>
2023-05-03 05:41:58,025 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36681
2023-05-03 05:41:58,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41962
2023-05-03 05:41:58,038 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41962; closing.
2023-05-03 05:41:58,038 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36681', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:58,038 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36681
2023-05-03 05:41:58,038 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:58,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:41:58,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:41:58,526 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-05-03 05:41:58,699 - distributed.comm.tcp - INFO - Connection from tcp://127.0.0.1:41968 closed before handshake completed
2023-05-03 05:41:59,377 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39427
2023-05-03 05:41:59,377 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39427
2023-05-03 05:41:59,377 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40451
2023-05-03 05:41:59,377 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-05-03 05:41:59,377 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:59,377 - distributed.worker - INFO -               Threads:                          1
2023-05-03 05:41:59,377 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-05-03 05:41:59,377 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bghazl4t
2023-05-03 05:41:59,378 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f587a1c-5c9e-4668-b8a1-623e2948b723
2023-05-03 05:41:59,378 - distributed.worker - INFO - Starting Worker plugin PreImport-8fff0a20-6fb0-412e-ab6d-0dd24ffbfaec
2023-05-03 05:41:59,378 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f510ac52-67b7-49a0-8fc6-a0eb8d728a7c
2023-05-03 05:41:59,440 - distributed.scheduler - INFO - Receive client connection: Client-38f431bb-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:59,441 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41972
2023-05-03 05:41:59,491 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:59,520 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39427', status: init, memory: 0, processing: 0>
2023-05-03 05:41:59,521 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39427
2023-05-03 05:41:59,521 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42004
2023-05-03 05:41:59,521 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-05-03 05:41:59,522 - distributed.worker - INFO - -------------------------------------------------
2023-05-03 05:41:59,524 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-05-03 05:41:59,549 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-05-03 05:41:59,554 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:59,558 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:59,559 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:59,562 - distributed.scheduler - INFO - Remove client Client-38f431bb-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:59,562 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41972; closing.
2023-05-03 05:41:59,562 - distributed.scheduler - INFO - Remove client Client-38f431bb-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:59,563 - distributed.scheduler - INFO - Close client connection: Client-38f431bb-e975-11ed-b865-d8c49764f6bb
2023-05-03 05:41:59,584 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-05-03 05:41:59,589 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-05-03 05:41:59,593 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:59,594 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-05-03 05:41:59,596 - distributed.scheduler - INFO - Remove client Client-3650a3c6-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:59,597 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41948; closing.
2023-05-03 05:41:59,597 - distributed.scheduler - INFO - Remove client Client-3650a3c6-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:59,597 - distributed.scheduler - INFO - Close client connection: Client-3650a3c6-e975-11ed-b0d3-d8c49764f6bb
2023-05-03 05:41:59,598 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46233'. Reason: nanny-close
2023-05-03 05:41:59,599 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-05-03 05:41:59,600 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39427. Reason: nanny-close
2023-05-03 05:41:59,602 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-05-03 05:41:59,602 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42004; closing.
2023-05-03 05:41:59,602 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39427', status: closing, memory: 0, processing: 0>
2023-05-03 05:41:59,602 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39427
2023-05-03 05:41:59,603 - distributed.scheduler - INFO - Lost all workers
2023-05-03 05:41:59,603 - distributed.nanny - INFO - Worker closed
2023-05-03 05:42:00,665 - distributed._signals - INFO - Received signal SIGINT (2)
2023-05-03 05:42:00,665 - distributed.scheduler - INFO - Scheduler closing...
2023-05-03 05:42:00,666 - distributed.scheduler - INFO - Scheduler closing all comms
2023-05-03 05:42:00,667 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-05-03 05:42:00,667 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38335 instead
  warnings.warn(
2023-05-03 05:42:10,874 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:10,874 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:10,875 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:10,875 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:10,938 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:10,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:10,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:10,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:10,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:10,961 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:10,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:10,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:10,973 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:10,973 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:11,002 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:11,003 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41779 instead
  warnings.warn(
2023-05-03 05:42:20,507 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:20,507 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:20,559 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:20,559 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:20,609 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:20,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:20,621 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:20,621 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:20,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:20,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:20,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:20,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:20,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:20,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:20,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:20,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45253 instead
  warnings.warn(
2023-05-03 05:42:30,054 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:30,054 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:30,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:30,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:30,171 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:30,171 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:30,194 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:30,195 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:30,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:30,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:30,211 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:30,211 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:30,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:30,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:30,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:30,261 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33787 instead
  warnings.warn(
2023-05-03 05:42:42,353 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:42,353 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:42,385 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:42,385 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:42,397 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:42,397 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:42,399 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:42,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:42,405 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:42,405 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:42,477 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:42,477 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:42,477 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:42,477 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:42,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:42,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41107 instead
  warnings.warn(
2023-05-03 05:42:56,159 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:56,159 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:56,181 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:56,181 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:56,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:56,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:56,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:56,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:56,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:56,250 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:56,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:56,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:56,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:56,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:42:56,253 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:42:56,253 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35187 instead
  warnings.warn(
2023-05-03 05:43:07,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:07,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:07,889 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:07,889 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:07,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:07,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:07,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:07,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:07,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:07,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:07,978 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:07,978 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:07,979 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:07,979 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:07,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:07,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34349 instead
  warnings.warn(
2023-05-03 05:43:19,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:19,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:19,246 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:19,246 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:19,247 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:19,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:19,249 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:19,249 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:19,318 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:19,318 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:19,318 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:19,319 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:19,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:19,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:19,367 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:19,368 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40199 instead
  warnings.warn(
2023-05-03 05:43:30,472 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:30,472 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:30,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:30,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:30,536 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:30,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:30,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:30,539 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:30,573 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:30,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:30,580 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:30,580 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:30,658 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:30,658 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-03 05:43:30,683 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-03 05:43:30,683 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46121 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37783 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46629 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40563 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44645 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41225 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35775 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45619 instead
  warnings.warn(
2023-05-03 05:45:07,169 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 21, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 57, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 122, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 88, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 435, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 664, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-05-03 05:45:07,380 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-877b7fdc-8113-45d2-a650-bed00f17d18c
Function:  _run_coroutine_on_worker
args:      (82880876975892984918351160413766573978, <function shuffle_task at 0x7f0e23ed35e0>, ('explicit-comms-shuffle-1af1fd2a0d0a803c7e4d3c1b0827de98', {0: set(), 1: {"('from_pandas-6cfe5cca90d2b9dc7cc279513487a6d6', 0)"}}, {0: {0}, 1: set()}, ['key'], 1, False, 1, 1))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

Process SpawnProcess-16:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 153, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 314, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 599, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 3186, in get
    results = self.gather(packed, asynchronous=asynchronous, direct=direct)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2345, in gather
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 349, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 416, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 389, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2208, in _gather
    raise exception.with_traceback(traceback)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 102, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 446, in result
    return self.__get_result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 99, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 446, in result
    return self.__get_result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 379, in shuffle_task
    await send_recv_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 296, in send_recv_partitions
    await asyncio.gather(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 124, in recv
    await asyncio.gather(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 120, in read_msg
    msg: Dict[int, DataFrame] = nested_deserialize(await eps[rank].read())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 59, in dask_loads
    typ = pickle.loads(header["type-serialized"])
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 96, in loads
    return pickle.loads(x)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 21, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 9, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 57, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 28, in <module>
    from cudf.core.udf.groupby_utils import jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 122, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 88, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 435, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 664, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 320, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 388, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40307 instead
  warnings.warn(
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-03 05:45:20,996 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-03 05:45:21,008 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-dd152170-9f6d-47cd-9982-7c956787b8e8
Function:  _run_coroutine_on_worker
args:      (145768789415960198899041074397078087023, <function shuffle_task at 0x7fcf9b9e14c0>, ('explicit-comms-shuffle-6ed698cc872d93ae5811c1078586d783', {0: set(), 1: set(), 2: {"('from_pandas-279b6fb66ea03d82dad9c5e6ed4d9293', 0)"}}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 1))
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

Process SpawnProcess-17:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 153, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 314, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 599, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 3186, in get
    results = self.gather(packed, asynchronous=asynchronous, direct=direct)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2345, in gather
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 349, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 416, in sync
    raise exc.with_traceback(tb)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 389, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2208, in _gather
    raise exception.with_traceback(traceback)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 102, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 446, in result
    return self.__get_result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 99, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 446, in result
    return self.__get_result()
  File "/opt/conda/envs/gdf/lib/python3.9/concurrent/futures/_base.py", line 391, in __get_result
    raise self._exception
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 379, in shuffle_task
    await send_recv_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 296, in send_recv_partitions
    await asyncio.gather(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 124, in recv
    await asyncio.gather(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 120, in read_msg
    msg: Dict[int, DataFrame] = nested_deserialize(await eps[rank].read())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 168, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44177 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38025 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33929 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46709 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46457 instead
  warnings.warn(
2023-05-03 05:46:35,833 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-03 05:46:35,869 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f6e2a35adc0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
