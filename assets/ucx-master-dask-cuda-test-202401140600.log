============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-14 06:32:44,743 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:32:44,748 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41167 instead
  warnings.warn(
2024-01-14 06:32:44,752 - distributed.scheduler - INFO - State start
2024-01-14 06:32:44,775 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:32:44,776 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-14 06:32:44,776 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41167/status
2024-01-14 06:32:44,777 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:32:44,906 - distributed.scheduler - INFO - Receive client connection: Client-b8d31b28-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:32:44,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60710
2024-01-14 06:32:45,000 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41617'
2024-01-14 06:32:45,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33109'
2024-01-14 06:32:45,024 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40707'
2024-01-14 06:32:45,033 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44893'
2024-01-14 06:32:46,819 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:46,819 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:46,823 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:46,824 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45797
2024-01-14 06:32:46,824 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45797
2024-01-14 06:32:46,824 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45805
2024-01-14 06:32:46,824 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-14 06:32:46,824 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,824 - distributed.worker - INFO -               Threads:                          4
2024-01-14 06:32:46,824 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-14 06:32:46,824 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-48ljh_g1
2024-01-14 06:32:46,824 - distributed.worker - INFO - Starting Worker plugin PreImport-81da8e55-9396-4776-b855-6fa4500d2473
2024-01-14 06:32:46,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06b106ef-bd65-491b-8a55-242d2f4da790
2024-01-14 06:32:46,825 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85a8c6b6-9cba-4a9b-b0e0-4b758112b6b1
2024-01-14 06:32:46,825 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:46,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:46,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:46,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:46,831 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:46,831 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:46,831 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43797
2024-01-14 06:32:46,832 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43797
2024-01-14 06:32:46,832 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44285
2024-01-14 06:32:46,832 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-14 06:32:46,832 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,832 - distributed.worker - INFO -               Threads:                          4
2024-01-14 06:32:46,832 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-14 06:32:46,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ttiwk0sz
2024-01-14 06:32:46,832 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37119
2024-01-14 06:32:46,832 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37119
2024-01-14 06:32:46,832 - distributed.worker - INFO - Starting Worker plugin PreImport-ba0807be-b372-4436-8bd4-93aa100a5635
2024-01-14 06:32:46,832 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44755
2024-01-14 06:32:46,832 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-14 06:32:46,832 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af829c99-e55c-4d5f-8e2b-882cb4e33b57
2024-01-14 06:32:46,832 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,832 - distributed.worker - INFO -               Threads:                          4
2024-01-14 06:32:46,832 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-14 06:32:46,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-yeup0p9s
2024-01-14 06:32:46,833 - distributed.worker - INFO - Starting Worker plugin PreImport-d9f70693-976c-488e-83af-bd17075ea6be
2024-01-14 06:32:46,833 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9812363f-2410-4dda-b9b5-a8a60454e2f2
2024-01-14 06:32:46,833 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2617a3f-a58c-4597-a202-b4026b8ad68d
2024-01-14 06:32:46,833 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,834 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9419ebcb-aa40-43f4-98dd-62e300a509fe
2024-01-14 06:32:46,835 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,845 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:46,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:46,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:46,850 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36729
2024-01-14 06:32:46,850 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36729
2024-01-14 06:32:46,850 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39581
2024-01-14 06:32:46,850 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-14 06:32:46,850 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,850 - distributed.worker - INFO -               Threads:                          4
2024-01-14 06:32:46,850 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-14 06:32:46,850 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-fmt6g_le
2024-01-14 06:32:46,851 - distributed.worker - INFO - Starting Worker plugin PreImport-5f6103a6-1b0e-461d-8861-18dcf4ae5267
2024-01-14 06:32:46,851 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d038ad02-76b1-4d91-a810-cc2e87d01828
2024-01-14 06:32:46,851 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed3ce8f7-1692-4f87-87e2-faa9b9b6fd3a
2024-01-14 06:32:46,851 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,940 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45797', status: init, memory: 0, processing: 0>
2024-01-14 06:32:46,941 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45797
2024-01-14 06:32:46,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60774
2024-01-14 06:32:46,942 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:46,943 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-14 06:32:46,943 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,944 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-14 06:32:46,953 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37119', status: init, memory: 0, processing: 0>
2024-01-14 06:32:46,953 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37119
2024-01-14 06:32:46,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60788
2024-01-14 06:32:46,954 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:46,955 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-14 06:32:46,955 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,955 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43797', status: init, memory: 0, processing: 0>
2024-01-14 06:32:46,956 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43797
2024-01-14 06:32:46,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60790
2024-01-14 06:32:46,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-14 06:32:46,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:46,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-14 06:32:46,958 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-14 06:32:46,965 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36729', status: init, memory: 0, processing: 0>
2024-01-14 06:32:46,966 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36729
2024-01-14 06:32:46,966 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60792
2024-01-14 06:32:46,967 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:46,968 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-14 06:32:46,968 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:46,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-14 06:32:47,066 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-14 06:32:47,066 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-14 06:32:47,067 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-14 06:32:47,067 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-14 06:32:47,071 - distributed.scheduler - INFO - Remove client Client-b8d31b28-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:32:47,072 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60710; closing.
2024-01-14 06:32:47,072 - distributed.scheduler - INFO - Remove client Client-b8d31b28-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:32:47,072 - distributed.scheduler - INFO - Close client connection: Client-b8d31b28-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:32:47,074 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41617'. Reason: nanny-close
2024-01-14 06:32:47,074 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:32:47,075 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33109'. Reason: nanny-close
2024-01-14 06:32:47,075 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:32:47,075 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36729. Reason: nanny-close
2024-01-14 06:32:47,075 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40707'. Reason: nanny-close
2024-01-14 06:32:47,076 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:32:47,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44893'. Reason: nanny-close
2024-01-14 06:32:47,076 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45797. Reason: nanny-close
2024-01-14 06:32:47,076 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:32:47,076 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37119. Reason: nanny-close
2024-01-14 06:32:47,077 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43797. Reason: nanny-close
2024-01-14 06:32:47,077 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-14 06:32:47,077 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60792; closing.
2024-01-14 06:32:47,078 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-14 06:32:47,078 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705213967.0782738')
2024-01-14 06:32:47,078 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-14 06:32:47,078 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:47,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60788; closing.
2024-01-14 06:32:47,079 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-14 06:32:47,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60774; closing.
2024-01-14 06:32:47,079 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:47,079 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:47,080 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37119', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705213967.080129')
2024-01-14 06:32:47,080 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45797', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705213967.0805123')
2024-01-14 06:32:47,080 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:47,080 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60790; closing.
2024-01-14 06:32:47,081 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:60788>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:60788>: Stream is closed
2024-01-14 06:32:47,082 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43797', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705213967.082447')
2024-01-14 06:32:47,082 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:32:47,839 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:32:47,839 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:32:47,840 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:32:47,841 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-14 06:32:47,841 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-14 06:32:50,032 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:32:50,036 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37443 instead
  warnings.warn(
2024-01-14 06:32:50,041 - distributed.scheduler - INFO - State start
2024-01-14 06:32:50,066 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:32:50,067 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-14 06:32:50,068 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:32:50,068 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-14 06:32:50,273 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39431'
2024-01-14 06:32:50,294 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34689'
2024-01-14 06:32:50,311 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35767'
2024-01-14 06:32:50,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36277'
2024-01-14 06:32:50,338 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39299'
2024-01-14 06:32:50,350 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40649'
2024-01-14 06:32:50,361 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38537'
2024-01-14 06:32:50,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44901'
2024-01-14 06:32:52,219 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:52,219 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:52,220 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:52,220 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:52,224 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:52,224 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:52,225 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36455
2024-01-14 06:32:52,225 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36455
2024-01-14 06:32:52,225 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38067
2024-01-14 06:32:52,225 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:32:52,225 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33149
2024-01-14 06:32:52,225 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:52,225 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33149
2024-01-14 06:32:52,225 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:32:52,225 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38329
2024-01-14 06:32:52,225 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:32:52,225 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:32:52,225 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5i91blfm
2024-01-14 06:32:52,225 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:52,225 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:32:52,225 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:32:52,225 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o6jl5tr2
2024-01-14 06:32:52,225 - distributed.worker - INFO - Starting Worker plugin RMMSetup-529a106c-b1db-4e1b-9eef-c547f9157a59
2024-01-14 06:32:52,226 - distributed.worker - INFO - Starting Worker plugin PreImport-7f055e9b-4c52-42a0-90f4-5f63fc402c0a
2024-01-14 06:32:52,226 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c2a86d87-ba17-4786-bba7-250736a02492
2024-01-14 06:32:52,226 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ba249cf-df23-4199-b4e6-d81b36f7a63e
2024-01-14 06:32:52,270 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:52,270 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:52,277 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:52,277 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:52,277 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:52,278 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44607
2024-01-14 06:32:52,279 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44607
2024-01-14 06:32:52,279 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38011
2024-01-14 06:32:52,279 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:32:52,279 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:52,279 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:32:52,279 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:32:52,279 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ayaulcml
2024-01-14 06:32:52,280 - distributed.worker - INFO - Starting Worker plugin RMMSetup-400c6384-cad0-444d-a563-af96b51d18c8
2024-01-14 06:32:52,281 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:52,282 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43319
2024-01-14 06:32:52,282 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43319
2024-01-14 06:32:52,282 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41649
2024-01-14 06:32:52,282 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:32:52,282 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:52,282 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:32:52,282 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:32:52,283 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-__6qz7f7
2024-01-14 06:32:52,283 - distributed.worker - INFO - Starting Worker plugin RMMSetup-41dbf6f7-a236-469e-9191-9f6edc8997e7
2024-01-14 06:32:52,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:52,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:52,287 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:52,287 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:52,287 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:52,288 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43863
2024-01-14 06:32:52,288 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43863
2024-01-14 06:32:52,288 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42935
2024-01-14 06:32:52,288 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:32:52,288 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:52,288 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:32:52,288 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:32:52,288 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vpgd1ssk
2024-01-14 06:32:52,288 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b62187f6-dea8-4e37-a992-14de773dbe8a
2024-01-14 06:32:52,291 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:52,292 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44265
2024-01-14 06:32:52,292 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44265
2024-01-14 06:32:52,292 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45257
2024-01-14 06:32:52,292 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:32:52,292 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:52,293 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:32:52,293 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:32:52,293 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sw15fnrw
2024-01-14 06:32:52,293 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eeb8c261-503c-4a95-8006-bea20eb84a1c
2024-01-14 06:32:52,301 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:52,301 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:52,309 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:52,310 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46823
2024-01-14 06:32:52,310 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46823
2024-01-14 06:32:52,311 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38339
2024-01-14 06:32:52,311 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:32:52,311 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:52,311 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:32:52,311 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:32:52,311 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tb_k44m3
2024-01-14 06:32:52,312 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08c1b36a-edc9-4a9d-9b3a-5cf620c0def1
2024-01-14 06:32:52,325 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:32:52,326 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:32:52,332 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:32:52,334 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41921
2024-01-14 06:32:52,334 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41921
2024-01-14 06:32:52,334 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34365
2024-01-14 06:32:52,334 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:32:52,334 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:52,334 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:32:52,334 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:32:52,334 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-no57_bde
2024-01-14 06:32:52,335 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a194f5f5-b701-41c9-8ab1-00a9407e3249
2024-01-14 06:32:54,431 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,443 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e24286e9-2f6a-4235-a8e2-1b2a23bb113a
2024-01-14 06:32:54,445 - distributed.worker - INFO - Starting Worker plugin PreImport-7317476f-94ac-46d9-aab5-ada39e04b05c
2024-01-14 06:32:54,446 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,510 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:54,511 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:32:54,511 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,513 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:54,513 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:32:54,515 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:32:54,516 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:32:54,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab1450a4-94c4-4299-81b2-2c335e152694
2024-01-14 06:32:54,531 - distributed.worker - INFO - Starting Worker plugin PreImport-fedbc710-39a0-4cf9-9882-eb17f78c95b3
2024-01-14 06:32:54,532 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,534 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e78fe85-cf2c-441d-b704-0252cbe9148f
2024-01-14 06:32:54,535 - distributed.worker - INFO - Starting Worker plugin PreImport-81bac9e9-c32c-43ba-8f76-fa135c20ee77
2024-01-14 06:32:54,535 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7173b3d8-52de-44a5-b8d9-1c47cb6eed74
2024-01-14 06:32:54,541 - distributed.worker - INFO - Starting Worker plugin PreImport-2614949e-1330-4d01-b3e9-ba3b0640f0cf
2024-01-14 06:32:54,542 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,546 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39431'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,547 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,548 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36455. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,551 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:32:54,553 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:54,555 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44901'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,556 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,557 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33149. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,559 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:32:54,561 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:54,564 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:54,565 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:32:54,565 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:32:54,572 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:54,573 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:32:54,573 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,575 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:32:54,579 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:54,580 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:32:54,580 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,582 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:32:54,584 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c071b48-7d8c-4bb5-aba1-2396b4457549
2024-01-14 06:32:54,584 - distributed.worker - INFO - Starting Worker plugin PreImport-85accb66-2cd3-4e21-88cf-6a134f6976f0
2024-01-14 06:32:54,585 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,591 - distributed.worker - INFO - Starting Worker plugin PreImport-c060fc9f-c0fc-411a-a0e6-1c4819d44e6f
2024-01-14 06:32:54,591 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9181a4ce-d533-4819-8cd0-39995d2fd432
2024-01-14 06:32:54,593 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,597 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35767'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,597 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,598 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39299'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,598 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,599 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44265. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,599 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38537'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,599 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,600 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43863. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,600 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44607. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,601 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:32:54,602 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:54,603 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:32:54,603 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:32:54,605 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:54,606 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:54,612 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-753294ec-f652-4ac7-a19d-751d68788ac6
2024-01-14 06:32:54,612 - distributed.worker - INFO - Starting Worker plugin PreImport-9ffdd6c1-8b58-4918-b255-89a9bc6e643d
2024-01-14 06:32:54,612 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,626 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:54,628 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:32:54,628 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:32:54,633 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:54,634 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:32:54,635 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:32:54,638 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:32:54,639 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:32:54,640 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:32:54,641 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:32:54,648 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34689'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,648 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,649 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36277'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,649 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,650 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43319. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,650 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40649'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,650 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,651 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41921. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,651 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46823. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-01-14 06:32:54,652 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:32:54,653 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:32:54,654 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:32:54,654 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:54,655 - distributed.nanny - INFO - Worker closed
2024-01-14 06:32:54,656 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:44484 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-01-14 06:32:54,991 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61341 parent=61144 started daemon>
2024-01-14 06:32:54,992 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61337 parent=61144 started daemon>
2024-01-14 06:32:54,992 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61333 parent=61144 started daemon>
2024-01-14 06:32:54,992 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61329 parent=61144 started daemon>
2024-01-14 06:32:54,992 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61324 parent=61144 started daemon>
2024-01-14 06:32:54,992 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61314 parent=61144 started daemon>
2024-01-14 06:32:54,992 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=61310 parent=61144 started daemon>
2024-01-14 06:32:55,210 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 61341 exit status was already read will report exitcode 255
2024-01-14 06:32:55,241 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 61329 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-14 06:33:29,491 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:33:29,496 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46781 instead
  warnings.warn(
2024-01-14 06:33:29,500 - distributed.scheduler - INFO - State start
2024-01-14 06:33:29,523 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:33:29,524 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-14 06:33:29,524 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46781/status
2024-01-14 06:33:29,525 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:33:29,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41005'
2024-01-14 06:33:29,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42297'
2024-01-14 06:33:29,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46191'
2024-01-14 06:33:29,756 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43281'
2024-01-14 06:33:29,759 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33203'
2024-01-14 06:33:29,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46641'
2024-01-14 06:33:29,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35327'
2024-01-14 06:33:29,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35203'
2024-01-14 06:33:31,597 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:31,597 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:31,601 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:31,602 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37131
2024-01-14 06:33:31,602 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37131
2024-01-14 06:33:31,602 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35207
2024-01-14 06:33:31,602 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:31,602 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:31,602 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:31,602 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:31,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jpysexag
2024-01-14 06:33:31,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dcb02eda-8406-4368-9b72-a1c2ad344007
2024-01-14 06:33:31,603 - distributed.worker - INFO - Starting Worker plugin PreImport-683e330c-46da-4464-8fb3-853499ccd0ce
2024-01-14 06:33:31,603 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3df642c4-bc57-4728-b385-fa6be775f64f
2024-01-14 06:33:31,620 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:31,620 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:31,624 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:31,625 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40073
2024-01-14 06:33:31,625 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40073
2024-01-14 06:33:31,625 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46043
2024-01-14 06:33:31,625 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:31,625 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:31,625 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:31,626 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:31,626 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_n30po5b
2024-01-14 06:33:31,626 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0be8bc9-f6b4-4b9d-8ada-bf7e6eb76b84
2024-01-14 06:33:31,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:31,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:31,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:31,644 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:31,644 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:31,644 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:31,648 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:31,648 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:31,648 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:31,649 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38783
2024-01-14 06:33:31,649 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33029
2024-01-14 06:33:31,649 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38783
2024-01-14 06:33:31,649 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33029
2024-01-14 06:33:31,649 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37289
2024-01-14 06:33:31,649 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34643
2024-01-14 06:33:31,649 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33993
2024-01-14 06:33:31,649 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:31,649 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:31,649 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:31,649 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33993
2024-01-14 06:33:31,649 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:31,649 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:31,649 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42675
2024-01-14 06:33:31,649 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:31,649 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:31,649 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:31,649 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:31,649 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6uzp8uz8
2024-01-14 06:33:31,649 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:31,649 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-km6wylzn
2024-01-14 06:33:31,649 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:31,649 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:31,649 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-06rn1lwa
2024-01-14 06:33:31,649 - distributed.worker - INFO - Starting Worker plugin PreImport-64a995e4-3e47-4cec-b7e0-dbc49213e93d
2024-01-14 06:33:31,649 - distributed.worker - INFO - Starting Worker plugin PreImport-1179a13e-225d-4c50-b272-caf07a7a30cb
2024-01-14 06:33:31,650 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b71aaff1-3ac7-4538-97c9-cf02224ec8d3
2024-01-14 06:33:31,650 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49a66a23-2b2b-40ec-b828-3799712a314a
2024-01-14 06:33:31,650 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c31c75dc-78aa-4034-a1cd-900a4b4fd80a
2024-01-14 06:33:31,650 - distributed.worker - INFO - Starting Worker plugin RMMSetup-539397c3-3cb1-4906-a38f-504feaec5cff
2024-01-14 06:33:31,650 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b0842b99-dc5f-4477-ae90-210ce9cf19a2
2024-01-14 06:33:31,662 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:31,662 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:31,666 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:31,667 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40837
2024-01-14 06:33:31,667 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40837
2024-01-14 06:33:31,667 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35101
2024-01-14 06:33:31,667 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:31,667 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:31,667 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:31,667 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:31,667 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6kkam4zo
2024-01-14 06:33:31,668 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0721c493-be21-47e4-a094-a31a98b9771d
2024-01-14 06:33:31,668 - distributed.worker - INFO - Starting Worker plugin PreImport-5621eccb-6533-427a-97e1-c5b3191567f2
2024-01-14 06:33:31,668 - distributed.worker - INFO - Starting Worker plugin RMMSetup-34cf11c8-293d-4e6b-9875-c0e70d4215d4
2024-01-14 06:33:31,899 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:31,899 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:31,900 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:31,901 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:31,904 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:31,905 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38077
2024-01-14 06:33:31,905 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38077
2024-01-14 06:33:31,905 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44235
2024-01-14 06:33:31,905 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:31,905 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:31,905 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:31,905 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:31,905 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w76sxoy5
2024-01-14 06:33:31,905 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6a12c3d-a1a7-4684-b3c4-a240143ee2d4
2024-01-14 06:33:31,905 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:31,906 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36709
2024-01-14 06:33:31,907 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36709
2024-01-14 06:33:31,907 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36027
2024-01-14 06:33:31,907 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:31,907 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:31,907 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:31,907 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:31,907 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-znhnerc3
2024-01-14 06:33:31,907 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d677b4eb-8e1e-430b-b561-9f85506e4dd0
2024-01-14 06:33:33,383 - distributed.scheduler - INFO - Receive client connection: Client-d37b4498-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:33,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42820
2024-01-14 06:33:33,620 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:33,646 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37131', status: init, memory: 0, processing: 0>
2024-01-14 06:33:33,648 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37131
2024-01-14 06:33:33,648 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42838
2024-01-14 06:33:33,649 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:33,650 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:33,650 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:33,651 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:33,797 - distributed.worker - INFO - Starting Worker plugin PreImport-85e5f050-ef62-49bf-a9ac-9b31d23f2edd
2024-01-14 06:33:33,797 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e9b1bcd-2186-4f07-8ef6-8c378d6ab3a4
2024-01-14 06:33:33,798 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:33,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa94dfb0-ca94-46cc-89ad-6c08f4f20b53
2024-01-14 06:33:33,799 - distributed.worker - INFO - Starting Worker plugin PreImport-14095886-2ac5-4d63-896a-8f6010d69bf9
2024-01-14 06:33:33,799 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:33,821 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33993', status: init, memory: 0, processing: 0>
2024-01-14 06:33:33,821 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33993
2024-01-14 06:33:33,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42840
2024-01-14 06:33:33,822 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:33,823 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:33,823 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:33,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:33,829 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40073', status: init, memory: 0, processing: 0>
2024-01-14 06:33:33,830 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40073
2024-01-14 06:33:33,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42854
2024-01-14 06:33:33,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:33,833 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:33,833 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:33,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:33,878 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:33,911 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33029', status: init, memory: 0, processing: 0>
2024-01-14 06:33:33,912 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33029
2024-01-14 06:33:33,912 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42868
2024-01-14 06:33:33,913 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:33,914 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:33,915 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:33,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:33,965 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:33,987 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40837', status: init, memory: 0, processing: 0>
2024-01-14 06:33:33,989 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40837
2024-01-14 06:33:33,989 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42874
2024-01-14 06:33:33,990 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:33,991 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:33,991 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:33,992 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:34,032 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:34,040 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-21e7c9c3-c65b-4430-9b4b-c2fbb6dd88c2
2024-01-14 06:33:34,041 - distributed.worker - INFO - Starting Worker plugin PreImport-1c94254a-b4be-4019-96cb-fb410b9b0c45
2024-01-14 06:33:34,042 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:34,042 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5174349b-0631-47d6-8bf8-cdf3fcd8c899
2024-01-14 06:33:34,044 - distributed.worker - INFO - Starting Worker plugin PreImport-79312d8d-422e-4b1c-91db-324444911f7d
2024-01-14 06:33:34,045 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:34,055 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38783', status: init, memory: 0, processing: 0>
2024-01-14 06:33:34,056 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38783
2024-01-14 06:33:34,056 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42878
2024-01-14 06:33:34,057 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:34,057 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:34,058 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:34,059 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:34,079 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36709', status: init, memory: 0, processing: 0>
2024-01-14 06:33:34,079 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36709
2024-01-14 06:33:34,079 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42880
2024-01-14 06:33:34,081 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:34,083 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:34,083 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:34,085 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38077', status: init, memory: 0, processing: 0>
2024-01-14 06:33:34,086 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:34,086 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38077
2024-01-14 06:33:34,086 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42882
2024-01-14 06:33:34,087 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:34,089 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:34,089 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:34,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:34,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:34,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:34,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:34,128 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:34,129 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:34,129 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:34,129 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:34,129 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:34,134 - distributed.scheduler - INFO - Remove client Client-d37b4498-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:34,135 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42820; closing.
2024-01-14 06:33:34,135 - distributed.scheduler - INFO - Remove client Client-d37b4498-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:34,135 - distributed.scheduler - INFO - Close client connection: Client-d37b4498-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:34,136 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41005'. Reason: nanny-close
2024-01-14 06:33:34,137 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:34,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42297'. Reason: nanny-close
2024-01-14 06:33:34,138 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:34,138 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46191'. Reason: nanny-close
2024-01-14 06:33:34,138 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37131. Reason: nanny-close
2024-01-14 06:33:34,139 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:34,139 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43281'. Reason: nanny-close
2024-01-14 06:33:34,139 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38783. Reason: nanny-close
2024-01-14 06:33:34,139 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:34,139 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33203'. Reason: nanny-close
2024-01-14 06:33:34,139 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40073. Reason: nanny-close
2024-01-14 06:33:34,139 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:34,140 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46641'. Reason: nanny-close
2024-01-14 06:33:34,140 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33029. Reason: nanny-close
2024-01-14 06:33:34,140 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:34,140 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:34,140 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35327'. Reason: nanny-close
2024-01-14 06:33:34,140 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33993. Reason: nanny-close
2024-01-14 06:33:34,140 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42838; closing.
2024-01-14 06:33:34,140 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:34,141 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37131', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214014.1409473')
2024-01-14 06:33:34,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35203'. Reason: nanny-close
2024-01-14 06:33:34,141 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40837. Reason: nanny-close
2024-01-14 06:33:34,141 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:34,141 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:34,141 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38077. Reason: nanny-close
2024-01-14 06:33:34,141 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:34,142 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:34,142 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36709. Reason: nanny-close
2024-01-14 06:33:34,142 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:34,143 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42878; closing.
2024-01-14 06:33:34,143 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:34,143 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:34,143 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:34,144 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42854; closing.
2024-01-14 06:33:34,144 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:34,144 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38783', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214014.144247')
2024-01-14 06:33:34,144 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:34,144 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:34,144 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42840; closing.
2024-01-14 06:33:34,144 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42868; closing.
2024-01-14 06:33:34,145 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40073', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214014.1453598')
2024-01-14 06:33:34,145 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:34,145 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214014.1457672')
2024-01-14 06:33:34,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:34,146 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33029', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214014.146033')
2024-01-14 06:33:34,146 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42874; closing.
2024-01-14 06:33:34,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:34,147 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40837', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214014.1470191')
2024-01-14 06:33:34,147 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42882; closing.
2024-01-14 06:33:34,148 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214014.1479714')
2024-01-14 06:33:34,148 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42880; closing.
2024-01-14 06:33:34,148 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:34,148 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36709', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214014.1487336')
2024-01-14 06:33:34,148 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:34,148 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:33:34,149 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42880>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-14 06:33:35,102 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:33:35,103 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:33:35,104 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:33:35,105 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-14 06:33:35,106 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-14 06:33:37,457 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:33:37,461 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45421 instead
  warnings.warn(
2024-01-14 06:33:37,465 - distributed.scheduler - INFO - State start
2024-01-14 06:33:37,488 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:33:37,489 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-14 06:33:37,490 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45421/status
2024-01-14 06:33:37,490 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:33:37,925 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33287'
2024-01-14 06:33:37,938 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35963'
2024-01-14 06:33:37,953 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35721'
2024-01-14 06:33:37,964 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41921'
2024-01-14 06:33:37,967 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41715'
2024-01-14 06:33:37,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33089'
2024-01-14 06:33:37,984 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46597'
2024-01-14 06:33:37,994 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40691'
2024-01-14 06:33:39,016 - distributed.scheduler - INFO - Receive client connection: Client-d832973e-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:39,035 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43066
2024-01-14 06:33:39,897 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:39,897 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:39,897 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:39,897 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:39,901 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:39,902 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:39,902 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40521
2024-01-14 06:33:39,902 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40521
2024-01-14 06:33:39,902 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45837
2024-01-14 06:33:39,902 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:39,902 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45385
2024-01-14 06:33:39,902 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:39,902 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:39,902 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45385
2024-01-14 06:33:39,903 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:39,903 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43201
2024-01-14 06:33:39,903 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iz8zegtz
2024-01-14 06:33:39,903 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:39,903 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:39,903 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:39,903 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:39,903 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cplfpbx7
2024-01-14 06:33:39,903 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32db7e3c-dd14-4840-8f80-96fea7cb5a98
2024-01-14 06:33:39,903 - distributed.worker - INFO - Starting Worker plugin PreImport-3eafb808-c556-47b5-97de-16c30ad9fa51
2024-01-14 06:33:39,903 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c8cd5ae-c1a2-4496-847d-33b6650d33c9
2024-01-14 06:33:39,903 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9d56c286-db54-4771-9af5-93a0e3a98ffb
2024-01-14 06:33:39,913 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:39,913 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:39,914 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:39,915 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:39,915 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:39,915 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:39,917 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:39,918 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41141
2024-01-14 06:33:39,918 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41141
2024-01-14 06:33:39,918 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40189
2024-01-14 06:33:39,918 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:39,918 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:39,918 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:39,918 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:39,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wu4jav1f
2024-01-14 06:33:39,919 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d6872aaa-5acb-4c9c-96e5-617b3b408d95
2024-01-14 06:33:39,919 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:39,919 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:39,920 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36745
2024-01-14 06:33:39,920 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36745
2024-01-14 06:33:39,920 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35833
2024-01-14 06:33:39,920 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:39,920 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:39,920 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:39,920 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:39,920 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5v66pw3r
2024-01-14 06:33:39,920 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e54d3caa-19e4-4d80-b4fd-418195a05e7c
2024-01-14 06:33:39,920 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37625
2024-01-14 06:33:39,920 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37625
2024-01-14 06:33:39,920 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43517
2024-01-14 06:33:39,921 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:39,921 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:39,921 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:39,921 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:39,921 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_j86bvji
2024-01-14 06:33:39,921 - distributed.worker - INFO - Starting Worker plugin RMMSetup-985f4a7d-080a-46c1-b4a6-35b9d24308ae
2024-01-14 06:33:39,928 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:39,928 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:39,933 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:39,934 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39531
2024-01-14 06:33:39,934 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39531
2024-01-14 06:33:39,934 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33873
2024-01-14 06:33:39,934 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:39,934 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:39,935 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:39,935 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:39,935 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3gryze95
2024-01-14 06:33:39,935 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5b67f810-4054-4d14-8130-1a7b80923716
2024-01-14 06:33:40,027 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:40,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:40,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:40,033 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46347
2024-01-14 06:33:40,033 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46347
2024-01-14 06:33:40,033 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38649
2024-01-14 06:33:40,033 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:40,033 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:40,033 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:40,033 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:40,033 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0hp4kv2p
2024-01-14 06:33:40,034 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8a40e43-1a5e-4424-a3aa-11de71458475
2024-01-14 06:33:40,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:40,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:40,062 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:40,063 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45875
2024-01-14 06:33:40,063 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45875
2024-01-14 06:33:40,063 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33731
2024-01-14 06:33:40,063 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:40,063 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:40,063 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:40,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:40,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qpzheo8t
2024-01-14 06:33:40,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-016df0a2-6b32-4f99-88ec-34fbe20e6db3
2024-01-14 06:33:42,335 - distributed.worker - INFO - Starting Worker plugin PreImport-2906766b-119f-4aef-9105-dbf56f07e220
2024-01-14 06:33:42,335 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8114465b-ea6a-4fbb-bbb3-a34d9d527778
2024-01-14 06:33:42,336 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,359 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40521', status: init, memory: 0, processing: 0>
2024-01-14 06:33:42,362 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40521
2024-01-14 06:33:42,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53568
2024-01-14 06:33:42,363 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:42,363 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:42,364 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:42,426 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d799167-80c2-4b43-b60b-aa3307e64c7c
2024-01-14 06:33:42,426 - distributed.worker - INFO - Starting Worker plugin PreImport-0b851768-a063-488b-90d8-18c1b156da81
2024-01-14 06:33:42,427 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,437 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5b9c8af-9440-405b-a924-53a486351f43
2024-01-14 06:33:42,438 - distributed.worker - INFO - Starting Worker plugin PreImport-a14daa40-0a19-4dc8-98bd-34a20dbe8f40
2024-01-14 06:33:42,438 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,444 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,456 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3beb920b-7110-40be-909a-cc4a55ed3f04
2024-01-14 06:33:42,457 - distributed.worker - INFO - Starting Worker plugin PreImport-3466e62e-b790-4630-9d89-de0ff5275aea
2024-01-14 06:33:42,457 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,461 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36745', status: init, memory: 0, processing: 0>
2024-01-14 06:33:42,462 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36745
2024-01-14 06:33:42,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53582
2024-01-14 06:33:42,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:42,463 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:42,464 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,464 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37625', status: init, memory: 0, processing: 0>
2024-01-14 06:33:42,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:42,465 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37625
2024-01-14 06:33:42,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53570
2024-01-14 06:33:42,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:42,468 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:42,468 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,470 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:42,474 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45385', status: init, memory: 0, processing: 0>
2024-01-14 06:33:42,475 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45385
2024-01-14 06:33:42,475 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53598
2024-01-14 06:33:42,476 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:42,477 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:42,477 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:42,481 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41141', status: init, memory: 0, processing: 0>
2024-01-14 06:33:42,482 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41141
2024-01-14 06:33:42,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53612
2024-01-14 06:33:42,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:42,483 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:42,483 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:42,510 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9581242-2f13-4df9-b65d-f418a2c8695d
2024-01-14 06:33:42,512 - distributed.worker - INFO - Starting Worker plugin PreImport-58e3ef5d-f94c-4d7f-96ec-025ebea0404f
2024-01-14 06:33:42,513 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,539 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39531', status: init, memory: 0, processing: 0>
2024-01-14 06:33:42,539 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a175d7cd-3588-4166-8b60-742a411f2dc4
2024-01-14 06:33:42,539 - distributed.worker - INFO - Starting Worker plugin PreImport-323691df-bdcf-4a6c-a3f6-fb2682668f12
2024-01-14 06:33:42,539 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39531
2024-01-14 06:33:42,540 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53616
2024-01-14 06:33:42,540 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,541 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:42,542 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:42,542 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,543 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:42,557 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f74b1b02-b0d2-474c-97cf-6743521c05cd
2024-01-14 06:33:42,558 - distributed.worker - INFO - Starting Worker plugin PreImport-22491487-212c-466b-8b4d-9ba8c4e4e8c3
2024-01-14 06:33:42,559 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,574 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46347', status: init, memory: 0, processing: 0>
2024-01-14 06:33:42,575 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46347
2024-01-14 06:33:42,575 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53624
2024-01-14 06:33:42,576 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:42,578 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:42,578 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,580 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:42,589 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45875', status: init, memory: 0, processing: 0>
2024-01-14 06:33:42,589 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45875
2024-01-14 06:33:42,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53638
2024-01-14 06:33:42,591 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:42,592 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:42,592 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:42,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:42,611 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:42,611 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:42,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:42,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:42,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:42,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:42,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:42,612 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:33:42,624 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:42,624 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:42,624 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:42,625 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:42,625 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:42,625 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:42,625 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:42,625 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:42,634 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:42,636 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:42,639 - distributed.scheduler - INFO - Remove client Client-d832973e-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:42,639 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43066; closing.
2024-01-14 06:33:42,639 - distributed.scheduler - INFO - Remove client Client-d832973e-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:42,640 - distributed.scheduler - INFO - Close client connection: Client-d832973e-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:42,640 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33287'. Reason: nanny-close
2024-01-14 06:33:42,641 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:42,641 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35963'. Reason: nanny-close
2024-01-14 06:33:42,642 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:42,642 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35721'. Reason: nanny-close
2024-01-14 06:33:42,642 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39531. Reason: nanny-close
2024-01-14 06:33:42,642 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:42,642 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41921'. Reason: nanny-close
2024-01-14 06:33:42,642 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41141. Reason: nanny-close
2024-01-14 06:33:42,643 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:42,643 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41715'. Reason: nanny-close
2024-01-14 06:33:42,643 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45875. Reason: nanny-close
2024-01-14 06:33:42,643 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:42,643 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33089'. Reason: nanny-close
2024-01-14 06:33:42,644 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37625. Reason: nanny-close
2024-01-14 06:33:42,644 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:42,644 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36745. Reason: nanny-close
2024-01-14 06:33:42,644 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46597'. Reason: nanny-close
2024-01-14 06:33:42,644 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:42,644 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53612; closing.
2024-01-14 06:33:42,644 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:42,644 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40691'. Reason: nanny-close
2024-01-14 06:33:42,644 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40521. Reason: nanny-close
2024-01-14 06:33:42,644 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41141', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214022.6449075')
2024-01-14 06:33:42,645 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:42,645 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:42,645 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46347. Reason: nanny-close
2024-01-14 06:33:42,645 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:42,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:42,646 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:42,646 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45385. Reason: nanny-close
2024-01-14 06:33:42,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:42,646 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:42,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:42,647 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53638; closing.
2024-01-14 06:33:42,647 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53616; closing.
2024-01-14 06:33:42,647 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:42,647 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:42,647 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:42,648 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:42,648 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:42,648 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214022.6482165')
2024-01-14 06:33:42,648 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39531', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214022.6486385')
2024-01-14 06:33:42,648 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:42,649 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53570; closing.
2024-01-14 06:33:42,649 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53582; closing.
2024-01-14 06:33:42,649 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214022.6498346')
2024-01-14 06:33:42,649 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:42,650 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36745', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214022.6502297')
2024-01-14 06:33:42,650 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:42,650 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53568; closing.
2024-01-14 06:33:42,651 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40521', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214022.6513393')
2024-01-14 06:33:42,651 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53624; closing.
2024-01-14 06:33:42,651 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53598; closing.
2024-01-14 06:33:42,652 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46347', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214022.652415')
2024-01-14 06:33:42,652 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45385', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214022.6528726')
2024-01-14 06:33:42,653 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:33:44,708 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:33:44,709 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:33:44,710 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:33:44,711 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-14 06:33:44,712 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-14 06:33:47,129 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:33:47,134 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-14 06:33:47,137 - distributed.scheduler - INFO - State start
2024-01-14 06:33:47,159 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:33:47,160 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-14 06:33:47,161 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-14 06:33:47,161 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:33:47,239 - distributed.scheduler - INFO - Receive client connection: Client-dddfa729-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:47,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53742
2024-01-14 06:33:47,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46385'
2024-01-14 06:33:47,418 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37991'
2024-01-14 06:33:47,423 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36635'
2024-01-14 06:33:47,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32873'
2024-01-14 06:33:47,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39445'
2024-01-14 06:33:47,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33893'
2024-01-14 06:33:47,463 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46361'
2024-01-14 06:33:47,475 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42601'
2024-01-14 06:33:49,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:49,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:49,315 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:49,316 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34669
2024-01-14 06:33:49,316 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34669
2024-01-14 06:33:49,316 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45457
2024-01-14 06:33:49,316 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:49,316 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:49,316 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:49,316 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:49,316 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9oyfvzho
2024-01-14 06:33:49,316 - distributed.worker - INFO - Starting Worker plugin PreImport-ad861086-eb6e-4594-839f-6c36b83a534e
2024-01-14 06:33:49,316 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91896f5b-fc69-4230-8c52-89153bd04629
2024-01-14 06:33:49,316 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d508fcf1-cbe8-4564-a023-a17cf7848bbe
2024-01-14 06:33:49,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:49,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:49,560 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:49,561 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35903
2024-01-14 06:33:49,561 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35903
2024-01-14 06:33:49,561 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44249
2024-01-14 06:33:49,561 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:49,561 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:49,561 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:49,562 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:49,562 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1pa20c4y
2024-01-14 06:33:49,562 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:49,562 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eda28515-e318-4502-a642-52d29d30786b
2024-01-14 06:33:49,562 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:49,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:49,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:49,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:49,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:49,566 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:49,567 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37807
2024-01-14 06:33:49,567 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37807
2024-01-14 06:33:49,567 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45437
2024-01-14 06:33:49,567 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:49,567 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:49,567 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:49,567 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:49,567 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d22palaa
2024-01-14 06:33:49,567 - distributed.worker - INFO - Starting Worker plugin RMMSetup-539b6707-ee98-4c28-9f99-337e7a8d982f
2024-01-14 06:33:49,569 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:49,569 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:49,569 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:49,569 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:49,570 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45477
2024-01-14 06:33:49,570 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44163
2024-01-14 06:33:49,570 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45477
2024-01-14 06:33:49,570 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:49,570 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44163
2024-01-14 06:33:49,570 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39293
2024-01-14 06:33:49,570 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41549
2024-01-14 06:33:49,570 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:49,570 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:49,570 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:49,570 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:49,570 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:49,570 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:49,570 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:49,570 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:49,570 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mhygcwqt
2024-01-14 06:33:49,570 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:49,570 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gdmwqmfs
2024-01-14 06:33:49,570 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d7c30234-ea83-4e53-bf74-7cb7f45721d9
2024-01-14 06:33:49,570 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c617c27b-6f8e-494e-8af8-af1e0b8d3af7
2024-01-14 06:33:49,571 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:49,572 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:49,573 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:49,574 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42905
2024-01-14 06:33:49,574 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42905
2024-01-14 06:33:49,574 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35853
2024-01-14 06:33:49,574 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:49,574 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:49,574 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:49,574 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:49,575 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0dfu2xz2
2024-01-14 06:33:49,575 - distributed.worker - INFO - Starting Worker plugin PreImport-db7f43e8-092d-42e7-9d44-468a11b2d25b
2024-01-14 06:33:49,575 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:49,575 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f41bb4a-2ec8-4174-9a66-07341c1bc999
2024-01-14 06:33:49,575 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3aa78b7-4558-411e-9aa3-35ac2208b841
2024-01-14 06:33:49,576 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32909
2024-01-14 06:33:49,576 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32909
2024-01-14 06:33:49,576 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40931
2024-01-14 06:33:49,576 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:49,576 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:49,576 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:49,576 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:49,576 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uzqhelfd
2024-01-14 06:33:49,576 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:49,576 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e28ecdc1-a132-4bca-8495-a67855cb381c
2024-01-14 06:33:49,577 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44729
2024-01-14 06:33:49,577 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44729
2024-01-14 06:33:49,577 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44185
2024-01-14 06:33:49,577 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:49,578 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:49,578 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:49,578 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:49,578 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aidj_xla
2024-01-14 06:33:49,578 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc109e68-7bb4-4f66-97be-a666b2545668
2024-01-14 06:33:49,809 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:49,833 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34669', status: init, memory: 0, processing: 0>
2024-01-14 06:33:49,834 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34669
2024-01-14 06:33:49,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53838
2024-01-14 06:33:49,835 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:49,836 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:49,836 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:49,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:51,557 - distributed.worker - INFO - Starting Worker plugin PreImport-862f8ba7-b5c9-4d31-9f9d-48414e696649
2024-01-14 06:33:51,557 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0790b524-c8b5-49e3-b4d6-34aab95cd288
2024-01-14 06:33:51,558 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,584 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35903', status: init, memory: 0, processing: 0>
2024-01-14 06:33:51,584 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35903
2024-01-14 06:33:51,585 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33636
2024-01-14 06:33:51,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:51,586 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:51,587 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:51,596 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-714a8476-0e9e-4de9-ae81-fc327ac9c5b6
2024-01-14 06:33:51,597 - distributed.worker - INFO - Starting Worker plugin PreImport-cd2edd91-ec0b-4c6f-b2fd-7438f88d7760
2024-01-14 06:33:51,597 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,610 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b3f5511-cbd9-4ebc-9121-234e4d90b84f
2024-01-14 06:33:51,610 - distributed.worker - INFO - Starting Worker plugin PreImport-4d34a693-ad6e-474e-bc79-3cb7abe852dd
2024-01-14 06:33:51,611 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,620 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45477', status: init, memory: 0, processing: 0>
2024-01-14 06:33:51,620 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45477
2024-01-14 06:33:51,621 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33640
2024-01-14 06:33:51,621 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,621 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:51,622 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:51,622 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:51,628 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-416544c3-c501-49dd-ac9f-4fa056f268f7
2024-01-14 06:33:51,629 - distributed.worker - INFO - Starting Worker plugin PreImport-ffd324bf-ab19-4b41-81b6-f57110c2444c
2024-01-14 06:33:51,629 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,631 - distributed.worker - INFO - Starting Worker plugin PreImport-38b9b99c-29e4-4bc1-8a43-f06b9be22e09
2024-01-14 06:33:51,631 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8359ccd9-9def-4a05-af9f-91253a47232a
2024-01-14 06:33:51,632 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,633 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ed41d59-d5c3-49b2-a492-cd78c7c89c29
2024-01-14 06:33:51,634 - distributed.worker - INFO - Starting Worker plugin PreImport-b4a5f242-7823-4ed6-a6d0-13643e3fa19b
2024-01-14 06:33:51,634 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,644 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42905', status: init, memory: 0, processing: 0>
2024-01-14 06:33:51,645 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42905
2024-01-14 06:33:51,645 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33658
2024-01-14 06:33:51,645 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44729', status: init, memory: 0, processing: 0>
2024-01-14 06:33:51,646 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:51,646 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44729
2024-01-14 06:33:51,646 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33648
2024-01-14 06:33:51,646 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:51,647 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,647 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:51,648 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:51,649 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:51,649 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,651 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:51,656 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32909', status: init, memory: 0, processing: 0>
2024-01-14 06:33:51,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32909
2024-01-14 06:33:51,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33684
2024-01-14 06:33:51,658 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:51,659 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:51,659 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:51,813 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44163', status: init, memory: 0, processing: 0>
2024-01-14 06:33:51,814 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44163
2024-01-14 06:33:51,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33672
2024-01-14 06:33:51,815 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37807', status: init, memory: 0, processing: 0>
2024-01-14 06:33:51,815 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37807
2024-01-14 06:33:51,815 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33700
2024-01-14 06:33:51,816 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:51,817 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:33:51,817 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:51,817 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,818 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:33:51,818 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:51,820 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:51,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:33:51,927 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:51,927 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:51,928 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:51,928 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:51,928 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:51,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:51,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:51,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:51,941 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:51,941 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:51,941 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:51,941 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:51,941 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:51,942 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:51,942 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:51,942 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:33:51,950 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:51,952 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:33:51,954 - distributed.scheduler - INFO - Remove client Client-dddfa729-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:51,954 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53742; closing.
2024-01-14 06:33:51,954 - distributed.scheduler - INFO - Remove client Client-dddfa729-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:51,955 - distributed.scheduler - INFO - Close client connection: Client-dddfa729-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:51,956 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46385'. Reason: nanny-close
2024-01-14 06:33:51,956 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:51,956 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37991'. Reason: nanny-close
2024-01-14 06:33:51,957 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:51,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36635'. Reason: nanny-close
2024-01-14 06:33:51,957 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35903. Reason: nanny-close
2024-01-14 06:33:51,958 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:51,958 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32873'. Reason: nanny-close
2024-01-14 06:33:51,958 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42905. Reason: nanny-close
2024-01-14 06:33:51,958 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:51,958 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39445'. Reason: nanny-close
2024-01-14 06:33:51,959 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:51,959 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37807. Reason: nanny-close
2024-01-14 06:33:51,959 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33893'. Reason: nanny-close
2024-01-14 06:33:51,959 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34669. Reason: nanny-close
2024-01-14 06:33:51,959 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:51,959 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46361'. Reason: nanny-close
2024-01-14 06:33:51,959 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32909. Reason: nanny-close
2024-01-14 06:33:51,959 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:51,960 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42601'. Reason: nanny-close
2024-01-14 06:33:51,960 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33636; closing.
2024-01-14 06:33:51,960 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:51,960 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:51,960 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45477. Reason: nanny-close
2024-01-14 06:33:51,960 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:33:51,960 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35903', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214031.9604576')
2024-01-14 06:33:51,960 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44163. Reason: nanny-close
2024-01-14 06:33:51,961 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33658; closing.
2024-01-14 06:33:51,961 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44729. Reason: nanny-close
2024-01-14 06:33:51,961 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:51,961 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:51,961 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:51,962 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:51,962 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:51,962 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42905', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214031.9621992')
2024-01-14 06:33:51,962 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:51,963 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:51,963 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:51,963 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:51,963 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:51,963 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:33:51,964 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53838; closing.
2024-01-14 06:33:51,965 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:51,965 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:51,965 - distributed.nanny - INFO - Worker closed
2024-01-14 06:33:51,965 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33658>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-14 06:33:51,968 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34669', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214031.9683325')
2024-01-14 06:33:51,968 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33684; closing.
2024-01-14 06:33:51,969 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33700; closing.
2024-01-14 06:33:51,969 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33640; closing.
2024-01-14 06:33:51,970 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32909', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214031.9705155')
2024-01-14 06:33:51,971 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214031.9710534')
2024-01-14 06:33:51,971 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214031.9716222')
2024-01-14 06:33:51,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33672; closing.
2024-01-14 06:33:51,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33648; closing.
2024-01-14 06:33:51,973 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44163', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214031.9731023')
2024-01-14 06:33:51,973 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214031.9736407')
2024-01-14 06:33:51,973 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:33:53,273 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:33:53,273 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:33:53,273 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:33:53,275 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-14 06:33:53,275 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-14 06:33:55,722 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:33:55,734 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-14 06:33:55,742 - distributed.scheduler - INFO - State start
2024-01-14 06:33:55,944 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:33:55,945 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-14 06:33:55,946 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-14 06:33:55,946 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:33:56,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35711'
2024-01-14 06:33:56,950 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46355'
2024-01-14 06:33:56,965 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43535'
2024-01-14 06:33:56,977 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38731'
2024-01-14 06:33:56,980 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39033'
2024-01-14 06:33:56,990 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38009'
2024-01-14 06:33:56,999 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43575'
2024-01-14 06:33:57,011 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34033'
2024-01-14 06:33:58,114 - distributed.scheduler - INFO - Receive client connection: Client-e2f7effa-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:33:58,128 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33870
2024-01-14 06:33:58,838 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:58,838 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:58,843 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:58,843 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41349
2024-01-14 06:33:58,843 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41349
2024-01-14 06:33:58,844 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41223
2024-01-14 06:33:58,844 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:58,844 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:58,844 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:58,844 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:58,844 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_h7_yj1w
2024-01-14 06:33:58,844 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9b465340-4665-48cd-9ae1-2bc66d37eddc
2024-01-14 06:33:58,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:58,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:58,873 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:58,874 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40143
2024-01-14 06:33:58,874 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40143
2024-01-14 06:33:58,874 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36987
2024-01-14 06:33:58,874 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:58,874 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:58,874 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:58,874 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:58,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ojmr4tqo
2024-01-14 06:33:58,874 - distributed.worker - INFO - Starting Worker plugin PreImport-10bc43b7-fdf9-47a6-af57-82cecdf8b9c0
2024-01-14 06:33:58,875 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a08f3fc2-831d-423d-bd53-98ac1735f6ad
2024-01-14 06:33:58,943 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:58,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:58,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:58,949 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37087
2024-01-14 06:33:58,949 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37087
2024-01-14 06:33:58,949 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33869
2024-01-14 06:33:58,949 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:58,949 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:58,949 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:58,949 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:58,949 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jadl4r3w
2024-01-14 06:33:58,949 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57faa38b-3323-48a9-97b2-7f1efce63944
2024-01-14 06:33:59,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:59,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:59,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:59,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:59,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:59,187 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:59,188 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:59,188 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:59,188 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:59,189 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44189
2024-01-14 06:33:59,189 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44189
2024-01-14 06:33:59,189 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34261
2024-01-14 06:33:59,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:59,189 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:59,190 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:59,190 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:59,190 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2eecohnf
2024-01-14 06:33:59,190 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:59,190 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4e3be48c-4f1a-467b-aed3-0c7b38b6a619
2024-01-14 06:33:59,191 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36943
2024-01-14 06:33:59,191 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36943
2024-01-14 06:33:59,191 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46271
2024-01-14 06:33:59,191 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:59,191 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:59,191 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:59,191 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:59,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_ls7n8l9
2024-01-14 06:33:59,191 - distributed.worker - INFO - Starting Worker plugin PreImport-b849bf50-2d47-4874-b32e-a28f73bc2bc6
2024-01-14 06:33:59,191 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8f97ba21-b3f3-4a21-9fe2-ee1a26418acd
2024-01-14 06:33:59,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-02602789-c3ea-4113-8b03-7f9cb195d212
2024-01-14 06:33:59,192 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:59,193 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41017
2024-01-14 06:33:59,193 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41017
2024-01-14 06:33:59,193 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43311
2024-01-14 06:33:59,193 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:59,193 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:59,193 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:59,193 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:59,193 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dl3djjox
2024-01-14 06:33:59,193 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2394cd02-18fa-4044-85a6-20022aa77c45
2024-01-14 06:33:59,193 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:59,194 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35211
2024-01-14 06:33:59,195 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35211
2024-01-14 06:33:59,195 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40437
2024-01-14 06:33:59,195 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:59,195 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:59,195 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:59,195 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:59,195 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hgno6add
2024-01-14 06:33:59,195 - distributed.worker - INFO - Starting Worker plugin RMMSetup-decf2970-bc15-411f-8b14-03629fbaa6ed
2024-01-14 06:33:59,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:33:59,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:33:59,202 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:33:59,203 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38679
2024-01-14 06:33:59,203 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38679
2024-01-14 06:33:59,203 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37293
2024-01-14 06:33:59,203 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:33:59,204 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:33:59,204 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:33:59,204 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:33:59,204 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-auikrqaa
2024-01-14 06:33:59,204 - distributed.worker - INFO - Starting Worker plugin RMMSetup-062e2278-2680-4b99-ac57-a3ab55ddff01
2024-01-14 06:34:01,125 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d7ae4b2-e8fd-4821-b47f-a65d08fb30be
2024-01-14 06:34:01,125 - distributed.worker - INFO - Starting Worker plugin PreImport-b573b7f8-2f81-4ec3-b44d-b15c91075c67
2024-01-14 06:34:01,126 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,149 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41349', status: init, memory: 0, processing: 0>
2024-01-14 06:34:01,150 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41349
2024-01-14 06:34:01,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58178
2024-01-14 06:34:01,151 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:01,152 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:01,152 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,154 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:01,180 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8bc8ae08-0f7e-4c84-ad49-7e70d543f431
2024-01-14 06:34:01,181 - distributed.worker - INFO - Starting Worker plugin PreImport-ba56e23e-c136-4cd8-aa51-dc4d6c547e92
2024-01-14 06:34:01,181 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,197 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-823ccae0-5fb8-4778-b6aa-41f525547754
2024-01-14 06:34:01,198 - distributed.worker - INFO - Starting Worker plugin PreImport-4926f622-f61d-427a-811f-361dfa3d267a
2024-01-14 06:34:01,199 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,202 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,205 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37087', status: init, memory: 0, processing: 0>
2024-01-14 06:34:01,205 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37087
2024-01-14 06:34:01,205 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58192
2024-01-14 06:34:01,206 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-362920d1-f2aa-4bc1-9411-bdc754cd93a1
2024-01-14 06:34:01,206 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,206 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:01,207 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:01,207 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,209 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:01,220 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c570a54-257d-46ad-a353-a0518da4023a
2024-01-14 06:34:01,221 - distributed.worker - INFO - Starting Worker plugin PreImport-c64adfc8-0be2-46c5-9b4c-2e0abb2b8219
2024-01-14 06:34:01,222 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,226 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e8c52be-9dee-49da-a648-1c5e573a274d
2024-01-14 06:34:01,227 - distributed.worker - INFO - Starting Worker plugin PreImport-aa7edcdd-74b4-4ee5-a92d-41656a05b534
2024-01-14 06:34:01,228 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,231 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40143', status: init, memory: 0, processing: 0>
2024-01-14 06:34:01,232 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40143
2024-01-14 06:34:01,232 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58224
2024-01-14 06:34:01,233 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:01,233 - distributed.worker - INFO - Starting Worker plugin PreImport-e7e5ea4d-4146-45c1-8f81-f1869394c2a5
2024-01-14 06:34:01,234 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d137b27-8021-4985-93c8-9600f8b48823
2024-01-14 06:34:01,234 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,234 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:01,234 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,234 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41017', status: init, memory: 0, processing: 0>
2024-01-14 06:34:01,235 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41017
2024-01-14 06:34:01,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58200
2024-01-14 06:34:01,236 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:01,236 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:01,237 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36943', status: init, memory: 0, processing: 0>
2024-01-14 06:34:01,237 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36943
2024-01-14 06:34:01,237 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58212
2024-01-14 06:34:01,238 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:01,238 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,239 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:01,240 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:01,240 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:01,240 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,242 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:01,255 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35211', status: init, memory: 0, processing: 0>
2024-01-14 06:34:01,256 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35211
2024-01-14 06:34:01,256 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58240
2024-01-14 06:34:01,257 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38679', status: init, memory: 0, processing: 0>
2024-01-14 06:34:01,258 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:01,258 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38679
2024-01-14 06:34:01,258 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58256
2024-01-14 06:34:01,259 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:01,259 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:01,259 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,259 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:01,260 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,260 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44189', status: init, memory: 0, processing: 0>
2024-01-14 06:34:01,261 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44189
2024-01-14 06:34:01,261 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:01,261 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58246
2024-01-14 06:34:01,261 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:01,262 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:01,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:01,264 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:01,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:01,334 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:01,334 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:01,334 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:01,334 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:01,335 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:01,335 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:01,335 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:01,335 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:01,339 - distributed.scheduler - INFO - Remove client Client-e2f7effa-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:01,340 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33870; closing.
2024-01-14 06:34:01,340 - distributed.scheduler - INFO - Remove client Client-e2f7effa-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:01,340 - distributed.scheduler - INFO - Close client connection: Client-e2f7effa-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:01,341 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35711'. Reason: nanny-close
2024-01-14 06:34:01,341 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:01,342 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46355'. Reason: nanny-close
2024-01-14 06:34:01,342 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:01,342 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40143. Reason: nanny-close
2024-01-14 06:34:01,342 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43535'. Reason: nanny-close
2024-01-14 06:34:01,343 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:01,343 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38731'. Reason: nanny-close
2024-01-14 06:34:01,343 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41349. Reason: nanny-close
2024-01-14 06:34:01,343 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:01,343 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39033'. Reason: nanny-close
2024-01-14 06:34:01,344 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35211. Reason: nanny-close
2024-01-14 06:34:01,344 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:01,344 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38009'. Reason: nanny-close
2024-01-14 06:34:01,344 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:01,344 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58224; closing.
2024-01-14 06:34:01,344 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44189. Reason: nanny-close
2024-01-14 06:34:01,344 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:01,345 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43575'. Reason: nanny-close
2024-01-14 06:34:01,345 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37087. Reason: nanny-close
2024-01-14 06:34:01,345 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40143', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214041.3451931')
2024-01-14 06:34:01,345 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:01,345 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34033'. Reason: nanny-close
2024-01-14 06:34:01,345 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:01,345 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38679. Reason: nanny-close
2024-01-14 06:34:01,345 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:01,346 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41017. Reason: nanny-close
2024-01-14 06:34:01,346 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:01,346 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36943. Reason: nanny-close
2024-01-14 06:34:01,346 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58178; closing.
2024-01-14 06:34:01,347 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:01,347 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:01,347 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214041.3476193')
2024-01-14 06:34:01,347 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:01,347 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:01,347 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:01,348 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58240; closing.
2024-01-14 06:34:01,348 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:01,348 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35211', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214041.3485906')
2024-01-14 06:34:01,348 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58192; closing.
2024-01-14 06:34:01,349 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:01,349 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:01,349 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:01,349 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37087', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214041.3498049')
2024-01-14 06:34:01,350 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:01,350 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58246; closing.
2024-01-14 06:34:01,350 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58256; closing.
2024-01-14 06:34:01,350 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:01,351 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:01,351 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44189', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214041.3510137')
2024-01-14 06:34:01,351 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38679', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214041.351414')
2024-01-14 06:34:01,351 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:01,351 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58200; closing.
2024-01-14 06:34:01,352 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58212; closing.
2024-01-14 06:34:01,352 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41017', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214041.352427')
2024-01-14 06:34:01,352 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214041.3528614')
2024-01-14 06:34:01,353 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:34:02,307 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:34:02,308 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:34:02,308 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:34:02,309 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-14 06:34:02,310 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-14 06:34:04,591 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:04,596 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45677 instead
  warnings.warn(
2024-01-14 06:34:04,600 - distributed.scheduler - INFO - State start
2024-01-14 06:34:04,623 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:04,624 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-14 06:34:04,625 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45677/status
2024-01-14 06:34:04,625 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:34:04,687 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37283'
2024-01-14 06:34:06,483 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:06,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:06,729 - distributed.scheduler - INFO - Receive client connection: Client-e85830a1-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:06,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58328
2024-01-14 06:34:07,048 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:07,049 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41321
2024-01-14 06:34:07,049 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41321
2024-01-14 06:34:07,050 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-14 06:34:07,050 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:07,050 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:07,050 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:07,050 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-14 06:34:07,050 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ug85jvs_
2024-01-14 06:34:07,050 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f878e24-4d74-481e-9c3a-76e0bd8b23ca
2024-01-14 06:34:07,050 - distributed.worker - INFO - Starting Worker plugin PreImport-54cfeae7-062c-4d5a-97f1-133f3ce4a2f4
2024-01-14 06:34:07,050 - distributed.worker - INFO - Starting Worker plugin RMMSetup-83b671eb-a204-40fe-8b69-650d8a89eef5
2024-01-14 06:34:07,050 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:07,104 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41321', status: init, memory: 0, processing: 0>
2024-01-14 06:34:07,105 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41321
2024-01-14 06:34:07,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58338
2024-01-14 06:34:07,106 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:07,107 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:07,107 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:07,108 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:07,164 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:07,167 - distributed.scheduler - INFO - Remove client Client-e85830a1-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:07,167 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58328; closing.
2024-01-14 06:34:07,167 - distributed.scheduler - INFO - Remove client Client-e85830a1-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:07,168 - distributed.scheduler - INFO - Close client connection: Client-e85830a1-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:07,169 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37283'. Reason: nanny-close
2024-01-14 06:34:07,169 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:07,170 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41321. Reason: nanny-close
2024-01-14 06:34:07,172 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:07,172 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58338; closing.
2024-01-14 06:34:07,173 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41321', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214047.1729722')
2024-01-14 06:34:07,173 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:34:07,174 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:07,684 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:34:07,684 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:34:07,685 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:34:07,686 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-14 06:34:07,687 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-14 06:34:11,845 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:11,850 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42413 instead
  warnings.warn(
2024-01-14 06:34:11,854 - distributed.scheduler - INFO - State start
2024-01-14 06:34:11,911 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:11,912 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-14 06:34:11,914 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42413/status
2024-01-14 06:34:11,914 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:34:11,984 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36919'
2024-01-14 06:34:12,202 - distributed.scheduler - INFO - Receive client connection: Client-ecac776b-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:12,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52528
2024-01-14 06:34:13,989 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:13,989 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:14,584 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:14,585 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33337
2024-01-14 06:34:14,585 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33337
2024-01-14 06:34:14,585 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39267
2024-01-14 06:34:14,585 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:14,585 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:14,585 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:14,585 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-14 06:34:14,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-agj2w4nb
2024-01-14 06:34:14,585 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f3ed87d-8247-4d7a-8a3c-bd14f10c438a
2024-01-14 06:34:14,585 - distributed.worker - INFO - Starting Worker plugin PreImport-11ceafe8-3ab8-4720-a42b-41fd751c0df2
2024-01-14 06:34:14,586 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4ec14d8-8a4e-4624-ae50-3007de9523af
2024-01-14 06:34:14,587 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:14,778 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33337', status: init, memory: 0, processing: 0>
2024-01-14 06:34:14,780 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33337
2024-01-14 06:34:14,780 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52538
2024-01-14 06:34:14,781 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:14,781 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:14,781 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:14,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:14,872 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:14,874 - distributed.scheduler - INFO - Remove client Client-ecac776b-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:14,875 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52528; closing.
2024-01-14 06:34:14,875 - distributed.scheduler - INFO - Remove client Client-ecac776b-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:14,876 - distributed.scheduler - INFO - Close client connection: Client-ecac776b-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:14,876 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36919'. Reason: nanny-close
2024-01-14 06:34:14,877 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:14,877 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33337. Reason: nanny-close
2024-01-14 06:34:14,879 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:14,879 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52538; closing.
2024-01-14 06:34:14,879 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33337', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214054.8798468')
2024-01-14 06:34:14,880 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:34:14,880 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:15,491 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:34:15,492 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:34:15,492 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:34:15,493 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-14 06:34:15,494 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-14 06:34:17,883 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:17,889 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33371 instead
  warnings.warn(
2024-01-14 06:34:17,893 - distributed.scheduler - INFO - State start
2024-01-14 06:34:18,013 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:18,014 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-14 06:34:18,014 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33371/status
2024-01-14 06:34:18,015 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:34:20,502 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:52546'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52546>: Stream is closed
2024-01-14 06:34:20,860 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:34:20,860 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:34:20,860 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:34:20,861 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-14 06:34:20,861 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-14 06:34:23,013 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:23,018 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33379 instead
  warnings.warn(
2024-01-14 06:34:23,022 - distributed.scheduler - INFO - State start
2024-01-14 06:34:23,044 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:23,045 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-14 06:34:23,045 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33379/status
2024-01-14 06:34:23,046 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:34:23,157 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39815'
2024-01-14 06:34:24,967 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:24,967 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:24,971 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:24,972 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42975
2024-01-14 06:34:24,972 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42975
2024-01-14 06:34:24,972 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33899
2024-01-14 06:34:24,972 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-14 06:34:24,972 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:24,972 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:24,972 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-14 06:34:24,972 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-15o4cpm1
2024-01-14 06:34:24,972 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b4748d0d-b9f0-4bd4-a2e7-90b69dd51d4f
2024-01-14 06:34:24,973 - distributed.worker - INFO - Starting Worker plugin PreImport-c2079125-20f2-4ac6-93d4-683d81a37cd7
2024-01-14 06:34:24,973 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a839a555-aa3d-4c7a-adc2-c88374566455
2024-01-14 06:34:24,973 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:25,168 - distributed.scheduler - INFO - Receive client connection: Client-f358c228-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:25,180 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48186
2024-01-14 06:34:26,364 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42975', status: init, memory: 0, processing: 0>
2024-01-14 06:34:26,365 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42975
2024-01-14 06:34:26,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48178
2024-01-14 06:34:26,366 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:26,367 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-14 06:34:26,367 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:26,368 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-14 06:34:26,391 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:26,394 - distributed.scheduler - INFO - Remove client Client-f358c228-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:26,394 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48186; closing.
2024-01-14 06:34:26,394 - distributed.scheduler - INFO - Remove client Client-f358c228-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:26,394 - distributed.scheduler - INFO - Close client connection: Client-f358c228-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:26,395 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39815'. Reason: nanny-close
2024-01-14 06:34:26,396 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:26,396 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42975. Reason: nanny-close
2024-01-14 06:34:26,398 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-14 06:34:26,398 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48178; closing.
2024-01-14 06:34:26,398 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42975', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214066.3988826')
2024-01-14 06:34:26,399 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:34:26,399 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:26,910 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:34:26,911 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:34:26,911 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:34:26,912 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-14 06:34:26,912 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-14 06:34:29,184 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:29,189 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35303 instead
  warnings.warn(
2024-01-14 06:34:29,193 - distributed.scheduler - INFO - State start
2024-01-14 06:34:29,216 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:29,217 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-14 06:34:29,217 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35303/status
2024-01-14 06:34:29,218 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:34:29,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41461'
2024-01-14 06:34:29,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46463'
2024-01-14 06:34:29,438 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42415'
2024-01-14 06:34:29,447 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41829'
2024-01-14 06:34:29,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34987'
2024-01-14 06:34:29,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40181'
2024-01-14 06:34:29,474 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42261'
2024-01-14 06:34:29,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44939'
2024-01-14 06:34:29,597 - distributed.scheduler - INFO - Receive client connection: Client-f6f9edac-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:29,615 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40188
2024-01-14 06:34:31,349 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:31,349 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:31,354 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:31,355 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35071
2024-01-14 06:34:31,355 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35071
2024-01-14 06:34:31,355 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44597
2024-01-14 06:34:31,355 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:31,355 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:31,355 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:31,355 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:34:31,355 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xrf99rpu
2024-01-14 06:34:31,356 - distributed.worker - INFO - Starting Worker plugin PreImport-fe03d4d8-cbaa-4648-ba8e-2f8d66620e6d
2024-01-14 06:34:31,356 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-128c1f93-2b68-4e35-99d7-2f4f44f3b11e
2024-01-14 06:34:31,356 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7733bc01-84be-4e90-9487-2c99a9513452
2024-01-14 06:34:31,627 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:31,628 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:31,632 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:31,633 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44295
2024-01-14 06:34:31,633 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44295
2024-01-14 06:34:31,633 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44299
2024-01-14 06:34:31,633 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:31,633 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:31,633 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:31,633 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:31,633 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:34:31,633 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:31,634 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lq1ag05l
2024-01-14 06:34:31,634 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b373fc3-85dd-49ab-b5d9-e71fe0d9bfd6
2024-01-14 06:34:31,634 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:31,634 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:31,634 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:31,634 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:31,636 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:31,636 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:31,638 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:31,638 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:31,638 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33455
2024-01-14 06:34:31,639 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33455
2024-01-14 06:34:31,639 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41631
2024-01-14 06:34:31,639 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:31,639 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:31,639 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:31,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:31,639 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38461
2024-01-14 06:34:31,639 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:31,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:31,639 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:34:31,639 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38461
2024-01-14 06:34:31,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0mzj42zz
2024-01-14 06:34:31,639 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43063
2024-01-14 06:34:31,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:31,639 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:31,639 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:31,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:31,639 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:31,639 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:34:31,639 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ff8c282-06db-4249-a3de-939652c8ac66
2024-01-14 06:34:31,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aqk2l6yc
2024-01-14 06:34:31,639 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7103322-3ecf-42d5-8958-f7966666bb4a
2024-01-14 06:34:31,640 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43007
2024-01-14 06:34:31,640 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43007
2024-01-14 06:34:31,640 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40295
2024-01-14 06:34:31,640 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:31,640 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:31,640 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:31,640 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:34:31,640 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d3dzqdnw
2024-01-14 06:34:31,641 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e3896995-0b72-4d1d-aafb-c623740764ea
2024-01-14 06:34:31,641 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:31,642 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46161
2024-01-14 06:34:31,642 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46161
2024-01-14 06:34:31,642 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38505
2024-01-14 06:34:31,642 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:31,642 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:31,643 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:31,643 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:34:31,643 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-majdoh28
2024-01-14 06:34:31,643 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c17da07f-bdd0-4c30-8f44-ddfccc9c9638
2024-01-14 06:34:31,644 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:31,644 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:31,645 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39569
2024-01-14 06:34:31,645 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39569
2024-01-14 06:34:31,645 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35055
2024-01-14 06:34:31,645 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:31,645 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:31,645 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36115
2024-01-14 06:34:31,645 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:31,645 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36115
2024-01-14 06:34:31,645 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:34:31,645 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37785
2024-01-14 06:34:31,645 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xzbjx4c0
2024-01-14 06:34:31,645 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:31,645 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:31,645 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:31,645 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-14 06:34:31,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4188f5b-304a-487f-ba10-e9bb5dbb130b
2024-01-14 06:34:31,645 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aykkn539
2024-01-14 06:34:31,646 - distributed.worker - INFO - Starting Worker plugin RMMSetup-882f122a-1a84-4dee-a409-9df5d11c0f82
2024-01-14 06:34:32,091 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:32,115 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35071', status: init, memory: 0, processing: 0>
2024-01-14 06:34:32,117 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35071
2024-01-14 06:34:32,117 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38968
2024-01-14 06:34:32,118 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:32,119 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:32,119 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:32,120 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:33,680 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93cbf72b-9b03-4924-becf-57fd44fa862f
2024-01-14 06:34:33,680 - distributed.worker - INFO - Starting Worker plugin PreImport-441a1536-0a67-400d-a06c-ac2a9aa5816d
2024-01-14 06:34:33,681 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,705 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33455', status: init, memory: 0, processing: 0>
2024-01-14 06:34:33,705 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33455
2024-01-14 06:34:33,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38980
2024-01-14 06:34:33,706 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:33,707 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:33,707 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,709 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:33,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e4ced20-ff5f-43b0-ad6d-7cce27044566
2024-01-14 06:34:33,753 - distributed.worker - INFO - Starting Worker plugin PreImport-c51b3a4d-b2d3-4f48-a613-8e91b2f9b30b
2024-01-14 06:34:33,754 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,775 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc007cae-8aa4-40e8-9d72-178d0aef24ab
2024-01-14 06:34:33,776 - distributed.worker - INFO - Starting Worker plugin PreImport-11bdbb61-364c-4aa4-87e0-82643ae99c64
2024-01-14 06:34:33,776 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,786 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39569', status: init, memory: 0, processing: 0>
2024-01-14 06:34:33,787 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39569
2024-01-14 06:34:33,787 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38994
2024-01-14 06:34:33,788 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85f06cf8-3a25-4f5e-81e3-ff0cf50f7d86
2024-01-14 06:34:33,788 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:33,789 - distributed.worker - INFO - Starting Worker plugin PreImport-8cd787a3-789b-4a3f-af68-08cb6fba41a4
2024-01-14 06:34:33,789 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,790 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:33,790 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,792 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:33,799 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44295', status: init, memory: 0, processing: 0>
2024-01-14 06:34:33,800 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44295
2024-01-14 06:34:33,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39000
2024-01-14 06:34:33,801 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:33,801 - distributed.worker - INFO - Starting Worker plugin PreImport-b0fe12c5-42ac-465a-9bb8-37763e17bf02
2024-01-14 06:34:33,802 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e7e4fe3-c744-46f9-ae2a-c880719fb776
2024-01-14 06:34:33,802 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:33,802 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,803 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,803 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:33,811 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46161', status: init, memory: 0, processing: 0>
2024-01-14 06:34:33,812 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46161
2024-01-14 06:34:33,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39010
2024-01-14 06:34:33,813 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:33,814 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:33,814 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,815 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:33,818 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d3c8b9b-4b49-48f9-9af0-292c636653cd
2024-01-14 06:34:33,819 - distributed.worker - INFO - Starting Worker plugin PreImport-d84525d1-e414-4304-a19f-54592051d572
2024-01-14 06:34:33,820 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,839 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38461', status: init, memory: 0, processing: 0>
2024-01-14 06:34:33,840 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38461
2024-01-14 06:34:33,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39020
2024-01-14 06:34:33,841 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:33,843 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:33,843 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,845 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:33,854 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43007', status: init, memory: 0, processing: 0>
2024-01-14 06:34:33,854 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43007
2024-01-14 06:34:33,855 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39026
2024-01-14 06:34:33,857 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:33,858 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:33,858 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:33,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:33,985 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac942c4e-ee09-4acd-885e-cbfc9f9fe453
2024-01-14 06:34:33,986 - distributed.worker - INFO - Starting Worker plugin PreImport-081b12dd-de01-4161-9157-810e4f2c3dc7
2024-01-14 06:34:33,987 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:34,020 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36115', status: init, memory: 0, processing: 0>
2024-01-14 06:34:34,021 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36115
2024-01-14 06:34:34,021 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39030
2024-01-14 06:34:34,023 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:34,024 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:34,024 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:34,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:34,092 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:34,092 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:34,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:34,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:34,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:34,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:34,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:34,093 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-14 06:34:34,107 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:34,107 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:34,107 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:34,107 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:34,107 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:34,107 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:34,108 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:34,108 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:34,112 - distributed.scheduler - INFO - Remove client Client-f6f9edac-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:34,112 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40188; closing.
2024-01-14 06:34:34,112 - distributed.scheduler - INFO - Remove client Client-f6f9edac-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:34,113 - distributed.scheduler - INFO - Close client connection: Client-f6f9edac-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:34,114 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46463'. Reason: nanny-close
2024-01-14 06:34:34,115 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:34,115 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42415'. Reason: nanny-close
2024-01-14 06:34:34,116 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:34,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41829'. Reason: nanny-close
2024-01-14 06:34:34,116 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43007. Reason: nanny-close
2024-01-14 06:34:34,116 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:34,117 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34987'. Reason: nanny-close
2024-01-14 06:34:34,117 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39569. Reason: nanny-close
2024-01-14 06:34:34,117 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:34,117 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40181'. Reason: nanny-close
2024-01-14 06:34:34,117 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46161. Reason: nanny-close
2024-01-14 06:34:34,118 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:34,118 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42261'. Reason: nanny-close
2024-01-14 06:34:34,118 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44295. Reason: nanny-close
2024-01-14 06:34:34,118 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:34,118 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44939'. Reason: nanny-close
2024-01-14 06:34:34,119 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:34,119 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36115. Reason: nanny-close
2024-01-14 06:34:34,119 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41461'. Reason: nanny-close
2024-01-14 06:34:34,119 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:34,119 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:34,119 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39026; closing.
2024-01-14 06:34:34,119 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38461. Reason: nanny-close
2024-01-14 06:34:34,119 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:34,119 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33455. Reason: nanny-close
2024-01-14 06:34:34,119 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:34,119 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43007', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214074.1199007')
2024-01-14 06:34:34,120 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:34,120 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39000; closing.
2024-01-14 06:34:34,120 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35071. Reason: nanny-close
2024-01-14 06:34:34,121 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:34,121 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44295', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214074.1210759')
2024-01-14 06:34:34,121 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:34,121 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:34,121 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38994; closing.
2024-01-14 06:34:34,121 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:34,121 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:34,121 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:34,121 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39010; closing.
2024-01-14 06:34:34,121 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:34,122 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39569', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214074.1226876')
2024-01-14 06:34:34,123 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:34,123 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46161', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214074.123073')
2024-01-14 06:34:34,123 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:34,123 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:34,124 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:39000>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-14 06:34:34,126 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39030; closing.
2024-01-14 06:34:34,126 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39020; closing.
2024-01-14 06:34:34,126 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38980; closing.
2024-01-14 06:34:34,126 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214074.1267009')
2024-01-14 06:34:34,127 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38461', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214074.1270852')
2024-01-14 06:34:34,127 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33455', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214074.127418')
2024-01-14 06:34:34,125 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:38970 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-01-14 06:34:34,129 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:34,129 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38968; closing.
2024-01-14 06:34:34,129 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214074.1294765')
2024-01-14 06:34:34,129 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:34:34,130 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:35,130 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:34:35,131 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:34:35,132 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:34:35,133 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-14 06:34:35,134 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-14 06:34:37,428 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:37,433 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34123 instead
  warnings.warn(
2024-01-14 06:34:37,438 - distributed.scheduler - INFO - State start
2024-01-14 06:34:37,466 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:37,467 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-14 06:34:37,467 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34123/status
2024-01-14 06:34:37,468 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:34:37,651 - distributed.scheduler - INFO - Receive client connection: Client-fbf1a674-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:37,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39112
2024-01-14 06:34:37,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42937'
2024-01-14 06:34:40,140 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:40,140 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:40,145 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:40,146 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40479
2024-01-14 06:34:40,146 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40479
2024-01-14 06:34:40,146 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46349
2024-01-14 06:34:40,146 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:40,147 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:40,147 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:40,147 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-14 06:34:40,147 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pqocv4zt
2024-01-14 06:34:40,147 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d7f25e2-1ffe-481e-a816-7afde73195db
2024-01-14 06:34:41,207 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49850ad9-53bc-44eb-942a-7f0e3d0bf99f
2024-01-14 06:34:41,207 - distributed.worker - INFO - Starting Worker plugin PreImport-37d30800-622c-4c28-a0e6-0c9a36d6da32
2024-01-14 06:34:41,207 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:41,261 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40479', status: init, memory: 0, processing: 0>
2024-01-14 06:34:41,262 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40479
2024-01-14 06:34:41,262 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52286
2024-01-14 06:34:41,263 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:41,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:41,264 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:41,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:41,289 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:34:41,294 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:41,295 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:41,298 - distributed.scheduler - INFO - Remove client Client-fbf1a674-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:41,298 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39112; closing.
2024-01-14 06:34:41,298 - distributed.scheduler - INFO - Remove client Client-fbf1a674-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:41,299 - distributed.scheduler - INFO - Close client connection: Client-fbf1a674-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:41,299 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42937'. Reason: nanny-close
2024-01-14 06:34:41,300 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:41,301 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40479. Reason: nanny-close
2024-01-14 06:34:41,302 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:41,302 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52286; closing.
2024-01-14 06:34:41,303 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40479', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214081.30302')
2024-01-14 06:34:41,303 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:34:41,304 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:41,965 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:34:41,965 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:34:41,966 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:34:41,967 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-14 06:34:41,967 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-14 06:34:44,368 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:44,373 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44349 instead
  warnings.warn(
2024-01-14 06:34:44,378 - distributed.scheduler - INFO - State start
2024-01-14 06:34:44,411 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-14 06:34:44,414 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-14 06:34:44,416 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44349/status
2024-01-14 06:34:44,417 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-14 06:34:44,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33775'
2024-01-14 06:34:45,048 - distributed.scheduler - INFO - Receive client connection: Client-fffae38a-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:45,061 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52400
2024-01-14 06:34:46,339 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-14 06:34:46,339 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-14 06:34:46,343 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-14 06:34:46,344 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32793
2024-01-14 06:34:46,344 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32793
2024-01-14 06:34:46,344 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45045
2024-01-14 06:34:46,344 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-14 06:34:46,344 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:46,344 - distributed.worker - INFO -               Threads:                          1
2024-01-14 06:34:46,345 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-14 06:34:46,345 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_3cyahsx
2024-01-14 06:34:46,345 - distributed.worker - INFO - Starting Worker plugin PreImport-87e7caf6-eb51-413a-b970-a631b6e2cdd7
2024-01-14 06:34:46,345 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a3ce1307-403c-4f5a-ad5a-5b6fbd00b941
2024-01-14 06:34:46,633 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57ba8e3a-b155-41e9-a3e4-a314ad9ec58f
2024-01-14 06:34:46,634 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:46,702 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32793', status: init, memory: 0, processing: 0>
2024-01-14 06:34:46,704 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32793
2024-01-14 06:34:46,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52424
2024-01-14 06:34:46,705 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-14 06:34:46,706 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-14 06:34:46,706 - distributed.worker - INFO - -------------------------------------------------
2024-01-14 06:34:46,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-14 06:34:46,746 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-14 06:34:46,751 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-14 06:34:46,755 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:46,757 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-14 06:34:46,759 - distributed.scheduler - INFO - Remove client Client-fffae38a-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:46,759 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52400; closing.
2024-01-14 06:34:46,759 - distributed.scheduler - INFO - Remove client Client-fffae38a-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:46,760 - distributed.scheduler - INFO - Close client connection: Client-fffae38a-b2a6-11ee-ad4e-d8c49764f6bb
2024-01-14 06:34:46,760 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33775'. Reason: nanny-close
2024-01-14 06:34:46,761 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-14 06:34:46,762 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32793. Reason: nanny-close
2024-01-14 06:34:46,764 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-14 06:34:46,764 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52424; closing.
2024-01-14 06:34:46,764 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32793', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705214086.7643828')
2024-01-14 06:34:46,764 - distributed.scheduler - INFO - Lost all workers
2024-01-14 06:34:46,765 - distributed.nanny - INFO - Worker closed
2024-01-14 06:34:47,376 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-14 06:34:47,376 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-14 06:34:47,377 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-14 06:34:47,378 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-14 06:34:47,379 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33629 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] 2024-01-14 06:35:23,046 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-14 06:35:23,048 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-14 06:35:23,049 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
[1705214123.922865] [dgx13:64600] UCXPY  WARNING Listener object is being destroyed, but 3 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45959 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46569 instead
  warnings.warn(
2024-01-14 06:35:57,551 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:53001 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 51 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
