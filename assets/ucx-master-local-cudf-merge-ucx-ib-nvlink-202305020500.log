2023-05-02 07:03:39,368 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 07:03:39,368 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 07:03:39,392 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 07:03:39,392 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 07:03:39,394 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 07:03:39,394 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 07:03:39,426 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 07:03:39,426 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 07:03:39,426 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 07:03:39,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 07:03:39,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 07:03:39,453 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 07:03:39,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 07:03:39,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 07:03:39,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 07:03:39,548 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1683011033.177205] [dgx13:73371:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_2: LRU push returned Unsupported operation
[dgx13:73371:0:73371]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  73371) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7fbf68051dec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7fbf6804ed28]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_format+0x114) [0x7fbf6804ee44]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x830c5) [0x7fbf61f950c5]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xc9) [0x7fbf61f69fc9]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x20) [0x7fbf61fafc60]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x71e) [0x7fbf61fb4b7e]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x52) [0x7fbf61fb4332]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x495d8) [0x7fbf680d35d8]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x55758c20edc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x55758c20d1a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55758c1f3d36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55758c1ed27a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55758c1fec05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55758c1ef3cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55758c1ed27a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55758c1fec05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55758c1ef3cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55758c21370e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55758c1f4923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55758c21370e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55758c1f4923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55758c21370e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55758c1f4923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55758c21370e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55758c1f4923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55758c21370e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55758c1f4923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55758c21370e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7fbf86c6b2fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7fbf86c6bb4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55758c1f72bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55758c1aa817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55758c1f5f83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55758c1f3d36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55758c1feef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55758c1ee81b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55758c1feef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55758c1ee81b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55758c1feef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55758c1ee81b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55758c1feef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55758c1ee81b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55758c1ed27a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55758c1fec05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55758c1f2fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55758c1ed27a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55758c20c935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55758c20d104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55758c2d3fc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55758c1f72bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55758c1f21bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55758c1feef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55758c20cc72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55758c1f21bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55758c1feef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55758c1ee81b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55758c1ed27a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55758c1fec05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55758c1ee81b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55758c1feef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55758c1ee568]
=================================
2023-05-02 07:03:53,376 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54956
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7f4cc4fe0200, tag: 0xe3b624f716ab4fd1, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7f4cc4fe0200, tag: 0xe3b624f716ab4fd1, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-02 07:03:53,377 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54956
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #022] ep: 0x7f331d0e4200, tag: 0x56ed9cfb8b972693, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #022] ep: 0x7f331d0e4200, tag: 0x56ed9cfb8b972693, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-02 07:03:53,377 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54956
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7f4fac1db180, tag: 0xc3500ab3259d1d2f, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7f4fac1db180, tag: 0xc3500ab3259d1d2f, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-02 07:03:53,378 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54956
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #050] ep: 0x7fad4089c200, tag: 0xf2af38d372d236d8, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #050] ep: 0x7fad4089c200, tag: 0xf2af38d372d236d8, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-02 07:03:53,378 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54956
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7fd72c450200, tag: 0xa0a72a2ff5a9074f, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7fd72c450200, tag: 0xa0a72a2ff5a9074f, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-02 07:03:53,381 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54956
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #017] ep: 0x7f6205605280, tag: 0x658d4155755e8a45, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #017] ep: 0x7f6205605280, tag: 0x658d4155755e8a45, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-02 07:03:53,382 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54956
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #025] ep: 0x7fc6e803a180, tag: 0xf91909b027d89833, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #025] ep: 0x7fc6e803a180, tag: 0xf91909b027d89833, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-02 07:03:53,826 - distributed.nanny - WARNING - Restarting worker
[1683011034.528994] [dgx13:73356:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_0: LRU push returned Unsupported operation
[dgx13:73356:0:73356]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  73356) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7fc6f8f21dec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7fc6f8f1ed28]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_format+0x114) [0x7fc6f8f1ee44]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x830c5) [0x7fc6f8fdd0c5]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xc9) [0x7fc6f8fb1fc9]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x20) [0x7fc6f8ff7c60]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x71e) [0x7fc6f8ffcb7e]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x52) [0x7fc6f8ffc332]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x495d8) [0x7fc6f90915d8]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x5577032f7dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x5577032f61a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5577032dcd36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5577032d627a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5577032e7c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5577032d83cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5577032d627a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5577032e7c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5577032d83cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5577032fc70e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x5577032dd923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5577032fc70e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x5577032dd923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5577032fc70e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x5577032dd923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5577032fc70e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x5577032dd923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5577032fc70e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x5577032dd923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5577032fc70e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7fc709c242fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7fc709c24b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5577032e02bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x557703293817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5577032def83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5577032dcd36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5577032e7ef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5577032d781b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5577032e7ef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5577032d781b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5577032e7ef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5577032d781b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5577032e7ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5577032d781b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5577032d627a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5577032e7c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5577032dbfa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5577032d627a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5577032f5935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5577032f6104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5577033bcfc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5577032e02bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5577032db1bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5577032e7ef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5577032f5c72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5577032db1bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5577032e7ef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5577032d781b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5577032d627a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5577032e7c05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5577032d781b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5577032e7ef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5577032d7568]
=================================
2023-05-02 07:03:54,734 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56903
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #046] ep: 0x7f4cc4fe0400, tag: 0xd184b9d4cef719a5, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #046] ep: 0x7f4cc4fe0400, tag: 0xd184b9d4cef719a5, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-02 07:03:54,736 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56903
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #048] ep: 0x7f331d0e4180, tag: 0x99a150a9bf26781b, nbytes: 99965008, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #048] ep: 0x7f331d0e4180, tag: 0x99a150a9bf26781b, nbytes: 99965008, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-02 07:03:54,736 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56903
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #042] ep: 0x7fad4089c140, tag: 0x7a61bd80494cb2e9, nbytes: 100038160, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #042] ep: 0x7fad4089c140, tag: 0x7a61bd80494cb2e9, nbytes: 100038160, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-02 07:03:54,736 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56903
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #031] ep: 0x7f4fac1db300, tag: 0x4d40bda7ba8e2b6d, nbytes: 99981272, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #031] ep: 0x7f4fac1db300, tag: 0x4d40bda7ba8e2b6d, nbytes: 99981272, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-02 07:03:54,736 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56903
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #039] ep: 0x7fd72c450380, tag: 0xcf5a87ca2741065f, nbytes: 100007032, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #039] ep: 0x7fd72c450380, tag: 0xcf5a87ca2741065f, nbytes: 100007032, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-02 07:03:54,737 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:56903
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #032] ep: 0x7f6205605180, tag: 0x582ee6f010096e8b, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #032] ep: 0x7f6205605180, tag: 0x582ee6f010096e8b, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-05-02 07:03:55,139 - distributed.nanny - WARNING - Restarting worker
2023-05-02 07:03:55,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 07:03:55,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1683011036.301233] [dgx13:74011:0]            sock.c:470  UCX  ERROR bind(fd=163 addr=0.0.0.0:44404) failed: Address already in use
2023-05-02 07:03:56,721 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-02 07:03:56,721 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-02 07:03:56,773 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-8c8769293f10f5c23d0a929731b7200c', 0)
Function:  subgraph_callable-3b3274db-47cb-4aff-adee-d0093e6a
args:      (               key   payload
shuffle                     
0            19922  73837060
0           608938  58021213
0           313640  50084022
0           464730  35228675
0           317908  75231701
...            ...       ...
7        799926884  11789960
7        799894611  52437760
7        799671482   4104075
7        799847285  63224172
7        799858157  90376829

[99999977 rows x 2 columns],                  key   payload
11503      822460200  16275859
22660      810671777  71954753
11504      102166330  85385922
22663      864282477  34950428
11506      703961298  89323727
...              ...       ...
99881830   393672519  94791574
99881843  1545772995  78936724
99881845  1532915638  93716128
99881846  1565716252  59077496
99881853  1565601783  79365795

[100005446 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-02 07:03:56,782 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-8c8769293f10f5c23d0a929731b7200c', 6)
Function:  subgraph_callable-3b3274db-47cb-4aff-adee-d0093e6a
args:      (               key   payload
shuffle                     
0           317190  90831769
0           613580  75628404
0           283124  62498638
0           627282  95224715
0           351453  31947715
...            ...       ...
7        799796546  73151477
7        799845283  29370438
7        799850754    907538
7        799819754  90480344
7        799749324  12728680

[99977935 rows x 2 columns],                  key   payload
11502      828072745  75671208
22657      403472411  62880038
22667      311707130  79923201
43015      851032910  59566571
43024      836076219   1471704
...              ...       ...
99983281   196781757  35273708
99983284  1511251114  21163645
99881826  1508794312  22940572
99881834  1569633680  44635251
99881841  1530184136  76661230

[100006787 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-02 07:04:04,098 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 07:04:04,099 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 07:04:04,135 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 07:04:04,135 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-02 07:04:04,246 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-3d1af2bf4e51f735d47ca8e9785a27ed', 1)
Function:  <dask.layers.CallableLazyImport object at 0x7f1218
args:      ([               key   payload
shuffle                     
0            45316  23425321
0           348335  92868879
0           252710  19679302
0           304928  23493694
0           863077  75544432
...            ...       ...
0        799562909  60831808
0        799607991  20074141
0        799554063  84532306
0        799576712  68474750
0        799610532  32732628

[12497508 rows x 2 columns],                key   payload
shuffle                     
1           327652  26787466
1           144720  51572181
1           164724  94743746
1           212438  71380315
1            83267  52264183
...            ...       ...
1        799997425  60065997
1        799892685  28016334
1        799825518  30343494
1        799953859  78093511
1        799981938  63112716

[12503907 rows x 2 columns],                key   payload
shuffle                     
2           583337  61718567
2            28266   9689966
2           373042  46623753
2            79080  39962522
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-05-02 07:04:04,501 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-3d1af2bf4e51f735d47ca8e9785a27ed', 3)
Function:  <dask.layers.CallableLazyImport object at 0x7f2a05
args:      ([               key   payload
shuffle                     
0           270147  54086377
0           358641  34926524
0           259341   4476426
0           324085  20067885
0           288629  81210721
...            ...       ...
0        799664025  62008258
0        799683020  45246104
0        799718676  28983721
0        799631426   2156858
0        799577098  97432035

[12497076 rows x 2 columns],                key   payload
shuffle                     
1           238483  61791293
1           281072  61914244
1           264092  75092781
1           159589   6141688
1            78873   1945948
...            ...       ...
1        799945759  33395218
1        799776290  97282072
1        799842934  31879554
1        799865830  67491249
1        799905816  32942206

[12501362 rows x 2 columns],                key   payload
shuffle                     
2           646807  18802852
2            30450  34355294
2           422387    359970
2            93900  32808481
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-05-02 07:04:04,624 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-3d1af2bf4e51f735d47ca8e9785a27ed', 7)
Function:  <dask.layers.CallableLazyImport object at 0x7f2a05
args:      ([               key   payload
shuffle                     
0           319941   3080739
0             4887  30963746
0           387733  68813033
0          1010827  23580199
0            58722  30343986
...            ...       ...
0        799635217  62053657
0        799650875  93165213
0        799592701  61319345
0        799661415  46298723
0        799607911  24864096

[12498151 rows x 2 columns],                key   payload
shuffle                     
1           254867  76948071
1           271287  38874992
1           167182   5721945
1           206647  60945516
1            56898  39431014
...            ...       ...
1        799968503  79628188
1        799995993  42616376
1        799940827  57849407
1        799906959  73707453
1        799859398   2739492

[12498591 rows x 2 columns],                key   payload
shuffle                     
2           571131  12264493
2            59620  80269080
2           396682  59463000
2              109  35160437
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-05-02 07:04:04,827 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-5cb1085724b046458ef02d79d6608214', 5)
Function:  subgraph_callable-ca5fe426-ac51-40dd-b380-ec42593e
args:      (               key   payload
shuffle                     
0           367771  27031479
0           364623  20008162
0           299358  39820774
0           288248  16953300
0           336042  25760349
...            ...       ...
7        799667876  26604402
7        799838662  94262920
7        799700160  73434108
7        799826758  87957604
7        799737415  93687528

[99993166 rows x 2 columns],                  key   payload
125091     106365702  26884337
125094     104443127  72386209
125096     838298315   8926972
125099     819940141  24668765
134242     811583848  50545002
...              ...       ...
99994046  1518871611  80403837
99993995   289781557  71288093
99994006  1508255209  27166607
99994011   399505156  27895516
99994014   393210571  44928793

[99997899 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
