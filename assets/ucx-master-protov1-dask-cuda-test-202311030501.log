============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-11-03 05:25:16,970 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:16,974 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:25:16,978 - distributed.scheduler - INFO - State start
2023-11-03 05:25:17,000 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:17,001 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-03 05:25:17,001 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:25:17,002 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:25:17,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44575'
2023-11-03 05:25:17,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43615'
2023-11-03 05:25:17,124 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42499'
2023-11-03 05:25:17,131 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38705'
2023-11-03 05:25:17,252 - distributed.scheduler - INFO - Receive client connection: Client-5e6d443f-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:17,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50748
2023-11-03 05:25:18,770 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:18,770 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:18,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:18,773 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:18,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:18,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:18,774 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:18,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:18,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:18,777 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:18,777 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:18,779 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-11-03 05:25:18,790 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34447
2023-11-03 05:25:18,791 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34447
2023-11-03 05:25:18,791 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46477
2023-11-03 05:25:18,791 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-03 05:25:18,791 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:18,791 - distributed.worker - INFO -               Threads:                          4
2023-11-03 05:25:18,791 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-03 05:25:18,791 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-94j4ed16
2023-11-03 05:25:18,791 - distributed.worker - INFO - Starting Worker plugin RMMSetup-065121fc-5504-4897-8884-5a891bf1941f
2023-11-03 05:25:18,791 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86cc7b77-b0f9-4ffb-888b-a94df95f278d
2023-11-03 05:25:18,791 - distributed.worker - INFO - Starting Worker plugin PreImport-5b790627-ff48-4f29-bf08-0af07889fbd1
2023-11-03 05:25:18,792 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:19,202 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34447', status: init, memory: 0, processing: 0>
2023-11-03 05:25:19,203 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34447
2023-11-03 05:25:19,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50758
2023-11-03 05:25:19,204 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:19,205 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-03 05:25:19,205 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:19,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-03 05:25:19,982 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41897
2023-11-03 05:25:19,983 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41897
2023-11-03 05:25:19,983 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45553
2023-11-03 05:25:19,983 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-03 05:25:19,983 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:19,983 - distributed.worker - INFO -               Threads:                          4
2023-11-03 05:25:19,984 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-03 05:25:19,984 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-1z9h6muj
2023-11-03 05:25:19,984 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cb2bbd18-851a-4016-bc15-9d2853618ab7
2023-11-03 05:25:19,984 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-18fcf835-a934-4e64-8865-6f7ac76fa2aa
2023-11-03 05:25:19,985 - distributed.worker - INFO - Starting Worker plugin PreImport-4fe14211-9b82-4ef0-85e9-9a9dddbfd721
2023-11-03 05:25:19,984 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32825
2023-11-03 05:25:19,985 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32825
2023-11-03 05:25:19,985 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:19,985 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39833
2023-11-03 05:25:19,985 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-03 05:25:19,985 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:19,986 - distributed.worker - INFO -               Threads:                          4
2023-11-03 05:25:19,986 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-03 05:25:19,986 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ymyilftk
2023-11-03 05:25:19,986 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7963e8e7-87bd-4f75-bd8b-63a7a1be2ea3
2023-11-03 05:25:19,987 - distributed.worker - INFO - Starting Worker plugin PreImport-710aae92-1e8e-4ba7-b711-c150de95f7bd
2023-11-03 05:25:19,987 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e07ff93-664d-4fd2-a9b6-9e53fdc8fe05
2023-11-03 05:25:19,988 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:19,988 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39557
2023-11-03 05:25:19,988 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39557
2023-11-03 05:25:19,988 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35971
2023-11-03 05:25:19,989 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-03 05:25:19,989 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:19,989 - distributed.worker - INFO -               Threads:                          4
2023-11-03 05:25:19,989 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-03 05:25:19,989 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-uh5a1kf2
2023-11-03 05:25:19,989 - distributed.worker - INFO - Starting Worker plugin RMMSetup-350ba646-34a9-43ea-9f62-0e96f652fb5e
2023-11-03 05:25:19,990 - distributed.worker - INFO - Starting Worker plugin PreImport-c31c579c-49ab-49ca-a153-2484dc2f96be
2023-11-03 05:25:19,990 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-19892247-801b-4ea0-ac3c-6acf8e9aeddb
2023-11-03 05:25:19,990 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:20,012 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39557', status: init, memory: 0, processing: 0>
2023-11-03 05:25:20,013 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39557
2023-11-03 05:25:20,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48272
2023-11-03 05:25:20,014 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:20,015 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-03 05:25:20,015 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:20,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-03 05:25:20,018 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32825', status: init, memory: 0, processing: 0>
2023-11-03 05:25:20,018 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32825
2023-11-03 05:25:20,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48268
2023-11-03 05:25:20,019 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:20,019 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41897', status: init, memory: 0, processing: 0>
2023-11-03 05:25:20,020 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41897
2023-11-03 05:25:20,020 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48258
2023-11-03 05:25:20,020 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-03 05:25:20,020 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:20,021 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:20,021 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-03 05:25:20,022 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:20,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-03 05:25:20,024 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-03 05:25:20,126 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-03 05:25:20,126 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-03 05:25:20,126 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-03 05:25:20,126 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-03 05:25:20,131 - distributed.scheduler - INFO - Remove client Client-5e6d443f-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:20,131 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50748; closing.
2023-11-03 05:25:20,131 - distributed.scheduler - INFO - Remove client Client-5e6d443f-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:20,131 - distributed.scheduler - INFO - Close client connection: Client-5e6d443f-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:20,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44575'. Reason: nanny-close
2023-11-03 05:25:20,133 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:20,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43615'. Reason: nanny-close
2023-11-03 05:25:20,133 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:20,133 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41897. Reason: nanny-close
2023-11-03 05:25:20,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42499'. Reason: nanny-close
2023-11-03 05:25:20,134 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:20,134 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39557. Reason: nanny-close
2023-11-03 05:25:20,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38705'. Reason: nanny-close
2023-11-03 05:25:20,135 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:20,135 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32825. Reason: nanny-close
2023-11-03 05:25:20,135 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34447. Reason: nanny-close
2023-11-03 05:25:20,135 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48258; closing.
2023-11-03 05:25:20,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-03 05:25:20,136 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-03 05:25:20,136 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41897', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989120.1361728')
2023-11-03 05:25:20,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48272; closing.
2023-11-03 05:25:20,137 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-03 05:25:20,137 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-03 05:25:20,137 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:20,137 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:20,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48268; closing.
2023-11-03 05:25:20,137 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39557', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989120.1377413')
2023-11-03 05:25:20,138 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32825', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989120.1385458')
2023-11-03 05:25:20,138 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:20,138 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:20,138 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50758; closing.
2023-11-03 05:25:20,139 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34447', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989120.1391325')
2023-11-03 05:25:20,139 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:21,148 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:25:21,149 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:25:21,149 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:25:21,150 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-03 05:25:21,151 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-11-03 05:25:23,132 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:23,137 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:25:23,140 - distributed.scheduler - INFO - State start
2023-11-03 05:25:23,160 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:23,160 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-03 05:25:23,161 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:25:23,161 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:25:23,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36705'
2023-11-03 05:25:23,215 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34291'
2023-11-03 05:25:23,227 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38467'
2023-11-03 05:25:23,238 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39431'
2023-11-03 05:25:23,241 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35011'
2023-11-03 05:25:23,250 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40655'
2023-11-03 05:25:23,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38195'
2023-11-03 05:25:23,270 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39203'
2023-11-03 05:25:24,047 - distributed.scheduler - INFO - Receive client connection: Client-622745ad-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:24,061 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51372
2023-11-03 05:25:24,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:24,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:24,877 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:24,887 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:24,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:24,891 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:24,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:24,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:24,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:24,957 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:24,959 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:24,961 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:24,961 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:24,961 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:24,965 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:25,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:25,172 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:25,176 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:25,179 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:25,180 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:25,181 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:25,181 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:25,184 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:25,185 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:26,279 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35953
2023-11-03 05:25:26,279 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35953
2023-11-03 05:25:26,279 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38847
2023-11-03 05:25:26,279 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:26,279 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:26,279 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:26,280 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:26,280 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tngy75__
2023-11-03 05:25:26,280 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2e91dd28-09c9-4702-b11a-600ea4002705
2023-11-03 05:25:26,860 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e795e1da-4b1f-4799-86b5-25f9207fe339
2023-11-03 05:25:26,861 - distributed.worker - INFO - Starting Worker plugin PreImport-a1493d33-c239-450b-8ac9-e3fbd93fba74
2023-11-03 05:25:26,861 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:26,896 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35953', status: init, memory: 0, processing: 0>
2023-11-03 05:25:26,899 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35953
2023-11-03 05:25:26,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51400
2023-11-03 05:25:26,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:26,901 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:26,901 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:26,903 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:27,094 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42181
2023-11-03 05:25:27,095 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42181
2023-11-03 05:25:27,095 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42607
2023-11-03 05:25:27,095 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,095 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,096 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:27,096 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:27,096 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ldmym86h
2023-11-03 05:25:27,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3db78bb0-d5ae-423d-a9ba-2c1a39922c0a
2023-11-03 05:25:27,237 - distributed.worker - INFO - Starting Worker plugin PreImport-26667c3e-4459-4cf5-957c-626767d3901c
2023-11-03 05:25:27,238 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cca4f101-ab8b-4943-aa1b-b35718df6a08
2023-11-03 05:25:27,239 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,272 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42181', status: init, memory: 0, processing: 0>
2023-11-03 05:25:27,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42181
2023-11-03 05:25:27,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51404
2023-11-03 05:25:27,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:27,275 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,275 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:27,696 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39161
2023-11-03 05:25:27,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39161
2023-11-03 05:25:27,698 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43499
2023-11-03 05:25:27,698 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,698 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,698 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:27,698 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:27,698 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l0wxeceg
2023-11-03 05:25:27,699 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a9544d2-dc69-4c3c-bf72-1b2a0e030a81
2023-11-03 05:25:27,700 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a6da50a6-7816-4527-907c-7e88b990c294
2023-11-03 05:25:27,710 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43883
2023-11-03 05:25:27,711 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43883
2023-11-03 05:25:27,711 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34543
2023-11-03 05:25:27,711 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,711 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,711 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:27,711 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:27,711 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kzzfrfd_
2023-11-03 05:25:27,712 - distributed.worker - INFO - Starting Worker plugin RMMSetup-27b0cc21-3a24-44ec-bac3-98620eb68405
2023-11-03 05:25:27,719 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44429
2023-11-03 05:25:27,721 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44429
2023-11-03 05:25:27,721 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37993
2023-11-03 05:25:27,721 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,721 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,721 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:27,721 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:27,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z0lzyva0
2023-11-03 05:25:27,723 - distributed.worker - INFO - Starting Worker plugin RMMSetup-671138ac-dc20-4d70-a78b-bd598359dd95
2023-11-03 05:25:27,727 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32877
2023-11-03 05:25:27,728 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32877
2023-11-03 05:25:27,728 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45455
2023-11-03 05:25:27,728 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,728 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,728 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:27,728 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:27,728 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bgjn9g6v
2023-11-03 05:25:27,729 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eebac7d4-867b-4223-adff-562e820086f5
2023-11-03 05:25:27,729 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a0a40db-4c9d-460c-92c5-6056636c3698
2023-11-03 05:25:27,727 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41371
2023-11-03 05:25:27,729 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41371
2023-11-03 05:25:27,729 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45641
2023-11-03 05:25:27,729 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,729 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,730 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:27,730 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:27,730 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n388z40v
2023-11-03 05:25:27,731 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a29def67-4e26-4594-8e8f-b479e661ce67
2023-11-03 05:25:27,732 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38209
2023-11-03 05:25:27,734 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38209
2023-11-03 05:25:27,734 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42855
2023-11-03 05:25:27,734 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,734 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,734 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:27,734 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:27,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fefqf9hz
2023-11-03 05:25:27,736 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b386519-a3e5-4e08-bc52-45e72c62108b
2023-11-03 05:25:27,855 - distributed.worker - INFO - Starting Worker plugin PreImport-0128b9e5-2c8d-4ad3-996e-3b83413b41c7
2023-11-03 05:25:27,855 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,857 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9822add9-059e-49cc-a04d-7652bf314553
2023-11-03 05:25:27,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca46ba56-bb87-4150-9581-134ab57a46b1
2023-11-03 05:25:27,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f354bef-b8bc-4662-85df-00e23d96c29a
2023-11-03 05:25:27,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a60688e9-5f43-4f47-b130-3c8bb3c3b1a8
2023-11-03 05:25:27,858 - distributed.worker - INFO - Starting Worker plugin PreImport-b129c4e7-c9a2-4ddd-8518-290decf333a5
2023-11-03 05:25:27,858 - distributed.worker - INFO - Starting Worker plugin PreImport-825b5701-86ab-4cf6-9aed-743427f5f9c5
2023-11-03 05:25:27,858 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,858 - distributed.worker - INFO - Starting Worker plugin PreImport-8078299e-2a6e-4de4-ab3f-bbfbd1076999
2023-11-03 05:25:27,858 - distributed.worker - INFO - Starting Worker plugin PreImport-7580b963-eae1-48e4-898c-711ac41bbaeb
2023-11-03 05:25:27,858 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,858 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,858 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,859 - distributed.worker - INFO - Starting Worker plugin PreImport-f1da3f7a-b34b-4db5-ba75-3ccedd6e022b
2023-11-03 05:25:27,860 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,886 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32877', status: init, memory: 0, processing: 0>
2023-11-03 05:25:27,887 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32877
2023-11-03 05:25:27,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51422
2023-11-03 05:25:27,888 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41371', status: init, memory: 0, processing: 0>
2023-11-03 05:25:27,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:27,888 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41371
2023-11-03 05:25:27,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51436
2023-11-03 05:25:27,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,889 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,889 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38209', status: init, memory: 0, processing: 0>
2023-11-03 05:25:27,890 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38209
2023-11-03 05:25:27,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51432
2023-11-03 05:25:27,890 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:27,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:27,891 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,891 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,891 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39161', status: init, memory: 0, processing: 0>
2023-11-03 05:25:27,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:27,892 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39161
2023-11-03 05:25:27,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51418
2023-11-03 05:25:27,892 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,892 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:27,893 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:27,894 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43883', status: init, memory: 0, processing: 0>
2023-11-03 05:25:27,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:27,894 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43883
2023-11-03 05:25:27,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51452
2023-11-03 05:25:27,894 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,895 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44429', status: init, memory: 0, processing: 0>
2023-11-03 05:25:27,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:27,896 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44429
2023-11-03 05:25:27,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51460
2023-11-03 05:25:27,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:27,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,896 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,897 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:27,898 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:27,898 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:27,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:27,899 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:28,003 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:28,003 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:28,003 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:28,003 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:28,004 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:28,004 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:28,004 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:28,004 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:28,008 - distributed.scheduler - INFO - Remove client Client-622745ad-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:28,008 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51372; closing.
2023-11-03 05:25:28,009 - distributed.scheduler - INFO - Remove client Client-622745ad-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:28,009 - distributed.scheduler - INFO - Close client connection: Client-622745ad-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:28,010 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36705'. Reason: nanny-close
2023-11-03 05:25:28,010 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:28,011 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34291'. Reason: nanny-close
2023-11-03 05:25:28,011 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:28,011 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42181. Reason: nanny-close
2023-11-03 05:25:28,012 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38467'. Reason: nanny-close
2023-11-03 05:25:28,012 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:28,012 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43883. Reason: nanny-close
2023-11-03 05:25:28,012 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39431'. Reason: nanny-close
2023-11-03 05:25:28,013 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:28,013 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39161. Reason: nanny-close
2023-11-03 05:25:28,013 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35011'. Reason: nanny-close
2023-11-03 05:25:28,013 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:28,014 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44429. Reason: nanny-close
2023-11-03 05:25:28,014 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40655'. Reason: nanny-close
2023-11-03 05:25:28,014 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:28,014 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51404; closing.
2023-11-03 05:25:28,014 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:28,014 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42181', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989128.0147443')
2023-11-03 05:25:28,014 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41371. Reason: nanny-close
2023-11-03 05:25:28,015 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38195'. Reason: nanny-close
2023-11-03 05:25:28,015 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:28,015 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:28,015 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35953. Reason: nanny-close
2023-11-03 05:25:28,015 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39203'. Reason: nanny-close
2023-11-03 05:25:28,015 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:28,016 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32877. Reason: nanny-close
2023-11-03 05:25:28,016 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:28,016 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:28,016 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51418; closing.
2023-11-03 05:25:28,017 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51452; closing.
2023-11-03 05:25:28,017 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:28,017 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:28,017 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38209. Reason: nanny-close
2023-11-03 05:25:28,017 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:28,017 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39161', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989128.017902')
2023-11-03 05:25:28,018 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:28,018 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43883', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989128.0183237')
2023-11-03 05:25:28,018 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:28,018 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51460; closing.
2023-11-03 05:25:28,018 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:28,019 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:28,019 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44429', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989128.0195556')
2023-11-03 05:25:28,019 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:28,019 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51436; closing.
2023-11-03 05:25:28,020 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:28,020 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:28,020 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:28,020 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41371', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989128.020804')
2023-11-03 05:25:28,021 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:28,021 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51400; closing.
2023-11-03 05:25:28,021 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51422; closing.
2023-11-03 05:25:28,022 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35953', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989128.0219457')
2023-11-03 05:25:28,022 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32877', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989128.022351')
2023-11-03 05:25:28,022 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51432; closing.
2023-11-03 05:25:28,023 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38209', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989128.023325')
2023-11-03 05:25:28,023 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:28,023 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51432>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-03 05:25:29,527 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:25:29,527 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:25:29,528 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:25:29,529 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-03 05:25:29,529 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-11-03 05:25:31,541 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:31,545 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:25:31,548 - distributed.scheduler - INFO - State start
2023-11-03 05:25:31,570 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:31,572 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-03 05:25:31,572 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:25:31,572 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:25:31,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39307'
2023-11-03 05:25:31,695 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35425'
2023-11-03 05:25:31,704 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35967'
2023-11-03 05:25:31,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46087'
2023-11-03 05:25:31,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33933'
2023-11-03 05:25:31,731 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38727'
2023-11-03 05:25:31,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41319'
2023-11-03 05:25:31,749 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45225'
2023-11-03 05:25:31,924 - distributed.scheduler - INFO - Receive client connection: Client-685b8933-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:25:31,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53454
2023-11-03 05:25:33,431 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:33,431 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:33,435 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:33,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:33,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:33,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:33,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:33,545 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:33,545 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:33,548 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:33,548 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:33,549 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:33,747 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:33,747 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:33,747 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:33,747 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:33,751 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:33,751 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:33,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:33,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:33,775 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:33,775 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:33,775 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:33,780 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:35,316 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44797
2023-11-03 05:25:35,317 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44797
2023-11-03 05:25:35,317 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41291
2023-11-03 05:25:35,317 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:35,317 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:35,317 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:35,317 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:35,317 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zth0m7q4
2023-11-03 05:25:35,318 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4197dd71-197e-43ce-a201-74558ad857f0
2023-11-03 05:25:35,358 - distributed.worker - INFO - Starting Worker plugin PreImport-c21d54db-eb45-486d-8b09-9da173acb569
2023-11-03 05:25:35,358 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11c3ebaa-4621-41cc-bbe0-c71a32bcc99f
2023-11-03 05:25:35,359 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:35,385 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44797', status: init, memory: 0, processing: 0>
2023-11-03 05:25:35,387 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44797
2023-11-03 05:25:35,387 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53530
2023-11-03 05:25:35,388 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:35,389 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:35,389 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:35,391 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:35,782 - distributed.scheduler - INFO - Receive client connection: Client-6728caa0-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:35,782 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53540
2023-11-03 05:25:36,545 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42321
2023-11-03 05:25:36,546 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42321
2023-11-03 05:25:36,546 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34365
2023-11-03 05:25:36,546 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,546 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,546 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:36,546 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:36,546 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w885cs_w
2023-11-03 05:25:36,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb749e2f-6313-4eb5-9dcd-b20ce4e13304
2023-11-03 05:25:36,556 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45043
2023-11-03 05:25:36,557 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45043
2023-11-03 05:25:36,557 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42657
2023-11-03 05:25:36,557 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,557 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,557 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:36,557 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:36,557 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ido60xl2
2023-11-03 05:25:36,558 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1e9d8c28-82e7-4890-a577-2ff091859bc5
2023-11-03 05:25:36,558 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af06a39e-7856-473e-aa7c-fb3b866e140d
2023-11-03 05:25:36,559 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36815
2023-11-03 05:25:36,560 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36815
2023-11-03 05:25:36,560 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42949
2023-11-03 05:25:36,560 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,560 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,560 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:36,560 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:36,560 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wi8tqtit
2023-11-03 05:25:36,560 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36831
2023-11-03 05:25:36,561 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36831
2023-11-03 05:25:36,561 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34045
2023-11-03 05:25:36,561 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,561 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,561 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a39bda00-2807-43e4-bad3-80398b24d2ae
2023-11-03 05:25:36,561 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:36,561 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:36,561 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xyt1nbth
2023-11-03 05:25:36,562 - distributed.worker - INFO - Starting Worker plugin PreImport-c5348fe4-9213-400b-a47b-8ecc73ecc360
2023-11-03 05:25:36,562 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-41798b7d-61b6-4d09-a721-b296bde7fb2e
2023-11-03 05:25:36,562 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b25a623-901f-44bd-9e58-3b11429da0e0
2023-11-03 05:25:36,566 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45345
2023-11-03 05:25:36,567 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45345
2023-11-03 05:25:36,567 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45913
2023-11-03 05:25:36,567 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,567 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,567 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:36,568 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:36,568 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ssvnqy_9
2023-11-03 05:25:36,568 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1463650-556f-4a9d-8385-29651b5c8fa8
2023-11-03 05:25:36,568 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41915
2023-11-03 05:25:36,569 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41915
2023-11-03 05:25:36,569 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39413
2023-11-03 05:25:36,568 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33661
2023-11-03 05:25:36,569 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,569 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33661
2023-11-03 05:25:36,569 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,569 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37739
2023-11-03 05:25:36,569 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,569 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,569 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:36,569 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:36,569 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5_a15_zp
2023-11-03 05:25:36,569 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:36,569 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:36,570 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dupybkn4
2023-11-03 05:25:36,570 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ffe9f23d-62e6-4ea4-9f9b-1d50f2b495db
2023-11-03 05:25:36,570 - distributed.worker - INFO - Starting Worker plugin PreImport-b6345cd8-78dd-450f-99db-345fe9ab53d2
2023-11-03 05:25:36,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a1ca0ef2-4526-49f2-8e83-bc38496022d3
2023-11-03 05:25:36,570 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9de257fa-3007-4c74-885b-b7319da2ef20
2023-11-03 05:25:36,629 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a5868c4-a7b1-4cea-96ce-e0cb5b58dec6
2023-11-03 05:25:36,629 - distributed.worker - INFO - Starting Worker plugin PreImport-4ea05f14-b7d5-4825-a8ca-9dd769509a7c
2023-11-03 05:25:36,630 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,633 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7a8a1bcc-96d5-45ab-8728-0558c883230b
2023-11-03 05:25:36,635 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,635 - distributed.worker - INFO - Starting Worker plugin PreImport-ddcb0236-98a7-4c18-ad4d-c3de3457831c
2023-11-03 05:25:36,635 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,638 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58f13505-c4df-4bd4-b472-8bf7d35ca6cc
2023-11-03 05:25:36,638 - distributed.worker - INFO - Starting Worker plugin PreImport-bfb03978-6087-4505-a7b5-c44c171e3f2e
2023-11-03 05:25:36,638 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,639 - distributed.worker - INFO - Starting Worker plugin PreImport-1a8d21cf-cb4e-4a97-b4de-df2c8b3ccd5d
2023-11-03 05:25:36,639 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,640 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,644 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99c4856c-2ef6-4fb3-9030-e38047167a12
2023-11-03 05:25:36,647 - distributed.worker - INFO - Starting Worker plugin PreImport-b7c4d559-1d62-4d46-94da-a19a7e698c91
2023-11-03 05:25:36,647 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,655 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42321', status: init, memory: 0, processing: 0>
2023-11-03 05:25:36,656 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42321
2023-11-03 05:25:36,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53560
2023-11-03 05:25:36,657 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:36,658 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,658 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:36,660 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36831', status: init, memory: 0, processing: 0>
2023-11-03 05:25:36,660 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36831
2023-11-03 05:25:36,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53574
2023-11-03 05:25:36,661 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:36,662 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45345', status: init, memory: 0, processing: 0>
2023-11-03 05:25:36,662 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,662 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,662 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45345
2023-11-03 05:25:36,663 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53588
2023-11-03 05:25:36,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:36,664 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45043', status: init, memory: 0, processing: 0>
2023-11-03 05:25:36,664 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:36,664 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45043
2023-11-03 05:25:36,664 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,664 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53592
2023-11-03 05:25:36,664 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,665 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:36,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:36,666 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,666 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,667 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:36,668 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36815', status: init, memory: 0, processing: 0>
2023-11-03 05:25:36,669 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36815
2023-11-03 05:25:36,669 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53578
2023-11-03 05:25:36,670 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:36,671 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,671 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,673 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:36,674 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33661', status: init, memory: 0, processing: 0>
2023-11-03 05:25:36,675 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33661
2023-11-03 05:25:36,675 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53594
2023-11-03 05:25:36,676 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:36,677 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,677 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,680 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:36,680 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41915', status: init, memory: 0, processing: 0>
2023-11-03 05:25:36,680 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41915
2023-11-03 05:25:36,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53598
2023-11-03 05:25:36,682 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:36,683 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:36,683 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:36,685 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:36,708 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:36,709 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:36,709 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:36,709 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:36,709 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:36,709 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:36,709 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:36,709 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:36,715 - distributed.scheduler - INFO - Remove client Client-6728caa0-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:36,715 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53540; closing.
2023-11-03 05:25:36,715 - distributed.scheduler - INFO - Remove client Client-6728caa0-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:36,716 - distributed.scheduler - INFO - Close client connection: Client-6728caa0-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:36,717 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39307'. Reason: nanny-close
2023-11-03 05:25:36,717 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:36,719 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35425'. Reason: nanny-close
2023-11-03 05:25:36,720 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44797. Reason: nanny-close
2023-11-03 05:25:36,720 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:36,720 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35967'. Reason: nanny-close
2023-11-03 05:25:36,720 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:36,721 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33661. Reason: nanny-close
2023-11-03 05:25:36,721 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46087'. Reason: nanny-close
2023-11-03 05:25:36,721 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:36,721 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42321. Reason: nanny-close
2023-11-03 05:25:36,721 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33933'. Reason: nanny-close
2023-11-03 05:25:36,722 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:36,722 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:36,722 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45043. Reason: nanny-close
2023-11-03 05:25:36,722 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53530; closing.
2023-11-03 05:25:36,722 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38727'. Reason: nanny-close
2023-11-03 05:25:36,722 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:36,722 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44797', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989136.7226562')
2023-11-03 05:25:36,722 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36815. Reason: nanny-close
2023-11-03 05:25:36,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41319'. Reason: nanny-close
2023-11-03 05:25:36,723 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:36,723 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53594; closing.
2023-11-03 05:25:36,723 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:36,723 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:36,723 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41915. Reason: nanny-close
2023-11-03 05:25:36,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45225'. Reason: nanny-close
2023-11-03 05:25:36,723 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:36,723 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33661', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989136.7237542')
2023-11-03 05:25:36,723 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:36,723 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45345. Reason: nanny-close
2023-11-03 05:25:36,723 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:36,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36831. Reason: nanny-close
2023-11-03 05:25:36,724 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:36,724 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53560; closing.
2023-11-03 05:25:36,724 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:36,725 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:36,725 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:36,725 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:36,725 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:36,726 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:36,725 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53594>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-03 05:25:36,727 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:36,727 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42321', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989136.727259')
2023-11-03 05:25:36,727 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:36,727 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53592; closing.
2023-11-03 05:25:36,727 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:36,728 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:36,728 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45043', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989136.7282279')
2023-11-03 05:25:36,728 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53578; closing.
2023-11-03 05:25:36,729 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36815', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989136.7291582')
2023-11-03 05:25:36,729 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53598; closing.
2023-11-03 05:25:36,729 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53588; closing.
2023-11-03 05:25:36,730 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41915', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989136.730165')
2023-11-03 05:25:36,730 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45345', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989136.7305353')
2023-11-03 05:25:36,730 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53574; closing.
2023-11-03 05:25:36,731 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36831', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989136.7312622')
2023-11-03 05:25:36,731 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:36,731 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53574>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-03 05:25:38,346 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44757', status: init, memory: 0, processing: 0>
2023-11-03 05:25:38,346 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44757
2023-11-03 05:25:38,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53604
2023-11-03 05:25:38,585 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:25:38,585 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:25:38,585 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:25:38,586 - distributed.core - INFO - Connection to tcp://127.0.0.1:53604 has been closed.
2023-11-03 05:25:38,587 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44757', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989138.586941')
2023-11-03 05:25:38,587 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:38,589 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-03 05:25:38,590 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-11-03 05:25:40,754 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:40,758 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:25:40,761 - distributed.scheduler - INFO - State start
2023-11-03 05:25:40,783 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:40,784 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-03 05:25:40,785 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:25:40,785 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:25:40,898 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33413'
2023-11-03 05:25:40,911 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46721'
2023-11-03 05:25:40,923 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39319'
2023-11-03 05:25:40,933 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39307'
2023-11-03 05:25:40,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39917'
2023-11-03 05:25:40,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43081'
2023-11-03 05:25:40,954 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35619'
2023-11-03 05:25:40,963 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36155'
2023-11-03 05:25:41,077 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45441', status: init, memory: 0, processing: 0>
2023-11-03 05:25:41,088 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45441
2023-11-03 05:25:41,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40362
2023-11-03 05:25:41,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40362; closing.
2023-11-03 05:25:41,137 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45441', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989141.1374917')
2023-11-03 05:25:41,137 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:41,206 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41673', status: init, memory: 0, processing: 0>
2023-11-03 05:25:41,206 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41673
2023-11-03 05:25:41,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40366
2023-11-03 05:25:41,234 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40366; closing.
2023-11-03 05:25:41,235 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41673', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989141.2351363')
2023-11-03 05:25:41,235 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:41,482 - distributed.scheduler - INFO - Receive client connection: Client-685b8933-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:25:41,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40370
2023-11-03 05:25:41,548 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37523', status: init, memory: 0, processing: 0>
2023-11-03 05:25:41,548 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37523
2023-11-03 05:25:41,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40372
2023-11-03 05:25:41,571 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41255', status: init, memory: 0, processing: 0>
2023-11-03 05:25:41,572 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41255
2023-11-03 05:25:41,572 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40386
2023-11-03 05:25:41,591 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40372; closing.
2023-11-03 05:25:41,591 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37523', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989141.5915182')
2023-11-03 05:25:41,592 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40386; closing.
2023-11-03 05:25:41,592 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41255', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989141.5928504')
2023-11-03 05:25:41,593 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:41,818 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41059', status: init, memory: 0, processing: 0>
2023-11-03 05:25:41,819 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41059
2023-11-03 05:25:41,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40406
2023-11-03 05:25:41,845 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40406; closing.
2023-11-03 05:25:41,846 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41059', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989141.846115')
2023-11-03 05:25:41,846 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:42,626 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:42,626 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:42,630 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:42,704 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:42,704 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:42,707 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:42,747 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:42,748 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:42,751 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:42,895 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:42,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:42,899 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:42,921 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:42,921 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:42,925 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:42,930 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:42,930 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:42,933 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:42,934 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:42,934 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:42,937 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:42,940 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:42,940 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:42,944 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:44,371 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33725
2023-11-03 05:25:44,372 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33725
2023-11-03 05:25:44,372 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43117
2023-11-03 05:25:44,372 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:44,372 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:44,372 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:44,372 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:44,372 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i7njhrnl
2023-11-03 05:25:44,373 - distributed.worker - INFO - Starting Worker plugin RMMSetup-48b27e77-ee06-4423-8265-624cf4011fd8
2023-11-03 05:25:44,699 - distributed.worker - INFO - Starting Worker plugin PreImport-122dd465-9dcc-4d64-a691-be9a6723c133
2023-11-03 05:25:44,700 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ad99120-d07b-4d7d-a213-fc779a8c7e29
2023-11-03 05:25:44,700 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:44,727 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33725', status: init, memory: 0, processing: 0>
2023-11-03 05:25:44,728 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33725
2023-11-03 05:25:44,728 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40410
2023-11-03 05:25:44,729 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:44,730 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:44,730 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:44,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:45,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46493
2023-11-03 05:25:45,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46493
2023-11-03 05:25:45,164 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34893
2023-11-03 05:25:45,164 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,164 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,164 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:45,164 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:45,164 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7691wjw4
2023-11-03 05:25:45,162 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41659
2023-11-03 05:25:45,164 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41659
2023-11-03 05:25:45,164 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40763
2023-11-03 05:25:45,164 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,164 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74c706f2-25e1-4733-8516-97affd9fd671
2023-11-03 05:25:45,165 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:45,165 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:45,165 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i1vr5wii
2023-11-03 05:25:45,166 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3548dc18-4f72-42f2-9104-c9d95ecd31c6
2023-11-03 05:25:45,166 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a75dbe84-10cb-44f5-86c7-2b09c6547bcf
2023-11-03 05:25:45,332 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46115
2023-11-03 05:25:45,333 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46115
2023-11-03 05:25:45,333 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37865
2023-11-03 05:25:45,333 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,333 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,333 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:45,333 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:45,333 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ptyczskc
2023-11-03 05:25:45,334 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb373bb0-9ed9-4829-8693-2f3c17b247d6
2023-11-03 05:25:45,333 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34451
2023-11-03 05:25:45,334 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34451
2023-11-03 05:25:45,334 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42953
2023-11-03 05:25:45,334 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,334 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,334 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:45,335 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:45,335 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xkodv24g
2023-11-03 05:25:45,334 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44839
2023-11-03 05:25:45,335 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44839
2023-11-03 05:25:45,335 - distributed.worker - INFO - Starting Worker plugin RMMSetup-69005f4e-9174-418d-9299-afa86fe60489
2023-11-03 05:25:45,335 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41113
2023-11-03 05:25:45,335 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,335 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,335 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:45,336 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:45,336 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bb30f7s1
2023-11-03 05:25:45,336 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51c98cce-5727-413e-bffe-8049f08010f8
2023-11-03 05:25:45,336 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34985
2023-11-03 05:25:45,336 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34985
2023-11-03 05:25:45,337 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43875
2023-11-03 05:25:45,337 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,337 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,337 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:45,337 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:45,337 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z06pw16h
2023-11-03 05:25:45,337 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bc367dc1-b661-4dd2-8b12-1a38e70450a5
2023-11-03 05:25:45,338 - distributed.worker - INFO - Starting Worker plugin RMMSetup-138577a4-993c-4ea3-aea0-2d35a0b7a201
2023-11-03 05:25:45,339 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40221
2023-11-03 05:25:45,339 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40221
2023-11-03 05:25:45,339 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37577
2023-11-03 05:25:45,340 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,340 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,340 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:45,340 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:45,340 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r3ax7_dm
2023-11-03 05:25:45,340 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0605966f-e15b-4111-84f6-b8a0842ad9f5
2023-11-03 05:25:45,363 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42209db3-8331-48b6-a5f3-deb66a96f50e
2023-11-03 05:25:45,364 - distributed.worker - INFO - Starting Worker plugin PreImport-fb455552-8d48-45fd-9ff9-211afb6d6eb2
2023-11-03 05:25:45,364 - distributed.worker - INFO - Starting Worker plugin PreImport-0bd91394-0c9b-42a4-a85c-5e427089e31f
2023-11-03 05:25:45,364 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,364 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,394 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46493', status: init, memory: 0, processing: 0>
2023-11-03 05:25:45,395 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46493
2023-11-03 05:25:45,395 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40414
2023-11-03 05:25:45,396 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:45,397 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,397 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,398 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41659', status: init, memory: 0, processing: 0>
2023-11-03 05:25:45,398 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41659
2023-11-03 05:25:45,398 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40416
2023-11-03 05:25:45,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:45,399 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:45,400 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,400 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:45,408 - distributed.scheduler - INFO - Receive client connection: Client-6cace4ab-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:45,408 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40424
2023-11-03 05:25:45,523 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7099ec96-7e32-4b24-b279-1c2eaff50cbf
2023-11-03 05:25:45,523 - distributed.worker - INFO - Starting Worker plugin PreImport-a34ced76-b003-4292-b3c9-e9b994bba41e
2023-11-03 05:25:45,524 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,526 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d874a0fb-a2a0-4681-90e2-2c6de8300f8a
2023-11-03 05:25:45,527 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-44bc5d78-1d3b-4c53-b463-fb0ab0acd387
2023-11-03 05:25:45,528 - distributed.worker - INFO - Starting Worker plugin PreImport-ff357d85-ab6e-4841-b108-05d4bebb5550
2023-11-03 05:25:45,527 - distributed.worker - INFO - Starting Worker plugin PreImport-b1d798fb-1dd4-426e-97d3-390bf560be71
2023-11-03 05:25:45,528 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,528 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dc6b496d-6e69-45de-a975-bed3ba9c993a
2023-11-03 05:25:45,532 - distributed.worker - INFO - Starting Worker plugin PreImport-76ab9e7a-b847-41d2-9121-f54b44e650ee
2023-11-03 05:25:45,533 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,536 - distributed.worker - INFO - Starting Worker plugin PreImport-cb8ef11d-54b9-4f9d-975e-ab216349fb80
2023-11-03 05:25:45,536 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,554 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40221', status: init, memory: 0, processing: 0>
2023-11-03 05:25:45,555 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40221
2023-11-03 05:25:45,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40442
2023-11-03 05:25:45,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:45,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,556 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,557 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34985', status: init, memory: 0, processing: 0>
2023-11-03 05:25:45,557 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34985
2023-11-03 05:25:45,557 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40460
2023-11-03 05:25:45,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:45,558 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:45,559 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,559 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,561 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:45,563 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44839', status: init, memory: 0, processing: 0>
2023-11-03 05:25:45,564 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44839
2023-11-03 05:25:45,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40434
2023-11-03 05:25:45,565 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:45,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34451', status: init, memory: 0, processing: 0>
2023-11-03 05:25:45,567 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,567 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,567 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34451
2023-11-03 05:25:45,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40456
2023-11-03 05:25:45,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:45,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,569 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,570 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:45,572 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:45,573 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46115', status: init, memory: 0, processing: 0>
2023-11-03 05:25:45,573 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46115
2023-11-03 05:25:45,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40474
2023-11-03 05:25:45,574 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:45,575 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:45,575 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:45,578 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:45,622 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:45,622 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:45,622 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:45,622 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:45,622 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:45,623 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:45,624 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:45,624 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:25:45,633 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:45,633 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:45,633 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:45,633 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:45,633 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:45,634 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:45,634 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:45,634 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:45,640 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:45,641 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:45,643 - distributed.scheduler - INFO - Remove client Client-6cace4ab-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:45,644 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40424; closing.
2023-11-03 05:25:45,644 - distributed.scheduler - INFO - Remove client Client-6cace4ab-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:45,644 - distributed.scheduler - INFO - Close client connection: Client-6cace4ab-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:45,645 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33413'. Reason: nanny-close
2023-11-03 05:25:45,645 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:45,646 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46721'. Reason: nanny-close
2023-11-03 05:25:45,646 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:45,646 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33725. Reason: nanny-close
2023-11-03 05:25:45,647 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39319'. Reason: nanny-close
2023-11-03 05:25:45,647 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:45,647 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44839. Reason: nanny-close
2023-11-03 05:25:45,647 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39307'. Reason: nanny-close
2023-11-03 05:25:45,648 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:45,648 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41659. Reason: nanny-close
2023-11-03 05:25:45,648 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39917'. Reason: nanny-close
2023-11-03 05:25:45,648 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:45,648 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46493. Reason: nanny-close
2023-11-03 05:25:45,648 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:45,648 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40410; closing.
2023-11-03 05:25:45,649 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43081'. Reason: nanny-close
2023-11-03 05:25:45,649 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:45,649 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33725', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989145.649351')
2023-11-03 05:25:45,649 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35619'. Reason: nanny-close
2023-11-03 05:25:45,649 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46115. Reason: nanny-close
2023-11-03 05:25:45,649 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:45,650 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34451. Reason: nanny-close
2023-11-03 05:25:45,650 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:45,650 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36155'. Reason: nanny-close
2023-11-03 05:25:45,650 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:45,650 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:45,650 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:45,650 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:45,650 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34985. Reason: nanny-close
2023-11-03 05:25:45,651 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40416; closing.
2023-11-03 05:25:45,651 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40221. Reason: nanny-close
2023-11-03 05:25:45,651 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:45,652 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41659', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989145.6520145')
2023-11-03 05:25:45,652 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:45,652 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40414; closing.
2023-11-03 05:25:45,652 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40434; closing.
2023-11-03 05:25:45,653 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:45,653 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:45,653 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:45,653 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:45,653 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46493', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989145.653276')
2023-11-03 05:25:45,653 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:45,653 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44839', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989145.6536584')
2023-11-03 05:25:45,654 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:45,654 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:45,654 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40474; closing.
2023-11-03 05:25:45,654 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40456; closing.
2023-11-03 05:25:45,655 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:45,655 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40460; closing.
2023-11-03 05:25:45,655 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989145.6555567')
2023-11-03 05:25:45,655 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:45,656 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34451', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989145.6559849')
2023-11-03 05:25:45,656 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34985', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989145.656392')
2023-11-03 05:25:45,656 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40442; closing.
2023-11-03 05:25:45,657 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40221', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989145.6571293')
2023-11-03 05:25:45,657 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:47,263 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:25:47,263 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:25:47,264 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:25:47,266 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-03 05:25:47,266 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-11-03 05:25:49,443 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:49,447 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:25:49,450 - distributed.scheduler - INFO - State start
2023-11-03 05:25:49,477 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:49,478 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-03 05:25:49,478 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:25:49,479 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:25:49,764 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44375'
2023-11-03 05:25:49,783 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39405'
2023-11-03 05:25:49,803 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32987'
2023-11-03 05:25:49,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36329'
2023-11-03 05:25:49,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32781'
2023-11-03 05:25:49,836 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44745'
2023-11-03 05:25:49,850 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46397'
2023-11-03 05:25:49,862 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43905'
2023-11-03 05:25:50,050 - distributed.scheduler - INFO - Receive client connection: Client-71c50d8b-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:50,066 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34200
2023-11-03 05:25:50,275 - distributed.scheduler - INFO - Receive client connection: Client-685b8933-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:25:50,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34214
2023-11-03 05:25:50,634 - distributed.scheduler - INFO - Remove client Client-685b8933-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:25:50,634 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34214; closing.
2023-11-03 05:25:50,635 - distributed.scheduler - INFO - Remove client Client-685b8933-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:25:50,635 - distributed.scheduler - INFO - Close client connection: Client-685b8933-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:25:51,134 - distributed.scheduler - INFO - Receive client connection: Client-73ceae98-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:25:51,134 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34258
2023-11-03 05:25:51,671 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:51,671 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:51,675 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:51,682 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:51,682 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:51,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:51,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:51,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:51,693 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:51,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:51,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:51,975 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:51,975 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:51,979 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:51,981 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:51,996 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:51,997 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:51,999 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:51,999 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:52,001 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:52,001 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:25:52,001 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:25:52,004 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:52,006 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:25:53,686 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39675
2023-11-03 05:25:53,686 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39675
2023-11-03 05:25:53,686 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38847
2023-11-03 05:25:53,686 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:53,686 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:53,687 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:53,687 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:53,687 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aiwk1hep
2023-11-03 05:25:53,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-05cc4d06-3abb-4b08-9a26-7b16d8d71750
2023-11-03 05:25:53,687 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1514c97-35d7-4474-a569-a5afbb74552f
2023-11-03 05:25:53,853 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35979
2023-11-03 05:25:53,855 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35979
2023-11-03 05:25:53,855 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36875
2023-11-03 05:25:53,855 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:53,855 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:53,855 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:53,855 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:53,855 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5tioer54
2023-11-03 05:25:53,856 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c15dd522-83d0-4c1d-a8f5-795b2f51ddf4
2023-11-03 05:25:53,859 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45469
2023-11-03 05:25:53,860 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45469
2023-11-03 05:25:53,860 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41177
2023-11-03 05:25:53,860 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:53,860 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:53,860 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:53,860 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:53,860 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s260hqdw
2023-11-03 05:25:53,861 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b75d15f-b78d-4e84-9263-185cff8a59cc
2023-11-03 05:25:54,060 - distributed.worker - INFO - Starting Worker plugin PreImport-7700ab83-5c2f-4429-ba22-a151a87c372e
2023-11-03 05:25:54,061 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,091 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39675', status: init, memory: 0, processing: 0>
2023-11-03 05:25:54,093 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39675
2023-11-03 05:25:54,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34366
2023-11-03 05:25:54,094 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:54,095 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,096 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:54,140 - distributed.worker - INFO - Starting Worker plugin PreImport-c54aed57-c542-4181-a81c-8c91b311d31d
2023-11-03 05:25:54,141 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95b4ef74-bb73-43d9-94fe-1be52e3079f2
2023-11-03 05:25:54,141 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,142 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab0c3e31-a10e-4934-889c-06fe3d473bb0
2023-11-03 05:25:54,143 - distributed.worker - INFO - Starting Worker plugin PreImport-e6225ee8-7060-4555-b08c-7dcd60b21306
2023-11-03 05:25:54,143 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,171 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35979', status: init, memory: 0, processing: 0>
2023-11-03 05:25:54,171 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35979
2023-11-03 05:25:54,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34376
2023-11-03 05:25:54,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:54,174 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,174 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45469', status: init, memory: 0, processing: 0>
2023-11-03 05:25:54,174 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,175 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45469
2023-11-03 05:25:54,175 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34388
2023-11-03 05:25:54,176 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:54,176 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:54,176 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,177 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,178 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:54,581 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41883
2023-11-03 05:25:54,582 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41883
2023-11-03 05:25:54,582 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35741
2023-11-03 05:25:54,582 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,582 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,582 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:54,582 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:54,582 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zrky9odq
2023-11-03 05:25:54,583 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f989f70-283b-4fad-920c-b80500c6e49b
2023-11-03 05:25:54,583 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c94d1b23-cadc-4a9d-9fc0-167a189b23b9
2023-11-03 05:25:54,624 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40513
2023-11-03 05:25:54,625 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40513
2023-11-03 05:25:54,625 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43357
2023-11-03 05:25:54,625 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,625 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,625 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:54,625 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:54,625 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q62in34o
2023-11-03 05:25:54,626 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6252150-6a21-4ff3-a633-0be130b5f95a
2023-11-03 05:25:54,628 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38939
2023-11-03 05:25:54,629 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38939
2023-11-03 05:25:54,629 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40203
2023-11-03 05:25:54,629 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,629 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,629 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:54,629 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:54,629 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9e78wgcb
2023-11-03 05:25:54,630 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21cd88ab-4487-4b8a-bb95-f43df9521338
2023-11-03 05:25:54,632 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39685
2023-11-03 05:25:54,633 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39685
2023-11-03 05:25:54,633 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35579
2023-11-03 05:25:54,633 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,633 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,633 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:54,633 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:54,633 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v6mzsanm
2023-11-03 05:25:54,633 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19bd86cf-893b-432d-aed7-ebc593ba40a7
2023-11-03 05:25:54,647 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43035
2023-11-03 05:25:54,649 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43035
2023-11-03 05:25:54,650 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41563
2023-11-03 05:25:54,650 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,650 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,650 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:25:54,650 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:25:54,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-464i2503
2023-11-03 05:25:54,651 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e63ebb3d-31d6-46d3-8349-02b421a85de8
2023-11-03 05:25:54,723 - distributed.worker - INFO - Starting Worker plugin PreImport-8375428f-1b73-4807-a8f0-8c8685740e8a
2023-11-03 05:25:54,724 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,765 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41883', status: init, memory: 0, processing: 0>
2023-11-03 05:25:54,766 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41883
2023-11-03 05:25:54,766 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34400
2023-11-03 05:25:54,767 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:54,768 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,768 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,770 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:54,795 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-83f490ea-13c4-46c7-b529-cc141ee6a236
2023-11-03 05:25:54,795 - distributed.worker - INFO - Starting Worker plugin PreImport-6ae9a472-a4b5-4819-869d-0be76c4d1d59
2023-11-03 05:25:54,795 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,803 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bead4819-42cb-4064-abcb-8211e2f240e9
2023-11-03 05:25:54,803 - distributed.worker - INFO - Starting Worker plugin PreImport-726e2457-74a4-451c-97cb-9b53e9fcd9ca
2023-11-03 05:25:54,803 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,805 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b1c176a3-e8a8-49a6-b18a-0496f8d58f75
2023-11-03 05:25:54,805 - distributed.worker - INFO - Starting Worker plugin PreImport-6474d08f-9103-4aed-9350-132dab518448
2023-11-03 05:25:54,805 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,813 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59a15b40-0447-403f-bed2-913f0041635c
2023-11-03 05:25:54,813 - distributed.worker - INFO - Starting Worker plugin PreImport-8965a77b-635f-4f58-a724-0b6066f94620
2023-11-03 05:25:54,813 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,822 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40513', status: init, memory: 0, processing: 0>
2023-11-03 05:25:54,822 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40513
2023-11-03 05:25:54,822 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34408
2023-11-03 05:25:54,823 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:54,824 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,824 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:54,833 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38939', status: init, memory: 0, processing: 0>
2023-11-03 05:25:54,833 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38939
2023-11-03 05:25:54,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34424
2023-11-03 05:25:54,834 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39685', status: init, memory: 0, processing: 0>
2023-11-03 05:25:54,834 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:54,835 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39685
2023-11-03 05:25:54,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34438
2023-11-03 05:25:54,835 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,835 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,836 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:54,836 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,836 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:54,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:54,845 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43035', status: init, memory: 0, processing: 0>
2023-11-03 05:25:54,846 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43035
2023-11-03 05:25:54,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34444
2023-11-03 05:25:54,847 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:25:54,848 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:25:54,848 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:25:54,850 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:25:54,873 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:54,873 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:54,873 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:54,873 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:54,873 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:54,874 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:54,874 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:54,874 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:54,885 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:54,885 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:54,885 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:54,885 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:54,886 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:54,886 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:54,886 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:54,886 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:25:54,892 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:54,894 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:25:54,896 - distributed.scheduler - INFO - Remove client Client-71c50d8b-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:54,896 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34200; closing.
2023-11-03 05:25:54,896 - distributed.scheduler - INFO - Remove client Client-71c50d8b-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:54,897 - distributed.scheduler - INFO - Close client connection: Client-71c50d8b-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:25:54,898 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44375'. Reason: nanny-close
2023-11-03 05:25:54,898 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:54,899 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39405'. Reason: nanny-close
2023-11-03 05:25:54,899 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35979. Reason: nanny-close
2023-11-03 05:25:54,899 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:54,900 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36329'. Reason: nanny-close
2023-11-03 05:25:54,900 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:54,900 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40513. Reason: nanny-close
2023-11-03 05:25:54,900 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32781'. Reason: nanny-close
2023-11-03 05:25:54,900 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:54,901 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41883. Reason: nanny-close
2023-11-03 05:25:54,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32987'. Reason: nanny-close
2023-11-03 05:25:54,901 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:54,901 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43035. Reason: nanny-close
2023-11-03 05:25:54,901 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44745'. Reason: nanny-close
2023-11-03 05:25:54,902 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:54,902 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34376; closing.
2023-11-03 05:25:54,902 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:54,902 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38939. Reason: nanny-close
2023-11-03 05:25:54,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46397'. Reason: nanny-close
2023-11-03 05:25:54,902 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35979', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989154.902543')
2023-11-03 05:25:54,902 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:54,903 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:54,903 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39685. Reason: nanny-close
2023-11-03 05:25:54,903 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43905'. Reason: nanny-close
2023-11-03 05:25:54,903 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:25:54,903 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:54,903 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39675. Reason: nanny-close
2023-11-03 05:25:54,904 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:54,904 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:54,904 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45469. Reason: nanny-close
2023-11-03 05:25:54,904 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:54,905 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:54,905 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34400; closing.
2023-11-03 05:25:54,905 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:54,905 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:54,905 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34408; closing.
2023-11-03 05:25:54,905 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:54,905 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:54,906 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:54,906 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:25:54,907 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41883', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989154.9071329')
2023-11-03 05:25:54,907 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989154.9076962')
2023-11-03 05:25:54,907 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:54,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34444; closing.
2023-11-03 05:25:54,908 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:54,908 - distributed.nanny - INFO - Worker closed
2023-11-03 05:25:54,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34424; closing.
2023-11-03 05:25:54,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43035', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989154.9090018')
2023-11-03 05:25:54,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38939', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989154.9094174')
2023-11-03 05:25:54,909 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34438; closing.
2023-11-03 05:25:54,910 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39685', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989154.9107459')
2023-11-03 05:25:54,911 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34366; closing.
2023-11-03 05:25:54,911 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34388; closing.
2023-11-03 05:25:54,911 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39675', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989154.911765')
2023-11-03 05:25:54,912 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45469', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989154.9122088')
2023-11-03 05:25:54,912 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:54,912 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34366>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-03 05:25:56,565 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:25:56,566 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:25:56,566 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:25:56,569 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-03 05:25:56,570 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-11-03 05:25:58,868 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:58,872 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:25:58,875 - distributed.scheduler - INFO - State start
2023-11-03 05:25:58,895 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:25:58,896 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-03 05:25:58,897 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:25:58,897 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:25:58,943 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42635', status: init, memory: 0, processing: 0>
2023-11-03 05:25:58,958 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42635
2023-11-03 05:25:58,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35170
2023-11-03 05:25:58,961 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38619', status: init, memory: 0, processing: 0>
2023-11-03 05:25:58,962 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38619
2023-11-03 05:25:58,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35180
2023-11-03 05:25:58,963 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34347', status: init, memory: 0, processing: 0>
2023-11-03 05:25:58,963 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34347
2023-11-03 05:25:58,964 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35188
2023-11-03 05:25:58,964 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45073', status: init, memory: 0, processing: 0>
2023-11-03 05:25:58,965 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45073
2023-11-03 05:25:58,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35192
2023-11-03 05:25:58,966 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36421', status: init, memory: 0, processing: 0>
2023-11-03 05:25:58,967 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36421
2023-11-03 05:25:58,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35206
2023-11-03 05:25:58,989 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35206; closing.
2023-11-03 05:25:58,989 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36421', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989158.9894373')
2023-11-03 05:25:58,991 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35192; closing.
2023-11-03 05:25:58,992 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45073', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989158.9926026')
2023-11-03 05:25:58,993 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35180; closing.
2023-11-03 05:25:58,994 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38619', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989158.9944084')
2023-11-03 05:25:58,995 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35170; closing.
2023-11-03 05:25:58,996 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989158.9961824')
2023-11-03 05:25:58,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35188; closing.
2023-11-03 05:25:58,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34347', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989158.9970143')
2023-11-03 05:25:58,997 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:59,003 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46283', status: init, memory: 0, processing: 0>
2023-11-03 05:25:59,004 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46283
2023-11-03 05:25:59,004 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35218
2023-11-03 05:25:59,049 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35218; closing.
2023-11-03 05:25:59,049 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46283', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989159.049476')
2023-11-03 05:25:59,049 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:59,094 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35289'
2023-11-03 05:25:59,121 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38381'
2023-11-03 05:25:59,141 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34917'
2023-11-03 05:25:59,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33777'
2023-11-03 05:25:59,182 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46377'
2023-11-03 05:25:59,198 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39823'
2023-11-03 05:25:59,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32949', status: init, memory: 0, processing: 0>
2023-11-03 05:25:59,199 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32949
2023-11-03 05:25:59,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35306
2023-11-03 05:25:59,201 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38463'
2023-11-03 05:25:59,204 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43399', status: init, memory: 0, processing: 0>
2023-11-03 05:25:59,204 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43399
2023-11-03 05:25:59,204 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35310
2023-11-03 05:25:59,212 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40679'
2023-11-03 05:25:59,250 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35306; closing.
2023-11-03 05:25:59,250 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32949', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989159.2507505')
2023-11-03 05:25:59,251 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35310; closing.
2023-11-03 05:25:59,251 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43399', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989159.2515092')
2023-11-03 05:25:59,251 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:25:59,251 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35310>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-03 05:25:59,814 - distributed.scheduler - INFO - Receive client connection: Client-73ceae98-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:25:59,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35322
2023-11-03 05:26:00,781 - distributed.scheduler - INFO - Receive client connection: Client-77639bad-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:00,782 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37224
2023-11-03 05:26:01,078 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:01,078 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:01,082 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:01,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:01,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:01,304 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:01,307 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:01,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:01,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:01,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:01,308 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:01,308 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:01,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:01,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:01,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:01,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:01,312 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:01,314 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:01,315 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:01,316 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:01,317 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:01,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:01,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:01,346 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:04,120 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43309
2023-11-03 05:26:04,120 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43309
2023-11-03 05:26:04,120 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36793
2023-11-03 05:26:04,121 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,121 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,121 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:04,121 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:04,121 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cbiru0on
2023-11-03 05:26:04,121 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7b788b2-9d3b-4ec5-9f84-b948ee7abb80
2023-11-03 05:26:04,141 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39817
2023-11-03 05:26:04,144 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39817
2023-11-03 05:26:04,144 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44799
2023-11-03 05:26:04,145 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,145 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,145 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:04,145 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:04,145 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j1gmsx3v
2023-11-03 05:26:04,147 - distributed.worker - INFO - Starting Worker plugin RMMSetup-726d29a6-fe07-4f90-a286-cbeea827a891
2023-11-03 05:26:04,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45355
2023-11-03 05:26:04,164 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45355
2023-11-03 05:26:04,164 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42371
2023-11-03 05:26:04,164 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,164 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,164 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:04,164 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:04,164 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-69jj0_p_
2023-11-03 05:26:04,165 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55f13138-e0dc-46ba-80fe-1423da3828aa
2023-11-03 05:26:04,165 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc0d25f5-d22d-4590-b25e-241e9a10713f
2023-11-03 05:26:04,165 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35217
2023-11-03 05:26:04,166 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35217
2023-11-03 05:26:04,166 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39193
2023-11-03 05:26:04,166 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,166 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,166 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:04,166 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:04,166 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kzjf1aca
2023-11-03 05:26:04,167 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7fc4e6f2-2176-4799-a817-22bd5c8efbab
2023-11-03 05:26:04,172 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91e782a3-b688-49b8-81df-8a8a6312b91d
2023-11-03 05:26:04,223 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42409
2023-11-03 05:26:04,224 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42409
2023-11-03 05:26:04,224 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46575
2023-11-03 05:26:04,224 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,224 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,224 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:04,224 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:04,224 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ec6usb2p
2023-11-03 05:26:04,225 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b301095c-c995-435c-ab18-c02635d55d9a
2023-11-03 05:26:04,238 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36655
2023-11-03 05:26:04,240 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36655
2023-11-03 05:26:04,240 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35445
2023-11-03 05:26:04,240 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,240 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,240 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:04,240 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:04,240 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-24swcn2m
2023-11-03 05:26:04,241 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ffdd6de0-3e8d-4f35-86b9-deccf39f07c7
2023-11-03 05:26:04,241 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44033
2023-11-03 05:26:04,242 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44033
2023-11-03 05:26:04,242 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36465
2023-11-03 05:26:04,242 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,243 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,243 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:04,243 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:04,243 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3ahcycyh
2023-11-03 05:26:04,244 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a70fcfb4-fa94-4c60-91d9-efc62db67480
2023-11-03 05:26:04,242 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37245
2023-11-03 05:26:04,244 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37245
2023-11-03 05:26:04,244 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44363
2023-11-03 05:26:04,244 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,244 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,244 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:04,245 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:04,245 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_279u_mc
2023-11-03 05:26:04,246 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20e6d3cf-c25b-48c5-933d-41c271070de9
2023-11-03 05:26:04,293 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2e1ecd8-ad28-48d4-b1aa-422692d70998
2023-11-03 05:26:04,294 - distributed.worker - INFO - Starting Worker plugin PreImport-f3455dc7-9f97-4a42-b430-d72ca06bf746
2023-11-03 05:26:04,294 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,329 - distributed.worker - INFO - Starting Worker plugin PreImport-ee01eb6e-43e3-4913-8011-c86f5eb2371e
2023-11-03 05:26:04,329 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-630ead5a-c01c-4667-8bc6-a0b9c58bd5b8
2023-11-03 05:26:04,330 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,334 - distributed.worker - INFO - Starting Worker plugin PreImport-a7549491-db22-4c19-99fa-6d47838679aa
2023-11-03 05:26:04,334 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,341 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43309', status: init, memory: 0, processing: 0>
2023-11-03 05:26:04,342 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43309
2023-11-03 05:26:04,342 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37238
2023-11-03 05:26:04,344 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:04,346 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,346 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:04,364 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39817', status: init, memory: 0, processing: 0>
2023-11-03 05:26:04,365 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39817
2023-11-03 05:26:04,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37240
2023-11-03 05:26:04,366 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:04,367 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,367 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,369 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:04,371 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-023fdcf2-0b14-4540-b487-db0129a05e5a
2023-11-03 05:26:04,371 - distributed.worker - INFO - Starting Worker plugin PreImport-9ab290f0-9e99-4e24-a52d-a4cc1e3f46ca
2023-11-03 05:26:04,371 - distributed.worker - INFO - Starting Worker plugin PreImport-d12b70d4-84b6-40c5-ba3a-5e660213d01d
2023-11-03 05:26:04,372 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,372 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,376 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45355', status: init, memory: 0, processing: 0>
2023-11-03 05:26:04,376 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45355
2023-11-03 05:26:04,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37248
2023-11-03 05:26:04,378 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:04,379 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,379 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:04,383 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4dac1a77-31c4-4e88-bd07-81ea146b4b73
2023-11-03 05:26:04,383 - distributed.worker - INFO - Starting Worker plugin PreImport-12bb2711-77a8-499b-9daa-0561f2c5b282
2023-11-03 05:26:04,384 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,386 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6168b1f4-664b-4eaa-b4e6-a5b8a10b462b
2023-11-03 05:26:04,387 - distributed.worker - INFO - Starting Worker plugin PreImport-2c7ffc7e-64a8-41b5-8efe-ff1bc47efccd
2023-11-03 05:26:04,388 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,390 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c92d2f96-5e6a-40a5-b557-008032c26150
2023-11-03 05:26:04,391 - distributed.worker - INFO - Starting Worker plugin PreImport-dfb97bf3-4d96-4f64-bc7f-6dce7b77eca4
2023-11-03 05:26:04,392 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,412 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42409', status: init, memory: 0, processing: 0>
2023-11-03 05:26:04,413 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42409
2023-11-03 05:26:04,413 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37268
2023-11-03 05:26:04,414 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35217', status: init, memory: 0, processing: 0>
2023-11-03 05:26:04,414 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:04,415 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35217
2023-11-03 05:26:04,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37262
2023-11-03 05:26:04,415 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,415 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,416 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44033', status: init, memory: 0, processing: 0>
2023-11-03 05:26:04,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:04,417 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44033
2023-11-03 05:26:04,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37282
2023-11-03 05:26:04,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:04,417 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,417 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:04,419 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,419 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:04,420 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36655', status: init, memory: 0, processing: 0>
2023-11-03 05:26:04,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:04,420 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36655
2023-11-03 05:26:04,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37296
2023-11-03 05:26:04,421 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:04,422 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,422 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,423 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37245', status: init, memory: 0, processing: 0>
2023-11-03 05:26:04,424 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37245
2023-11-03 05:26:04,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:04,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37304
2023-11-03 05:26:04,425 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:04,426 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:04,426 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:04,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:04,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,500 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,501 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,501 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,501 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,501 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,504 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,505 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:04,509 - distributed.scheduler - INFO - Remove client Client-73ceae98-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:04,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35322; closing.
2023-11-03 05:26:04,509 - distributed.scheduler - INFO - Remove client Client-73ceae98-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:04,510 - distributed.scheduler - INFO - Close client connection: Client-73ceae98-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:04,510 - distributed.scheduler - INFO - Remove client Client-77639bad-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:04,511 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37224; closing.
2023-11-03 05:26:04,511 - distributed.scheduler - INFO - Remove client Client-77639bad-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:04,511 - distributed.scheduler - INFO - Close client connection: Client-77639bad-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:04,512 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35289'. Reason: nanny-close
2023-11-03 05:26:04,513 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:04,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38381'. Reason: nanny-close
2023-11-03 05:26:04,514 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:04,514 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39817. Reason: nanny-close
2023-11-03 05:26:04,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34917'. Reason: nanny-close
2023-11-03 05:26:04,515 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:04,515 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37245. Reason: nanny-close
2023-11-03 05:26:04,515 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33777'. Reason: nanny-close
2023-11-03 05:26:04,515 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35217. Reason: nanny-close
2023-11-03 05:26:04,515 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:04,516 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:04,516 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46377'. Reason: nanny-close
2023-11-03 05:26:04,516 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37240; closing.
2023-11-03 05:26:04,516 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:04,516 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43309. Reason: nanny-close
2023-11-03 05:26:04,517 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39817', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989164.516968')
2023-11-03 05:26:04,517 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:04,517 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39823'. Reason: nanny-close
2023-11-03 05:26:04,517 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:04,517 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36655. Reason: nanny-close
2023-11-03 05:26:04,517 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:04,517 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:04,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38463'. Reason: nanny-close
2023-11-03 05:26:04,518 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:04,518 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44033. Reason: nanny-close
2023-11-03 05:26:04,518 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:04,518 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40679'. Reason: nanny-close
2023-11-03 05:26:04,519 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:04,519 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:04,519 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45355. Reason: nanny-close
2023-11-03 05:26:04,519 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37262; closing.
2023-11-03 05:26:04,519 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37304; closing.
2023-11-03 05:26:04,519 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:04,519 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:04,520 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42409. Reason: nanny-close
2023-11-03 05:26:04,520 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35217', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989164.5203946')
2023-11-03 05:26:04,520 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37245', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989164.520855')
2023-11-03 05:26:04,520 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:04,521 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:04,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:04,521 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37238; closing.
2023-11-03 05:26:04,521 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:04,522 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:04,522 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43309', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989164.5223956')
2023-11-03 05:26:04,522 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37296; closing.
2023-11-03 05:26:04,523 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37282; closing.
2023-11-03 05:26:04,523 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36655', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989164.5235958')
2023-11-03 05:26:04,523 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:04,523 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:04,524 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44033', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989164.5239837')
2023-11-03 05:26:04,524 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:04,524 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37248; closing.
2023-11-03 05:26:04,524 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37268; closing.
2023-11-03 05:26:04,525 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45355', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989164.525016')
2023-11-03 05:26:04,525 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42409', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989164.5254686')
2023-11-03 05:26:04,525 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:26:04,678 - distributed.scheduler - INFO - Receive client connection: Client-7be164f9-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:04,678 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37310
2023-11-03 05:26:06,280 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:26:06,280 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:26:06,281 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:26:06,282 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-03 05:26:06,283 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-11-03 05:26:08,521 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:08,525 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33979 instead
  warnings.warn(
2023-11-03 05:26:08,529 - distributed.scheduler - INFO - State start
2023-11-03 05:26:08,558 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:08,559 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-11-03 05:26:08,560 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:26:08,561 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-11-03 05:26:08,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35225'
2023-11-03 05:26:10,684 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:10,685 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:11,353 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:11,570 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35225'. Reason: nanny-close
2023-11-03 05:26:12,540 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42315
2023-11-03 05:26:12,541 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42315
2023-11-03 05:26:12,541 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-11-03 05:26:12,541 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:12,541 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:12,541 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:12,542 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-03 05:26:12,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0vv71_4s
2023-11-03 05:26:12,543 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5abc1c4-da4a-4712-931b-1aa24f62f65d
2023-11-03 05:26:12,543 - distributed.worker - INFO - Starting Worker plugin PreImport-024b9ded-9012-4d9f-bcdd-842b56967f74
2023-11-03 05:26:12,543 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca7b2b88-35df-48c6-8b80-52f192b75d90
2023-11-03 05:26:12,544 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:12,572 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:12,573 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:12,573 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:12,575 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:12,592 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:12,594 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42315. Reason: nanny-close
2023-11-03 05:26:12,597 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:12,599 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-11-03 05:26:18,097 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:18,101 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42927 instead
  warnings.warn(
2023-11-03 05:26:18,104 - distributed.scheduler - INFO - State start
2023-11-03 05:26:18,125 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:18,126 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-11-03 05:26:18,126 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:26:18,127 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-11-03 05:26:18,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39551'
2023-11-03 05:26:20,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:20,408 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:21,095 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:21,396 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39551'. Reason: nanny-close
2023-11-03 05:26:22,410 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33543
2023-11-03 05:26:22,411 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33543
2023-11-03 05:26:22,411 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44463
2023-11-03 05:26:22,411 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:22,412 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:22,412 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:22,412 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-03 05:26:22,412 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k6f2uazz
2023-11-03 05:26:22,413 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f27ac84-adaf-452e-9b98-da47afc0fc03
2023-11-03 05:26:22,413 - distributed.worker - INFO - Starting Worker plugin PreImport-e58c9b92-95ea-4cd1-85dd-aae9a7a1e548
2023-11-03 05:26:22,415 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a5a1bf34-e21f-4302-b7bb-81a7ae9be0ce
2023-11-03 05:26:22,416 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:22,453 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:22,454 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:22,455 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:22,456 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:22,476 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:22,478 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33543. Reason: nanny-close
2023-11-03 05:26:22,480 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:22,482 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-11-03 05:26:25,353 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:25,357 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:26:25,360 - distributed.scheduler - INFO - State start
2023-11-03 05:26:25,381 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:25,382 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-03 05:26:25,383 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:26:25,383 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:26:25,859 - distributed.scheduler - INFO - Receive client connection: Client-875ad4ee-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:25,872 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55928
2023-11-03 05:26:30,970 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40735', status: init, memory: 0, processing: 0>
2023-11-03 05:26:30,972 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40735
2023-11-03 05:26:30,972 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50788
2023-11-03 05:26:31,333 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38309', status: init, memory: 0, processing: 0>
2023-11-03 05:26:31,333 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38309
2023-11-03 05:26:31,334 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50796
2023-11-03 05:26:31,338 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33635', status: init, memory: 0, processing: 0>
2023-11-03 05:26:31,339 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33635
2023-11-03 05:26:31,339 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50806
2023-11-03 05:26:31,352 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45945', status: init, memory: 0, processing: 0>
2023-11-03 05:26:31,353 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45945
2023-11-03 05:26:31,353 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50820
2023-11-03 05:26:31,359 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36225', status: init, memory: 0, processing: 0>
2023-11-03 05:26:31,359 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36225
2023-11-03 05:26:31,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50830
2023-11-03 05:26:31,362 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42477', status: init, memory: 0, processing: 0>
2023-11-03 05:26:31,363 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42477
2023-11-03 05:26:31,363 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50856
2023-11-03 05:26:31,365 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46873', status: init, memory: 0, processing: 0>
2023-11-03 05:26:31,366 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46873
2023-11-03 05:26:31,366 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50840
2023-11-03 05:26:31,366 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46043', status: init, memory: 0, processing: 0>
2023-11-03 05:26:31,367 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46043
2023-11-03 05:26:31,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50866
2023-11-03 05:26:31,430 - distributed.scheduler - INFO - Remove client Client-875ad4ee-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:31,430 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55928; closing.
2023-11-03 05:26:31,430 - distributed.scheduler - INFO - Remove client Client-875ad4ee-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:31,431 - distributed.scheduler - INFO - Close client connection: Client-875ad4ee-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:31,436 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50840; closing.
2023-11-03 05:26:31,437 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46873', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989191.4372306')
2023-11-03 05:26:31,439 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50796; closing.
2023-11-03 05:26:31,439 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50788; closing.
2023-11-03 05:26:31,439 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50820; closing.
2023-11-03 05:26:31,440 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38309', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989191.4400418')
2023-11-03 05:26:31,440 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40735', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989191.4404023')
2023-11-03 05:26:31,440 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45945', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989191.4409165')
2023-11-03 05:26:31,441 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50856; closing.
2023-11-03 05:26:31,442 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50866; closing.
2023-11-03 05:26:31,442 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50806; closing.
2023-11-03 05:26:31,442 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989191.4427192')
2023-11-03 05:26:31,443 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46043', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989191.443093')
2023-11-03 05:26:31,443 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989191.4434526')
2023-11-03 05:26:31,443 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50830; closing.
2023-11-03 05:26:31,444 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36225', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989191.4442112')
2023-11-03 05:26:31,444 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:26:31,949 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:55916'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55916>: Stream is closed
2023-11-03 05:26:32,427 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:26:32,428 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:26:32,429 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:26:32,431 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-03 05:26:32,432 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-11-03 05:26:34,705 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:34,709 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:26:34,712 - distributed.scheduler - INFO - State start
2023-11-03 05:26:34,732 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:34,733 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-03 05:26:34,733 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:26:34,734 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:26:34,906 - distributed.scheduler - INFO - Receive client connection: Client-8cd1f13a-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:34,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37836
2023-11-03 05:26:34,921 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35271'
2023-11-03 05:26:36,463 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:36,463 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:36,467 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:37,385 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38367
2023-11-03 05:26:37,386 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38367
2023-11-03 05:26:37,386 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34347
2023-11-03 05:26:37,386 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-03 05:26:37,386 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:37,386 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:37,386 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-03 05:26:37,386 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-flo_1bjb
2023-11-03 05:26:37,387 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bc677b2c-f2c0-455c-b454-d52b70678fbb
2023-11-03 05:26:37,387 - distributed.worker - INFO - Starting Worker plugin PreImport-f6f74aff-c214-4c35-8461-97589eaab2df
2023-11-03 05:26:37,388 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4e06ee4-7c9a-42d9-8a54-4f7bc39c9790
2023-11-03 05:26:37,388 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:37,417 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38367', status: init, memory: 0, processing: 0>
2023-11-03 05:26:37,418 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38367
2023-11-03 05:26:37,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37862
2023-11-03 05:26:37,419 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:37,419 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-03 05:26:37,420 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:37,422 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-03 05:26:37,424 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:37,427 - distributed.scheduler - INFO - Remove client Client-8cd1f13a-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:37,427 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37836; closing.
2023-11-03 05:26:37,427 - distributed.scheduler - INFO - Remove client Client-8cd1f13a-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:37,428 - distributed.scheduler - INFO - Close client connection: Client-8cd1f13a-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:37,429 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35271'. Reason: nanny-close
2023-11-03 05:26:37,450 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:37,451 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38367. Reason: nanny-close
2023-11-03 05:26:37,453 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-03 05:26:37,453 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37862; closing.
2023-11-03 05:26:37,453 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38367', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989197.4532635')
2023-11-03 05:26:37,453 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:26:37,454 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:38,595 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:26:38,595 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:26:38,596 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:26:38,597 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-03 05:26:38,597 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-11-03 05:26:40,770 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:40,774 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:26:40,777 - distributed.scheduler - INFO - State start
2023-11-03 05:26:40,798 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:40,799 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-03 05:26:40,800 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:26:40,800 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:26:40,862 - distributed.scheduler - INFO - Receive client connection: Client-9055b737-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:40,876 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51132
2023-11-03 05:26:40,915 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36815'
2023-11-03 05:26:40,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43211'
2023-11-03 05:26:40,946 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34437'
2023-11-03 05:26:40,956 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46413'
2023-11-03 05:26:40,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44445'
2023-11-03 05:26:40,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36219'
2023-11-03 05:26:40,979 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40749'
2023-11-03 05:26:40,988 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34369'
2023-11-03 05:26:41,999 - distributed.scheduler - INFO - Receive client connection: Client-92202c4d-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:42,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51218
2023-11-03 05:26:42,760 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:42,760 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:42,760 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:42,760 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:42,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:42,763 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:42,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:42,763 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:42,764 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:42,765 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:42,767 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:42,768 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:42,779 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:42,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:42,783 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:42,790 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:42,790 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:42,794 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:42,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:42,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:42,799 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:42,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:42,804 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:42,808 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:45,237 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37395
2023-11-03 05:26:45,238 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37395
2023-11-03 05:26:45,238 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40613
2023-11-03 05:26:45,238 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,238 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,238 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:45,238 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:45,238 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yz7t6chr
2023-11-03 05:26:45,239 - distributed.worker - INFO - Starting Worker plugin RMMSetup-104737bd-15ac-451e-a05b-fadebcaea7c1
2023-11-03 05:26:45,239 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40315
2023-11-03 05:26:45,239 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40315
2023-11-03 05:26:45,239 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39087
2023-11-03 05:26:45,240 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,240 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,240 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:45,240 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:45,240 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-11lypllf
2023-11-03 05:26:45,240 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-12e9df08-9ab2-455b-98da-669c7fb7c5a6
2023-11-03 05:26:45,240 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1478dc29-d6a0-4ca1-8978-c8879b223828
2023-11-03 05:26:45,242 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46079
2023-11-03 05:26:45,243 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46079
2023-11-03 05:26:45,243 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41239
2023-11-03 05:26:45,243 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,243 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,243 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:45,243 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:45,243 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3s0o4lf7
2023-11-03 05:26:45,244 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4700a5b-3e67-472e-831c-445b66a25991
2023-11-03 05:26:45,243 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32975
2023-11-03 05:26:45,245 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32975
2023-11-03 05:26:45,245 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36621
2023-11-03 05:26:45,245 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,245 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,245 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:45,246 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:45,246 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6a8pc7p8
2023-11-03 05:26:45,247 - distributed.worker - INFO - Starting Worker plugin RMMSetup-88d9117e-7bb8-4f96-9dd4-f927dd665e8a
2023-11-03 05:26:45,366 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b2971ad-4744-4424-bab0-1480cd357081
2023-11-03 05:26:45,366 - distributed.worker - INFO - Starting Worker plugin PreImport-3c2d1983-f1eb-43ae-bace-5abbff38c4ff
2023-11-03 05:26:45,366 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,377 - distributed.worker - INFO - Starting Worker plugin PreImport-b3d8f9ce-75f5-4a63-a3c5-221232bf8cb0
2023-11-03 05:26:45,377 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,377 - distributed.worker - INFO - Starting Worker plugin PreImport-402af88c-5f9b-4ebe-9f38-f27c5cfba81b
2023-11-03 05:26:45,378 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-09bca297-aae6-4bd7-be7a-6db698a2cb6c
2023-11-03 05:26:45,378 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,384 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39759
2023-11-03 05:26:45,385 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39759
2023-11-03 05:26:45,385 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38283
2023-11-03 05:26:45,385 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,385 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,385 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:45,385 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:45,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_n08vu6h
2023-11-03 05:26:45,386 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6c777943-7235-4a9e-b9e7-c69406fd9816
2023-11-03 05:26:45,386 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7546b111-6844-43dc-8f84-81997b5e04aa
2023-11-03 05:26:45,387 - distributed.worker - INFO - Starting Worker plugin PreImport-61e7b6fb-499b-49ca-b2bf-40c582743058
2023-11-03 05:26:45,387 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,392 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35473
2023-11-03 05:26:45,393 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35473
2023-11-03 05:26:45,393 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42355
2023-11-03 05:26:45,393 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,393 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,393 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:45,393 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:45,393 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0ppjrm7i
2023-11-03 05:26:45,394 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9474b0b2-1d77-49cb-bc9e-45d09b2fb813
2023-11-03 05:26:45,395 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41991
2023-11-03 05:26:45,396 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41991
2023-11-03 05:26:45,396 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43597
2023-11-03 05:26:45,396 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,396 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,395 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32975', status: init, memory: 0, processing: 0>
2023-11-03 05:26:45,396 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:45,396 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:45,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wle65ggn
2023-11-03 05:26:45,397 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32975
2023-11-03 05:26:45,397 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f198498-6b85-4ab6-9d34-b144e4e20b54
2023-11-03 05:26:45,397 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51248
2023-11-03 05:26:45,397 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ac4a2d41-f889-4ddc-b3ec-ba731489815e
2023-11-03 05:26:45,397 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43529
2023-11-03 05:26:45,397 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43529
2023-11-03 05:26:45,398 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35351
2023-11-03 05:26:45,398 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,398 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,398 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:45,398 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-03 05:26:45,398 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f118cj84
2023-11-03 05:26:45,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:45,398 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5936a2d-9629-462a-8574-4e4e2c553fca
2023-11-03 05:26:45,399 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,399 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,401 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:45,403 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40315', status: init, memory: 0, processing: 0>
2023-11-03 05:26:45,404 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40315
2023-11-03 05:26:45,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51260
2023-11-03 05:26:45,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:45,405 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,405 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:45,408 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:45,409 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:45,411 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37395', status: init, memory: 0, processing: 0>
2023-11-03 05:26:45,411 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37395
2023-11-03 05:26:45,411 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51264
2023-11-03 05:26:45,412 - distributed.scheduler - INFO - Remove client Client-92202c4d-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:45,412 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51218; closing.
2023-11-03 05:26:45,412 - distributed.scheduler - INFO - Remove client Client-92202c4d-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:45,413 - distributed.scheduler - INFO - Close client connection: Client-92202c4d-7a09-11ee-913e-d8c49764f6bb
2023-11-03 05:26:45,413 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:45,414 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,414 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,416 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:45,418 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46079', status: init, memory: 0, processing: 0>
2023-11-03 05:26:45,419 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46079
2023-11-03 05:26:45,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51268
2023-11-03 05:26:45,420 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:45,421 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,421 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,423 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:45,510 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ccc51603-86eb-48a3-aae1-40cc0686396d
2023-11-03 05:26:45,511 - distributed.worker - INFO - Starting Worker plugin PreImport-7d50268c-e704-416d-bd4d-2a5023e678b2
2023-11-03 05:26:45,511 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,517 - distributed.worker - INFO - Starting Worker plugin PreImport-74bd8f2d-1cdc-4908-afd3-48a28e60dfad
2023-11-03 05:26:45,518 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f5d8151-f9d4-498a-a55c-66c9b4bc94cd
2023-11-03 05:26:45,518 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14bd7e66-963d-456e-b0c1-7022ff3bd273
2023-11-03 05:26:45,518 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,518 - distributed.worker - INFO - Starting Worker plugin PreImport-a2ca20cc-b967-4dd0-a9b5-5e62a07629e9
2023-11-03 05:26:45,518 - distributed.worker - INFO - Starting Worker plugin PreImport-9a726c54-fcaa-469a-9b5c-23c6d457fb9c
2023-11-03 05:26:45,518 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,518 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,534 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39759', status: init, memory: 0, processing: 0>
2023-11-03 05:26:45,535 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39759
2023-11-03 05:26:45,535 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51270
2023-11-03 05:26:45,536 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:45,537 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,537 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,539 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:45,543 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41991', status: init, memory: 0, processing: 0>
2023-11-03 05:26:45,543 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41991
2023-11-03 05:26:45,543 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51272
2023-11-03 05:26:45,544 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35473', status: init, memory: 0, processing: 0>
2023-11-03 05:26:45,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:45,544 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35473
2023-11-03 05:26:45,545 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51282
2023-11-03 05:26:45,545 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,545 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,545 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:45,546 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,546 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,546 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:45,548 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:45,552 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43529', status: init, memory: 0, processing: 0>
2023-11-03 05:26:45,552 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43529
2023-11-03 05:26:45,552 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51298
2023-11-03 05:26:45,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:45,554 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:45,555 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:45,557 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:45,617 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:45,617 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:45,617 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:45,618 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:45,618 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:45,618 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:45,618 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:45,619 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-03 05:26:45,630 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:45,630 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:45,631 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:45,631 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:45,631 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:45,631 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:45,631 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:45,631 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:45,635 - distributed.scheduler - INFO - Remove client Client-9055b737-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:45,635 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51132; closing.
2023-11-03 05:26:45,635 - distributed.scheduler - INFO - Remove client Client-9055b737-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:45,636 - distributed.scheduler - INFO - Close client connection: Client-9055b737-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:45,637 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36815'. Reason: nanny-close
2023-11-03 05:26:45,637 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:45,638 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43211'. Reason: nanny-close
2023-11-03 05:26:45,638 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:45,639 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37395. Reason: nanny-close
2023-11-03 05:26:45,639 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34437'. Reason: nanny-close
2023-11-03 05:26:45,639 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:45,639 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46079. Reason: nanny-close
2023-11-03 05:26:45,640 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46413'. Reason: nanny-close
2023-11-03 05:26:45,640 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40315. Reason: nanny-close
2023-11-03 05:26:45,640 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:45,640 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44445'. Reason: nanny-close
2023-11-03 05:26:45,641 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:45,641 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35473. Reason: nanny-close
2023-11-03 05:26:45,641 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36219'. Reason: nanny-close
2023-11-03 05:26:45,641 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:45,642 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43529. Reason: nanny-close
2023-11-03 05:26:45,642 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51264; closing.
2023-11-03 05:26:45,642 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:45,642 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40749'. Reason: nanny-close
2023-11-03 05:26:45,642 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:45,642 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37395', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989205.6426396')
2023-11-03 05:26:45,643 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32975. Reason: nanny-close
2023-11-03 05:26:45,643 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34369'. Reason: nanny-close
2023-11-03 05:26:45,643 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:45,643 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:45,643 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:45,643 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41991. Reason: nanny-close
2023-11-03 05:26:45,643 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:45,643 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51282; closing.
2023-11-03 05:26:45,644 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39759. Reason: nanny-close
2023-11-03 05:26:45,644 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:45,644 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35473', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989205.6448944')
2023-11-03 05:26:45,645 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:45,645 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:45,645 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51268; closing.
2023-11-03 05:26:45,645 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:45,645 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:45,645 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51260; closing.
2023-11-03 05:26:45,645 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:45,646 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:45,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:45,646 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:45,647 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:45,647 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:45,646 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51282>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51282>: Stream is closed
2023-11-03 05:26:45,648 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:45,648 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46079', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989205.6486251')
2023-11-03 05:26:45,649 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51298; closing.
2023-11-03 05:26:45,649 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40315', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989205.6493347')
2023-11-03 05:26:45,650 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43529', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989205.6501222')
2023-11-03 05:26:45,650 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51248; closing.
2023-11-03 05:26:45,650 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51272; closing.
2023-11-03 05:26:45,651 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32975', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989205.6513615')
2023-11-03 05:26:45,651 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41991', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989205.651778')
2023-11-03 05:26:45,652 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51270; closing.
2023-11-03 05:26:45,652 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39759', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989205.6525893')
2023-11-03 05:26:45,652 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:26:45,653 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51270>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-03 05:26:47,104 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:26:47,104 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:26:47,105 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:26:47,106 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-03 05:26:47,106 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-11-03 05:26:49,015 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:49,019 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:26:49,022 - distributed.scheduler - INFO - State start
2023-11-03 05:26:49,042 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:49,043 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-03 05:26:49,044 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:26:49,044 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:26:49,212 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46085'
2023-11-03 05:26:49,404 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33489', status: init, memory: 0, processing: 0>
2023-11-03 05:26:49,416 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33489
2023-11-03 05:26:49,416 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51476
2023-11-03 05:26:49,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51476; closing.
2023-11-03 05:26:49,426 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33489', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989209.426641')
2023-11-03 05:26:49,427 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:26:50,719 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:26:50,719 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:26:50,723 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:26:51,541 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43849
2023-11-03 05:26:51,541 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43849
2023-11-03 05:26:51,541 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32801
2023-11-03 05:26:51,541 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:26:51,541 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:51,542 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:26:51,542 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-03 05:26:51,542 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c__3tao9
2023-11-03 05:26:51,542 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24b93292-6f4a-470b-89e9-454f360a214f
2023-11-03 05:26:51,633 - distributed.worker - INFO - Starting Worker plugin PreImport-6783c026-b113-41b8-a7e1-72a22ef215ee
2023-11-03 05:26:51,634 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45621633-2f1a-44c0-b4a5-9478fb61f745
2023-11-03 05:26:51,634 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:51,658 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43849', status: init, memory: 0, processing: 0>
2023-11-03 05:26:51,658 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43849
2023-11-03 05:26:51,658 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58532
2023-11-03 05:26:51,659 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:26:51,660 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:26:51,660 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:26:51,661 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:26:52,378 - distributed.scheduler - INFO - Receive client connection: Client-956cf96f-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:52,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58548
2023-11-03 05:26:52,385 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:26:52,389 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:52,390 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:26:52,393 - distributed.scheduler - INFO - Remove client Client-956cf96f-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:52,393 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58548; closing.
2023-11-03 05:26:52,393 - distributed.scheduler - INFO - Remove client Client-956cf96f-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:52,393 - distributed.scheduler - INFO - Close client connection: Client-956cf96f-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:26:52,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46085'. Reason: nanny-close
2023-11-03 05:26:52,395 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:26:52,396 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43849. Reason: nanny-close
2023-11-03 05:26:52,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:26:52,397 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58532; closing.
2023-11-03 05:26:52,398 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43849', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989212.3981133')
2023-11-03 05:26:52,398 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:26:52,399 - distributed.nanny - INFO - Worker closed
2023-11-03 05:26:53,260 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:26:53,260 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:26:53,261 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:26:53,262 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-03 05:26:53,262 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-11-03 05:26:55,241 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:26:55,245 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-11-03 05:27:06,512 - distributed.scheduler - INFO - State start
2023-11-03 05:27:06,536 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-03 05:27:06,537 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-03 05:27:06,538 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-11-03 05:27:06,538 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-03 05:27:07,926 - distributed.scheduler - INFO - Receive client connection: Client-990fe2fa-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:27:07,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39452
2023-11-03 05:27:09,451 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44633'
2023-11-03 05:27:11,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-03 05:27:11,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-03 05:27:11,064 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-03 05:27:11,847 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37421
2023-11-03 05:27:11,848 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37421
2023-11-03 05:27:11,848 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38833
2023-11-03 05:27:11,848 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-03 05:27:11,848 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:27:11,848 - distributed.worker - INFO -               Threads:                          1
2023-11-03 05:27:11,848 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-03 05:27:11,848 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a0bn1wvf
2023-11-03 05:27:11,849 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9fff3f18-56a5-4035-9afa-acd8a50b4197
2023-11-03 05:27:11,849 - distributed.worker - INFO - Starting Worker plugin PreImport-050d9180-e54d-44a1-9560-50a6e809456a
2023-11-03 05:27:11,849 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2a35421-7b16-45ee-9ecb-1da79fc9b9e4
2023-11-03 05:27:11,954 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:27:11,979 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37421', status: init, memory: 0, processing: 0>
2023-11-03 05:27:11,980 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37421
2023-11-03 05:27:11,980 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47546
2023-11-03 05:27:11,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-03 05:27:11,982 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-03 05:27:11,982 - distributed.worker - INFO - -------------------------------------------------
2023-11-03 05:27:11,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-03 05:27:12,011 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-11-03 05:27:12,015 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-03 05:27:12,018 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:27:12,019 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-03 05:27:12,022 - distributed.scheduler - INFO - Remove client Client-990fe2fa-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:27:12,022 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39452; closing.
2023-11-03 05:27:12,022 - distributed.scheduler - INFO - Remove client Client-990fe2fa-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:27:12,022 - distributed.scheduler - INFO - Close client connection: Client-990fe2fa-7a09-11ee-90fa-d8c49764f6bb
2023-11-03 05:27:12,024 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44633'. Reason: nanny-close
2023-11-03 05:27:12,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-03 05:27:12,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37421. Reason: nanny-close
2023-11-03 05:27:12,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-03 05:27:12,038 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47546; closing.
2023-11-03 05:27:12,038 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37421', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1698989232.0384665')
2023-11-03 05:27:12,038 - distributed.scheduler - INFO - Lost all workers
2023-11-03 05:27:12,039 - distributed.nanny - INFO - Worker closed
2023-11-03 05:27:12,939 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-03 05:27:12,940 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-03 05:27:12,940 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-03 05:27:12,941 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-03 05:27:12,942 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43117 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] [1698989339.865223] [dgx13:58971:0]            sock.c:470  UCX  ERROR bind(fd=133 addr=0.0.0.0:45950) failed: Address already in use
[1698989339.865316] [dgx13:58971:0]            sock.c:470  UCX  ERROR bind(fd=133 addr=0.0.0.0:33234) failed: Address already in use
[1698989339.865349] [dgx13:58971:0]            sock.c:470  UCX  ERROR bind(fd=133 addr=0.0.0.0:60518) failed: Address already in use
[1698989339.865402] [dgx13:58971:0]            sock.c:470  UCX  ERROR bind(fd=133 addr=0.0.0.0:33166) failed: Address already in use
[1698989339.866020] [dgx13:58971:0]            sock.c:470  UCX  ERROR bind(fd=136 addr=0.0.0.0:52866) failed: Address already in use
[1698989341.424365] [dgx13:59064:0]            sock.c:470  UCX  ERROR bind(fd=122 addr=0.0.0.0:37250) failed: Address already in use
[1698989343.449429] [dgx13:59057:0]            sock.c:470  UCX  ERROR bind(fd=128 addr=0.0.0.0:35758) failed: Address already in use
[1698989344.147624] [dgx13:59068:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:38047) failed: Address already in use
[1698989344.147725] [dgx13:59068:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:50686) failed: Address already in use
[1698989344.147761] [dgx13:59068:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:39732) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42243 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45585 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37877 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35707 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34211 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44859 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46305 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41623 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33951 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34781 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39025 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34517 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42049 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39839 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] [1698989732.603569] [dgx13:67208:0]            sock.c:470  UCX  ERROR bind(fd=159 addr=0.0.0.0:45458) failed: Address already in use
[1698989733.959882] [dgx13:67380:0]            sock.c:470  UCX  ERROR bind(fd=122 addr=0.0.0.0:47006) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34781 instead
  warnings.warn(
[1698989757.305444] [dgx13:67790:0]            sock.c:470  UCX  ERROR bind(fd=122 addr=0.0.0.0:40554) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39867 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32985 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37593 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44361 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37367 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36079 instead
  warnings.warn(
[1698989849.528953] [dgx13:68650:0]            sock.c:470  UCX  ERROR bind(fd=128 addr=0.0.0.0:46453) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] 2023-11-03 05:38:02,266 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-03 05:38:02,275 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-03 05:38:02,485 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-03 05:38:02,493 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-03 05:38:02,511 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:36493'.
2023-11-03 05:38:02,514 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:36493'. Shutting down.
2023-11-03 05:38:02,541 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f3c90363790>>, <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-03 05:38:02,543 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7ff0e5808790>>, <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-17' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-03 05:38:04,544 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-11-03 05:38:04,546 - distributed.nanny - ERROR - Worker process died unexpectedly
