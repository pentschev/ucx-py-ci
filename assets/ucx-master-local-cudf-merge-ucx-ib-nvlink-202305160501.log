2023-05-16 05:56:15,722 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-16 05:56:15,722 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-16 05:56:15,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-16 05:56:15,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-16 05:56:15,783 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-16 05:56:15,783 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-16 05:56:15,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-16 05:56:15,784 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-16 05:56:15,785 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-16 05:56:15,785 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-16 05:56:15,799 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-16 05:56:15,799 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-16 05:56:15,846 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-16 05:56:15,846 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-16 05:56:15,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-16 05:56:15,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1684216586.909274] [dgx13:81358:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_0: LRU push returned Unsupported operation
[dgx13:81358:0:81358]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  81358) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f9df044ddec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7f9df044ad28]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_format+0x114) [0x7f9df044ae44]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x83605) [0x7f9df0509605]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xc9) [0x7f9df04de069]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x20) [0x7f9df05241a0]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x71e) [0x7f9df05290be]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x52) [0x7f9df0528872]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x495d8) [0x7f9df05be5d8]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x55668e7c1dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x55668e7c01a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55668e7a6d36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55668e7a027a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55668e7b1c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55668e7a23cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55668e7a027a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55668e7b1c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55668e7a23cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55668e7c670e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55668e7a7923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55668e7c670e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55668e7a7923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55668e7c670e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55668e7a7923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55668e7c670e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55668e7a7923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55668e7c670e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x55668e7a7923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55668e7c670e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f9e0f1862fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7f9e0f186b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55668e7aa2bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55668e75d817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55668e7a8f83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55668e7a6d36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55668e7b1ef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55668e7a181b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55668e7b1ef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55668e7a181b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55668e7b1ef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55668e7a181b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55668e7b1ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55668e7a181b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55668e7a027a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55668e7b1c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55668e7a5fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55668e7a027a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55668e7bf935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55668e7c0104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55668e886fc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55668e7aa2bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55668e7a51bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55668e7b1ef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55668e7bfc72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55668e7a51bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55668e7b1ef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55668e7a181b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55668e7a027a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55668e7b1c05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55668e7a181b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55668e7b1ef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55668e7a1568]
=================================
2023-05-16 05:56:27,082 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:41366
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #029] ep: 0x7fef14021180, tag: 0xeb43225dcbe9d0a7, nbytes: 799823480, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #029] ep: 0x7fef14021180, tag: 0xeb43225dcbe9d0a7, nbytes: 799823480, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-16 05:56:27,540 - distributed.nanny - WARNING - Restarting worker
2023-05-16 05:56:29,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-16 05:56:29,069 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[1684216589.593430] [dgx13:81367:0]    ib_mlx5dv_md.c:462  UCX  ERROR mlx5_1: LRU push returned Unsupported operation
[dgx13:81367:0:81367]        rndv.c:166  Fatal: failed to pack rendezvous remote key: Unsupported operation
==== backtrace (tid:  81367) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2dc) [0x7f0973815dec]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_message+0xb8) [0x7f0973812d28]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_fatal_error_format+0x114) [0x7f0973812e44]
 3  /opt/conda/envs/gdf/lib/libucp.so.0(+0x83605) [0x7f09738d1605]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_do_am_single+0xc9) [0x7f09738a6069]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_proto_progress_tag_rndv_rts+0x20) [0x7f09738ec1a0]
 6  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nbx+0x71e) [0x7f09738f10be]
 7  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_tag_send_nb+0x52) [0x7f09738f0872]
 8  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x495d8) [0x7f09739865d8]
 9  /opt/conda/envs/gdf/bin/python(+0x149dc7) [0x5640a35a2dc7]
10  /opt/conda/envs/gdf/bin/python(PyObject_Call+0x158) [0x5640a35a11a8]
11  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5640a3587d36]
12  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640a358127a]
13  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5640a3592c05]
14  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5640a35833cb]
15  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640a358127a]
16  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5640a3592c05]
17  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5640a35833cb]
18  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5640a35a770e]
19  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x5640a3588923]
20  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5640a35a770e]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x5640a3588923]
22  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5640a35a770e]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x5640a3588923]
24  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5640a35a770e]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x5640a3588923]
26  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5640a35a770e]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x6793) [0x5640a3588923]
28  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5640a35a770e]
29  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f09925722fe]
30  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x8b4e) [0x7f0992572b4e]
31  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5640a358b2bc]
32  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x5640a353e817]
33  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5640a3589f83]
34  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5640a3587d36]
35  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640a3592ef3]
36  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640a358281b]
37  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640a3592ef3]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640a358281b]
39  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640a3592ef3]
40  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640a358281b]
41  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640a3592ef3]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640a358281b]
43  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640a358127a]
44  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5640a3592c05]
45  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5640a3586fa7]
46  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640a358127a]
47  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5640a35a0935]
48  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5640a35a1104]
49  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5640a3667fc8]
50  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5640a358b2bc]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5640a35861bb]
52  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640a3592ef3]
53  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5640a35a0c72]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5640a35861bb]
55  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640a3592ef3]
56  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640a358281b]
57  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5640a358127a]
58  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5640a3592c05]
59  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5640a358281b]
60  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5640a3592ef3]
61  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5640a3582568]
=================================
2023-05-16 05:56:29,783 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44137
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #031] ep: 0x7fef140211c0, tag: 0xb87079d598caf837, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #031] ep: 0x7fef140211c0, tag: 0xb87079d598caf837, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-16 05:56:29,784 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44137
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #031] ep: 0x7f0ac5aeb280, tag: 0xbaa4f9c181830cdb, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #031] ep: 0x7f0ac5aeb280, tag: 0xbaa4f9c181830cdb, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-16 05:56:29,784 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44137
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #043] ep: 0x7f7ce548b240, tag: 0x4565ccb8a57356a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #043] ep: 0x7f7ce548b240, tag: 0x4565ccb8a57356a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-16 05:56:29,784 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44137
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #031] ep: 0x7fb50d785200, tag: 0x71560cb2f97fb0c2, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #031] ep: 0x7fb50d785200, tag: 0x71560cb2f97fb0c2, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-16 05:56:29,785 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44137
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #038] ep: 0x7f28ada0b280, tag: 0x9f1c82b6c2b093dc, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #038] ep: 0x7f28ada0b280, tag: 0x9f1c82b6c2b093dc, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-16 05:56:29,785 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:44137
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #031] ep: 0x7ff45cde4140, tag: 0xf3d6a91485a71625, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #031] ep: 0x7ff45cde4140, tag: 0xf3d6a91485a71625, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-05-16 05:56:30,248 - distributed.nanny - WARNING - Restarting worker
2023-05-16 05:56:31,862 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-05-16 05:56:31,862 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-05-16 05:56:32,023 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-725abe67c74ae66d512a568d78d27104', 1)
Function:  <dask.layers.CallableLazyImport object at 0x7f2356
args:      ([               key   payload
shuffle                     
0           697887   8539426
0           565532  19524800
0           740989  78357208
0           712902  45388010
0           647013  46056678
...            ...       ...
0        799951656  53262973
0        799805646  69729112
0        799815219  43588210
0        799853066  69956193
0        799824401  99977581

[12497508 rows x 2 columns],                key   payload
shuffle                     
1           109249  80245479
1           639365  24736018
1            61480  93989177
1           219673   2851300
1           721486  63168929
...            ...       ...
1        799913628  84587011
1        799958741  48923921
1        799977378  70399852
1        799956732  61375045
1        799656715  43681380

[12503907 rows x 2 columns],                key   payload
shuffle                     
2           134560  19459887
2            42982  68660705
2           114100  38607634
2            65337  22351766
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-05-16 05:56:38,815 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-16 05:56:38,815 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-16 05:56:38,849 - distributed.core - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-16 05:56:38,849 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-05-16 05:56:38,956 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-515d7fed71fef99a84e30dffe3177fc4', 3)
Function:  <dask.layers.CallableLazyImport object at 0x7f9400
args:      ([               key   payload
shuffle                     
0           304590  51653531
0           380442  81591923
0           422014  58156238
0           154357  93038271
0            90072  80772772
...            ...       ...
0        799838798  42443001
0        799926390  17079235
0        799892673   3804342
0        799858817  91880819
0        799835299     95239

[12497076 rows x 2 columns],                key   payload
shuffle                     
1           547667  27750906
1           570675  66028433
1           463858  31254894
1           116332  29949669
1           195126  36632075
...            ...       ...
1        799960190   5069606
1        799955688  47523005
1        799904801  22578998
1        799903465  65335821
1        799929572  67090277

[12501362 rows x 2 columns],                key   payload
shuffle                     
2           201728  46905446
2            52820  79918826
2             5779  36127105
2           589007  57434322
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')"

2023-05-16 05:56:39,289 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-93c053ba913344a232d205b095249130', 5)
Function:  subgraph_callable-395569e9-1624-40aa-918d-424e58d2
args:      (               key   payload
shuffle                     
0            47884  73810576
0           375373  79627062
0           332369  15057169
0           119814  82009874
0           177970  63289755
...            ...       ...
7        799986177  50414045
7        799934502   1263012
7        799986185  25621059
7        799923930  44430965
7        799965874  10497616

[99993166 rows x 2 columns],                  key   payload
73738      410179484  30009285
73741      827093110  67065699
73745      849517815  61056717
73757      710936103  75719760
73758      808886729  31633077
...              ...       ...
99984845  1501533482  63095044
99984858  1545405279  94522359
99984811   792007948  79476776
99984812   388804905  81964517
99984817  1536597121  61282560

[99997899 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
