2023-06-01 06:50:07,062 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:07,062 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:07,143 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:07,143 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:07,163 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:07,163 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:07,187 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:07,188 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:07,188 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:07,188 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:07,188 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:07,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:07,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:07,213 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:07,213 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:07,214 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:71162:0:71162] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71162) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fd9c399e80d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a04) [0x7fd9c399ea04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28bca) [0x7fd9c399ebca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fda6a351420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7fd9c3a10987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fd9c3a2f549]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x1e9bf) [0x7fd9c395b9bf]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21be8) [0x7fd9c395ebe8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fd9c39a6df9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fd9c395dbad]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fd9c3a0e36a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7fd9c3aad17a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5631bac07b08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x5631babf8112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631babf127a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5631bac02c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631babf281b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631bac02ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x5631bac10a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x5631bad209b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x5631babae817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5631babf9f83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5631babf7d36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631bac02ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631babf281b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631bac02ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631babf281b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631bac02ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631babf281b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631bac02ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631babf281b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631babf127a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5631bac02c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5631babf6fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631babf127a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5631bac10935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5631bac11104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5631bacd7fc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5631babfb2bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5631babf61bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631bac02ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5631bac10c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5631babf61bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631bac02ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631babf281b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631babf127a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5631bac02c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5631babf281b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5631bac02ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5631babf2568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631babf127a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5631bac02c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5631babf33cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5631babf127a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x5631babf0f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5631babf0eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5631baca18bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5631baccfadc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5631baccbc24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5631bacc37ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5631bacc36bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x5631bacc28a2]
=================================
[dgx13:71159:0:71159] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71159) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fafbd51480d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a04) [0x7fafbd514a04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28bca) [0x7fafbd514bca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fb061eb2420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7fafbd586987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fafbd5a5549]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x1e9bf) [0x7fafbd4d19bf]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21be8) [0x7fafbd4d4be8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fafbd51cdf9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fafbd4d3bad]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fafbd58436a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7fafbd62317a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55afeb43cb08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55afeb42d112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55afeb42627a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55afeb437c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55afeb42781b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55afeb437ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x55afeb445a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x55afeb5559b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55afeb3e3817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55afeb42ef83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55afeb42cd36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55afeb437ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55afeb42781b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55afeb437ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55afeb42781b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55afeb437ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55afeb42781b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55afeb437ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55afeb42781b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55afeb42627a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55afeb437c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55afeb42bfa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55afeb42627a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55afeb445935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55afeb446104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55afeb50cfc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55afeb4302bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55afeb42b1bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55afeb437ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55afeb445c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55afeb42b1bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55afeb437ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55afeb42781b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55afeb42627a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55afeb437c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55afeb42781b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55afeb437ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55afeb427568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55afeb42627a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55afeb437c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55afeb4283cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55afeb42627a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55afeb425f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55afeb425eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55afeb4d68bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55afeb504adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55afeb500c24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55afeb4f87ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55afeb4f86bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x55afeb4f78a2]
=================================
Task exception was never retrieved
future: <Task finished name='Task-828' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Endpoint timeout')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Endpoint timeout
Task exception was never retrieved
future: <Task finished name='Task-841' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Endpoint timeout')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Endpoint timeout
2023-06-01 06:50:16,239 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:38243 -> ucx://127.0.0.1:37895
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f41281ed2c0, tag: 0xfb0721278fce8b1, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:16,240 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37895
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f5f344f0180, tag: 0xacfe4b750bc8c3b0, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f5f344f0180, tag: 0xacfe4b750bc8c3b0, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-01 06:50:16,241 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37895
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f41281ed1c0, tag: 0xe07122489f17d6a1, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f41281ed1c0, tag: 0xe07122489f17d6a1, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-01 06:50:16,245 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37895
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 468, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-06-01 06:50:16,248 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:37895
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 468, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-06-01 06:50:16,266 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59343
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fa64140c180, tag: 0x99ecb6da6827b5fb, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fa64140c180, tag: 0x99ecb6da6827b5fb, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-01 06:50:16,266 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:41281 -> ucx://127.0.0.1:59343
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f5f344f03c0, tag: 0xb8090cf1c05adde2, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:16,266 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59343
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7eff1001b240, tag: 0x5ba92932d6740242, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7eff1001b240, tag: 0x5ba92932d6740242, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-01 06:50:16,267 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59343
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f5f344f0100, tag: 0x4d93d53545680d5c, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f5f344f0100, tag: 0x4d93d53545680d5c, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-01 06:50:16,267 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51313 -> ucx://127.0.0.1:59343
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7eff1001b380, tag: 0xa1b0f00c527aed63, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:16,286 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:59343
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 329, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 469, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f41281ed280, tag: 0x7b18a4ba2e70a313, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 334, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:59343 after 30 s
2023-06-01 06:50:16,319 - distributed.nanny - WARNING - Restarting worker
[dgx13:71165:0:71165] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71165) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fc494e3d80d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a04) [0x7fc494e3da04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28bca) [0x7fc494e3dbca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fc5277f2420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7fc494eaf987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fc494ece549]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x1e9bf) [0x7fc494dfa9bf]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21be8) [0x7fc494dfdbe8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fc494e45df9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fc494dfcbad]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fc494ead36a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7fc494f4c17a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x556ff6b65b08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x556ff6b56112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556ff6b4f27a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556ff6b60c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556ff6b5081b]
17  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x556ff6b7570e]
18  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7fc4a71d12fe]
19  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x556ff6b592bc]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x556ff6b0c817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x556ff6b57f83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x556ff6b55d36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556ff6b60ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556ff6b5081b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556ff6b60ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556ff6b5081b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556ff6b60ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556ff6b5081b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556ff6b60ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556ff6b5081b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556ff6b4f27a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556ff6b60c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x556ff6b54fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556ff6b4f27a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x556ff6b6e935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x556ff6b6f104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x556ff6c35fc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x556ff6b592bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x556ff6b541bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556ff6b60ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x556ff6b6ec72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x556ff6b541bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556ff6b60ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556ff6b5081b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556ff6b4f27a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556ff6b60c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556ff6b5081b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556ff6b60ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x556ff6b50568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556ff6b4f27a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556ff6b60c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x556ff6b513cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556ff6b4f27a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x556ff6b4ef07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x556ff6b4eeb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x556ff6bff8bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x556ff6c2dadc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x556ff6c29c24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x556ff6c217ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x556ff6c216bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x556ff6c208a2]
=================================
[dgx13:71156:0:71156] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71156) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f64a824580d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a04) [0x7f64a8245a04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28bca) [0x7f64a8245bca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f6548aa2420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f64a82b7987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f64a82d6549]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x1e9bf) [0x7f64a82029bf]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21be8) [0x7f64a8205be8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f64a824ddf9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f64a8204bad]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f64a82b536a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7f64a835417a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55edbf634b08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55edbf625112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55edbf61e27a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55edbf62fc05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55edbf61f81b]
17  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55edbf64470e]
18  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f64c84782fe]
19  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55edbf6282bc]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55edbf5db817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55edbf626f83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55edbf624d36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55edbf62fef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55edbf61f81b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55edbf62fef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55edbf61f81b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55edbf62fef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55edbf61f81b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55edbf62fef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55edbf61f81b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55edbf61e27a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55edbf62fc05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55edbf623fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55edbf61e27a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55edbf63d935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55edbf63e104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55edbf704fc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55edbf6282bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55edbf6231bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55edbf62fef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55edbf63dc72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55edbf6231bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55edbf62fef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55edbf61f81b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55edbf61e27a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55edbf62fc05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55edbf61f81b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55edbf62fef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55edbf61f568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55edbf61e27a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55edbf62fc05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55edbf6203cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55edbf61e27a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55edbf61df07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55edbf61deb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55edbf6ce8bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55edbf6fcadc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55edbf6f8c24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55edbf6f07ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55edbf6f06bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x55edbf6ef8a2]
=================================
2023-06-01 06:50:16,398 - distributed.nanny - WARNING - Restarting worker
Task exception was never retrieved
future: <Task finished name='Task-834' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Endpoint timeout')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Endpoint timeout
2023-06-01 06:50:16,586 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46479
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f41281ed180, tag: 0xb461fffcbcecddba, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f41281ed180, tag: 0xb461fffcbcecddba, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-06-01 06:50:16,587 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46479
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7fa64140c140, tag: 0xd5350125beaa9b11, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 329, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7fa64140c140, tag: 0xd5350125beaa9b11, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 334, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:46479 after 30 s
2023-06-01 06:50:16,587 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46479
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7f5f344f0200, tag: 0xcd34060b897f7726, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 329, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7f5f344f0200, tag: 0xcd34060b897f7726, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 334, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:46479 after 30 s
2023-06-01 06:50:16,587 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:38243 -> ucx://127.0.0.1:46479
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f41281ed300, tag: 0xa7f8a53388d916c2, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
Task exception was never retrieved
future: <Task finished name='Task-844' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-06-01 06:50:16,588 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:46479
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7eff1001b1c0, tag: 0x39b311dbe04e2baf, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 329, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7eff1001b1c0, tag: 0x39b311dbe04e2baf, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 334, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:46479 after 30 s
2023-06-01 06:50:16,591 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51313 -> ucx://127.0.0.1:54019
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7eff1001b3c0, tag: 0xd76c5e0e03f532ec, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:16,592 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54019
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7eff1001b100, tag: 0xdfd4964b071ecad8, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7eff1001b100, tag: 0xdfd4964b071ecad8, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-06-01 06:50:16,592 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:41281 -> ucx://127.0.0.1:54019
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f5f344f0400, tag: 0x8850cf5d7652f71, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:16,613 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:38243 -> ucx://127.0.0.1:54019
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f41281ed440, tag: 0x4cde3c814d644562, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:16,610 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54019
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f5f344f0240, tag: 0xc46ecd20d961f049, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f5f344f0240, tag: 0xc46ecd20d961f049, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-01 06:50:16,622 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54019
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f41281ed100, tag: 0xd7e8c537932d3c7a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f41281ed100, tag: 0xd7e8c537932d3c7a, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Endpoint timeout")
2023-06-01 06:50:16,673 - distributed.nanny - WARNING - Restarting worker
2023-06-01 06:50:16,706 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:54019
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #010] ep: 0x7fa64140c240, tag: 0x9a7f1326efb17185, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #010] ep: 0x7fa64140c240, tag: 0x9a7f1326efb17185, nbytes: 50000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2023-06-01 06:50:16,728 - distributed.nanny - WARNING - Restarting worker
2023-06-01 06:50:17,892 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:17,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:17,963 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:17,963 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:18,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:18,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:18,247 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:18,247 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:71672:0:71672] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71672) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fd78c3d880d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a04) [0x7fd78c3d8a04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28bca) [0x7fd78c3d8bca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fd81ec4c420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7fd78c44a987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fd78c469549]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x1e9bf) [0x7fd78c3959bf]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21be8) [0x7fd78c398be8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fd78c3e0df9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fd78c397bad]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fd78c44836a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7fd78c4e717a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55997d7e8b08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55997d7d9112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55997d7d227a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55997d7e3c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55997d7d381b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55997d7e3ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x55997d7f1a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x55997d9019b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55997d78f817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55997d7daf83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55997d7d8d36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55997d7e3ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55997d7d381b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55997d7e3ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55997d7d381b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55997d7e3ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55997d7d381b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55997d7e3ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55997d7d381b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55997d7d227a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55997d7e3c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55997d7d7fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55997d7d227a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55997d7f1935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55997d7f2104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55997d8b8fc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55997d7dc2bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55997d7d71bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55997d7e3ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55997d7f1c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55997d7d71bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55997d7e3ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55997d7d381b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55997d7d227a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55997d7e3c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55997d7d381b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55997d7e3ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55997d7d3568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55997d7d227a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55997d7e3c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55997d7d43cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55997d7d227a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55997d7d1f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55997d7d1eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55997d8828bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55997d8b0adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55997d8acc24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55997d8a47ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55997d8a46bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x55997d8a38a2]
=================================
[dgx13:71675:0:71675] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71675) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f30bd5f380d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a04) [0x7f30bd5f3a04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28bca) [0x7f30bd5f3bca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f3161e72420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f30bd665987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f30bd684549]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x1e9bf) [0x7f30bd5b09bf]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21be8) [0x7f30bd5b3be8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f30bd5fbdf9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f30bd5b2bad]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f30bd66336a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7f30bd70217a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5569656a7b08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x556965698112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55696569127a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5569656a2c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55696569281b]
17  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x5569656b770e]
18  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f30e18512fe]
19  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55696569b2bc]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55696564e817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x556965699f83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x556965697d36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5569656a2ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55696569281b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5569656a2ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55696569281b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5569656a2ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55696569281b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5569656a2ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55696569281b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55696569127a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5569656a2c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x556965696fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55696569127a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5569656b0935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5569656b1104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x556965777fc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55696569b2bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5569656961bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5569656a2ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5569656b0c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5569656961bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5569656a2ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55696569281b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55696569127a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5569656a2c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55696569281b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5569656a2ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x556965692568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55696569127a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5569656a2c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5569656933cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55696569127a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x556965690f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x556965690eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5569657418bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55696576fadc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55696576bc24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5569657637ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5569657636bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x5569657628a2]
=================================
2023-06-01 06:50:19,538 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-01 06:50:19,538 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-01 06:50:19,550 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-01 06:50:19,551 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-01 06:50:19,567 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51531 -> ucx://127.0.0.1:49717
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa64140c140, tag: 0xb492d16ed66c2791, nbytes: 100025496, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:19,573 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51313 -> ucx://127.0.0.1:49717
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7eff1001b100, tag: 0x54173c385f410104, nbytes: 100015992, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:19,611 - distributed.nanny - WARNING - Restarting worker
2023-06-01 06:50:19,646 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51531 -> ucx://127.0.0.1:33923
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa64140c300, tag: 0x5fea3aefdf164aa2, nbytes: 99979552, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:19,646 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51313 -> ucx://127.0.0.1:33923
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7eff1001b3c0, tag: 0x9afd9d89ad11fd73, nbytes: 100028792, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:19,665 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 0)
Function:  <dask.layers.CallableLazyImport object at 0x7f3bce
args:      ([                key   payload
2693      834589921  76828704
2701      802410943  15534474
2703      210296572  64118960
2710         736698  86724145
114241    863545465  80572998
...             ...       ...
99989882  604878924   2285234
99949994  808250941  45683522
99950002  828449692  73574492
99950006  509200131  74451072
99950013  306526394  59981271

[12497168 rows x 2 columns],                 key   payload
2080      929429280  32437860
41472     909323769  24832286
2086      120909675  39106817
41496     932723975  20227126
2099      925409353  32918089
...             ...       ...
99998070  942898091  75197962
99969500  938612387  90023071
99969501  121406213  95559083
99969502  913522728   2663338
99956524  950521416  57188982

[12502889 rows x 2 columns],                  key   payload
20709     1041794682   2008305
20712     1025689621  20980081
20719      427984528  12042842
20731      525411306    543869
73280      326460787  70613113
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-01 06:50:19,665 - distributed.worker - ERROR - 'int' object is not subscriptable
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2902, in get_data_from_worker
    status = response["status"]
TypeError: 'int' object is not subscriptable
2023-06-01 06:50:19,705 - distributed.nanny - WARNING - Restarting worker
2023-06-01 06:50:19,715 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-01 06:50:19,715 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
[dgx13:71680:0:71680] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71680) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7efe0506780d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a04) [0x7efe05067a04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28bca) [0x7efe05067bca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7efea58da420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7efe050d9987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7efe050f8549]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x1e9bf) [0x7efe050249bf]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21be8) [0x7efe05027be8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7efe0506fdf9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7efe05026bad]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7efe050d736a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7efe0517617a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x556b572eab08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x556b572db112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556b572d427a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556b572e5c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556b572d581b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556b572e5ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x556b572f3a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x556b574039b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x556b57291817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x556b572dcf83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x556b572dad36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556b572e5ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556b572d581b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556b572e5ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556b572d581b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556b572e5ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556b572d581b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556b572e5ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556b572d581b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556b572d427a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556b572e5c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x556b572d9fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556b572d427a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x556b572f3935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x556b572f4104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x556b573bafc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x556b572de2bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x556b572d91bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556b572e5ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x556b572f3c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x556b572d91bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556b572e5ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556b572d581b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556b572d427a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556b572e5c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x556b572d581b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x556b572e5ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x556b572d5568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556b572d427a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x556b572e5c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x556b572d63cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x556b572d427a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x556b572d3f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x556b572d3eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x556b573848bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x556b573b2adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x556b573aec24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x556b573a67ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x556b573a66bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x556b573a58a2]
=================================
2023-06-01 06:50:19,745 - distributed.core - ERROR - not enough values to unpack (expected 2, got 0)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 376, in read
    cuda_recv_frames, recv_frames = zip(
ValueError: not enough values to unpack (expected 2, got 0)
2023-06-01 06:50:19,745 - distributed.worker - ERROR - not enough values to unpack (expected 2, got 0)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 376, in read
    cuda_recv_frames, recv_frames = zip(
ValueError: not enough values to unpack (expected 2, got 0)
2023-06-01 06:50:19,747 - distributed.core - ERROR - Exception while handling op get_data
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 830, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 987, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1794, in get_data
    assert response == "OK", response
AssertionError: {'op': 'get_data', 'keys': {"('split-simple-shuffle-dd2afed0cacaa172f75444002884d37e', 5, 5)"}, 'who': 'ucx://127.0.0.1:38243', 'max_connections': None, 'reply': True}
2023-06-01 06:50:19,774 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-01 06:50:19,774 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-01 06:50:19,855 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-3ebd756736d0dbb3e700d723f5010c29', 1)
Function:  subgraph_callable-ec95cf5c-c771-4b53-a369-5785e325
args:      (               key   payload
shuffle                     
0            35833  24808269
0           644608  71441988
0            44885  91745308
0           620049  72923971
0            73166  42814161
...            ...       ...
7        799980326  28053175
7        799946732  54661419
7        799939031  10667053
7        799901196  15375820
7        799819870  25554451

[100005187 rows x 2 columns],                  key   payload
2695       848034854  97390992
2708       840089177  18757948
2715       849442429  87310838
114249     836032895  61995576
43205      701072325  25103838
...              ...       ...
99994044  1539700890   2311635
99994045  1510718048  21382855
99994047  1557389422  54066081
99993984  1564539867  54689686
99994003   194363741  75733734

[99996328 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-01 06:50:19,945 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51313
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 359, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #090] ep: 0x7f41281ed240, tag: 0xfec680f0271ffb42, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #090] ep: 0x7f41281ed240, tag: 0xfec680f0271ffb42, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated")
[dgx13:71683:0:71683] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  71683) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f1920f3b80d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28a04) [0x7f1920f3ba04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x28bca) [0x7f1920f3bbca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f19c1798420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f1920fad987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f1920fcc549]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x1e9bf) [0x7f1920ef89bf]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x21be8) [0x7f1920efbbe8]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f1920f43df9]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f1920efabad]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f1920fab36a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7f192104a17a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55dfd55f8b08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55dfd55e9112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55dfd55e227a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55dfd55f3c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55dfd55e381b]
17  /opt/conda/envs/gdf/bin/python(+0x14e70e) [0x55dfd560870e]
18  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x82fe) [0x7f19411742fe]
19  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55dfd55ec2bc]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55dfd559f817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55dfd55eaf83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55dfd55e8d36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55dfd55f3ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55dfd55e381b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55dfd55f3ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55dfd55e381b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55dfd55f3ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55dfd55e381b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55dfd55f3ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55dfd55e381b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55dfd55e227a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55dfd55f3c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55dfd55e7fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55dfd55e227a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55dfd5601935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55dfd5602104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55dfd56c8fc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55dfd55ec2bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55dfd55e71bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55dfd55f3ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55dfd5601c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55dfd55e71bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55dfd55f3ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55dfd55e381b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55dfd55e227a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55dfd55f3c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55dfd55e381b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55dfd55f3ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55dfd55e3568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55dfd55e227a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55dfd55f3c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55dfd55e43cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55dfd55e227a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55dfd55e1f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55dfd55e1eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55dfd56928bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55dfd56c0adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55dfd56bcc24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55dfd56b47ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55dfd56b46bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x55dfd56b38a2]
=================================
2023-06-01 06:50:19,951 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51531 -> ucx://127.0.0.1:56431
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa64140c340, tag: 0x1478e2e4bb7cb132, nbytes: 99988080, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:19,960 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:41281 -> ucx://127.0.0.1:56431
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f5f344f0440, tag: 0x4c725d5a34aad684, nbytes: 99935688, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:19,993 - distributed.nanny - WARNING - Restarting worker
2023-06-01 06:50:20,046 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 4)
Function:  <dask.layers.CallableLazyImport object at 0x7f3bce
args:      ([                key   payload
2700      110911396  22114849
2704      834972940  49706776
2716      859066057  35547889
114242    846510340  68723466
43206     305174874  17388661
...             ...       ...
99950012  817393555  18722107
99989857  409479530  70465021
99989859  800588233  87298177
99989877  855881565  98426917
99989884  829509130  40354912

[12500589 rows x 2 columns],                 key   payload
2094      906369698   8978939
2100      516526844  50430733
2109      924039485  85392941
2111      966658270  24110099
21004     321160951  85515726
...             ...       ...
99969499  951132470   2251397
99956516  967795408  66007764
99956517  964587240  76411403
99956520  723978235  26431710
99956539  947329572  20471947

[12501715 rows x 2 columns],                  key   payload
20704      725158361  90611431
20706     1015558837  88513395
20716      232079658  13373216
20718     1028398657  79130374
73292      134303432  37292940
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-01 06:50:20,049 - distributed.worker - ERROR - ('Unexpected response', {'op': 'get_data', 'keys': {"('split-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 7, 0)"}, 'who': 'ucx://127.0.0.1:51531', 'max_connections': None, 'reply': True})
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2902, in get_data_from_worker
    status = response["status"]
KeyError: 'status'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2904, in get_data_from_worker
    raise ValueError("Unexpected response", response)
ValueError: ('Unexpected response', {'op': 'get_data', 'keys': {"('split-simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 7, 0)"}, 'who': 'ucx://127.0.0.1:51531', 'max_connections': None, 'reply': True})
Task exception was never retrieved
future: <Task finished name='Task-4865' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-06-01 06:50:20,173 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:41281 -> ucx://127.0.0.1:51113
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f5f344f0200, tag: 0x4044ec6d4dac4fd0, nbytes: 99979816, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:20,182 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6)
Function:  <dask.layers.CallableLazyImport object at 0x7ef9b4
args:      ([                key   payload
2688      869943971  78054016
2711      204586965  37036746
2717      809837039  17380707
2718      104205523  17364535
114248      5771091  41171914
...             ...       ...
99989871  851164216   9327824
99950011  805103299  56341136
99989873  807869291  31787687
99989880  826886451  67460054
99989883  411134236  79910949

[12497796 rows x 2 columns],                 key   payload
2083      947198925  67321337
41476     947717826  65274430
2106      935832656  56881203
41477     959616113  56823216
41479     969240723  83360708
...             ...       ...
99998073  917193179  15084851
99989402  224884232  26537511
99998076  905930140  23671255
99956543  947009882  77465898
99989404  917088907  66261341

[12497151 rows x 2 columns],                  key   payload
20707     1066491457  10682225
20721     1069950223   3349445
20726     1018517621  74090447
20729     1024569425  73335474
73282     1067787264  58857401
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-01 06:50:20,183 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51313 -> ucx://127.0.0.1:51113
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7eff1001b1c0, tag: 0xcf0502e4ea143ab0, nbytes: 99983960, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-01 06:50:20,192 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 7)
Function:  <dask.layers.CallableLazyImport object at 0x7fa0f7
args:      ([                key   payload
2691      821245047  22952703
2698      857074466  88865155
2699      848620190  25440737
114240    857092042  68953111
43201     853792335  54784973
...             ...       ...
99950007  835254206  36683613
99950008  801654125  68274415
99950014  869509430   7532692
99989879    4856172  47132349
99989886  308049286  27320032

[12500893 rows x 2 columns],                 key   payload
2081      615506569  68610567
41487     906472898    626237
2090      918104954  31702688
41492     947789650  85668090
2093      903705124   8928397
...             ...       ...
99956525  902442830  18511132
99956527  922019013  72374802
99956529  932247371  13652314
99956533  968348344  86097288
99956540  413461540  30943824

[12495890 rows x 2 columns],                  key   payload
20705     1057444155  69464206
20710      728224344   8167976
20715      133869692  73920622
20723     1035171659   5537655
73283     1024884967  95369104
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-01 06:50:20,197 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:51531 -> ucx://127.0.0.1:51113
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #006] ep: 0x7fa64140c180, tag: 0xb4a059b119753c37, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1793, in get_data
    response = await comm.read(deserializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #006] ep: 0x7fa64140c180, tag: 0xb4a059b119753c37, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-01 06:50:20,232 - distributed.nanny - WARNING - Restarting worker
2023-06-01 06:50:20,246 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-01 06:50:20,247 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-01 06:50:20,413 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 5)
Function:  <dask.layers.CallableLazyImport object at 0x7ef9b4
args:      ([                key   payload
2689      827396126  41414059
2702      602656371  80253457
2709      848913182   1960371
114247    511450655  44832632
43214     824832536  65009678
...             ...       ...
99949998  102065973  61353955
99989861  403641031   2361517
99989868  611318375  97341589
99950010  846442446  29234151
99989885  842790643  59423702

[12498923 rows x 2 columns],                 key   payload
2087       16616638  80761411
41483     943477256  73691541
2091       16222663  38507108
41489     420221758  13224942
2103      950683628   9193079
...             ...       ...
99998071  968964168   4389773
99998075  923070826  83681681
99956528  969976116  57314290
99956534  214020546  84506281
99956542  939476631  83386449

[12501128 rows x 2 columns],                  key   payload
20720       34335946  83432833
73293     1057301495   9578551
78977     1028919006  92318925
73295     1044698880  21924434
78981     1068205670  92364980
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-01 06:50:20,537 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-dd2afed0cacaa172f75444002884d37e', 2)
Function:  <dask.layers.CallableLazyImport object at 0x7fa0f7
args:      ([               key   payload
shuffle                     
0           113643  18260707
0           559228  54427318
0            49518  24861751
0           617172  60024224
0           404935   4873192
...            ...       ...
0        799956298  10690035
0        799956302   7407768
0        799970091  85693009
0        799883291  56435043
0        799961254  24697421

[12497244 rows x 2 columns],                key   payload
shuffle                     
1           965419  76532246
1          1083111  26527602
1           936724  38495898
1           325529  50895846
1           125809  66021115
...            ...       ...
1        799908837   4726461
1        799766085  46544850
1        799763029   7666645
1        799787154  80199724
1        799778041  17805921

[12500558 rows x 2 columns],                key   payload
shuffle                     
2           250617  23650817
2            32231  97884483
2           299641  70804411
2           323795  20797525
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-01 06:50:21,131 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:21,131 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:21,255 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:21,255 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:21,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:21,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-01 06:50:21,768 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-01 06:50:21,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
