============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-15 06:32:38,462 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:32:38,467 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33943 instead
  warnings.warn(
2024-01-15 06:32:38,471 - distributed.scheduler - INFO - State start
2024-01-15 06:32:38,494 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:32:38,495 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-15 06:32:38,496 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33943/status
2024-01-15 06:32:38,496 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:32:38,749 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45433'
2024-01-15 06:32:38,769 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38841'
2024-01-15 06:32:38,772 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39979'
2024-01-15 06:32:38,780 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44965'
2024-01-15 06:32:39,516 - distributed.scheduler - INFO - Receive client connection: Client-df797784-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:39,531 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56600
2024-01-15 06:32:40,540 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:40,540 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:40,542 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:40,542 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:40,544 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:40,545 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40537
2024-01-15 06:32:40,545 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40537
2024-01-15 06:32:40,545 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41121
2024-01-15 06:32:40,545 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-15 06:32:40,545 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:40,545 - distributed.worker - INFO -               Threads:                          4
2024-01-15 06:32:40,545 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-15 06:32:40,545 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-d7ulap1d
2024-01-15 06:32:40,545 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dcc7548c-de9c-4f44-9867-8fe66603cecf
2024-01-15 06:32:40,545 - distributed.worker - INFO - Starting Worker plugin PreImport-0c0c42fb-05fa-4397-9281-5f75a58b0e94
2024-01-15 06:32:40,546 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a872bcf-2351-4bad-9e64-d7e7a8900c82
2024-01-15 06:32:40,546 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:40,546 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:40,546 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32993
2024-01-15 06:32:40,546 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32993
2024-01-15 06:32:40,547 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45195
2024-01-15 06:32:40,547 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-15 06:32:40,547 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:40,547 - distributed.worker - INFO -               Threads:                          4
2024-01-15 06:32:40,547 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-15 06:32:40,547 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ta4tcq98
2024-01-15 06:32:40,547 - distributed.worker - INFO - Starting Worker plugin PreImport-305f7ea1-1777-4cc5-9fa3-4e52b3f97b53
2024-01-15 06:32:40,547 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b60b2cf-45d4-4613-ac8f-25a5122ddaee
2024-01-15 06:32:40,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f3cd21a-757e-4e70-96e9-1ec73d1c9ac6
2024-01-15 06:32:40,548 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:40,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:40,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:40,586 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:40,586 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:40,586 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:40,587 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40169
2024-01-15 06:32:40,587 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40169
2024-01-15 06:32:40,587 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39229
2024-01-15 06:32:40,587 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-15 06:32:40,588 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:40,588 - distributed.worker - INFO -               Threads:                          4
2024-01-15 06:32:40,588 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-15 06:32:40,588 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-gqcvey0x
2024-01-15 06:32:40,588 - distributed.worker - INFO - Starting Worker plugin PreImport-e0c5c9f8-bfc3-464d-a85a-39db08f70e89
2024-01-15 06:32:40,588 - distributed.worker - INFO - Starting Worker plugin RMMSetup-130eef9f-1ee3-41be-9fef-415d11acbe9b
2024-01-15 06:32:40,588 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac9d5f39-5d69-421a-a449-b94f1026b43a
2024-01-15 06:32:40,588 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:40,590 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:40,591 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35685
2024-01-15 06:32:40,591 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35685
2024-01-15 06:32:40,591 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36713
2024-01-15 06:32:40,591 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-15 06:32:40,591 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:40,591 - distributed.worker - INFO -               Threads:                          4
2024-01-15 06:32:40,591 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-15 06:32:40,591 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-xbxk88ww
2024-01-15 06:32:40,591 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c18333d9-8a8e-4d90-8bc4-8838843afb01
2024-01-15 06:32:40,592 - distributed.worker - INFO - Starting Worker plugin PreImport-82873d57-40c3-4af5-bfce-849f576f0de1
2024-01-15 06:32:40,592 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b2f2ffd-4014-43e4-bacc-c5857bceb029
2024-01-15 06:32:40,592 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:41,861 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35685', status: init, memory: 0, processing: 0>
2024-01-15 06:32:41,863 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35685
2024-01-15 06:32:41,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43608
2024-01-15 06:32:41,864 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:41,864 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-15 06:32:41,864 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:41,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-15 06:32:41,880 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40537', status: init, memory: 0, processing: 0>
2024-01-15 06:32:41,881 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40537
2024-01-15 06:32:41,881 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43584
2024-01-15 06:32:41,882 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:41,883 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-15 06:32:41,883 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:41,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-15 06:32:41,891 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40169', status: init, memory: 0, processing: 0>
2024-01-15 06:32:41,891 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40169
2024-01-15 06:32:41,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43600
2024-01-15 06:32:41,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32993', status: init, memory: 0, processing: 0>
2024-01-15 06:32:41,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:41,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32993
2024-01-15 06:32:41,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43588
2024-01-15 06:32:41,894 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-15 06:32:41,894 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:41,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:41,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-15 06:32:41,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-15 06:32:41,896 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:41,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-15 06:32:41,959 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-15 06:32:41,959 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-15 06:32:41,959 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-15 06:32:41,959 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-15 06:32:41,964 - distributed.scheduler - INFO - Remove client Client-df797784-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:41,964 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56600; closing.
2024-01-15 06:32:41,965 - distributed.scheduler - INFO - Remove client Client-df797784-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:41,965 - distributed.scheduler - INFO - Close client connection: Client-df797784-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:41,966 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45433'. Reason: nanny-close
2024-01-15 06:32:41,966 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:41,966 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38841'. Reason: nanny-close
2024-01-15 06:32:41,967 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:41,967 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39979'. Reason: nanny-close
2024-01-15 06:32:41,967 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32993. Reason: nanny-close
2024-01-15 06:32:41,967 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:41,968 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44965'. Reason: nanny-close
2024-01-15 06:32:41,968 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40537. Reason: nanny-close
2024-01-15 06:32:41,968 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:41,968 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40169. Reason: nanny-close
2024-01-15 06:32:41,968 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35685. Reason: nanny-close
2024-01-15 06:32:41,969 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-15 06:32:41,969 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43588; closing.
2024-01-15 06:32:41,969 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-15 06:32:41,970 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-15 06:32:41,970 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300361.9702835')
2024-01-15 06:32:41,970 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-15 06:32:41,970 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43608; closing.
2024-01-15 06:32:41,970 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43584; closing.
2024-01-15 06:32:41,970 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:41,971 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:41,971 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35685', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300361.9713602')
2024-01-15 06:32:41,971 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:41,971 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40537', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300361.9717457')
2024-01-15 06:32:41,971 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:41,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43600; closing.
2024-01-15 06:32:41,972 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40169', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300361.9724426')
2024-01-15 06:32:41,972 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:32:41,972 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:43608>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-15 06:32:41,974 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:43584>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-15 06:32:42,681 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:32:42,681 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:32:42,682 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:32:42,683 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-15 06:32:42,684 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-15 06:32:44,936 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:32:44,941 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39041 instead
  warnings.warn(
2024-01-15 06:32:44,945 - distributed.scheduler - INFO - State start
2024-01-15 06:32:44,967 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:32:44,968 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:32:44,969 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39041/status
2024-01-15 06:32:44,970 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:32:45,192 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35051'
2024-01-15 06:32:45,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44435'
2024-01-15 06:32:45,220 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40055'
2024-01-15 06:32:45,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34253'
2024-01-15 06:32:45,240 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34779'
2024-01-15 06:32:45,251 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43055'
2024-01-15 06:32:45,265 - distributed.scheduler - INFO - Receive client connection: Client-e3461b13-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:45,266 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34723'
2024-01-15 06:32:45,278 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33235'
2024-01-15 06:32:45,282 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56296
2024-01-15 06:32:47,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:47,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:47,025 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:47,026 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41291
2024-01-15 06:32:47,026 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41291
2024-01-15 06:32:47,026 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38517
2024-01-15 06:32:47,026 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:47,026 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:47,026 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:47,027 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:47,027 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sdrh05ve
2024-01-15 06:32:47,027 - distributed.worker - INFO - Starting Worker plugin RMMSetup-49d3664d-abdf-43e5-9d0c-87331e9e507b
2024-01-15 06:32:47,259 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:47,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:47,263 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:47,264 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35437
2024-01-15 06:32:47,264 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35437
2024-01-15 06:32:47,264 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40535
2024-01-15 06:32:47,264 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:47,264 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:47,265 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:47,265 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:47,265 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y3v_nnh5
2024-01-15 06:32:47,265 - distributed.worker - INFO - Starting Worker plugin RMMSetup-329b785c-4ef4-42c0-b512-e1b157647e2d
2024-01-15 06:32:47,309 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:47,309 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:47,309 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:47,309 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:47,310 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:47,310 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:47,313 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:47,314 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:47,314 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43403
2024-01-15 06:32:47,314 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43403
2024-01-15 06:32:47,315 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39917
2024-01-15 06:32:47,315 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35851
2024-01-15 06:32:47,315 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:47,315 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35851
2024-01-15 06:32:47,315 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:47,315 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33897
2024-01-15 06:32:47,315 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:47,315 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:47,315 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:47,315 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:47,315 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9mak5tlv
2024-01-15 06:32:47,315 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:47,315 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:47,315 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1msooplj
2024-01-15 06:32:47,315 - distributed.worker - INFO - Starting Worker plugin RMMSetup-139cf661-f286-4777-b47e-6c87565aac6f
2024-01-15 06:32:47,315 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:47,315 - distributed.worker - INFO - Starting Worker plugin RMMSetup-39a7c61a-1fe1-4833-a6ba-ce371c8acff3
2024-01-15 06:32:47,316 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40889
2024-01-15 06:32:47,316 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40889
2024-01-15 06:32:47,316 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42577
2024-01-15 06:32:47,316 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:47,316 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:47,316 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:47,316 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:47,316 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:47,316 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yd6pv0sj
2024-01-15 06:32:47,316 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:47,317 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aed6fa52-32d4-44f8-babf-d9d6f1c335f3
2024-01-15 06:32:47,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:47,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:47,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:47,321 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:47,321 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:47,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33093
2024-01-15 06:32:47,322 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33093
2024-01-15 06:32:47,322 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34939
2024-01-15 06:32:47,322 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:47,322 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:47,322 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:47,322 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:47,322 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hr8m1no3
2024-01-15 06:32:47,323 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b47c1a91-8e30-4aa4-b6c5-9eebcd2505b6
2024-01-15 06:32:47,324 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:47,325 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40769
2024-01-15 06:32:47,325 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40769
2024-01-15 06:32:47,325 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44027
2024-01-15 06:32:47,325 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:47,325 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:47,325 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:47,326 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:47,326 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-459bi0vn
2024-01-15 06:32:47,326 - distributed.worker - INFO - Starting Worker plugin PreImport-89d777cd-1666-426e-919d-1e62f7deed3f
2024-01-15 06:32:47,326 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58ce8c2b-143d-4a65-8e29-564db3c3efbd
2024-01-15 06:32:47,326 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf5e23d0-207b-4e85-8757-36f6485ef477
2024-01-15 06:32:47,328 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:47,329 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43241
2024-01-15 06:32:47,330 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43241
2024-01-15 06:32:47,330 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33697
2024-01-15 06:32:47,330 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:47,330 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:47,330 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:47,330 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:47,330 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hh8gkff3
2024-01-15 06:32:47,330 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0b5e8ff-a9bf-4678-be29-680d7fe44e2e
2024-01-15 06:32:47,541 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-317605f2-dec0-41d0-94b2-8532a11591dc
2024-01-15 06:32:47,542 - distributed.worker - INFO - Starting Worker plugin PreImport-28be3a13-d15c-4498-a641-bd2324cb3a8f
2024-01-15 06:32:47,542 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:47,565 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41291', status: init, memory: 0, processing: 0>
2024-01-15 06:32:47,567 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41291
2024-01-15 06:32:47,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56318
2024-01-15 06:32:47,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:47,568 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:47,569 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:47,570 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:49,146 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3adcf1fc-1282-4261-923d-1bd8142e062e
2024-01-15 06:32:49,147 - distributed.worker - INFO - Starting Worker plugin PreImport-dc27c341-44ea-45be-92a3-040ed1416f3f
2024-01-15 06:32:49,148 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,196 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35437', status: init, memory: 0, processing: 0>
2024-01-15 06:32:49,197 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35437
2024-01-15 06:32:49,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56336
2024-01-15 06:32:49,199 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:49,201 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:49,201 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,204 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:49,217 - distributed.worker - INFO - Starting Worker plugin PreImport-aa3a213e-a569-442c-9700-0ce577025f31
2024-01-15 06:32:49,217 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11036260-cdea-4484-8220-492e51ff8ded
2024-01-15 06:32:49,218 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,234 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ba171b9-aa2b-44b5-bd20-27f5512e5d0d
2024-01-15 06:32:49,235 - distributed.worker - INFO - Starting Worker plugin PreImport-e74c7d08-4a01-4e78-9611-a10e43c73049
2024-01-15 06:32:49,235 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,240 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-838065b8-ab4d-40ea-868e-6c90afe2c223
2024-01-15 06:32:49,241 - distributed.worker - INFO - Starting Worker plugin PreImport-16a08370-937e-41c2-83f0-1410932dedcc
2024-01-15 06:32:49,241 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,244 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d133f80-2f91-41ca-8f6f-77d3925f44f4
2024-01-15 06:32:49,247 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,247 - distributed.worker - INFO - Starting Worker plugin PreImport-43ce362d-1f51-4ca6-ba5c-a7f1547548de
2024-01-15 06:32:49,248 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,252 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f370abd-85b6-46f1-bee8-695e6553e850
2024-01-15 06:32:49,253 - distributed.worker - INFO - Starting Worker plugin PreImport-0a1464fd-a61c-46dc-ae4b-3d891caa6761
2024-01-15 06:32:49,253 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,259 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33093', status: init, memory: 0, processing: 0>
2024-01-15 06:32:49,260 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33093
2024-01-15 06:32:49,260 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56354
2024-01-15 06:32:49,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:49,262 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:49,263 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:49,267 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40889', status: init, memory: 0, processing: 0>
2024-01-15 06:32:49,268 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40889
2024-01-15 06:32:49,268 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56370
2024-01-15 06:32:49,269 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:49,270 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35851', status: init, memory: 0, processing: 0>
2024-01-15 06:32:49,270 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:49,270 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,270 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35851
2024-01-15 06:32:49,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56352
2024-01-15 06:32:49,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:49,273 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40769', status: init, memory: 0, processing: 0>
2024-01-15 06:32:49,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:49,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40769
2024-01-15 06:32:49,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56384
2024-01-15 06:32:49,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:49,275 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:49,275 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,275 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:49,275 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:49,278 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:49,283 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43241', status: init, memory: 0, processing: 0>
2024-01-15 06:32:49,284 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43241
2024-01-15 06:32:49,284 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56410
2024-01-15 06:32:49,285 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:49,286 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:49,286 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,287 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:49,289 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43403', status: init, memory: 0, processing: 0>
2024-01-15 06:32:49,290 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43403
2024-01-15 06:32:49,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56396
2024-01-15 06:32:49,292 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:49,293 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:49,293 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:49,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:49,352 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:49,353 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:49,353 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:49,353 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:49,354 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:49,355 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:49,355 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:49,355 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:49,362 - distributed.scheduler - INFO - Remove client Client-e3461b13-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:49,363 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56296; closing.
2024-01-15 06:32:49,363 - distributed.scheduler - INFO - Remove client Client-e3461b13-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:49,364 - distributed.scheduler - INFO - Close client connection: Client-e3461b13-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:49,366 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35051'. Reason: nanny-close
2024-01-15 06:32:49,366 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:49,367 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44435'. Reason: nanny-close
2024-01-15 06:32:49,368 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:49,368 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40055'. Reason: nanny-close
2024-01-15 06:32:49,368 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41291. Reason: nanny-close
2024-01-15 06:32:49,369 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:49,369 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34253'. Reason: nanny-close
2024-01-15 06:32:49,370 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:49,370 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35437. Reason: nanny-close
2024-01-15 06:32:49,370 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34779'. Reason: nanny-close
2024-01-15 06:32:49,370 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43241. Reason: nanny-close
2024-01-15 06:32:49,370 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:49,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43055'. Reason: nanny-close
2024-01-15 06:32:49,371 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40889. Reason: nanny-close
2024-01-15 06:32:49,371 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:49,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34723'. Reason: nanny-close
2024-01-15 06:32:49,371 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:49,372 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:49,372 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56318; closing.
2024-01-15 06:32:49,372 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43403. Reason: nanny-close
2024-01-15 06:32:49,372 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33235'. Reason: nanny-close
2024-01-15 06:32:49,373 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41291', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300369.3729746')
2024-01-15 06:32:49,373 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:49,373 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:49,373 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35851. Reason: nanny-close
2024-01-15 06:32:49,374 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40769. Reason: nanny-close
2024-01-15 06:32:49,374 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:49,374 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33093. Reason: nanny-close
2024-01-15 06:32:49,374 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:49,374 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56336; closing.
2024-01-15 06:32:49,376 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35437', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300369.376437')
2024-01-15 06:32:49,377 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:49,377 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56410; closing.
2024-01-15 06:32:49,377 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:49,377 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:49,377 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:49,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:49,379 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:49,380 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:49,380 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:49,378 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56336>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56336>: Stream is closed
2024-01-15 06:32:49,380 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:49,381 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43241', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300369.3814032')
2024-01-15 06:32:49,381 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:49,382 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56370; closing.
2024-01-15 06:32:49,383 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:49,383 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300369.3835804')
2024-01-15 06:32:49,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56396; closing.
2024-01-15 06:32:49,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56384; closing.
2024-01-15 06:32:49,384 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:49,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56352; closing.
2024-01-15 06:32:49,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43403', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300369.3858335')
2024-01-15 06:32:49,386 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40769', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300369.3865192')
2024-01-15 06:32:49,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35851', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300369.3870304')
2024-01-15 06:32:49,387 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56354; closing.
2024-01-15 06:32:49,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33093', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300369.3884928')
2024-01-15 06:32:49,388 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:32:49,389 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56354>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-15 06:32:50,432 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:32:50,432 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:32:50,433 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:32:50,435 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:32:50,435 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-15 06:32:52,777 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:32:52,782 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-15 06:32:52,786 - distributed.scheduler - INFO - State start
2024-01-15 06:32:52,814 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:32:52,815 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:32:52,816 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-15 06:32:52,816 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:32:52,852 - distributed.scheduler - INFO - Receive client connection: Client-e7ec0c5a-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:52,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37134
2024-01-15 06:32:53,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35083'
2024-01-15 06:32:53,135 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40251'
2024-01-15 06:32:53,152 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33811'
2024-01-15 06:32:53,155 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46635'
2024-01-15 06:32:53,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39815'
2024-01-15 06:32:53,174 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43791'
2024-01-15 06:32:53,184 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40211'
2024-01-15 06:32:53,194 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39371'
2024-01-15 06:32:55,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:55,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:55,065 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:55,065 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37731
2024-01-15 06:32:55,066 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37731
2024-01-15 06:32:55,066 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33689
2024-01-15 06:32:55,066 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:55,066 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:55,066 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:55,066 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:55,066 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0fjlp4ys
2024-01-15 06:32:55,066 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d105f48-d934-46de-bf96-d72371d97c02
2024-01-15 06:32:55,071 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:55,071 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:55,071 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:55,071 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:55,072 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:55,072 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:55,075 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:55,075 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:55,076 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38529
2024-01-15 06:32:55,076 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38529
2024-01-15 06:32:55,076 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42423
2024-01-15 06:32:55,076 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:55,076 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37085
2024-01-15 06:32:55,076 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:55,076 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37085
2024-01-15 06:32:55,076 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:55,076 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38315
2024-01-15 06:32:55,076 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:55,076 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:55,076 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-meh5e6x6
2024-01-15 06:32:55,076 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:55,076 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:55,076 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:55,076 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:55,077 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hqjvas3_
2024-01-15 06:32:55,077 - distributed.worker - INFO - Starting Worker plugin PreImport-01c1e459-247c-40d6-891c-65ab6106bafd
2024-01-15 06:32:55,077 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a33e119c-7fef-4ae5-bd10-59c3238c2afc
2024-01-15 06:32:55,077 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce3774ff-0386-4a0c-b6d1-eaaf37ea387b
2024-01-15 06:32:55,077 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b4eecaa-0173-478b-a4e8-4e943b2cbc40
2024-01-15 06:32:55,077 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36877
2024-01-15 06:32:55,077 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36877
2024-01-15 06:32:55,077 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41203
2024-01-15 06:32:55,078 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:55,078 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:55,078 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:55,078 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:55,078 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ekn75ofr
2024-01-15 06:32:55,078 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30ad9841-9280-426c-9648-9ffdf066d523
2024-01-15 06:32:55,094 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:55,094 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:55,099 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:55,100 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36427
2024-01-15 06:32:55,100 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36427
2024-01-15 06:32:55,100 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35919
2024-01-15 06:32:55,100 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:55,100 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:55,100 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:55,100 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:55,100 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0y4boy0g
2024-01-15 06:32:55,100 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72af4e10-46f2-4540-b8ec-b49ca2f5f3b5
2024-01-15 06:32:55,119 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:55,119 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:55,123 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:55,124 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34547
2024-01-15 06:32:55,124 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34547
2024-01-15 06:32:55,124 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40965
2024-01-15 06:32:55,124 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:55,124 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:55,124 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:55,124 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:55,124 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ozeorps
2024-01-15 06:32:55,124 - distributed.worker - INFO - Starting Worker plugin RMMSetup-23dd3e11-a811-41a9-adff-9fd2717d12a3
2024-01-15 06:32:55,132 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:55,132 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:55,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:55,138 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33767
2024-01-15 06:32:55,138 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33767
2024-01-15 06:32:55,138 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35993
2024-01-15 06:32:55,138 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:55,138 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:55,138 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:55,138 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:55,138 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uzqqhsa_
2024-01-15 06:32:55,138 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f73ea739-b943-47df-8035-ce80a5617935
2024-01-15 06:32:55,138 - distributed.worker - INFO - Starting Worker plugin PreImport-764134bb-0cc1-40b1-bf01-5dd5f42f713b
2024-01-15 06:32:55,139 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1a421547-923e-44cd-ae05-e0979d5b44f5
2024-01-15 06:32:55,146 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:32:55,146 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:32:55,151 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:32:55,151 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42105
2024-01-15 06:32:55,151 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42105
2024-01-15 06:32:55,152 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36097
2024-01-15 06:32:55,152 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:32:55,152 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:55,152 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:32:55,152 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:32:55,152 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nz_stusf
2024-01-15 06:32:55,152 - distributed.worker - INFO - Starting Worker plugin PreImport-fbed0c88-b7a4-4a27-888d-88cf35d7c939
2024-01-15 06:32:55,152 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-507c1506-db8b-4d59-b878-b41c8336357e
2024-01-15 06:32:55,152 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cac21575-44c0-4a03-b255-4e68b847ba0d
2024-01-15 06:32:57,107 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-022b451c-5417-4952-928d-8ad567ffc177
2024-01-15 06:32:57,108 - distributed.worker - INFO - Starting Worker plugin PreImport-afa074fe-5fda-41a6-bd5f-f5ed4ac51705
2024-01-15 06:32:57,109 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,146 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,147 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-553aae5d-f4f6-4e9d-ab04-7a7acbca5cf9
2024-01-15 06:32:57,146 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37731', status: init, memory: 0, processing: 0>
2024-01-15 06:32:57,147 - distributed.worker - INFO - Starting Worker plugin PreImport-fcc9c8aa-d881-4e48-9437-a6e68d3d2e38
2024-01-15 06:32:57,148 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37731
2024-01-15 06:32:57,148 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37222
2024-01-15 06:32:57,148 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:57,151 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:57,151 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:57,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5183a895-b294-4d46-adbe-a6252824efe1
2024-01-15 06:32:57,160 - distributed.worker - INFO - Starting Worker plugin PreImport-aebbbd12-a996-4ed6-b588-20ad8cf5070b
2024-01-15 06:32:57,161 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d7816b5-8c92-45f6-a21a-e69846e5547d
2024-01-15 06:32:57,161 - distributed.worker - INFO - Starting Worker plugin PreImport-b5fb1d76-e817-40b5-9623-22de8aa88eea
2024-01-15 06:32:57,162 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,171 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,176 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37085', status: init, memory: 0, processing: 0>
2024-01-15 06:32:57,177 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37085
2024-01-15 06:32:57,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37234
2024-01-15 06:32:57,178 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:57,179 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:57,179 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,180 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:57,184 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38529', status: init, memory: 0, processing: 0>
2024-01-15 06:32:57,184 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38529
2024-01-15 06:32:57,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37248
2024-01-15 06:32:57,184 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37c5f0c6-eada-45c3-9402-0fb1c2f96c80
2024-01-15 06:32:57,185 - distributed.worker - INFO - Starting Worker plugin PreImport-4bb0e674-99dd-4f06-9372-ea2ff1ab6b63
2024-01-15 06:32:57,186 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:57,186 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,187 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36427', status: init, memory: 0, processing: 0>
2024-01-15 06:32:57,187 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36427
2024-01-15 06:32:57,187 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:57,187 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37262
2024-01-15 06:32:57,187 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,188 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:57,189 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:57,189 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:57,190 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:57,196 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33767', status: init, memory: 0, processing: 0>
2024-01-15 06:32:57,196 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,197 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33767
2024-01-15 06:32:57,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37282
2024-01-15 06:32:57,198 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:57,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34547', status: init, memory: 0, processing: 0>
2024-01-15 06:32:57,199 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:57,199 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34547
2024-01-15 06:32:57,199 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37268
2024-01-15 06:32:57,200 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:57,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:57,202 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:57,202 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,204 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:57,220 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42105', status: init, memory: 0, processing: 0>
2024-01-15 06:32:57,221 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42105
2024-01-15 06:32:57,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37310
2024-01-15 06:32:57,222 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36877', status: init, memory: 0, processing: 0>
2024-01-15 06:32:57,222 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:57,222 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36877
2024-01-15 06:32:57,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37296
2024-01-15 06:32:57,223 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:57,223 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:32:57,224 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:57,225 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:32:57,225 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:32:57,227 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:32:57,252 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:57,253 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:57,253 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:57,253 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:57,253 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:57,253 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:57,254 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:57,254 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:32:57,258 - distributed.scheduler - INFO - Remove client Client-e7ec0c5a-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:57,258 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37134; closing.
2024-01-15 06:32:57,258 - distributed.scheduler - INFO - Remove client Client-e7ec0c5a-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:57,259 - distributed.scheduler - INFO - Close client connection: Client-e7ec0c5a-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:32:57,259 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35083'. Reason: nanny-close
2024-01-15 06:32:57,260 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:57,260 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40251'. Reason: nanny-close
2024-01-15 06:32:57,261 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33811'. Reason: nanny-close
2024-01-15 06:32:57,261 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:57,261 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46635'. Reason: nanny-close
2024-01-15 06:32:57,261 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37731. Reason: nanny-close
2024-01-15 06:32:57,261 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:57,261 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39815'. Reason: nanny-close
2024-01-15 06:32:57,262 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37085. Reason: nanny-close
2024-01-15 06:32:57,262 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:57,262 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43791'. Reason: nanny-close
2024-01-15 06:32:57,262 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36427. Reason: nanny-close
2024-01-15 06:32:57,262 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:57,262 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40211'. Reason: nanny-close
2024-01-15 06:32:57,263 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38529. Reason: nanny-close
2024-01-15 06:32:57,263 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:57,263 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39371'. Reason: nanny-close
2024-01-15 06:32:57,263 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34547. Reason: nanny-close
2024-01-15 06:32:57,263 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:57,264 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42105. Reason: nanny-close
2024-01-15 06:32:57,264 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37222; closing.
2024-01-15 06:32:57,264 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:57,264 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33767. Reason: nanny-close
2024-01-15 06:32:57,264 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37731', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300377.2643962')
2024-01-15 06:32:57,264 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:57,265 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:57,265 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:57,265 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:57,266 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:57,266 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37234; closing.
2024-01-15 06:32:57,266 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:57,266 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:57,266 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:57,267 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:57,267 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:57,267 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:57,267 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37248; closing.
2024-01-15 06:32:57,267 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37085', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300377.2677646')
2024-01-15 06:32:57,268 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37268; closing.
2024-01-15 06:32:57,268 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:57,268 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37310; closing.
2024-01-15 06:32:57,268 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37262; closing.
2024-01-15 06:32:57,268 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:57,269 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38529', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300377.268971')
2024-01-15 06:32:57,269 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34547', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300377.269316')
2024-01-15 06:32:57,269 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42105', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300377.2696018')
2024-01-15 06:32:57,269 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36427', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300377.2699366')
2024-01-15 06:32:57,270 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37282; closing.
2024-01-15 06:32:57,270 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33767', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300377.2706463')
2024-01-15 06:32:57,276 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:32:57,277 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36877. Reason: nanny-close
2024-01-15 06:32:57,280 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37296; closing.
2024-01-15 06:32:57,280 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:32:57,280 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36877', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300377.2802563')
2024-01-15 06:32:57,280 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:32:57,282 - distributed.nanny - INFO - Worker closed
2024-01-15 06:32:58,176 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:32:58,176 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:32:58,176 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:32:58,178 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:32:58,178 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-15 06:33:00,251 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:00,256 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-15 06:33:00,260 - distributed.scheduler - INFO - State start
2024-01-15 06:33:00,283 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:00,284 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:33:00,285 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-15 06:33:00,285 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:33:00,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33331'
2024-01-15 06:33:00,675 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34745'
2024-01-15 06:33:00,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46521'
2024-01-15 06:33:00,698 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43919'
2024-01-15 06:33:00,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35493'
2024-01-15 06:33:00,718 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35797'
2024-01-15 06:33:00,727 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44105'
2024-01-15 06:33:00,736 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35781'
2024-01-15 06:33:01,560 - distributed.scheduler - INFO - Receive client connection: Client-ec73c268-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:01,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35094
2024-01-15 06:33:02,580 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:02,580 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:02,584 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:02,585 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39083
2024-01-15 06:33:02,585 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39083
2024-01-15 06:33:02,586 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38655
2024-01-15 06:33:02,586 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:02,586 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:02,586 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:02,586 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:02,586 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9mo3t8z4
2024-01-15 06:33:02,586 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88bd4ce3-8473-4760-b91a-78478186784a
2024-01-15 06:33:02,586 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e03cb9c-c06c-44af-b82b-edb778b094c6
2024-01-15 06:33:02,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:02,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:02,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:02,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:02,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:02,836 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:02,836 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:02,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:02,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:02,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:02,838 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:02,839 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35615
2024-01-15 06:33:02,839 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35615
2024-01-15 06:33:02,839 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46367
2024-01-15 06:33:02,839 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:02,839 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:02,839 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:02,839 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:02,839 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h62r0rtn
2024-01-15 06:33:02,840 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da781bfc-832e-4d18-96d3-0933cd567c3d
2024-01-15 06:33:02,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:02,842 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:02,842 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:02,842 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:02,842 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:02,842 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40545
2024-01-15 06:33:02,842 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:02,843 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40545
2024-01-15 06:33:02,843 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43223
2024-01-15 06:33:02,843 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:02,843 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:02,843 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:02,843 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34429
2024-01-15 06:33:02,843 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:02,843 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34429
2024-01-15 06:33:02,843 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sym0powt
2024-01-15 06:33:02,843 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40261
2024-01-15 06:33:02,843 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:02,843 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:02,843 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:02,843 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:02,843 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08168332-1e1a-45ca-a8e6-b4356a0b5223
2024-01-15 06:33:02,843 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dpvhqzg7
2024-01-15 06:33:02,843 - distributed.worker - INFO - Starting Worker plugin PreImport-4b058590-e46a-4d6a-9846-32f3397318f3
2024-01-15 06:33:02,844 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32999
2024-01-15 06:33:02,844 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-52be578a-8a1f-4293-8911-cb3036ef2027
2024-01-15 06:33:02,844 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32999
2024-01-15 06:33:02,844 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35645
2024-01-15 06:33:02,844 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38303
2024-01-15 06:33:02,844 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:02,844 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:02,844 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38303
2024-01-15 06:33:02,844 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:02,844 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:02,844 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34947
2024-01-15 06:33:02,844 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d7dc5fb-a75d-42d9-9103-cff767061395
2024-01-15 06:33:02,844 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mjxqxssq
2024-01-15 06:33:02,844 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:02,844 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:02,844 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:02,844 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:02,844 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2727dddc-2b2f-4190-9533-3eced0cc393f
2024-01-15 06:33:02,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:02,844 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:02,844 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8j2m9se7
2024-01-15 06:33:02,845 - distributed.worker - INFO - Starting Worker plugin RMMSetup-063856ac-2c99-43a3-9e3f-4a121b7b811a
2024-01-15 06:33:02,847 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:02,848 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38235
2024-01-15 06:33:02,848 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38235
2024-01-15 06:33:02,848 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44535
2024-01-15 06:33:02,848 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:02,849 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:02,849 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:02,849 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:02,849 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-whv6hwi7
2024-01-15 06:33:02,849 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b30c73d6-5727-4741-ad89-fb4e65a0963a
2024-01-15 06:33:02,851 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:02,853 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46659
2024-01-15 06:33:02,853 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46659
2024-01-15 06:33:02,853 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40361
2024-01-15 06:33:02,853 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:02,853 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:02,853 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:02,853 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:02,853 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-snm918nj
2024-01-15 06:33:02,853 - distributed.worker - INFO - Starting Worker plugin RMMSetup-05a73e10-3930-4e99-bf54-fa7d07fd76b4
2024-01-15 06:33:03,130 - distributed.worker - INFO - Starting Worker plugin PreImport-c1d14744-925d-4d6e-a31e-e5079c6a0d05
2024-01-15 06:33:03,131 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:03,159 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39083', status: init, memory: 0, processing: 0>
2024-01-15 06:33:03,160 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39083
2024-01-15 06:33:03,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35104
2024-01-15 06:33:03,161 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:03,162 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:03,162 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:03,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:05,128 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3fec5412-9a1c-4cb1-a9b5-065d94d0ae53
2024-01-15 06:33:05,130 - distributed.worker - INFO - Starting Worker plugin PreImport-969d7d73-59f8-4c24-8d87-745474d958f9
2024-01-15 06:33:05,130 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,151 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29bbbf68-636f-4cc8-8a44-310c8e268e30
2024-01-15 06:33:05,153 - distributed.worker - INFO - Starting Worker plugin PreImport-45a5b51d-79d1-4db7-9c50-bf34ab499e85
2024-01-15 06:33:05,154 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,159 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35615', status: init, memory: 0, processing: 0>
2024-01-15 06:33:05,160 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35615
2024-01-15 06:33:05,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35126
2024-01-15 06:33:05,161 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:05,162 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:05,162 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7bdaa0c5-a244-444a-916b-753944f9d04e
2024-01-15 06:33:05,164 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:05,164 - distributed.worker - INFO - Starting Worker plugin PreImport-21b6c6d8-c453-4c0c-8ce8-02e216f78d26
2024-01-15 06:33:05,165 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,169 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55eb9688-accf-4fc9-8a42-d033865b8a22
2024-01-15 06:33:05,170 - distributed.worker - INFO - Starting Worker plugin PreImport-22d791dc-28b8-434c-9aba-73c23ca5b7b2
2024-01-15 06:33:05,171 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,182 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,186 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25e2e8bc-b38b-4781-b50f-577fbc1f1b9c
2024-01-15 06:33:05,187 - distributed.worker - INFO - Starting Worker plugin PreImport-6680864c-09ef-4e5e-a495-1cb27df1aa4d
2024-01-15 06:33:05,187 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,190 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c76b7385-1ea5-4f0f-a18b-b3cfd65aa781
2024-01-15 06:33:05,191 - distributed.worker - INFO - Starting Worker plugin PreImport-786bba85-71b4-48ae-8878-015988cbc085
2024-01-15 06:33:05,191 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,194 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46659', status: init, memory: 0, processing: 0>
2024-01-15 06:33:05,195 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46659
2024-01-15 06:33:05,195 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35132
2024-01-15 06:33:05,197 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:05,198 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:05,198 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,200 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:05,201 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38235', status: init, memory: 0, processing: 0>
2024-01-15 06:33:05,202 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38235
2024-01-15 06:33:05,202 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35140
2024-01-15 06:33:05,203 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:05,205 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:05,205 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38303', status: init, memory: 0, processing: 0>
2024-01-15 06:33:05,207 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:05,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38303
2024-01-15 06:33:05,207 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35142
2024-01-15 06:33:05,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:05,210 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:05,210 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,212 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:05,214 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40545', status: init, memory: 0, processing: 0>
2024-01-15 06:33:05,215 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40545
2024-01-15 06:33:05,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35160
2024-01-15 06:33:05,216 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:05,217 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:05,217 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32999', status: init, memory: 0, processing: 0>
2024-01-15 06:33:05,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:05,218 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32999
2024-01-15 06:33:05,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35168
2024-01-15 06:33:05,219 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34429', status: init, memory: 0, processing: 0>
2024-01-15 06:33:05,219 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:05,219 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34429
2024-01-15 06:33:05,219 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35148
2024-01-15 06:33:05,220 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:05,220 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,221 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:05,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:05,222 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:05,222 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:05,224 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:05,258 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:05,259 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:05,259 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:05,259 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:05,259 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:05,259 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:05,259 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:05,260 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:05,269 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:05,270 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:05,270 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:05,270 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:05,270 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:05,270 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:05,270 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:05,270 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:05,278 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:05,280 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:05,282 - distributed.scheduler - INFO - Remove client Client-ec73c268-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:05,283 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35094; closing.
2024-01-15 06:33:05,283 - distributed.scheduler - INFO - Remove client Client-ec73c268-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:05,283 - distributed.scheduler - INFO - Close client connection: Client-ec73c268-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:05,363 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33331'. Reason: nanny-close
2024-01-15 06:33:05,363 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:05,363 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34745'. Reason: nanny-close
2024-01-15 06:33:05,364 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:05,364 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46521'. Reason: nanny-close
2024-01-15 06:33:05,364 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40545. Reason: nanny-close
2024-01-15 06:33:05,365 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:05,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43919'. Reason: nanny-close
2024-01-15 06:33:05,365 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32999. Reason: nanny-close
2024-01-15 06:33:05,365 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:05,365 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35493'. Reason: nanny-close
2024-01-15 06:33:05,366 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:05,366 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38235. Reason: nanny-close
2024-01-15 06:33:05,366 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35797'. Reason: nanny-close
2024-01-15 06:33:05,366 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:05,366 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38303. Reason: nanny-close
2024-01-15 06:33:05,366 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35160; closing.
2024-01-15 06:33:05,366 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44105'. Reason: nanny-close
2024-01-15 06:33:05,366 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:05,366 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35615. Reason: nanny-close
2024-01-15 06:33:05,367 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:05,367 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40545', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300385.3670304')
2024-01-15 06:33:05,367 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35781'. Reason: nanny-close
2024-01-15 06:33:05,367 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:05,367 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39083. Reason: nanny-close
2024-01-15 06:33:05,367 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:05,367 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46659. Reason: nanny-close
2024-01-15 06:33:05,368 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:05,368 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34429. Reason: nanny-close
2024-01-15 06:33:05,368 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:05,368 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35168; closing.
2024-01-15 06:33:05,368 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:05,369 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:05,369 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:05,369 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32999', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300385.3695734')
2024-01-15 06:33:05,369 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:05,369 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35140; closing.
2024-01-15 06:33:05,370 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:05,370 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:05,370 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:05,370 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38235', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300385.3706598')
2024-01-15 06:33:05,370 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:05,371 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35142; closing.
2024-01-15 06:33:05,371 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:05,371 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:05,371 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35126; closing.
2024-01-15 06:33:05,371 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38303', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300385.371858')
2024-01-15 06:33:05,372 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35615', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300385.3722308')
2024-01-15 06:33:05,372 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:05,372 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35104; closing.
2024-01-15 06:33:05,372 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35132; closing.
2024-01-15 06:33:05,372 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:05,373 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39083', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300385.3732307')
2024-01-15 06:33:05,373 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46659', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300385.3736546')
2024-01-15 06:33:05,374 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35148; closing.
2024-01-15 06:33:05,374 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34429', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300385.374371')
2024-01-15 06:33:05,374 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:33:06,401 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:33:06,401 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:33:06,401 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:33:06,403 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:33:06,403 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-15 06:33:08,549 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:08,554 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-15 06:33:08,558 - distributed.scheduler - INFO - State start
2024-01-15 06:33:08,580 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:08,581 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:33:08,582 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-15 06:33:08,582 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:33:08,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39129'
2024-01-15 06:33:08,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37249'
2024-01-15 06:33:08,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42547'
2024-01-15 06:33:08,842 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38869'
2024-01-15 06:33:08,850 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34081'
2024-01-15 06:33:08,859 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46833'
2024-01-15 06:33:08,867 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35845'
2024-01-15 06:33:08,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45565'
2024-01-15 06:33:08,998 - distributed.scheduler - INFO - Receive client connection: Client-f16ecf78-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:09,011 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35332
2024-01-15 06:33:10,733 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:10,733 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:10,737 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:10,738 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37785
2024-01-15 06:33:10,738 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37785
2024-01-15 06:33:10,738 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36155
2024-01-15 06:33:10,738 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:10,738 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:10,738 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:10,738 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:10,738 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-otzf6m_v
2024-01-15 06:33:10,738 - distributed.worker - INFO - Starting Worker plugin RMMSetup-51f6d579-5ac2-4c8e-90a0-9030df634524
2024-01-15 06:33:10,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:10,761 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:10,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:10,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:10,765 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:10,766 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:10,766 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40903
2024-01-15 06:33:10,766 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40903
2024-01-15 06:33:10,766 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39791
2024-01-15 06:33:10,766 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:10,766 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:10,766 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:10,766 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:10,766 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cbttgnjx
2024-01-15 06:33:10,767 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce32224d-1fa6-40fa-b115-a01de6668feb
2024-01-15 06:33:10,767 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35829
2024-01-15 06:33:10,767 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35829
2024-01-15 06:33:10,767 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37535
2024-01-15 06:33:10,767 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:10,767 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:10,767 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:10,767 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:10,767 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a7y2vcih
2024-01-15 06:33:10,767 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7837d679-cc74-48ac-84e1-3edd90577dbc
2024-01-15 06:33:10,799 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:10,799 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:10,803 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:10,803 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:10,803 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:10,804 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41275
2024-01-15 06:33:10,804 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41275
2024-01-15 06:33:10,804 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37561
2024-01-15 06:33:10,804 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:10,804 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:10,804 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:10,804 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:10,804 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a4k5imlh
2024-01-15 06:33:10,804 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8a30396d-65bb-476b-b1e2-59a2e61fff31
2024-01-15 06:33:10,807 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:10,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38741
2024-01-15 06:33:10,808 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38741
2024-01-15 06:33:10,808 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45755
2024-01-15 06:33:10,808 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:10,809 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:10,809 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:10,809 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:10,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6hsfkaqq
2024-01-15 06:33:10,809 - distributed.worker - INFO - Starting Worker plugin RMMSetup-925723f2-f1ed-4318-88ea-aeffe2b1472f
2024-01-15 06:33:10,817 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:10,817 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:10,817 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:10,817 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:10,821 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:10,822 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:10,822 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38701
2024-01-15 06:33:10,822 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38701
2024-01-15 06:33:10,822 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45399
2024-01-15 06:33:10,823 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:10,823 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:10,823 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41675
2024-01-15 06:33:10,823 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:10,823 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41675
2024-01-15 06:33:10,823 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:10,823 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39091
2024-01-15 06:33:10,823 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rdys63b_
2024-01-15 06:33:10,823 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:10,823 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:10,823 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:10,823 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:10,823 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6a5e0940-6718-4011-afb8-e3d590032314
2024-01-15 06:33:10,823 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2zj9sfjj
2024-01-15 06:33:10,823 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c6880101-c78b-453b-b0a9-81db90d4ad6a
2024-01-15 06:33:10,863 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:10,863 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:10,867 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:10,868 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34831
2024-01-15 06:33:10,868 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34831
2024-01-15 06:33:10,868 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37893
2024-01-15 06:33:10,868 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:10,868 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:10,868 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:10,868 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:10,868 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uyhuua5z
2024-01-15 06:33:10,868 - distributed.worker - INFO - Starting Worker plugin PreImport-b32d8455-8907-4ea7-90cd-cec817037f38
2024-01-15 06:33:10,868 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72a5ca61-6d65-4056-88d1-cbcea639c5a8
2024-01-15 06:33:10,869 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c925006-ce55-4226-96c3-7c92c66b82f1
2024-01-15 06:33:13,391 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f99f9b2-3c77-42e8-8a31-301092657758
2024-01-15 06:33:13,391 - distributed.worker - INFO - Starting Worker plugin PreImport-17b6e09d-de45-4bbc-a2a7-7a45e8b4e98f
2024-01-15 06:33:13,392 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,415 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35829', status: init, memory: 0, processing: 0>
2024-01-15 06:33:13,416 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35829
2024-01-15 06:33:13,416 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56832
2024-01-15 06:33:13,417 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:13,418 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:13,418 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,419 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:13,424 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-981386fe-cf19-487b-9b40-d9de4c0b7f70
2024-01-15 06:33:13,424 - distributed.worker - INFO - Starting Worker plugin PreImport-76f63607-6d9f-4c49-8f45-96da3c61e705
2024-01-15 06:33:13,425 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,447 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38741', status: init, memory: 0, processing: 0>
2024-01-15 06:33:13,447 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38741
2024-01-15 06:33:13,447 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56836
2024-01-15 06:33:13,448 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:13,448 - distributed.worker - INFO - Starting Worker plugin PreImport-350677cb-6209-4835-b5b0-b440e9f117b6
2024-01-15 06:33:13,449 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff0e9fb3-ad1c-4549-beb3-9d336aba403f
2024-01-15 06:33:13,449 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:13,449 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,450 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:13,464 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-402ad083-5d8f-4de4-8303-1bb3db9d851f
2024-01-15 06:33:13,465 - distributed.worker - INFO - Starting Worker plugin PreImport-7f94f3ee-3bea-4626-9aae-409ce41d815f
2024-01-15 06:33:13,465 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,466 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee874c7d-1845-4fe0-8768-9da1f0622804
2024-01-15 06:33:13,467 - distributed.worker - INFO - Starting Worker plugin PreImport-cbeaec7d-2792-4d67-a1ef-a617df76f63c
2024-01-15 06:33:13,468 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,470 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,472 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-17d867bb-2bec-4649-82fb-fa891d650bd5
2024-01-15 06:33:13,472 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5a24b14-7301-498d-a93f-0dea56a871b0
2024-01-15 06:33:13,473 - distributed.worker - INFO - Starting Worker plugin PreImport-45ca0db3-e32e-4604-b0df-20ffd8a89f50
2024-01-15 06:33:13,473 - distributed.worker - INFO - Starting Worker plugin PreImport-284c3e9e-ee74-4aaf-91c4-62a73acc9e40
2024-01-15 06:33:13,474 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,474 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,483 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41675', status: init, memory: 0, processing: 0>
2024-01-15 06:33:13,483 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41675
2024-01-15 06:33:13,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56838
2024-01-15 06:33:13,485 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:13,486 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:13,486 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,487 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41275', status: init, memory: 0, processing: 0>
2024-01-15 06:33:13,487 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41275
2024-01-15 06:33:13,487 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56850
2024-01-15 06:33:13,488 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:13,488 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:13,489 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:13,489 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,490 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:13,491 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34831', status: init, memory: 0, processing: 0>
2024-01-15 06:33:13,492 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34831
2024-01-15 06:33:13,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56872
2024-01-15 06:33:13,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:13,494 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:13,494 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:13,505 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38701', status: init, memory: 0, processing: 0>
2024-01-15 06:33:13,505 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38701
2024-01-15 06:33:13,505 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56894
2024-01-15 06:33:13,507 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:13,507 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40903', status: init, memory: 0, processing: 0>
2024-01-15 06:33:13,508 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40903
2024-01-15 06:33:13,508 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56858
2024-01-15 06:33:13,508 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:13,508 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,509 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:13,510 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:13,511 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:13,511 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,511 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37785', status: init, memory: 0, processing: 0>
2024-01-15 06:33:13,512 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37785
2024-01-15 06:33:13,512 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56886
2024-01-15 06:33:13,512 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:13,513 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:13,514 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:13,514 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:13,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:13,552 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:13,552 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:13,553 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:13,553 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:13,553 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:13,553 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:13,553 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:13,554 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:13,564 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:13,564 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:13,564 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:13,565 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:13,565 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:13,565 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:13,565 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:13,565 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:13,573 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:13,574 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:13,577 - distributed.scheduler - INFO - Remove client Client-f16ecf78-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:13,577 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35332; closing.
2024-01-15 06:33:13,577 - distributed.scheduler - INFO - Remove client Client-f16ecf78-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:13,578 - distributed.scheduler - INFO - Close client connection: Client-f16ecf78-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:13,578 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39129'. Reason: nanny-close
2024-01-15 06:33:13,579 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:13,579 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37249'. Reason: nanny-close
2024-01-15 06:33:13,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:13,580 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42547'. Reason: nanny-close
2024-01-15 06:33:13,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37785. Reason: nanny-close
2024-01-15 06:33:13,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:13,580 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38869'. Reason: nanny-close
2024-01-15 06:33:13,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40903. Reason: nanny-close
2024-01-15 06:33:13,581 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:13,581 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34081'. Reason: nanny-close
2024-01-15 06:33:13,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35829. Reason: nanny-close
2024-01-15 06:33:13,581 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:13,582 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46833'. Reason: nanny-close
2024-01-15 06:33:13,582 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38741. Reason: nanny-close
2024-01-15 06:33:13,582 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:13,582 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35845'. Reason: nanny-close
2024-01-15 06:33:13,582 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38701. Reason: nanny-close
2024-01-15 06:33:13,582 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:13,582 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45565'. Reason: nanny-close
2024-01-15 06:33:13,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:13,583 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:13,583 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41675. Reason: nanny-close
2024-01-15 06:33:13,583 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41275. Reason: nanny-close
2024-01-15 06:33:13,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:13,584 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:13,584 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:13,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56886; closing.
2024-01-15 06:33:13,585 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:13,585 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:13,585 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:13,585 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:13,585 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:13,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37785', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300393.5859098')
2024-01-15 06:33:13,586 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34831. Reason: nanny-close
2024-01-15 06:33:13,586 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:13,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56858; closing.
2024-01-15 06:33:13,586 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:13,586 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:13,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56832; closing.
2024-01-15 06:33:13,587 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:13,587 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56836; closing.
2024-01-15 06:33:13,587 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:13,587 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:13,588 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40903', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300393.5879512')
2024-01-15 06:33:13,588 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56894; closing.
2024-01-15 06:33:13,588 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35829', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300393.588546')
2024-01-15 06:33:13,588 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56850; closing.
2024-01-15 06:33:13,589 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56838; closing.
2024-01-15 06:33:13,589 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:13,590 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38741', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300393.5900834')
2024-01-15 06:33:13,590 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38701', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300393.5904841')
2024-01-15 06:33:13,590 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41275', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300393.59089')
2024-01-15 06:33:13,591 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41675', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300393.5913155')
2024-01-15 06:33:13,591 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56832>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-15 06:33:13,593 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56858>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-15 06:33:13,593 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56872; closing.
2024-01-15 06:33:13,594 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34831', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300393.5940146')
2024-01-15 06:33:13,594 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:33:14,645 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:33:14,645 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:33:14,646 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:33:14,647 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:33:14,648 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-15 06:33:17,022 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:17,027 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-15 06:33:17,030 - distributed.scheduler - INFO - State start
2024-01-15 06:33:17,053 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:17,054 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:33:17,054 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-15 06:33:17,054 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:33:17,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39905'
2024-01-15 06:33:17,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35763'
2024-01-15 06:33:17,235 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39113'
2024-01-15 06:33:17,237 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35669'
2024-01-15 06:33:17,246 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37507'
2024-01-15 06:33:17,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34645'
2024-01-15 06:33:17,266 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35263'
2024-01-15 06:33:17,275 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39493'
2024-01-15 06:33:17,571 - distributed.scheduler - INFO - Receive client connection: Client-f65ffe2e-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:17,588 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57072
2024-01-15 06:33:19,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:19,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:19,083 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:19,084 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43775
2024-01-15 06:33:19,084 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43775
2024-01-15 06:33:19,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34647
2024-01-15 06:33:19,084 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:19,084 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:19,084 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:19,084 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:19,084 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tyzrul5_
2024-01-15 06:33:19,084 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a02805de-d424-4405-8df6-96283ddce2e8
2024-01-15 06:33:19,132 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:19,132 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:19,132 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:19,132 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:19,136 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:19,137 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:19,137 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36685
2024-01-15 06:33:19,137 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36685
2024-01-15 06:33:19,137 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39495
2024-01-15 06:33:19,137 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:19,137 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:19,137 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:19,137 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:19,137 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42707
2024-01-15 06:33:19,137 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-68tibp69
2024-01-15 06:33:19,137 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42707
2024-01-15 06:33:19,138 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45349
2024-01-15 06:33:19,138 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:19,138 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:19,138 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9473b93-9d61-4e60-b8f2-a2d41265d428
2024-01-15 06:33:19,138 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:19,138 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:19,138 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7bmuxats
2024-01-15 06:33:19,138 - distributed.worker - INFO - Starting Worker plugin RMMSetup-661145c6-4ee5-482c-919a-dc08252c2730
2024-01-15 06:33:19,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:19,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:19,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:19,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:19,340 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:19,340 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:19,341 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39397
2024-01-15 06:33:19,341 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39397
2024-01-15 06:33:19,341 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34005
2024-01-15 06:33:19,341 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43757
2024-01-15 06:33:19,341 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:19,341 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34005
2024-01-15 06:33:19,341 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:19,341 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46613
2024-01-15 06:33:19,341 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:19,341 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:19,342 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:19,342 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:19,342 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f_trn02z
2024-01-15 06:33:19,342 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:19,342 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:19,342 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-agt4hhi2
2024-01-15 06:33:19,342 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1aa7329e-61f2-4755-964a-909ced7f8069
2024-01-15 06:33:19,342 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9b14514-f04b-45b8-94d1-8aa5d86a5796
2024-01-15 06:33:19,351 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:19,351 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:19,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:19,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:19,356 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:19,356 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:19,356 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:19,357 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44753
2024-01-15 06:33:19,357 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44753
2024-01-15 06:33:19,357 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37745
2024-01-15 06:33:19,357 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:19,357 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:19,357 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:19,357 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:19,357 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-onvd49_w
2024-01-15 06:33:19,357 - distributed.worker - INFO - Starting Worker plugin PreImport-f5904c16-b567-46f2-b71a-a9955257d9ad
2024-01-15 06:33:19,358 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2902853c-7aa2-4e98-9742-400af2faadef
2024-01-15 06:33:19,358 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c65a0ac-3488-4959-942d-11798cb77f59
2024-01-15 06:33:19,359 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:19,360 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37651
2024-01-15 06:33:19,360 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37651
2024-01-15 06:33:19,360 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43285
2024-01-15 06:33:19,360 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:19,360 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:19,360 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:19,360 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:19,360 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c2iznx24
2024-01-15 06:33:19,361 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:19,361 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea7536a3-6b42-4077-b978-b4b177136afe
2024-01-15 06:33:19,361 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42507
2024-01-15 06:33:19,361 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42507
2024-01-15 06:33:19,361 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37817
2024-01-15 06:33:19,362 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:19,362 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:19,362 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:19,362 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:19,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4nid04k1
2024-01-15 06:33:19,362 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e356790-adf6-4ff8-b7ea-d2cf8924e3a0
2024-01-15 06:33:19,565 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6f24a81-32a1-4930-a802-2114ac4c871b
2024-01-15 06:33:19,566 - distributed.worker - INFO - Starting Worker plugin PreImport-a7672c66-e8a5-4659-9c7a-f6d4aafa3db6
2024-01-15 06:33:19,567 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:19,593 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43775', status: init, memory: 0, processing: 0>
2024-01-15 06:33:19,594 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43775
2024-01-15 06:33:19,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57088
2024-01-15 06:33:19,595 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:19,596 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:19,596 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:19,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:21,085 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ebc8b2d8-15bf-4a4c-a159-19027d6161f2
2024-01-15 06:33:21,086 - distributed.worker - INFO - Starting Worker plugin PreImport-e323ce4a-305a-4f3b-a805-17cbe0e82b61
2024-01-15 06:33:21,086 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,109 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36685', status: init, memory: 0, processing: 0>
2024-01-15 06:33:21,110 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36685
2024-01-15 06:33:21,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57522
2024-01-15 06:33:21,111 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:21,112 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:21,112 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,113 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:21,132 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c8356e5-e8d4-44f0-8736-296cadae2433
2024-01-15 06:33:21,132 - distributed.worker - INFO - Starting Worker plugin PreImport-8a574319-d8d5-4657-b3d2-e5490cbf69de
2024-01-15 06:33:21,133 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,165 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42707', status: init, memory: 0, processing: 0>
2024-01-15 06:33:21,166 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42707
2024-01-15 06:33:21,166 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57538
2024-01-15 06:33:21,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:21,169 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58daae15-4480-4860-97ee-fbbfa2866872
2024-01-15 06:33:21,169 - distributed.worker - INFO - Starting Worker plugin PreImport-e859edd1-8777-4185-a7b7-31aa0ba826a2
2024-01-15 06:33:21,169 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,169 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:21,170 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:21,175 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,192 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39397', status: init, memory: 0, processing: 0>
2024-01-15 06:33:21,192 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39397
2024-01-15 06:33:21,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57548
2024-01-15 06:33:21,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:21,194 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:21,194 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:21,197 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44753', status: init, memory: 0, processing: 0>
2024-01-15 06:33:21,197 - distributed.worker - INFO - Starting Worker plugin PreImport-03c52c19-927c-4280-b468-bfb3173f4c1d
2024-01-15 06:33:21,198 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44753
2024-01-15 06:33:21,198 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-da8ce503-5dce-44c6-bb42-aaa74639b2f0
2024-01-15 06:33:21,198 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57554
2024-01-15 06:33:21,199 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:21,199 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,200 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:21,200 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:21,211 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2388233-4f4f-46d4-8228-4c7c8f5c2842
2024-01-15 06:33:21,212 - distributed.worker - INFO - Starting Worker plugin PreImport-0dc7c238-87b3-4817-9a0b-348bb49da29f
2024-01-15 06:33:21,212 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,228 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-134d21d4-0691-491b-988b-2d3fb09bca97
2024-01-15 06:33:21,229 - distributed.worker - INFO - Starting Worker plugin PreImport-4a2e20d5-ded1-4777-87ed-98181ed6aeb8
2024-01-15 06:33:21,230 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,233 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42507', status: init, memory: 0, processing: 0>
2024-01-15 06:33:21,233 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42507
2024-01-15 06:33:21,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57558
2024-01-15 06:33:21,235 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:21,236 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:21,237 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,239 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:21,243 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34005', status: init, memory: 0, processing: 0>
2024-01-15 06:33:21,243 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34005
2024-01-15 06:33:21,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57570
2024-01-15 06:33:21,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:21,246 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:21,246 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:21,260 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37651', status: init, memory: 0, processing: 0>
2024-01-15 06:33:21,261 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37651
2024-01-15 06:33:21,261 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57582
2024-01-15 06:33:21,262 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:21,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:21,264 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:21,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:21,366 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:21,367 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:21,367 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:21,367 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:21,367 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:21,367 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:21,367 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:21,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:21,372 - distributed.scheduler - INFO - Remove client Client-f65ffe2e-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:21,372 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57072; closing.
2024-01-15 06:33:21,373 - distributed.scheduler - INFO - Remove client Client-f65ffe2e-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:21,373 - distributed.scheduler - INFO - Close client connection: Client-f65ffe2e-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:21,374 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39905'. Reason: nanny-close
2024-01-15 06:33:21,374 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:21,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35763'. Reason: nanny-close
2024-01-15 06:33:21,375 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:21,375 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39113'. Reason: nanny-close
2024-01-15 06:33:21,376 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34005. Reason: nanny-close
2024-01-15 06:33:21,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:21,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35669'. Reason: nanny-close
2024-01-15 06:33:21,376 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37651. Reason: nanny-close
2024-01-15 06:33:21,376 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:21,376 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37507'. Reason: nanny-close
2024-01-15 06:33:21,377 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43775. Reason: nanny-close
2024-01-15 06:33:21,377 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:21,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34645'. Reason: nanny-close
2024-01-15 06:33:21,377 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36685. Reason: nanny-close
2024-01-15 06:33:21,377 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:21,377 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35263'. Reason: nanny-close
2024-01-15 06:33:21,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42707. Reason: nanny-close
2024-01-15 06:33:21,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:21,378 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:21,378 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57570; closing.
2024-01-15 06:33:21,378 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39493'. Reason: nanny-close
2024-01-15 06:33:21,378 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42507. Reason: nanny-close
2024-01-15 06:33:21,378 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34005', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300401.378709')
2024-01-15 06:33:21,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:21,378 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:21,379 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39397. Reason: nanny-close
2024-01-15 06:33:21,379 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:21,379 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:21,379 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44753. Reason: nanny-close
2024-01-15 06:33:21,379 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57522; closing.
2024-01-15 06:33:21,380 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:21,380 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:21,380 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:21,380 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:21,380 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:21,380 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:21,381 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:21,381 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36685', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300401.3810394')
2024-01-15 06:33:21,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57582; closing.
2024-01-15 06:33:21,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57088; closing.
2024-01-15 06:33:21,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:21,382 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:21,382 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:21,382 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:21,383 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:21,382 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:57522>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:57522>: Stream is closed
2024-01-15 06:33:21,384 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37651', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300401.384704')
2024-01-15 06:33:21,385 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43775', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300401.3851893')
2024-01-15 06:33:21,385 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57538; closing.
2024-01-15 06:33:21,386 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42707', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300401.3862436')
2024-01-15 06:33:21,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57558; closing.
2024-01-15 06:33:21,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57548; closing.
2024-01-15 06:33:21,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42507', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300401.3873456')
2024-01-15 06:33:21,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39397', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300401.3877513')
2024-01-15 06:33:21,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57554; closing.
2024-01-15 06:33:21,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44753', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300401.3885362')
2024-01-15 06:33:21,388 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:33:21,388 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:57554>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-15 06:33:22,290 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:33:22,291 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:33:22,291 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:33:22,292 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:33:22,293 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-15 06:33:24,615 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:24,619 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-15 06:33:24,623 - distributed.scheduler - INFO - State start
2024-01-15 06:33:24,698 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:24,699 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:33:24,699 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-15 06:33:24,699 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:33:24,909 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37143'
2024-01-15 06:33:25,637 - distributed.scheduler - INFO - Receive client connection: Client-faee981e-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:25,651 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57678
2024-01-15 06:33:26,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:26,806 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:27,468 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:27,469 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40389
2024-01-15 06:33:27,469 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40389
2024-01-15 06:33:27,469 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-15 06:33:27,469 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:27,469 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:27,469 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:27,469 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-15 06:33:27,469 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s0gne80v
2024-01-15 06:33:27,470 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de33967c-e4f0-4bda-af59-9ab196310463
2024-01-15 06:33:27,470 - distributed.worker - INFO - Starting Worker plugin PreImport-2022008e-617f-4ec6-9120-e86e8009823a
2024-01-15 06:33:27,470 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-93fb3e98-6bfd-4306-98e7-e1651afa93b8
2024-01-15 06:33:27,470 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:27,523 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40389', status: init, memory: 0, processing: 0>
2024-01-15 06:33:27,524 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40389
2024-01-15 06:33:27,524 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57694
2024-01-15 06:33:27,525 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:27,526 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:27,526 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:27,528 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:27,584 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:27,587 - distributed.scheduler - INFO - Remove client Client-faee981e-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:27,587 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57678; closing.
2024-01-15 06:33:27,588 - distributed.scheduler - INFO - Remove client Client-faee981e-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:27,588 - distributed.scheduler - INFO - Close client connection: Client-faee981e-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:27,589 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37143'. Reason: nanny-close
2024-01-15 06:33:27,589 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:27,590 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40389. Reason: nanny-close
2024-01-15 06:33:27,592 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:27,592 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57694; closing.
2024-01-15 06:33:27,592 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40389', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300407.5923796')
2024-01-15 06:33:27,592 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:33:27,593 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:28,304 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:33:28,305 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:33:28,305 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:33:28,306 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:33:28,307 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-15 06:33:32,691 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:32,696 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44615 instead
  warnings.warn(
2024-01-15 06:33:32,699 - distributed.scheduler - INFO - State start
2024-01-15 06:33:32,770 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:32,771 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:33:32,772 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44615/status
2024-01-15 06:33:32,772 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:33:32,879 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43673'
2024-01-15 06:33:33,909 - distributed.scheduler - INFO - Receive client connection: Client-ffc4bb88-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:33,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58496
2024-01-15 06:33:34,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:34,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:35,473 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:35,474 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36047
2024-01-15 06:33:35,474 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36047
2024-01-15 06:33:35,474 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36995
2024-01-15 06:33:35,474 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:35,474 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:35,474 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:35,474 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-15 06:33:35,474 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h0_07f23
2024-01-15 06:33:35,474 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b11334a-72a4-4f36-9593-6216ece4ab08
2024-01-15 06:33:35,475 - distributed.worker - INFO - Starting Worker plugin PreImport-c9f1db02-ab2d-4e3c-b423-7d2b81c65be6
2024-01-15 06:33:35,476 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42aa15fd-f44a-4573-b874-0bd6a1911ecb
2024-01-15 06:33:35,476 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:35,540 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36047', status: init, memory: 0, processing: 0>
2024-01-15 06:33:35,542 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36047
2024-01-15 06:33:35,542 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58522
2024-01-15 06:33:35,543 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:35,544 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:35,544 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:35,546 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:35,587 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:35,589 - distributed.scheduler - INFO - Remove client Client-ffc4bb88-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:35,590 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58496; closing.
2024-01-15 06:33:35,590 - distributed.scheduler - INFO - Remove client Client-ffc4bb88-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:35,590 - distributed.scheduler - INFO - Close client connection: Client-ffc4bb88-b36f-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:35,591 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43673'. Reason: nanny-close
2024-01-15 06:33:35,592 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:35,593 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36047. Reason: nanny-close
2024-01-15 06:33:35,595 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58522; closing.
2024-01-15 06:33:35,596 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:35,596 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36047', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300415.5962505')
2024-01-15 06:33:35,596 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:33:35,597 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:36,507 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:33:36,507 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:33:36,508 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:33:36,509 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:33:36,510 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-15 06:33:38,841 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:38,846 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33949 instead
  warnings.warn(
2024-01-15 06:33:38,850 - distributed.scheduler - INFO - State start
2024-01-15 06:33:38,873 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:38,875 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:33:38,875 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33949/status
2024-01-15 06:33:38,876 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:33:41,328 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:58534'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58534>: Stream is closed
2024-01-15 06:33:41,721 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:33:41,722 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:33:41,722 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:33:41,723 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:33:41,723 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-15 06:33:43,924 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:43,929 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36957 instead
  warnings.warn(
2024-01-15 06:33:43,934 - distributed.scheduler - INFO - State start
2024-01-15 06:33:43,957 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:43,958 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-15 06:33:43,959 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36957/status
2024-01-15 06:33:43,959 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:33:44,301 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40161'
2024-01-15 06:33:44,950 - distributed.scheduler - INFO - Receive client connection: Client-067214ca-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:44,968 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52202
2024-01-15 06:33:46,048 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:46,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:46,052 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:46,053 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40657
2024-01-15 06:33:46,053 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40657
2024-01-15 06:33:46,053 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38563
2024-01-15 06:33:46,053 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-15 06:33:46,053 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:46,053 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:46,053 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-15 06:33:46,054 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-e46_z48o
2024-01-15 06:33:46,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1e4639c4-c2cb-4eea-be16-edff2c4d243e
2024-01-15 06:33:46,054 - distributed.worker - INFO - Starting Worker plugin PreImport-f0c13890-747e-4515-9c08-9c39b6b49e84
2024-01-15 06:33:46,054 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5038e0cd-0bb5-48ac-8138-f1130372ae16
2024-01-15 06:33:46,054 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:46,113 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40657', status: init, memory: 0, processing: 0>
2024-01-15 06:33:46,114 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40657
2024-01-15 06:33:46,114 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52226
2024-01-15 06:33:46,115 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:46,116 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-15 06:33:46,116 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:46,117 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-15 06:33:46,195 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:46,200 - distributed.scheduler - INFO - Remove client Client-067214ca-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:46,200 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52202; closing.
2024-01-15 06:33:46,201 - distributed.scheduler - INFO - Remove client Client-067214ca-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:46,201 - distributed.scheduler - INFO - Close client connection: Client-067214ca-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:46,202 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40161'. Reason: nanny-close
2024-01-15 06:33:46,202 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:46,203 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40657. Reason: nanny-close
2024-01-15 06:33:46,205 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52226; closing.
2024-01-15 06:33:46,205 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-15 06:33:46,205 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40657', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300426.2056465')
2024-01-15 06:33:46,205 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:33:46,206 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:46,767 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:33:46,767 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:33:46,768 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:33:46,769 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-15 06:33:46,769 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-15 06:33:49,047 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:49,052 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42195 instead
  warnings.warn(
2024-01-15 06:33:49,057 - distributed.scheduler - INFO - State start
2024-01-15 06:33:49,555 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:49,557 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:33:49,558 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42195/status
2024-01-15 06:33:49,558 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:33:49,728 - distributed.scheduler - INFO - Receive client connection: Client-0973d4f8-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:49,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60378
2024-01-15 06:33:50,250 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36321'
2024-01-15 06:33:50,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45921'
2024-01-15 06:33:50,276 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35917'
2024-01-15 06:33:50,290 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39793'
2024-01-15 06:33:50,293 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41851'
2024-01-15 06:33:50,303 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37069'
2024-01-15 06:33:50,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34819'
2024-01-15 06:33:50,323 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42393'
2024-01-15 06:33:52,119 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:52,119 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:52,123 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:52,124 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40539
2024-01-15 06:33:52,124 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40539
2024-01-15 06:33:52,124 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42969
2024-01-15 06:33:52,124 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:52,124 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:52,124 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:52,124 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:52,124 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f7m54p99
2024-01-15 06:33:52,124 - distributed.worker - INFO - Starting Worker plugin PreImport-60d73754-f46c-4cfd-8e24-ebde097f9006
2024-01-15 06:33:52,124 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c71ba67-aa92-4697-9f32-7348a987e999
2024-01-15 06:33:52,125 - distributed.worker - INFO - Starting Worker plugin RMMSetup-455cb328-9087-4990-8765-e9b8af8c0fc5
2024-01-15 06:33:52,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:52,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:52,154 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:52,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:52,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:52,155 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46331
2024-01-15 06:33:52,155 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46331
2024-01-15 06:33:52,155 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40843
2024-01-15 06:33:52,155 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:52,155 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:52,156 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:52,156 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:52,156 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bqd2gwfc
2024-01-15 06:33:52,156 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e687a4de-f0d3-4708-97e5-ef13e7c01615
2024-01-15 06:33:52,159 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:52,160 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39887
2024-01-15 06:33:52,160 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39887
2024-01-15 06:33:52,160 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37801
2024-01-15 06:33:52,160 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:52,160 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:52,160 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:52,160 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:52,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r6lprzby
2024-01-15 06:33:52,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a18bedd-4476-4a23-91e2-4c8a34237ca1
2024-01-15 06:33:52,176 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:52,176 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:52,176 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:52,176 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:52,176 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:52,176 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:52,180 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:52,180 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:52,180 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:52,181 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42305
2024-01-15 06:33:52,181 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35551
2024-01-15 06:33:52,181 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40107
2024-01-15 06:33:52,181 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35551
2024-01-15 06:33:52,181 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42305
2024-01-15 06:33:52,181 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40107
2024-01-15 06:33:52,181 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38545
2024-01-15 06:33:52,181 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37813
2024-01-15 06:33:52,181 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38905
2024-01-15 06:33:52,181 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:52,181 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:52,181 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:52,181 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:52,181 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:52,181 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:52,181 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:52,181 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:52,181 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:52,181 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:52,181 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:52,181 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:52,181 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jrqnwlof
2024-01-15 06:33:52,181 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dihpasa5
2024-01-15 06:33:52,181 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ymcbviz4
2024-01-15 06:33:52,181 - distributed.worker - INFO - Starting Worker plugin PreImport-26978bde-bb19-4162-88e9-94dadff5597c
2024-01-15 06:33:52,181 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7befc883-1db1-4684-bb37-98c007fcaae7
2024-01-15 06:33:52,181 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d5640ac6-21b0-4fbe-91f9-84dbf230682e
2024-01-15 06:33:52,181 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-df626eec-d46c-402a-8e7e-8c2a72881020
2024-01-15 06:33:52,182 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c85987d2-5411-4121-8975-950ce5078135
2024-01-15 06:33:52,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:52,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:52,194 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:52,194 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39757
2024-01-15 06:33:52,194 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39757
2024-01-15 06:33:52,195 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36949
2024-01-15 06:33:52,195 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:52,195 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:52,195 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:52,195 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:52,195 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7f_rip0x
2024-01-15 06:33:52,195 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1a7044d6-eadc-451e-9798-51237acb905c
2024-01-15 06:33:52,372 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:52,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:52,377 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:52,378 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38441
2024-01-15 06:33:52,378 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38441
2024-01-15 06:33:52,378 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38953
2024-01-15 06:33:52,378 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:52,378 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:52,378 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:52,378 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 06:33:52,378 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d07j20ns
2024-01-15 06:33:52,378 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e189b4aa-400f-4921-8178-a2e454b7c0e0
2024-01-15 06:33:54,116 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,149 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40539', status: init, memory: 0, processing: 0>
2024-01-15 06:33:54,150 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40539
2024-01-15 06:33:54,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52214
2024-01-15 06:33:54,152 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:54,153 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:54,153 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:54,204 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08875215-6b6c-45e2-8531-9182e6db7316
2024-01-15 06:33:54,205 - distributed.worker - INFO - Starting Worker plugin PreImport-9acdd291-48fb-439e-b6ce-38cce64b5d28
2024-01-15 06:33:54,206 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,238 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46331', status: init, memory: 0, processing: 0>
2024-01-15 06:33:54,239 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46331
2024-01-15 06:33:54,239 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52226
2024-01-15 06:33:54,240 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:54,241 - distributed.worker - INFO - Starting Worker plugin PreImport-f9e70c16-3940-485b-80f9-a84dee8b47a2
2024-01-15 06:33:54,241 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:54,242 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,242 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb007580-01a8-4efc-80a3-d803e35ee21d
2024-01-15 06:33:54,243 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:54,275 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39887', status: init, memory: 0, processing: 0>
2024-01-15 06:33:54,276 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39887
2024-01-15 06:33:54,276 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52228
2024-01-15 06:33:54,277 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:54,278 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:54,278 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:54,281 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,293 - distributed.worker - INFO - Starting Worker plugin PreImport-8f463316-1945-4cbc-adaa-c87c60148bb2
2024-01-15 06:33:54,294 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6f9cd84-9fbf-465b-866d-682f1675b665
2024-01-15 06:33:54,294 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,305 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a367784-d677-4f4d-bfaa-803cf2ae135e
2024-01-15 06:33:54,306 - distributed.worker - INFO - Starting Worker plugin PreImport-3ea3118b-fb3a-48c3-bbba-86cc6b6c9247
2024-01-15 06:33:54,306 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40107', status: init, memory: 0, processing: 0>
2024-01-15 06:33:54,306 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,307 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40107
2024-01-15 06:33:54,307 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52232
2024-01-15 06:33:54,308 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:54,309 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:54,309 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:54,317 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42305', status: init, memory: 0, processing: 0>
2024-01-15 06:33:54,318 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42305
2024-01-15 06:33:54,318 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52238
2024-01-15 06:33:54,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:54,318 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a0291baa-7188-4354-86ce-0427d423aa8a
2024-01-15 06:33:54,319 - distributed.worker - INFO - Starting Worker plugin PreImport-e4b05104-4875-4894-8ea7-f06a01a4c0bb
2024-01-15 06:33:54,319 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,319 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:54,319 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:54,321 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d2f07d65-12e9-4dbc-9992-e90b5a9c7c5e
2024-01-15 06:33:54,322 - distributed.worker - INFO - Starting Worker plugin PreImport-537784b7-b51f-4c23-bfd5-24e105e05903
2024-01-15 06:33:54,324 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,328 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39757', status: init, memory: 0, processing: 0>
2024-01-15 06:33:54,328 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39757
2024-01-15 06:33:54,328 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52246
2024-01-15 06:33:54,329 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:54,330 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:54,330 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,331 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:54,340 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38441', status: init, memory: 0, processing: 0>
2024-01-15 06:33:54,340 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38441
2024-01-15 06:33:54,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52254
2024-01-15 06:33:54,341 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:54,342 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:54,342 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,343 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:54,358 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35551', status: init, memory: 0, processing: 0>
2024-01-15 06:33:54,358 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35551
2024-01-15 06:33:54,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52268
2024-01-15 06:33:54,360 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:54,361 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:54,361 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:54,363 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:54,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:54,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:54,384 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:54,384 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:54,384 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:54,384 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:54,385 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:54,385 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-15 06:33:54,399 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:54,399 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:54,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:54,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:54,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:54,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:54,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:54,400 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:54,405 - distributed.scheduler - INFO - Remove client Client-0973d4f8-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:54,405 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60378; closing.
2024-01-15 06:33:54,405 - distributed.scheduler - INFO - Remove client Client-0973d4f8-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:54,406 - distributed.scheduler - INFO - Close client connection: Client-0973d4f8-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:54,406 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36321'. Reason: nanny-close
2024-01-15 06:33:54,406 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:54,407 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45921'. Reason: nanny-close
2024-01-15 06:33:54,407 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:54,407 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35917'. Reason: nanny-close
2024-01-15 06:33:54,408 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39887. Reason: nanny-close
2024-01-15 06:33:54,408 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:54,408 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39793'. Reason: nanny-close
2024-01-15 06:33:54,408 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40539. Reason: nanny-close
2024-01-15 06:33:54,408 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:54,408 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41851'. Reason: nanny-close
2024-01-15 06:33:54,408 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42305. Reason: nanny-close
2024-01-15 06:33:54,409 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:54,409 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37069'. Reason: nanny-close
2024-01-15 06:33:54,409 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40107. Reason: nanny-close
2024-01-15 06:33:54,409 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:54,409 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34819'. Reason: nanny-close
2024-01-15 06:33:54,409 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46331. Reason: nanny-close
2024-01-15 06:33:54,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:54,410 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52228; closing.
2024-01-15 06:33:54,410 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:54,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42393'. Reason: nanny-close
2024-01-15 06:33:54,410 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:54,410 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35551. Reason: nanny-close
2024-01-15 06:33:54,410 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39887', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300434.4105725')
2024-01-15 06:33:54,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:54,410 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:54,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39757. Reason: nanny-close
2024-01-15 06:33:54,411 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38441. Reason: nanny-close
2024-01-15 06:33:54,411 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52238; closing.
2024-01-15 06:33:54,411 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:54,411 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:54,412 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:54,412 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:54,412 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52232; closing.
2024-01-15 06:33:54,412 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:54,412 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42305', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300434.4127228')
2024-01-15 06:33:54,413 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52214; closing.
2024-01-15 06:33:54,413 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:54,413 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:54,413 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:54,413 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:54,413 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40107', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300434.4139001')
2024-01-15 06:33:54,414 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:54,414 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40539', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300434.414317')
2024-01-15 06:33:54,414 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:54,414 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52226; closing.
2024-01-15 06:33:54,414 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:54,415 - distributed.nanny - INFO - Worker closed
2024-01-15 06:33:54,415 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46331', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300434.4153')
2024-01-15 06:33:54,415 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52268; closing.
2024-01-15 06:33:54,415 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52246; closing.
2024-01-15 06:33:54,416 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35551', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300434.4163628')
2024-01-15 06:33:54,416 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300434.4167407')
2024-01-15 06:33:54,417 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52254; closing.
2024-01-15 06:33:54,417 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38441', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300434.41746')
2024-01-15 06:33:54,417 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:33:55,472 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:33:55,473 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:33:55,473 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:33:55,474 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:33:55,475 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-15 06:33:57,626 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:57,631 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38385 instead
  warnings.warn(
2024-01-15 06:33:57,635 - distributed.scheduler - INFO - State start
2024-01-15 06:33:57,658 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:33:57,659 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:33:57,660 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38385/status
2024-01-15 06:33:57,660 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:33:57,785 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35135'
2024-01-15 06:33:58,077 - distributed.scheduler - INFO - Receive client connection: Client-0ea32fa9-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:58,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52386
2024-01-15 06:33:59,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:33:59,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:33:59,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:33:59,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45449
2024-01-15 06:33:59,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45449
2024-01-15 06:33:59,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41707
2024-01-15 06:33:59,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:33:59,535 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:59,535 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:33:59,535 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-15 06:33:59,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nvftz93g
2024-01-15 06:33:59,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-167f4f32-ab9a-4c2c-a335-381aae340174
2024-01-15 06:33:59,828 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cdee9d0d-3863-4636-bf61-10e917f0286b
2024-01-15 06:33:59,829 - distributed.worker - INFO - Starting Worker plugin PreImport-e1afcd24-729a-430a-83d8-ccb4dcc9093f
2024-01-15 06:33:59,829 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:59,885 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45449', status: init, memory: 0, processing: 0>
2024-01-15 06:33:59,887 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45449
2024-01-15 06:33:59,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52404
2024-01-15 06:33:59,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:33:59,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:33:59,889 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:33:59,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:33:59,964 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:33:59,968 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:59,970 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:33:59,972 - distributed.scheduler - INFO - Remove client Client-0ea32fa9-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:59,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52386; closing.
2024-01-15 06:33:59,973 - distributed.scheduler - INFO - Remove client Client-0ea32fa9-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:59,973 - distributed.scheduler - INFO - Close client connection: Client-0ea32fa9-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:33:59,974 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35135'. Reason: nanny-close
2024-01-15 06:33:59,974 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:33:59,976 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45449. Reason: nanny-close
2024-01-15 06:33:59,977 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52404; closing.
2024-01-15 06:33:59,977 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:33:59,977 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45449', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300439.977877')
2024-01-15 06:33:59,978 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:33:59,979 - distributed.nanny - INFO - Worker closed
2024-01-15 06:34:00,639 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:34:00,639 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:34:00,640 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:34:00,641 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:34:00,642 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-15 06:34:02,947 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:34:02,952 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43125 instead
  warnings.warn(
2024-01-15 06:34:02,957 - distributed.scheduler - INFO - State start
2024-01-15 06:34:03,059 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-15 06:34:03,061 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-15 06:34:03,062 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43125/status
2024-01-15 06:34:03,062 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-15 06:34:03,234 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34991'
2024-01-15 06:34:03,977 - distributed.scheduler - INFO - Receive client connection: Client-11b6fbf0-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:34:03,993 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43170
2024-01-15 06:34:05,031 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 06:34:05,031 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 06:34:05,035 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 06:34:05,036 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34115
2024-01-15 06:34:05,036 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34115
2024-01-15 06:34:05,036 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35639
2024-01-15 06:34:05,037 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-15 06:34:05,037 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:34:05,037 - distributed.worker - INFO -               Threads:                          1
2024-01-15 06:34:05,037 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-15 06:34:05,037 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1y9swgd7
2024-01-15 06:34:05,037 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f13f900-7ff3-44d4-bed4-052276eca643
2024-01-15 06:34:05,355 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-845d856f-8ac7-4c54-8e8e-706d633e52c0
2024-01-15 06:34:05,356 - distributed.worker - INFO - Starting Worker plugin PreImport-1429c8c3-dda8-404d-ad5a-67b887490c57
2024-01-15 06:34:05,357 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:34:05,405 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34115', status: init, memory: 0, processing: 0>
2024-01-15 06:34:05,406 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34115
2024-01-15 06:34:05,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43188
2024-01-15 06:34:05,407 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 06:34:05,408 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-15 06:34:05,408 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 06:34:05,410 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-15 06:34:05,456 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-15 06:34:05,460 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-15 06:34:05,465 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:34:05,466 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 06:34:05,469 - distributed.scheduler - INFO - Remove client Client-11b6fbf0-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:34:05,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43170; closing.
2024-01-15 06:34:05,469 - distributed.scheduler - INFO - Remove client Client-11b6fbf0-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:34:05,470 - distributed.scheduler - INFO - Close client connection: Client-11b6fbf0-b370-11ee-ad35-d8c49764f6bb
2024-01-15 06:34:05,470 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34991'. Reason: nanny-close
2024-01-15 06:34:05,471 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-15 06:34:05,472 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34115. Reason: nanny-close
2024-01-15 06:34:05,474 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43188; closing.
2024-01-15 06:34:05,474 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-15 06:34:05,474 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705300445.4743419')
2024-01-15 06:34:05,474 - distributed.scheduler - INFO - Lost all workers
2024-01-15 06:34:05,475 - distributed.nanny - INFO - Worker closed
2024-01-15 06:34:06,136 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-15 06:34:06,136 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-15 06:34:06,137 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-15 06:34:06,138 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-15 06:34:06,138 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43385 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43621 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43243 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] 2024-01-15 06:35:17,032 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:60403 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
Process SpawnProcess-6:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 200, in _test_ucx_infiniband_nvlink
    assert all(client.run(check_ucx_options).values())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2998, in run
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2903, in _run
    raise exc
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36627 instead
  warnings.warn(
2024-01-15 06:35:45,670 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 836, in wait
  File "libucxx.pyx", line 820, in wait_yield
  File "libucxx.pyx", line 815, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44129 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44643 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37993 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36809 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44913 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32939 instead
  warnings.warn(
[1705300619.806345] [dgx13:67174:0]            sock.c:481  UCX  ERROR bind(fd=168 addr=0.0.0.0:47601) failed: Address already in use
2024-01-15 06:37:08,132 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 406, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 836, in wait
  File "libucxx.pyx", line 820, in wait_yield
  File "libucxx.pyx", line 815, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-15 06:37:08,136 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 836, in wait
  File "libucxx.pyx", line 820, in wait_yield
  File "libucxx.pyx", line 815, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46117 instead
  warnings.warn(
2024-01-15 06:37:17,488 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:60494 remote=tcp://127.0.0.1:42411>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44615 instead
  warnings.warn(
[1705300657.258615] [dgx13:67833:0]            sock.c:481  UCX  ERROR bind(fd=153 addr=0.0.0.0:35375) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38451 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40045 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36617 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39565 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43487 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34895 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] [1705300813.896639] [dgx13:70638:0]            sock.c:481  UCX  ERROR bind(fd=128 addr=0.0.0.0:54742) failed: Address already in use
2024-01-15 06:40:25,093 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #060] ep: 0x7fd14ca2a0c0, tag: 0xfeba34c397bf1cc9, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #060] ep: 0x7fd14ca2a0c0, tag: 0xfeba34c397bf1cc9, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-01-15 06:40:25,095 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #060] ep: 0x7f5a248540c0, tag: 0xbbb67052ebbaf3d0, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #060] ep: 0x7f5a248540c0, tag: 0xbbb67052ebbaf3d0, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] [1705300855.131884] [dgx13:71489:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:57023) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38891 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43571 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40005 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33407 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44297 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34265 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41099 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44209 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33981 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32799 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41561 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42323 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45295 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41461 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45163 instead
  warnings.warn(
[1705301337.717224] [dgx13:78799:0]            sock.c:481  UCX  ERROR bind(fd=128 addr=0.0.0.0:37710) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] [1705301387.332387] [dgx13:79358:0]            sock.c:481  UCX  ERROR bind(fd=124 addr=0.0.0.0:54118) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46559 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43509 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39509 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40123 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42577 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37225 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42285 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43759 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41607 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37455 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44689 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46055 instead
  warnings.warn(
2024-01-15 06:53:51,945 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45171 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35747 instead
  warnings.warn(
[1705301645.652091] [dgx13:82991:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:35404) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39391 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43121 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37147 instead
  warnings.warn(
[1705301692.516649] [dgx13:83678:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:52822) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42851 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46255 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33735 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44323 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32849 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36739 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45387 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46365 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1705301873.821894] [dgx13:85893:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:46878) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] 2024-01-15 06:58:06,826 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 836, in wait
  File "libucxx.pyx", line 820, in wait_yield
  File "libucxx.pyx", line 815, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37127 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40625 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43653 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38579 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33053 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46595 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35893 instead
  warnings.warn(
[1705301955.360647] [dgx13:86509:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:48324) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1705301963.844063] [dgx13:60725:0]            sock.c:481  UCX  ERROR bind(fd=246 addr=0.0.0.0:38914) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] [1705301970.947058] [dgx13:60725:0]            sock.c:481  UCX  ERROR bind(fd=247 addr=0.0.0.0:53344) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] [1705301979.079575] [dgx13:60725:0]            sock.c:481  UCX  ERROR bind(fd=245 addr=0.0.0.0:33428) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-15 07:00:05,021 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:05,022 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:05,057 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:05,057 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:05,144 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:05,145 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:05,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:05,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:05,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:05,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:05,174 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:05,174 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:05,174 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:05,175 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:05,311 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:05,311 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:05,646 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:05,647 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39077
2024-01-15 07:00:05,647 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39077
2024-01-15 07:00:05,647 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33073
2024-01-15 07:00:05,647 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,647 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,647 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:05,647 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qizzisb2
2024-01-15 07:00:05,648 - distributed.worker - INFO - Starting Worker plugin PreImport-54f6c00b-1a43-448a-a128-399c31827184
2024-01-15 07:00:05,648 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-326ff704-058c-4a61-a36c-80c392d0fff6
2024-01-15 07:00:05,649 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a63c359e-e40f-4b3f-98db-f13c3dada85b
2024-01-15 07:00:05,649 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,693 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:05,694 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40657
2024-01-15 07:00:05,694 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40657
2024-01-15 07:00:05,694 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34125
2024-01-15 07:00:05,694 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,694 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,694 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:05,694 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rt88ouya
2024-01-15 07:00:05,695 - distributed.worker - INFO - Starting Worker plugin PreImport-040679c4-4da0-44fd-955c-0010f5bad0db
2024-01-15 07:00:05,695 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9999a3a-e2a0-403f-9f87-14527c9e025b
2024-01-15 07:00:05,695 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c841203b-37c8-4470-9bb6-4682024ffa62
2024-01-15 07:00:05,696 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,723 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:05,723 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,723 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37325
2024-01-15 07:00:05,758 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:05,759 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,759 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,760 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37325
2024-01-15 07:00:05,769 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:05,770 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37283
2024-01-15 07:00:05,770 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37283
2024-01-15 07:00:05,770 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45301
2024-01-15 07:00:05,770 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,770 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,770 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:05,770 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pnlw3zjp
2024-01-15 07:00:05,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2db8f50d-af2d-4434-a1b5-efa3df8b50bf
2024-01-15 07:00:05,771 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7545089-1140-4ec7-ba25-d83d3878ecd8
2024-01-15 07:00:05,771 - distributed.worker - INFO - Starting Worker plugin PreImport-189868c8-2a28-4d5c-903b-5b453e0c39f8
2024-01-15 07:00:05,771 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,776 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:05,777 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39059
2024-01-15 07:00:05,777 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39059
2024-01-15 07:00:05,777 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33669
2024-01-15 07:00:05,777 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,777 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,777 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:05,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ogd6058_
2024-01-15 07:00:05,777 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b71f58fc-bc3b-4e1a-8108-4bdfad00ed32
2024-01-15 07:00:05,778 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5843aa23-f848-48c1-922a-863c04001dcd
2024-01-15 07:00:05,778 - distributed.worker - INFO - Starting Worker plugin PreImport-4861234f-225c-4513-91d8-28ec5245774a
2024-01-15 07:00:05,778 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,795 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:05,796 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42391
2024-01-15 07:00:05,796 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42391
2024-01-15 07:00:05,796 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42759
2024-01-15 07:00:05,796 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,796 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,796 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:05,796 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qvxt8ets
2024-01-15 07:00:05,796 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8265d9b7-790e-439e-8ae4-428a32f3fbcd
2024-01-15 07:00:05,796 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-79d92a68-e30d-4cfe-a8f9-d2c25bc7fc32
2024-01-15 07:00:05,797 - distributed.worker - INFO - Starting Worker plugin PreImport-ffd7f9ac-11c0-4f8b-a0d4-68c3f61a7275
2024-01-15 07:00:05,797 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,804 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:05,805 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38161
2024-01-15 07:00:05,805 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38161
2024-01-15 07:00:05,805 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45125
2024-01-15 07:00:05,805 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,805 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,805 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:05,805 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e7zwtpz_
2024-01-15 07:00:05,805 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f15cc075-ae0b-483f-91e7-206de5f8f49e
2024-01-15 07:00:05,806 - distributed.worker - INFO - Starting Worker plugin PreImport-921439a8-4dd5-4016-bb8c-77de3eae30ee
2024-01-15 07:00:05,807 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd2915d3-d191-4720-9fb9-b396d0c9d989
2024-01-15 07:00:05,807 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,808 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:05,809 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40115
2024-01-15 07:00:05,809 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40115
2024-01-15 07:00:05,809 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44563
2024-01-15 07:00:05,809 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,809 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,809 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:05,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_11e0fw0
2024-01-15 07:00:05,809 - distributed.worker - INFO - Starting Worker plugin PreImport-01a6d2c6-53fc-46c1-ac2c-5ec8bdef99a7
2024-01-15 07:00:05,809 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c4cc1bd-f894-4307-ac13-f0fbf5304fb0
2024-01-15 07:00:05,810 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c70dab9-47af-4b42-86c7-a945f9ea44cd
2024-01-15 07:00:05,810 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,927 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:05,928 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35387
2024-01-15 07:00:05,928 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35387
2024-01-15 07:00:05,928 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41941
2024-01-15 07:00:05,928 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,929 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,929 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:05,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-im14rsmd
2024-01-15 07:00:05,929 - distributed.worker - INFO - Starting Worker plugin PreImport-87e22221-13fd-4a51-a1d4-9df832f5eee2
2024-01-15 07:00:05,929 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ccf3306e-5e21-447d-b2e4-1baa79e843ad
2024-01-15 07:00:05,929 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9b965d2-22ea-4a1e-9477-c4c855d0adc1
2024-01-15 07:00:05,929 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,986 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:05,986 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,987 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37325
2024-01-15 07:00:05,997 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:05,998 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37325
2024-01-15 07:00:05,998 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:05,999 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37325
2024-01-15 07:00:06,014 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:06,015 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37325
2024-01-15 07:00:06,015 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:06,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37325
2024-01-15 07:00:06,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:06,033 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37325
2024-01-15 07:00:06,033 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:06,034 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:06,035 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37325
2024-01-15 07:00:06,035 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:06,035 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37325
2024-01-15 07:00:06,036 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37325
2024-01-15 07:00:06,053 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:06,053 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:37325
2024-01-15 07:00:06,053 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:06,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37325
2024-01-15 07:00:06,082 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 07:00:06,082 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 07:00:06,082 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 07:00:06,082 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 07:00:06,082 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 07:00:06,083 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 07:00:06,083 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 07:00:06,083 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-15 07:00:06,091 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39077. Reason: nanny-close
2024-01-15 07:00:06,092 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40657. Reason: nanny-close
2024-01-15 07:00:06,092 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39059. Reason: nanny-close
2024-01-15 07:00:06,093 - distributed.core - INFO - Connection to tcp://127.0.0.1:37325 has been closed.
2024-01-15 07:00:06,093 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40115. Reason: nanny-close
2024-01-15 07:00:06,093 - distributed.core - INFO - Connection to tcp://127.0.0.1:37325 has been closed.
2024-01-15 07:00:06,094 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38161. Reason: nanny-close
2024-01-15 07:00:06,094 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:06,094 - distributed.core - INFO - Connection to tcp://127.0.0.1:37325 has been closed.
2024-01-15 07:00:06,094 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37283. Reason: nanny-close
2024-01-15 07:00:06,095 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:06,095 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42391. Reason: nanny-close
2024-01-15 07:00:06,095 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:06,095 - distributed.core - INFO - Connection to tcp://127.0.0.1:37325 has been closed.
2024-01-15 07:00:06,096 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35387. Reason: nanny-close
2024-01-15 07:00:06,096 - distributed.core - INFO - Connection to tcp://127.0.0.1:37325 has been closed.
2024-01-15 07:00:06,096 - distributed.core - INFO - Connection to tcp://127.0.0.1:37325 has been closed.
2024-01-15 07:00:06,097 - distributed.core - INFO - Connection to tcp://127.0.0.1:37325 has been closed.
2024-01-15 07:00:06,097 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:06,098 - distributed.core - INFO - Connection to tcp://127.0.0.1:37325 has been closed.
2024-01-15 07:00:06,098 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:06,098 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:06,098 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:06,099 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool 2024-01-15 07:00:12,518 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-15 07:00:12,522 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async 2024-01-15 07:00:24,053 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:129: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 342, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:129: cudaErrorMemoryAllocation out of memory
2024-01-15 07:00:24,061 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 342, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:129: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size 2024-01-15 07:00:30,295 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:129: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 342, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:129: cudaErrorMemoryAllocation out of memory
2024-01-15 07:00:30,302 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 342, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:129: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging 2024-01-15 07:00:35,477 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1001, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-15 07:00:35,480 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1001, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-15 07:00:43,086 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:43,086 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:43,090 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:43,091 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41123
2024-01-15 07:00:43,091 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41123
2024-01-15 07:00:43,091 - distributed.worker - INFO -           Worker name:                          0
2024-01-15 07:00:43,092 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41131
2024-01-15 07:00:43,092 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39029
2024-01-15 07:00:43,092 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:43,092 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:43,092 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-15 07:00:43,092 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5hk99ctl
2024-01-15 07:00:43,092 - distributed.worker - INFO - Starting Worker plugin PreImport-98fe5897-251e-49de-8c8d-7671fa3b31bb
2024-01-15 07:00:43,097 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-15 07:00:43,098 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7f33726-c3d4-4797-aea3-4e1e86448ee4
2024-01-15 07:00:43,099 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5d1f44d7-8cc1-46d2-a48e-e992623575e1
2024-01-15 07:00:43,099 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41123. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-15 07:00:43,099 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-15 07:00:43,101 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-15 07:00:47,774 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:47,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:47,782 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:47,782 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:47,867 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:47,867 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:47,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:47,878 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:47,879 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:47,879 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:47,917 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:47,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:47,974 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:47,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:47,978 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-15 07:00:47,978 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-15 07:00:48,409 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:48,410 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44535
2024-01-15 07:00:48,410 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44535
2024-01-15 07:00:48,410 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35811
2024-01-15 07:00:48,410 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,410 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,410 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:48,410 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 07:00:48,410 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tf1t_8vl
2024-01-15 07:00:48,410 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-149489e3-dc98-4879-b1b3-d3d98b44f3b2
2024-01-15 07:00:48,411 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7470278-7d78-4038-8789-247fed8fa390
2024-01-15 07:00:48,411 - distributed.worker - INFO - Starting Worker plugin PreImport-a9f4b600-f730-4fea-ae31-5b275087e14f
2024-01-15 07:00:48,411 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,411 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:48,412 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45913
2024-01-15 07:00:48,412 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45913
2024-01-15 07:00:48,412 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34403
2024-01-15 07:00:48,412 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,412 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,412 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:48,412 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 07:00:48,412 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8abcgybe
2024-01-15 07:00:48,412 - distributed.worker - INFO - Starting Worker plugin RMMSetup-05a8fe06-0b00-4201-af24-7181bb809f40
2024-01-15 07:00:48,412 - distributed.worker - INFO - Starting Worker plugin PreImport-898a349b-e2ea-4b59-983a-d76147566c46
2024-01-15 07:00:48,413 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ebb72cd3-6806-48c8-99d0-4b2f2cfc5f60
2024-01-15 07:00:48,413 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,500 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:48,501 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,501 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,502 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39597
2024-01-15 07:00:48,503 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:48,504 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45989
2024-01-15 07:00:48,504 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45989
2024-01-15 07:00:48,504 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34507
2024-01-15 07:00:48,504 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,504 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:48,504 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,504 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:48,504 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 07:00:48,504 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hn9upvxl
2024-01-15 07:00:48,504 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c9f1c09b-e44a-49ac-94d6-1fd2a2c0b935
2024-01-15 07:00:48,504 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86d52fe4-55a9-465c-90a8-1b780cf7f3df
2024-01-15 07:00:48,505 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,505 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,505 - distributed.worker - INFO - Starting Worker plugin PreImport-b2cec7df-abd7-4ecb-86f7-07c6544730e6
2024-01-15 07:00:48,505 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,506 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39597
2024-01-15 07:00:48,514 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:48,515 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32823
2024-01-15 07:00:48,515 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32823
2024-01-15 07:00:48,515 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42413
2024-01-15 07:00:48,515 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,515 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,515 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:48,515 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 07:00:48,515 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ca__z5u9
2024-01-15 07:00:48,516 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f3d3dc5-3389-40b5-b507-a85d97cb8135
2024-01-15 07:00:48,516 - distributed.worker - INFO - Starting Worker plugin PreImport-a5e4523e-c56f-43a4-8617-e48f0752aa26
2024-01-15 07:00:48,516 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6800e23d-235f-4747-9c4b-071c3379743a
2024-01-15 07:00:48,516 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:48,525 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33197
2024-01-15 07:00:48,525 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33197
2024-01-15 07:00:48,525 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41981
2024-01-15 07:00:48,525 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,525 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,525 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:48,525 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 07:00:48,525 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6iaq1dwx
2024-01-15 07:00:48,526 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-83fc6ace-57ea-4448-9bbe-4f74a5f81b05
2024-01-15 07:00:48,526 - distributed.worker - INFO - Starting Worker plugin PreImport-706803e4-f2ea-4fda-8457-0dda70ebf9fa
2024-01-15 07:00:48,526 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1a83fdd-ff77-4bad-886d-676b0b16a3ec
2024-01-15 07:00:48,526 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:48,602 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,602 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39597
2024-01-15 07:00:48,606 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:48,607 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42681
2024-01-15 07:00:48,607 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42681
2024-01-15 07:00:48,607 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36687
2024-01-15 07:00:48,607 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,607 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,607 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:48,607 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 07:00:48,607 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xss3ifj8
2024-01-15 07:00:48,608 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e8d8cca-a873-4e1e-a80b-0d565ddec6cf
2024-01-15 07:00:48,608 - distributed.worker - INFO - Starting Worker plugin PreImport-5817e0e5-ac74-4b7d-85f7-eb4bf8b86c81
2024-01-15 07:00:48,608 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42954f27-27e2-4fa9-a80e-57d0655e38ad
2024-01-15 07:00:48,608 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,619 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:48,620 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38557
2024-01-15 07:00:48,620 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38557
2024-01-15 07:00:48,620 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42521
2024-01-15 07:00:48,620 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,620 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,620 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:48,620 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 07:00:48,620 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fu0cnd6c
2024-01-15 07:00:48,621 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c503786-8a66-4ba9-ae7a-d2401645fc37
2024-01-15 07:00:48,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be6aad09-3c74-4e3e-aae1-c365955d5b07
2024-01-15 07:00:48,621 - distributed.worker - INFO - Starting Worker plugin PreImport-90934fc7-be35-4eb9-b197-6853d8125126
2024-01-15 07:00:48,621 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-15 07:00:48,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33569
2024-01-15 07:00:48,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33569
2024-01-15 07:00:48,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44291
2024-01-15 07:00:48,627 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,628 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,628 - distributed.worker - INFO -               Threads:                          1
2024-01-15 07:00:48,628 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-15 07:00:48,628 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ywien_5
2024-01-15 07:00:48,628 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5b1c1490-5adc-4992-88d0-7039dec1e8ff
2024-01-15 07:00:48,629 - distributed.worker - INFO - Starting Worker plugin PreImport-d9cad9c0-ae0d-4ab7-a11e-3a4e8d9347bd
2024-01-15 07:00:48,629 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6922146-975f-4d6c-bcb4-4d0285f5cea1
2024-01-15 07:00:48,629 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:48,629 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,630 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,630 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39597
2024-01-15 07:00:48,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:48,637 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,637 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39597
2024-01-15 07:00:48,725 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:48,726 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,726 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,727 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39597
2024-01-15 07:00:48,743 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:48,743 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,743 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39597
2024-01-15 07:00:48,747 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-15 07:00:48,748 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:39597
2024-01-15 07:00:48,748 - distributed.worker - INFO - -------------------------------------------------
2024-01-15 07:00:48,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39597
2024-01-15 07:00:48,773 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45913. Reason: nanny-close
2024-01-15 07:00:48,774 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45989. Reason: nanny-close
2024-01-15 07:00:48,774 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44535. Reason: nanny-close
2024-01-15 07:00:48,775 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33197. Reason: nanny-close
2024-01-15 07:00:48,775 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32823. Reason: nanny-close
2024-01-15 07:00:48,776 - distributed.core - INFO - Connection to tcp://127.0.0.1:39597 has been closed.
2024-01-15 07:00:48,776 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33569. Reason: nanny-close
2024-01-15 07:00:48,776 - distributed.core - INFO - Connection to tcp://127.0.0.1:39597 has been closed.
2024-01-15 07:00:48,776 - distributed.core - INFO - Connection to tcp://127.0.0.1:39597 has been closed.
2024-01-15 07:00:48,776 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38557. Reason: nanny-close
2024-01-15 07:00:48,777 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42681. Reason: nanny-close
2024-01-15 07:00:48,777 - distributed.core - INFO - Connection to tcp://127.0.0.1:39597 has been closed.
2024-01-15 07:00:48,777 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:48,777 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:48,777 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:48,777 - distributed.core - INFO - Connection to tcp://127.0.0.1:39597 has been closed.
2024-01-15 07:00:48,778 - distributed.core - INFO - Connection to tcp://127.0.0.1:39597 has been closed.
2024-01-15 07:00:48,778 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:48,778 - distributed.core - INFO - Connection to tcp://127.0.0.1:39597 has been closed.
2024-01-15 07:00:48,778 - distributed.core - INFO - Connection to tcp://127.0.0.1:39597 has been closed.
2024-01-15 07:00:48,779 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:48,779 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:48,779 - distributed.nanny - INFO - Worker closed
2024-01-15 07:00:48,780 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations 2024-01-15 07:00:54,965 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-15 07:00:54,970 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration 2024-01-15 07:01:00,288 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-15 07:01:00,291 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-01-15 07:01:03,092 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-15 07:01:03,095 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] 2024-01-15 07:01:27,673 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-15 07:01:27,680 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f562afe8f40>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-15 07:01:29,685 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] 2024-01-15 07:01:59,167 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-0510f870e3f1398d4b018ddda308139b', 1)
Function:  subgraph_callable-84cdb754-21e4-4ed2-8167-174f04bf
args:      (   key
5    5
6    6
7    7
8    8
9    9, '_partitions', 'getitem-9d1eeab2dac9664ce3bf98396bd4334b', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-15 07:01:59,197 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-0510f870e3f1398d4b018ddda308139b', 0)
Function:  subgraph_callable-84cdb754-21e4-4ed2-8167-174f04bf
args:      (   key
0    0
1    1
2    2
3    3
4    4, '_partitions', 'getitem-9d1eeab2dac9664ce3bf98396bd4334b', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-01-15 07:02:04,786 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-73cabdfebe1dd4d17c2f37ae549713da', 2)
Function:  subgraph_callable-b6cbc17d-f537-45fd-a7a7-0a70d534
args:      (   key
7    7
8    8
9    9, '_partitions', 'getitem-6bc36e7fdccbf44303bd588a4a4ec92f', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-15 07:02:04,809 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('assign-73cabdfebe1dd4d17c2f37ae549713da', 1))" coro=<Worker.execute() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-01-15 07:02:10,117 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-0abd4a27d7ca2ade488131d8acb0d627', 0)
Function:  subgraph_callable-7044be8e-bd42-4f16-89cc-758728d3
args:      (<dask_cuda.proxy_object.ProxyObject at 0x7f31d1fcb910 of cudf.core.dataframe.DataFrame at 0x7f31d1fcb8b0>, '_partitions', 'getitem-70dbe22829b6adf716d57a3cf0a847ef', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-01-15 07:02:15,644 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-0510f870e3f1398d4b018ddda308139b', 1)
Function:  subgraph_callable-f86e8a52-ef47-4c4f-81f6-621e0676
args:      (<dask_cuda.proxy_object.ProxyObject at 0x7f20aa07d8b0 of cudf.core.dataframe.DataFrame at 0x7f20aa07d850>, '_partitions', 'getitem-9d1eeab2dac9664ce3bf98396bd4334b', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-15 07:02:15,662 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('assign-0510f870e3f1398d4b018ddda308139b', 0))" coro=<Worker.execute() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-01-15 07:02:20,987 - distributed.worker - WARNING - Compute Failed
Key:       ('assign-73cabdfebe1dd4d17c2f37ae549713da', 2)
Function:  subgraph_callable-53f5017b-9bb8-4feb-ab9a-5c8ee687
args:      (<dask_cuda.proxy_object.ProxyObject at 0x7fb9824708e0 of cudf.core.dataframe.DataFrame at 0x7fb982470880>, '_partitions', 'getitem-6bc36e7fdccbf44303bd588a4a4ec92f', ['key'])
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

2024-01-15 07:02:21,009 - distributed.worker.state_machine - WARNING - Async instruction for <Task cancelled name="execute(('assign-73cabdfebe1dd4d17c2f37ae549713da', 1))" coro=<Worker.execute() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker_state_machine.py:3615>> ended with CancelledError
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-15 07:02:31,096 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-15 07:02:31,101 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
