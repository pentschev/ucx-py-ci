============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-11-29 06:41:28,784 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:41:28,788 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44471 instead
  warnings.warn(
2023-11-29 06:41:28,792 - distributed.scheduler - INFO - State start
2023-11-29 06:41:28,823 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:41:28,824 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-29 06:41:28,824 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44471/status
2023-11-29 06:41:28,825 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:41:28,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38659'
2023-11-29 06:41:28,941 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38325'
2023-11-29 06:41:28,944 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34311'
2023-11-29 06:41:28,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42517'
2023-11-29 06:41:30,678 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:30,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:30,678 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:30,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:30,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:30,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:30,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:30,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:30,684 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:30,688 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:30,688 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:30,692 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-11-29 06:41:30,706 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46717
2023-11-29 06:41:30,706 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46717
2023-11-29 06:41:30,706 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44787
2023-11-29 06:41:30,706 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-29 06:41:30,707 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:30,707 - distributed.worker - INFO -               Threads:                          4
2023-11-29 06:41:30,707 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-29 06:41:30,707 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-uvwdg1j9
2023-11-29 06:41:30,707 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a699d597-6766-44e2-a766-1c8ab871ce1b
2023-11-29 06:41:30,707 - distributed.worker - INFO - Starting Worker plugin PreImport-1e6c46d5-9704-40df-bc5e-4aadabcc818d
2023-11-29 06:41:30,707 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ba18abd-68dd-41a9-b0af-d01b25a2e65e
2023-11-29 06:41:30,707 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:31,217 - distributed.scheduler - INFO - Receive client connection: Client-5236028f-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:31,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33002
2023-11-29 06:41:31,481 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46717', status: init, memory: 0, processing: 0>
2023-11-29 06:41:31,482 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46717
2023-11-29 06:41:31,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:32988
2023-11-29 06:41:31,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:31,484 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-29 06:41:31,484 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:31,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-29 06:41:32,312 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41751
2023-11-29 06:41:32,313 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41751
2023-11-29 06:41:32,313 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45749
2023-11-29 06:41:32,313 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-29 06:41:32,313 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:32,313 - distributed.worker - INFO -               Threads:                          4
2023-11-29 06:41:32,313 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33325
2023-11-29 06:41:32,313 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-29 06:41:32,313 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33325
2023-11-29 06:41:32,313 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-44p3q0fb
2023-11-29 06:41:32,313 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39207
2023-11-29 06:41:32,314 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-29 06:41:32,314 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:32,314 - distributed.worker - INFO -               Threads:                          4
2023-11-29 06:41:32,314 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e4af993-3110-4d86-a264-565edeaf9040
2023-11-29 06:41:32,314 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-29 06:41:32,314 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-drp8jcqt
2023-11-29 06:41:32,314 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-960adaae-2bbb-40c3-9163-be206528593b
2023-11-29 06:41:32,314 - distributed.worker - INFO - Starting Worker plugin PreImport-c0a3607b-199d-4b42-9499-b7ca0b2bd4c0
2023-11-29 06:41:32,314 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e40adf6b-02e4-4529-85ea-c1d0864ef718
2023-11-29 06:41:32,314 - distributed.worker - INFO - Starting Worker plugin PreImport-a2642e8a-9105-4ff2-8921-6a710b07d3c1
2023-11-29 06:41:32,314 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b87f283-3232-4a92-b0b0-b56f907eb413
2023-11-29 06:41:32,315 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:32,315 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:32,316 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43523
2023-11-29 06:41:32,319 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43523
2023-11-29 06:41:32,319 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39213
2023-11-29 06:41:32,319 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-29 06:41:32,319 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:32,319 - distributed.worker - INFO -               Threads:                          4
2023-11-29 06:41:32,319 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-29 06:41:32,319 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-7rmeti7d
2023-11-29 06:41:32,321 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-78dfac1f-94f8-4a14-ab3e-d12fec1c19a7
2023-11-29 06:41:32,321 - distributed.worker - INFO - Starting Worker plugin PreImport-0ebd7522-68da-4bda-81c0-03f852528153
2023-11-29 06:41:32,321 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6fb911f-9242-470f-a120-f9f4dda6a54d
2023-11-29 06:41:32,322 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:32,343 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33325', status: init, memory: 0, processing: 0>
2023-11-29 06:41:32,344 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33325
2023-11-29 06:41:32,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33028
2023-11-29 06:41:32,344 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:32,345 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-29 06:41:32,345 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:32,349 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-29 06:41:32,353 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41751', status: init, memory: 0, processing: 0>
2023-11-29 06:41:32,354 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41751
2023-11-29 06:41:32,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33042
2023-11-29 06:41:32,355 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43523', status: init, memory: 0, processing: 0>
2023-11-29 06:41:32,355 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43523
2023-11-29 06:41:32,355 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33044
2023-11-29 06:41:32,356 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:32,357 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:32,357 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-29 06:41:32,357 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:32,358 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-29 06:41:32,358 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:32,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-29 06:41:32,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-29 06:41:32,459 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-29 06:41:32,459 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-29 06:41:32,460 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-29 06:41:32,460 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-29 06:41:32,464 - distributed.scheduler - INFO - Remove client Client-5236028f-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:32,465 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33002; closing.
2023-11-29 06:41:32,465 - distributed.scheduler - INFO - Remove client Client-5236028f-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:32,465 - distributed.scheduler - INFO - Close client connection: Client-5236028f-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:32,467 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38659'. Reason: nanny-close
2023-11-29 06:41:32,467 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:32,468 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38325'. Reason: nanny-close
2023-11-29 06:41:32,468 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:32,468 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43523. Reason: nanny-close
2023-11-29 06:41:32,470 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33044; closing.
2023-11-29 06:41:32,470 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-29 06:41:32,470 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43523', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240092.4708748')
2023-11-29 06:41:32,472 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:32,473 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34311'. Reason: nanny-close
2023-11-29 06:41:32,473 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:32,473 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33325. Reason: nanny-close
2023-11-29 06:41:32,473 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42517'. Reason: nanny-close
2023-11-29 06:41:32,473 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:32,474 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41751. Reason: nanny-close
2023-11-29 06:41:32,474 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46717. Reason: nanny-close
2023-11-29 06:41:32,475 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-29 06:41:32,475 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33028; closing.
2023-11-29 06:41:32,475 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33325', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240092.4757822')
2023-11-29 06:41:32,476 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-29 06:41:32,476 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-29 06:41:32,476 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:32988; closing.
2023-11-29 06:41:32,476 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:32,476 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46717', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240092.476845')
2023-11-29 06:41:32,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33042; closing.
2023-11-29 06:41:32,477 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41751', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240092.477502')
2023-11-29 06:41:32,477 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:32,477 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:41:32,478 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:33,984 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:41:33,984 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:41:33,985 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:41:33,985 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-29 06:41:33,986 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-11-29 06:41:36,123 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:41:36,131 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44033 instead
  warnings.warn(
2023-11-29 06:41:36,136 - distributed.scheduler - INFO - State start
2023-11-29 06:41:36,171 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:41:36,174 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-29 06:41:36,176 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44033/status
2023-11-29 06:41:36,176 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:41:36,402 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43859'
2023-11-29 06:41:36,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41917'
2023-11-29 06:41:36,437 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41617'
2023-11-29 06:41:36,439 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34077'
2023-11-29 06:41:36,447 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46459'
2023-11-29 06:41:36,457 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40983'
2023-11-29 06:41:36,467 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46481'
2023-11-29 06:41:36,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33825'
2023-11-29 06:41:38,059 - distributed.scheduler - INFO - Receive client connection: Client-568a7481-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:38,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48980
2023-11-29 06:41:38,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:38,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:38,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:38,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:38,307 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:38,307 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:38,350 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:38,350 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:38,355 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:38,360 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:38,361 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:38,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:38,361 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:38,365 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:38,365 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:38,369 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:38,369 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:38,370 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:38,370 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:38,374 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:38,374 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:38,425 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:38,425 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:38,430 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:41,726 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41673
2023-11-29 06:41:41,727 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41673
2023-11-29 06:41:41,728 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45303
2023-11-29 06:41:41,728 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:41,728 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:41,728 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:41,728 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:41,728 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v_tlr6xw
2023-11-29 06:41:41,729 - distributed.worker - INFO - Starting Worker plugin RMMSetup-05a3ada5-b5c0-4022-ba22-6d099f1e5168
2023-11-29 06:41:41,733 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40935
2023-11-29 06:41:41,733 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40935
2023-11-29 06:41:41,733 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33463
2023-11-29 06:41:41,734 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:41,734 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:41,734 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:41,734 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:41,734 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mq1xnjo3
2023-11-29 06:41:41,734 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ebadaae-c831-46dd-b167-7de1c67072d7
2023-11-29 06:41:42,255 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39723
2023-11-29 06:41:42,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39723
2023-11-29 06:41:42,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46419
2023-11-29 06:41:42,256 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,256 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,256 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:42,256 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:42,256 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zhfuveeg
2023-11-29 06:41:42,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-39383d01-29a8-4f1e-a7d4-017468034b57
2023-11-29 06:41:42,600 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39549
2023-11-29 06:41:42,601 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39549
2023-11-29 06:41:42,601 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39293
2023-11-29 06:41:42,601 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,601 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,601 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:42,601 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:42,601 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j8pmrhk4
2023-11-29 06:41:42,602 - distributed.worker - INFO - Starting Worker plugin RMMSetup-49700c8b-78db-4eba-bb56-aa0896d69768
2023-11-29 06:41:42,616 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38527
2023-11-29 06:41:42,617 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38527
2023-11-29 06:41:42,617 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33301
2023-11-29 06:41:42,617 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,617 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,617 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:42,618 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:42,618 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ymy_81_y
2023-11-29 06:41:42,618 - distributed.worker - INFO - Starting Worker plugin PreImport-06a22392-5b9e-404b-a1df-8f9ac38b5a8c
2023-11-29 06:41:42,618 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b6e78000-302f-4f4a-a01a-c228f59f91f3
2023-11-29 06:41:42,618 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b918897b-5e52-41a3-ba54-217f0e01af11
2023-11-29 06:41:42,619 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46399
2023-11-29 06:41:42,621 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46399
2023-11-29 06:41:42,621 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36679
2023-11-29 06:41:42,621 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,621 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,621 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:42,622 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:42,622 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p0ngpbup
2023-11-29 06:41:42,623 - distributed.worker - INFO - Starting Worker plugin PreImport-fdf0f614-29c2-4329-af9d-b44f08e04ad1
2023-11-29 06:41:42,623 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e17eae6e-cb12-41fc-b337-ac42419826b9
2023-11-29 06:41:42,634 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46833
2023-11-29 06:41:42,635 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46833
2023-11-29 06:41:42,635 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35161
2023-11-29 06:41:42,635 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,635 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,635 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:42,635 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:42,635 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cztfascy
2023-11-29 06:41:42,636 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ef04c36-677d-4f1a-b999-d2a0cf658355
2023-11-29 06:41:42,636 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44221856-b018-4803-aac9-1a856dd6a2f9
2023-11-29 06:41:42,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43861
2023-11-29 06:41:42,638 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43861
2023-11-29 06:41:42,638 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40389
2023-11-29 06:41:42,638 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,638 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,638 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:42,638 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:42,638 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2w3rb5e3
2023-11-29 06:41:42,639 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6879cbc5-279a-4363-835d-4a47ffdb1543
2023-11-29 06:41:42,639 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c90692bf-0627-4939-8bd5-79341c0066ba
2023-11-29 06:41:42,777 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ba27c756-8e62-4735-9ffd-3f3b82becbb0
2023-11-29 06:41:42,780 - distributed.worker - INFO - Starting Worker plugin PreImport-a4ea16d0-8ee9-4d7a-98ac-388040c47ec8
2023-11-29 06:41:42,781 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dad8a186-4fba-41d8-bf77-7b21c668934a
2023-11-29 06:41:42,800 - distributed.worker - INFO - Starting Worker plugin PreImport-6192fef3-cf78-46e1-9722-c76df5182911
2023-11-29 06:41:42,800 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,829 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41673', status: init, memory: 0, processing: 0>
2023-11-29 06:41:42,832 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41673
2023-11-29 06:41:42,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58860
2023-11-29 06:41:42,834 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:42,844 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40935', status: init, memory: 0, processing: 0>
2023-11-29 06:41:42,845 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,845 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,845 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40935
2023-11-29 06:41:42,845 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58870
2023-11-29 06:41:42,847 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:42,847 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:42,856 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,856 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:42,867 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-353f1aed-37d9-48d1-9a3f-914ec2182543
2023-11-29 06:41:42,868 - distributed.worker - INFO - Starting Worker plugin PreImport-d3e76196-d270-4641-bd7e-efcc8db6b9fd
2023-11-29 06:41:42,868 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,908 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39723', status: init, memory: 0, processing: 0>
2023-11-29 06:41:42,909 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39723
2023-11-29 06:41:42,909 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58872
2023-11-29 06:41:42,910 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:42,914 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,914 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,914 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,916 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bbc2c296-05e7-453f-a1ba-dc4d9c221fe6
2023-11-29 06:41:42,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:42,916 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,919 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2519fb06-13be-4a11-958e-8d80c1716267
2023-11-29 06:41:42,920 - distributed.worker - INFO - Starting Worker plugin PreImport-302c5791-793f-44e7-b645-161deca79b91
2023-11-29 06:41:42,920 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,938 - distributed.worker - INFO - Starting Worker plugin PreImport-8234324d-d696-498d-b315-3c0a53b5c519
2023-11-29 06:41:42,938 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,938 - distributed.worker - INFO - Starting Worker plugin PreImport-33ab433b-9c7f-419b-b855-40987ab207a6
2023-11-29 06:41:42,939 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,953 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46399', status: init, memory: 0, processing: 0>
2023-11-29 06:41:42,953 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46399
2023-11-29 06:41:42,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58898
2023-11-29 06:41:42,954 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38527', status: init, memory: 0, processing: 0>
2023-11-29 06:41:42,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:42,955 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38527
2023-11-29 06:41:42,955 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58888
2023-11-29 06:41:42,956 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:42,958 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39549', status: init, memory: 0, processing: 0>
2023-11-29 06:41:42,959 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39549
2023-11-29 06:41:42,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58904
2023-11-29 06:41:42,960 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:42,960 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,960 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,960 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,961 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,961 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:42,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:42,963 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,963 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:42,975 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46833', status: init, memory: 0, processing: 0>
2023-11-29 06:41:42,976 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46833
2023-11-29 06:41:42,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58920
2023-11-29 06:41:42,977 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:42,981 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43861', status: init, memory: 0, processing: 0>
2023-11-29 06:41:42,982 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43861
2023-11-29 06:41:42,982 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58916
2023-11-29 06:41:42,983 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:42,984 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,984 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,986 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:42,987 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:42,987 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:42,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:43,017 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:43,017 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:43,017 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:43,017 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:43,018 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:43,018 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:43,018 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:43,018 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:43,023 - distributed.scheduler - INFO - Remove client Client-568a7481-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:43,023 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48980; closing.
2023-11-29 06:41:43,023 - distributed.scheduler - INFO - Remove client Client-568a7481-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:43,024 - distributed.scheduler - INFO - Close client connection: Client-568a7481-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:43,024 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43859'. Reason: nanny-close
2023-11-29 06:41:43,025 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:43,026 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41917'. Reason: nanny-close
2023-11-29 06:41:43,026 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:43,026 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46399. Reason: nanny-close
2023-11-29 06:41:43,026 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41617'. Reason: nanny-close
2023-11-29 06:41:43,026 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:43,027 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46833. Reason: nanny-close
2023-11-29 06:41:43,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34077'. Reason: nanny-close
2023-11-29 06:41:43,027 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:43,027 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39549. Reason: nanny-close
2023-11-29 06:41:43,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46459'. Reason: nanny-close
2023-11-29 06:41:43,028 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:43,028 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39723. Reason: nanny-close
2023-11-29 06:41:43,028 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40983'. Reason: nanny-close
2023-11-29 06:41:43,028 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:43,028 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:43,028 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41673. Reason: nanny-close
2023-11-29 06:41:43,028 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58898; closing.
2023-11-29 06:41:43,029 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46481'. Reason: nanny-close
2023-11-29 06:41:43,029 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46399', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240103.0292494')
2023-11-29 06:41:43,029 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:43,029 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:43,029 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:43,029 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33825'. Reason: nanny-close
2023-11-29 06:41:43,029 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40935. Reason: nanny-close
2023-11-29 06:41:43,029 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:43,030 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:43,030 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38527. Reason: nanny-close
2023-11-29 06:41:43,030 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43861. Reason: nanny-close
2023-11-29 06:41:43,030 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:43,031 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:43,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58872; closing.
2023-11-29 06:41:43,031 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:43,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58920; closing.
2023-11-29 06:41:43,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58904; closing.
2023-11-29 06:41:43,032 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:43,032 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:43,032 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:43,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39723', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240103.0325713')
2023-11-29 06:41:43,032 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:43,032 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:43,033 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46833', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240103.03299')
2023-11-29 06:41:43,033 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39549', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240103.0333905')
2023-11-29 06:41:43,033 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:43,034 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:43,034 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58860; closing.
2023-11-29 06:41:43,034 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:43,035 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:43,035 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41673', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240103.0350943')
2023-11-29 06:41:43,035 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58870; closing.
2023-11-29 06:41:43,035 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58888; closing.
2023-11-29 06:41:43,036 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58916; closing.
2023-11-29 06:41:43,036 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40935', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240103.0363345')
2023-11-29 06:41:43,036 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38527', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240103.0368147')
2023-11-29 06:41:43,037 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43861', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240103.0372455')
2023-11-29 06:41:43,037 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:41:44,843 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:41:44,843 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:41:44,844 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:41:44,845 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-29 06:41:44,846 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-11-29 06:41:47,310 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:41:47,315 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41693 instead
  warnings.warn(
2023-11-29 06:41:47,318 - distributed.scheduler - INFO - State start
2023-11-29 06:41:47,607 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:41:47,608 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-29 06:41:47,609 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41693/status
2023-11-29 06:41:47,609 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:41:48,211 - distributed.scheduler - INFO - Receive client connection: Client-5d1fbaa5-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:48,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58994
2023-11-29 06:41:48,538 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46139'
2023-11-29 06:41:48,557 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34379'
2023-11-29 06:41:48,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43291'
2023-11-29 06:41:48,592 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42417'
2023-11-29 06:41:48,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34723'
2023-11-29 06:41:48,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38301'
2023-11-29 06:41:48,616 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38767'
2023-11-29 06:41:48,626 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40675'
2023-11-29 06:41:48,960 - distributed.scheduler - INFO - Receive client connection: Client-5f393bb3-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:41:48,960 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59050
2023-11-29 06:41:50,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:50,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:50,451 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:50,480 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:50,480 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:50,481 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:50,481 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:50,484 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:50,485 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:50,486 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:50,486 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:50,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:50,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:50,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:50,561 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:50,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:50,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:50,578 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:50,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:50,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:50,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:50,592 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:41:50,592 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:41:50,598 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:41:53,682 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42853
2023-11-29 06:41:53,684 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42853
2023-11-29 06:41:53,684 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33121
2023-11-29 06:41:53,684 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:53,684 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,684 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:53,684 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:53,684 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4v4ela0f
2023-11-29 06:41:53,685 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b455cc15-dd57-4feb-9f19-224fe3a93ce1
2023-11-29 06:41:53,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-529cfc0c-0b43-467f-a693-1a13e4b26b8a
2023-11-29 06:41:53,752 - distributed.worker - INFO - Starting Worker plugin PreImport-790db75a-b4df-4a92-9145-fb16435920f9
2023-11-29 06:41:53,752 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,784 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34271
2023-11-29 06:41:53,785 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34271
2023-11-29 06:41:53,785 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46203
2023-11-29 06:41:53,785 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:53,785 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,785 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:53,786 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:53,786 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gsgm_8qo
2023-11-29 06:41:53,786 - distributed.worker - INFO - Starting Worker plugin PreImport-21ecd648-a81c-417c-9d8c-a6c486e6fbb5
2023-11-29 06:41:53,786 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be70386a-0055-4cea-b813-5d178d20578b
2023-11-29 06:41:53,786 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44741
2023-11-29 06:41:53,787 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44741
2023-11-29 06:41:53,787 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33771
2023-11-29 06:41:53,787 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:53,788 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,788 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:53,788 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:53,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vyr65hwf
2023-11-29 06:41:53,788 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ef37fc7-7d5b-48c4-abd8-262d21c133d1
2023-11-29 06:41:53,788 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41191
2023-11-29 06:41:53,789 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41191
2023-11-29 06:41:53,789 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38477
2023-11-29 06:41:53,789 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:53,789 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,789 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:53,789 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:53,789 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tlz_xdbw
2023-11-29 06:41:53,790 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7fd378f9-7c1b-4434-9bd2-ab55ebd56d16
2023-11-29 06:41:53,790 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff6fe098-6d2a-48ce-829c-ff839b0151be
2023-11-29 06:41:53,791 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42853', status: init, memory: 0, processing: 0>
2023-11-29 06:41:53,792 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42853
2023-11-29 06:41:53,792 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55740
2023-11-29 06:41:53,793 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:53,794 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:53,794 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:53,830 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35783
2023-11-29 06:41:53,832 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35783
2023-11-29 06:41:53,832 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42209
2023-11-29 06:41:53,832 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:53,832 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,832 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:53,832 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:53,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z4bvkrbi
2023-11-29 06:41:53,833 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c051693-65a6-4d9c-bd87-a525650588b7
2023-11-29 06:41:53,834 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce8e167d-026f-4987-b643-e97fc1d93a08
2023-11-29 06:41:53,849 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa8e5a0b-2b8f-4d29-8328-53b2de4dc204
2023-11-29 06:41:53,849 - distributed.worker - INFO - Starting Worker plugin PreImport-da27ab9b-ff9b-49bb-a892-503a14acdbe7
2023-11-29 06:41:53,849 - distributed.worker - INFO - Starting Worker plugin PreImport-f1e06c3b-f45f-4110-b26d-7605dfa6a1fd
2023-11-29 06:41:53,849 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,849 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,853 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96564a26-6a2d-4382-aaac-851a3e287578
2023-11-29 06:41:53,856 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,870 - distributed.worker - INFO - Starting Worker plugin PreImport-e7f44e7a-4da8-4088-9e55-68f909860f0a
2023-11-29 06:41:53,870 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44741', status: init, memory: 0, processing: 0>
2023-11-29 06:41:53,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44741
2023-11-29 06:41:53,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55748
2023-11-29 06:41:53,895 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41191', status: init, memory: 0, processing: 0>
2023-11-29 06:41:53,895 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:53,896 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41191
2023-11-29 06:41:53,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55764
2023-11-29 06:41:53,896 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:53,896 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,897 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34271', status: init, memory: 0, processing: 0>
2023-11-29 06:41:53,897 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:53,898 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34271
2023-11-29 06:41:53,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55766
2023-11-29 06:41:53,898 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:53,898 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,900 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35783', status: init, memory: 0, processing: 0>
2023-11-29 06:41:53,900 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:53,900 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35783
2023-11-29 06:41:53,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55768
2023-11-29 06:41:53,901 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:53,901 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,902 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:53,902 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:53,903 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:53,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:53,906 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:53,907 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:53,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:54,483 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42453
2023-11-29 06:41:54,484 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42453
2023-11-29 06:41:54,484 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41625
2023-11-29 06:41:54,484 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:54,484 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:54,484 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:54,484 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:54,484 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7g3qine_
2023-11-29 06:41:54,485 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fcb45b7a-bc91-40b1-8d26-8b257cb00543
2023-11-29 06:41:54,484 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46709
2023-11-29 06:41:54,486 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46709
2023-11-29 06:41:54,486 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36803
2023-11-29 06:41:54,486 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:54,486 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:54,486 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:54,486 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:54,486 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b76v6fy5
2023-11-29 06:41:54,485 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33351
2023-11-29 06:41:54,487 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33351
2023-11-29 06:41:54,487 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43249
2023-11-29 06:41:54,487 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:41:54,487 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:54,487 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:41:54,487 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:41:54,487 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0up19dxh
2023-11-29 06:41:54,487 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ad59a38a-0ff0-46be-99ae-180f44a361c5
2023-11-29 06:41:54,488 - distributed.worker - INFO - Starting Worker plugin PreImport-21e610a1-1062-4ec9-bdb1-7f50b37fb68d
2023-11-29 06:41:54,488 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d110c1d-1f51-48f6-9795-8a903b480c06
2023-11-29 06:41:54,488 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c19b4b5d-6cfd-495f-a43d-96f736d624ca
2023-11-29 06:41:54,490 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3cb0f7b4-26c0-4709-976e-0fc15b84075e
2023-11-29 06:41:54,557 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-14d4cbc3-68c4-4db2-be67-645c4b1634c6
2023-11-29 06:41:54,557 - distributed.worker - INFO - Starting Worker plugin PreImport-9fa8650f-8f2e-4ccb-9c30-5da1fa3bec29
2023-11-29 06:41:54,557 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:54,698 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:54,701 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42453', status: init, memory: 0, processing: 0>
2023-11-29 06:41:54,702 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42453
2023-11-29 06:41:54,702 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55776
2023-11-29 06:41:54,703 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:54,704 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:54,704 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:54,708 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:54,709 - distributed.worker - INFO - Starting Worker plugin PreImport-9ed9a243-3182-47cd-bf91-e1d11d18a627
2023-11-29 06:41:54,709 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:54,726 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46709', status: init, memory: 0, processing: 0>
2023-11-29 06:41:54,726 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46709
2023-11-29 06:41:54,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55792
2023-11-29 06:41:54,728 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:54,728 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:54,728 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:54,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:54,735 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33351', status: init, memory: 0, processing: 0>
2023-11-29 06:41:54,736 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33351
2023-11-29 06:41:54,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55798
2023-11-29 06:41:54,737 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:41:54,737 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:41:54,737 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:41:54,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:41:54,814 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,814 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,815 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,815 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,815 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,815 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,815 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,818 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,818 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,818 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,818 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,818 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,819 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,819 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,885 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,886 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:41:54,888 - distributed.scheduler - INFO - Remove client Client-5d1fbaa5-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:54,889 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58994; closing.
2023-11-29 06:41:54,889 - distributed.scheduler - INFO - Remove client Client-5d1fbaa5-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:54,890 - distributed.scheduler - INFO - Remove client Client-5f393bb3-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:41:54,891 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59050; closing.
2023-11-29 06:41:54,891 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46139'. Reason: nanny-close
2023-11-29 06:41:54,891 - distributed.scheduler - INFO - Close client connection: Client-5d1fbaa5-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:54,891 - distributed.scheduler - INFO - Remove client Client-5f393bb3-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:41:54,891 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:54,892 - distributed.scheduler - INFO - Close client connection: Client-5f393bb3-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:41:54,892 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34379'. Reason: nanny-close
2023-11-29 06:41:54,893 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34271. Reason: nanny-close
2023-11-29 06:41:54,893 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:54,893 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43291'. Reason: nanny-close
2023-11-29 06:41:54,893 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:54,893 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41191. Reason: nanny-close
2023-11-29 06:41:54,894 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42417'. Reason: nanny-close
2023-11-29 06:41:54,894 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:54,894 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35783. Reason: nanny-close
2023-11-29 06:41:54,895 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34723'. Reason: nanny-close
2023-11-29 06:41:54,895 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33351. Reason: nanny-close
2023-11-29 06:41:54,895 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:54,895 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:54,895 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55766; closing.
2023-11-29 06:41:54,895 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38301'. Reason: nanny-close
2023-11-29 06:41:54,896 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34271', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240114.8959248')
2023-11-29 06:41:54,896 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:54,896 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:54,896 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44741. Reason: nanny-close
2023-11-29 06:41:54,896 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38767'. Reason: nanny-close
2023-11-29 06:41:54,896 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:54,896 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:54,896 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42853. Reason: nanny-close
2023-11-29 06:41:54,897 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40675'. Reason: nanny-close
2023-11-29 06:41:54,897 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:54,897 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:41:54,897 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46709. Reason: nanny-close
2023-11-29 06:41:54,898 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:54,898 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42453. Reason: nanny-close
2023-11-29 06:41:54,898 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:54,898 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:54,898 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55768; closing.
2023-11-29 06:41:54,898 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:54,898 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55764; closing.
2023-11-29 06:41:54,899 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:54,899 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:54,899 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:54,900 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35783', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240114.90006')
2023-11-29 06:41:54,900 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:41:54,900 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41191', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240114.9007')
2023-11-29 06:41:54,900 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:54,901 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:54,901 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:54,901 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55798; closing.
2023-11-29 06:41:54,901 - distributed.nanny - INFO - Worker closed
2023-11-29 06:41:54,902 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33351', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240114.902407')
2023-11-29 06:41:54,903 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55748; closing.
2023-11-29 06:41:54,904 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44741', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240114.9040015')
2023-11-29 06:41:54,904 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55740; closing.
2023-11-29 06:41:54,904 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55792; closing.
2023-11-29 06:41:54,904 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55776; closing.
2023-11-29 06:41:54,905 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42853', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240114.9053445')
2023-11-29 06:41:54,905 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46709', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240114.905823')
2023-11-29 06:41:54,906 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42453', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240114.906187')
2023-11-29 06:41:54,906 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:41:54,906 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55776>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-29 06:41:54,908 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55740>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-29 06:41:54,908 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55792>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-29 06:41:56,809 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:41:56,809 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:41:56,810 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:41:56,813 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-29 06:41:56,813 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-11-29 06:41:59,157 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:41:59,163 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45847 instead
  warnings.warn(
2023-11-29 06:41:59,171 - distributed.scheduler - INFO - State start
2023-11-29 06:41:59,314 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:41:59,315 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-29 06:41:59,316 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45847/status
2023-11-29 06:41:59,317 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:41:59,543 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43793', status: init, memory: 0, processing: 0>
2023-11-29 06:41:59,561 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43793
2023-11-29 06:41:59,562 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55934
2023-11-29 06:41:59,564 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40105', status: init, memory: 0, processing: 0>
2023-11-29 06:41:59,564 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40105
2023-11-29 06:41:59,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55928
2023-11-29 06:41:59,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43751', status: init, memory: 0, processing: 0>
2023-11-29 06:41:59,566 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43751
2023-11-29 06:41:59,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55946
2023-11-29 06:41:59,568 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41935', status: init, memory: 0, processing: 0>
2023-11-29 06:41:59,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41935
2023-11-29 06:41:59,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55952
2023-11-29 06:41:59,575 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40603'
2023-11-29 06:41:59,582 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35751', status: init, memory: 0, processing: 0>
2023-11-29 06:41:59,583 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35751
2023-11-29 06:41:59,583 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55954
2023-11-29 06:41:59,588 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40681', status: init, memory: 0, processing: 0>
2023-11-29 06:41:59,589 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40681
2023-11-29 06:41:59,589 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55968
2023-11-29 06:41:59,602 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55946; closing.
2023-11-29 06:41:59,602 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43751', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240119.6028385')
2023-11-29 06:41:59,605 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55934; closing.
2023-11-29 06:41:59,605 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55952; closing.
2023-11-29 06:41:59,606 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43793', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240119.6060038')
2023-11-29 06:41:59,606 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41935', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240119.6064043')
2023-11-29 06:41:59,607 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39907', status: init, memory: 0, processing: 0>
2023-11-29 06:41:59,607 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39907
2023-11-29 06:41:59,607 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56058
2023-11-29 06:41:59,608 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33487'
2023-11-29 06:41:59,608 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55928; closing.
2023-11-29 06:41:59,608 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55954; closing.
2023-11-29 06:41:59,608 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40105', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240119.6089337')
2023-11-29 06:41:59,609 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35751', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240119.609315')
2023-11-29 06:41:59,610 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41121'
2023-11-29 06:41:59,610 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37277', status: init, memory: 0, processing: 0>
2023-11-29 06:41:59,610 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37277
2023-11-29 06:41:59,610 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55978
2023-11-29 06:41:59,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38049'
2023-11-29 06:41:59,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33797'
2023-11-29 06:41:59,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44119'
2023-11-29 06:41:59,653 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56058; closing.
2023-11-29 06:41:59,654 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39907', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240119.6541185')
2023-11-29 06:41:59,655 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55978; closing.
2023-11-29 06:41:59,655 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37277', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240119.6556978')
2023-11-29 06:41:59,656 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55968; closing.
2023-11-29 06:41:59,656 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40681', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240119.6563919')
2023-11-29 06:41:59,656 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:41:59,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39793'
2023-11-29 06:41:59,673 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46109'
2023-11-29 06:41:59,706 - distributed.scheduler - INFO - Receive client connection: Client-64355333-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:41:59,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56060
2023-11-29 06:42:01,368 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:01,368 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:01,369 - distributed.scheduler - INFO - Receive client connection: Client-669ed703-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:01,370 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42494
2023-11-29 06:42:01,372 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:01,439 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:01,439 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:01,444 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:01,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:01,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:01,450 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:01,458 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:01,458 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:01,462 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:01,677 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:01,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:01,679 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:01,679 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:01,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:01,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:01,680 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:01,680 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:01,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:01,683 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:01,684 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:01,684 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:03,083 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39383
2023-11-29 06:42:03,084 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39383
2023-11-29 06:42:03,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42521
2023-11-29 06:42:03,084 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:03,084 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:03,084 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:03,084 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:03,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0xen93ia
2023-11-29 06:42:03,085 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a8694e9-eb96-411a-b2eb-ab081fdfbf87
2023-11-29 06:42:03,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ffb789b-3a4a-47f8-9fc4-07030f2e64e5
2023-11-29 06:42:03,824 - distributed.worker - INFO - Starting Worker plugin PreImport-12db6441-c0a8-4e16-ab3e-a186db3a2280
2023-11-29 06:42:03,825 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:03,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39383', status: init, memory: 0, processing: 0>
2023-11-29 06:42:03,860 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39383
2023-11-29 06:42:03,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42586
2023-11-29 06:42:03,862 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:03,867 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:03,867 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:03,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:04,571 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38687
2023-11-29 06:42:04,572 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38687
2023-11-29 06:42:04,572 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44563
2023-11-29 06:42:04,572 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,572 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,572 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:04,573 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:04,573 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jrqkbe8f
2023-11-29 06:42:04,573 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9eef34ae-74c1-4b4d-af6a-7fc1218e46cf
2023-11-29 06:42:04,583 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33071
2023-11-29 06:42:04,584 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33071
2023-11-29 06:42:04,584 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44669
2023-11-29 06:42:04,584 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,584 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,584 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:04,584 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:04,584 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-03u56q4v
2023-11-29 06:42:04,585 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9364ac63-1108-4ec4-a7f6-6561cc95cb5a
2023-11-29 06:42:04,585 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44581
2023-11-29 06:42:04,586 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44581
2023-11-29 06:42:04,586 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42603
2023-11-29 06:42:04,586 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,586 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,586 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:04,586 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:04,586 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z9974jgm
2023-11-29 06:42:04,587 - distributed.worker - INFO - Starting Worker plugin RMMSetup-54c3d16a-5534-4e57-9a1d-186238e60686
2023-11-29 06:42:04,589 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35625
2023-11-29 06:42:04,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35625
2023-11-29 06:42:04,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38679
2023-11-29 06:42:04,590 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,590 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,590 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:04,590 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:04,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2zqnwca_
2023-11-29 06:42:04,591 - distributed.worker - INFO - Starting Worker plugin RMMSetup-08af2a49-d1d1-4ec2-9ecb-e21be3fe04f5
2023-11-29 06:42:04,592 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42023
2023-11-29 06:42:04,594 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42023
2023-11-29 06:42:04,594 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37095
2023-11-29 06:42:04,594 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,594 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,594 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:04,595 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:04,595 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5dpa5x10
2023-11-29 06:42:04,596 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f4033a5a-744e-459b-9fcc-c449cf65fbf1
2023-11-29 06:42:04,601 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42645
2023-11-29 06:42:04,601 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43003
2023-11-29 06:42:04,601 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42645
2023-11-29 06:42:04,601 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43003
2023-11-29 06:42:04,602 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46201
2023-11-29 06:42:04,602 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,602 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41479
2023-11-29 06:42:04,602 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,602 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,602 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,602 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:04,602 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:04,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aa5p1dge
2023-11-29 06:42:04,602 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:04,602 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:04,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-attlygk4
2023-11-29 06:42:04,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b728d56-acfa-4bae-9e29-4377824c5e96
2023-11-29 06:42:04,603 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8245044-c07e-47b8-9bc8-8d4f60cf06c0
2023-11-29 06:42:04,603 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f66254bd-06b4-44c1-bd8d-aa48cd486d77
2023-11-29 06:42:04,809 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82306bd8-3859-4acc-87a9-b63249e3e61a
2023-11-29 06:42:04,809 - distributed.worker - INFO - Starting Worker plugin PreImport-a10c590a-b70b-4305-be79-4f1094bb97fe
2023-11-29 06:42:04,809 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,820 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b5f559b-5acb-48a8-9b5a-14e746a950b3
2023-11-29 06:42:04,820 - distributed.worker - INFO - Starting Worker plugin PreImport-8bd1d601-6390-4a32-8d95-244a93e348b7
2023-11-29 06:42:04,821 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,822 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-083eb245-6e55-45ee-a6f7-aac379f428a6
2023-11-29 06:42:04,822 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69dcba6d-ee98-4322-b918-87003df55499
2023-11-29 06:42:04,823 - distributed.worker - INFO - Starting Worker plugin PreImport-b76ea227-d580-499c-9a0b-f9d8eb8cf14d
2023-11-29 06:42:04,823 - distributed.worker - INFO - Starting Worker plugin PreImport-3967044a-61e1-4bf0-9cb4-1c3c8d87f206
2023-11-29 06:42:04,823 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,823 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,823 - distributed.worker - INFO - Starting Worker plugin PreImport-fd246cb9-5fdf-4de0-bd5c-3e8237ce0914
2023-11-29 06:42:04,824 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f722ac0f-6278-4edf-a7aa-1819bc94005e
2023-11-29 06:42:04,825 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4606ae3-0866-4698-bdf0-f0279e7ae9cb
2023-11-29 06:42:04,825 - distributed.worker - INFO - Starting Worker plugin PreImport-030eb981-245d-4954-bf04-29df5a50a4ab
2023-11-29 06:42:04,826 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,833 - distributed.worker - INFO - Starting Worker plugin PreImport-79736450-eaa1-441e-8b24-d415b280127f
2023-11-29 06:42:04,833 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,841 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38687', status: init, memory: 0, processing: 0>
2023-11-29 06:42:04,841 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38687
2023-11-29 06:42:04,841 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42596
2023-11-29 06:42:04,842 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:04,846 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,846 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,847 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:04,854 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33071', status: init, memory: 0, processing: 0>
2023-11-29 06:42:04,854 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33071
2023-11-29 06:42:04,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42626
2023-11-29 06:42:04,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:04,856 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44581', status: init, memory: 0, processing: 0>
2023-11-29 06:42:04,856 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44581
2023-11-29 06:42:04,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42618
2023-11-29 06:42:04,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43003', status: init, memory: 0, processing: 0>
2023-11-29 06:42:04,857 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:04,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43003
2023-11-29 06:42:04,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42638
2023-11-29 06:42:04,858 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,858 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,859 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42023', status: init, memory: 0, processing: 0>
2023-11-29 06:42:04,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:04,859 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,859 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42023
2023-11-29 06:42:04,859 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42610
2023-11-29 06:42:04,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:04,860 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:04,861 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,862 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,862 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,863 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:04,863 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35625', status: init, memory: 0, processing: 0>
2023-11-29 06:42:04,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:04,864 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35625
2023-11-29 06:42:04,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42640
2023-11-29 06:42:04,866 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:04,867 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,867 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:04,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:04,878 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42645', status: init, memory: 0, processing: 0>
2023-11-29 06:42:04,878 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42645
2023-11-29 06:42:04,878 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42650
2023-11-29 06:42:04,880 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:04,887 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:04,887 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:04,889 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:04,924 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,925 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,925 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,925 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,925 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,925 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,925 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,925 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,938 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:04,938 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:04,938 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:04,938 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:04,938 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:04,939 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:04,939 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:04,939 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:04,950 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,951 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,951 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,951 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,951 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,951 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,951 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,951 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:04,952 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:04,955 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:04,958 - distributed.scheduler - INFO - Remove client Client-669ed703-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:04,958 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42494; closing.
2023-11-29 06:42:04,958 - distributed.scheduler - INFO - Remove client Client-64355333-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:04,958 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56060; closing.
2023-11-29 06:42:04,958 - distributed.scheduler - INFO - Remove client Client-669ed703-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:04,959 - distributed.scheduler - INFO - Remove client Client-64355333-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:04,959 - distributed.scheduler - INFO - Close client connection: Client-669ed703-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:04,960 - distributed.scheduler - INFO - Close client connection: Client-64355333-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:04,960 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40603'. Reason: nanny-close
2023-11-29 06:42:04,960 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:04,961 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33487'. Reason: nanny-close
2023-11-29 06:42:04,961 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:04,962 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35625. Reason: nanny-close
2023-11-29 06:42:04,962 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41121'. Reason: nanny-close
2023-11-29 06:42:04,962 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:04,962 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42023. Reason: nanny-close
2023-11-29 06:42:04,962 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38049'. Reason: nanny-close
2023-11-29 06:42:04,963 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:04,963 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44581. Reason: nanny-close
2023-11-29 06:42:04,963 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33797'. Reason: nanny-close
2023-11-29 06:42:04,963 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38687. Reason: nanny-close
2023-11-29 06:42:04,964 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:04,964 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:04,964 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42640; closing.
2023-11-29 06:42:04,964 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44119'. Reason: nanny-close
2023-11-29 06:42:04,964 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:04,964 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240124.9648316')
2023-11-29 06:42:04,964 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:04,964 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39383. Reason: nanny-close
2023-11-29 06:42:04,965 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39793'. Reason: nanny-close
2023-11-29 06:42:04,965 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:04,965 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:04,965 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:04,965 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42645. Reason: nanny-close
2023-11-29 06:42:04,966 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46109'. Reason: nanny-close
2023-11-29 06:42:04,966 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42618; closing.
2023-11-29 06:42:04,966 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:04,966 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:04,966 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43003. Reason: nanny-close
2023-11-29 06:42:04,966 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:04,966 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:04,966 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:04,967 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240124.967181')
2023-11-29 06:42:04,967 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:04,967 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42610; closing.
2023-11-29 06:42:04,967 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33071. Reason: nanny-close
2023-11-29 06:42:04,968 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:04,968 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:04,968 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:04,969 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:04,969 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:04,968 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42618>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42618>: Stream is closed
2023-11-29 06:42:04,970 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:04,970 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42596; closing.
2023-11-29 06:42:04,970 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240124.9702916')
2023-11-29 06:42:04,970 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:04,971 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38687', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240124.9709914')
2023-11-29 06:42:04,971 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42586; closing.
2023-11-29 06:42:04,972 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39383', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240124.9720275')
2023-11-29 06:42:04,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42650; closing.
2023-11-29 06:42:04,972 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42638; closing.
2023-11-29 06:42:04,973 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42645', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240124.9731445')
2023-11-29 06:42:04,973 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43003', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240124.9735065')
2023-11-29 06:42:04,973 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42626; closing.
2023-11-29 06:42:04,974 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240124.9743755')
2023-11-29 06:42:04,974 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:06,779 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:42:06,779 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:42:06,780 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:42:06,782 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-29 06:42:06,782 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-11-29 06:42:09,156 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:09,161 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42807 instead
  warnings.warn(
2023-11-29 06:42:09,165 - distributed.scheduler - INFO - State start
2023-11-29 06:42:09,187 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:09,188 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-29 06:42:09,189 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42807/status
2023-11-29 06:42:09,189 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:42:09,212 - distributed.scheduler - INFO - Receive client connection: Client-6a31622e-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:09,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43182
2023-11-29 06:42:09,271 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40767', status: init, memory: 0, processing: 0>
2023-11-29 06:42:09,273 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40767
2023-11-29 06:42:09,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43212
2023-11-29 06:42:09,277 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36261', status: init, memory: 0, processing: 0>
2023-11-29 06:42:09,278 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36261
2023-11-29 06:42:09,278 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43198
2023-11-29 06:42:09,304 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43212; closing.
2023-11-29 06:42:09,305 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40767', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240129.3050544')
2023-11-29 06:42:09,305 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43198; closing.
2023-11-29 06:42:09,306 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36261', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240129.3061879')
2023-11-29 06:42:09,306 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:09,309 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35719', status: init, memory: 0, processing: 0>
2023-11-29 06:42:09,309 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35719
2023-11-29 06:42:09,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43214
2023-11-29 06:42:09,338 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44243', status: init, memory: 0, processing: 0>
2023-11-29 06:42:09,338 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44243
2023-11-29 06:42:09,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43216
2023-11-29 06:42:09,344 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43815'
2023-11-29 06:42:09,347 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36427', status: init, memory: 0, processing: 0>
2023-11-29 06:42:09,347 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36427
2023-11-29 06:42:09,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43226
2023-11-29 06:42:09,354 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43214; closing.
2023-11-29 06:42:09,355 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35719', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240129.3550074')
2023-11-29 06:42:09,355 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43216; closing.
2023-11-29 06:42:09,356 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44243', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240129.356118')
2023-11-29 06:42:09,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43231'
2023-11-29 06:42:09,374 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34987'
2023-11-29 06:42:09,382 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38941'
2023-11-29 06:42:09,391 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42661'
2023-11-29 06:42:09,401 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38385'
2023-11-29 06:42:09,407 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43226; closing.
2023-11-29 06:42:09,407 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36427', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240129.4074275')
2023-11-29 06:42:09,407 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:09,410 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45215'
2023-11-29 06:42:09,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33801'
2023-11-29 06:42:09,527 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46335', status: init, memory: 0, processing: 0>
2023-11-29 06:42:09,527 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46335
2023-11-29 06:42:09,527 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43308
2023-11-29 06:42:09,528 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34941', status: init, memory: 0, processing: 0>
2023-11-29 06:42:09,528 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34941
2023-11-29 06:42:09,529 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43310
2023-11-29 06:42:09,558 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43310; closing.
2023-11-29 06:42:09,558 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34941', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240129.5587537')
2023-11-29 06:42:09,563 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43308; closing.
2023-11-29 06:42:09,563 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46335', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240129.5634985')
2023-11-29 06:42:09,563 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:11,006 - distributed.scheduler - INFO - Receive client connection: Client-6c5d48c5-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:11,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43576
2023-11-29 06:42:11,134 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:11,135 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:11,139 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:11,417 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:11,418 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:11,422 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:11,424 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:11,424 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:11,426 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:11,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:11,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:11,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:11,429 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:11,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:11,433 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:11,435 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:11,435 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:11,440 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:11,474 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:11,474 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:11,478 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:11,489 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:11,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:11,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:12,871 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41527
2023-11-29 06:42:12,873 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41527
2023-11-29 06:42:12,873 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43053
2023-11-29 06:42:12,873 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:12,873 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:12,873 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:12,873 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:12,873 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6oq_me0i
2023-11-29 06:42:12,874 - distributed.worker - INFO - Starting Worker plugin PreImport-343d0e65-3cb9-465e-9746-a74bf979dc1f
2023-11-29 06:42:12,874 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc335399-e362-4ddd-894b-798916ea4f5c
2023-11-29 06:42:13,409 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b94022f-7037-46a6-9602-d526a7e27fe2
2023-11-29 06:42:13,410 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:13,444 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41527', status: init, memory: 0, processing: 0>
2023-11-29 06:42:13,445 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41527
2023-11-29 06:42:13,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43670
2023-11-29 06:42:13,446 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:13,447 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:13,447 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:13,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:14,038 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34551
2023-11-29 06:42:14,039 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34551
2023-11-29 06:42:14,039 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37029
2023-11-29 06:42:14,039 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,039 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,039 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:14,039 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:14,039 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-r07wgdk2
2023-11-29 06:42:14,040 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8a491ac-d756-46a1-a9c7-f81b39f142a6
2023-11-29 06:42:14,050 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42449
2023-11-29 06:42:14,051 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42449
2023-11-29 06:42:14,051 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32965
2023-11-29 06:42:14,051 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,051 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,051 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:14,051 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:14,051 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ul1nuyj7
2023-11-29 06:42:14,052 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7975bc4c-9d4c-4fd7-8606-2dea2dd19fe4
2023-11-29 06:42:14,053 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33845
2023-11-29 06:42:14,054 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33845
2023-11-29 06:42:14,054 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35563
2023-11-29 06:42:14,054 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,054 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,054 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:14,054 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:14,054 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tcg7djsc
2023-11-29 06:42:14,055 - distributed.worker - INFO - Starting Worker plugin RMMSetup-acb965e2-8dae-4491-ba12-e650a5c11552
2023-11-29 06:42:14,057 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40093
2023-11-29 06:42:14,058 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40093
2023-11-29 06:42:14,058 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40867
2023-11-29 06:42:14,058 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,058 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,058 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:14,058 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:14,058 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sf5h4_uc
2023-11-29 06:42:14,059 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6f0e7593-5504-4bd0-97b9-2423d5e1d977
2023-11-29 06:42:14,059 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45637
2023-11-29 06:42:14,060 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45637
2023-11-29 06:42:14,060 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39401
2023-11-29 06:42:14,060 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,060 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,060 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:14,061 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:14,061 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i9iegpmj
2023-11-29 06:42:14,061 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b84ea2c-dc69-4afa-8dbc-e2df616078f0
2023-11-29 06:42:14,064 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38567
2023-11-29 06:42:14,065 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38567
2023-11-29 06:42:14,065 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42671
2023-11-29 06:42:14,065 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,065 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,065 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:14,066 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:14,066 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q5ygb_6j
2023-11-29 06:42:14,066 - distributed.worker - INFO - Starting Worker plugin RMMSetup-67082aaf-aaaa-43df-85e8-e78516df160f
2023-11-29 06:42:14,067 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39079
2023-11-29 06:42:14,068 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39079
2023-11-29 06:42:14,068 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41867
2023-11-29 06:42:14,068 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,068 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,068 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:14,068 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:14,068 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bjihgt7p
2023-11-29 06:42:14,069 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37fb5199-5467-4d83-ba7c-5557bbd4b313
2023-11-29 06:42:14,069 - distributed.worker - INFO - Starting Worker plugin RMMSetup-403d7d0e-6bc3-4f22-89dc-3a8facdce578
2023-11-29 06:42:14,257 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5cb416e7-ac78-4974-93a1-b9253fc35a57
2023-11-29 06:42:14,258 - distributed.worker - INFO - Starting Worker plugin PreImport-74c849bb-f93d-445d-aa6f-871bb5927aec
2023-11-29 06:42:14,258 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,267 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-312de57e-67a1-4718-8333-da81d8c5fbe9
2023-11-29 06:42:14,267 - distributed.worker - INFO - Starting Worker plugin PreImport-d2efa746-6a66-41d2-891e-43c2a5772f76
2023-11-29 06:42:14,267 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,276 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa5950f9-926c-4e02-85dd-24976449c495
2023-11-29 06:42:14,277 - distributed.worker - INFO - Starting Worker plugin PreImport-9b0a59ca-a01f-4873-9d6c-c9b2629fab42
2023-11-29 06:42:14,278 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,281 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1acad894-5327-46cc-a77b-67ca4db65bd2
2023-11-29 06:42:14,282 - distributed.worker - INFO - Starting Worker plugin PreImport-b12f4573-826a-403f-96d5-10963166ba42
2023-11-29 06:42:14,282 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,282 - distributed.worker - INFO - Starting Worker plugin PreImport-bbba2b4f-ba46-42f7-859b-4450e7ecce21
2023-11-29 06:42:14,282 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,286 - distributed.worker - INFO - Starting Worker plugin PreImport-14bdd91c-c1b5-4981-885d-2b44b28852b3
2023-11-29 06:42:14,287 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3dc64d80-f4ba-403d-8c7f-5f3e6e4affbc
2023-11-29 06:42:14,287 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,290 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-24abb020-e8bd-4539-bc26-895db1778ffd
2023-11-29 06:42:14,291 - distributed.worker - INFO - Starting Worker plugin PreImport-64b34615-4f7d-4c93-9f38-9935bb89aca9
2023-11-29 06:42:14,291 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,293 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42449', status: init, memory: 0, processing: 0>
2023-11-29 06:42:14,294 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42449
2023-11-29 06:42:14,294 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43696
2023-11-29 06:42:14,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:14,296 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,296 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,298 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34551', status: init, memory: 0, processing: 0>
2023-11-29 06:42:14,299 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34551
2023-11-29 06:42:14,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43682
2023-11-29 06:42:14,300 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:14,300 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:14,301 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,301 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,309 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:14,312 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39079', status: init, memory: 0, processing: 0>
2023-11-29 06:42:14,313 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39079
2023-11-29 06:42:14,313 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43724
2023-11-29 06:42:14,314 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:14,314 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,314 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,315 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38567', status: init, memory: 0, processing: 0>
2023-11-29 06:42:14,315 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38567
2023-11-29 06:42:14,315 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43744
2023-11-29 06:42:14,316 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:14,317 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,317 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,318 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45637', status: init, memory: 0, processing: 0>
2023-11-29 06:42:14,318 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:14,319 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45637
2023-11-29 06:42:14,319 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43710
2023-11-29 06:42:14,320 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:14,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:14,321 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,321 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,326 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33845', status: init, memory: 0, processing: 0>
2023-11-29 06:42:14,327 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33845
2023-11-29 06:42:14,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43732
2023-11-29 06:42:14,328 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:14,329 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:14,330 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,330 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:14,337 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40093', status: init, memory: 0, processing: 0>
2023-11-29 06:42:14,338 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40093
2023-11-29 06:42:14,338 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43758
2023-11-29 06:42:14,340 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:14,341 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:14,341 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:14,349 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:14,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:14,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:14,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:14,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:14,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:14,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:14,381 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:14,382 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:14,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:14,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:14,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:14,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:14,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:14,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:14,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:14,385 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:14,392 - distributed.scheduler - INFO - Remove client Client-6c5d48c5-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:14,393 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43576; closing.
2023-11-29 06:42:14,393 - distributed.scheduler - INFO - Remove client Client-6c5d48c5-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:14,394 - distributed.scheduler - INFO - Close client connection: Client-6c5d48c5-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:14,401 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:14,401 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:14,401 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:14,401 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:14,401 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:14,401 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:14,401 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:14,401 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:42:14,409 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:14,410 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:14,413 - distributed.scheduler - INFO - Remove client Client-6a31622e-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:14,413 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43182; closing.
2023-11-29 06:42:14,414 - distributed.scheduler - INFO - Remove client Client-6a31622e-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:14,414 - distributed.scheduler - INFO - Close client connection: Client-6a31622e-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:14,415 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43815'. Reason: nanny-close
2023-11-29 06:42:14,415 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:14,416 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43231'. Reason: nanny-close
2023-11-29 06:42:14,416 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38567. Reason: nanny-close
2023-11-29 06:42:14,416 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:14,417 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34987'. Reason: nanny-close
2023-11-29 06:42:14,417 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:14,417 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41527. Reason: nanny-close
2023-11-29 06:42:14,418 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38941'. Reason: nanny-close
2023-11-29 06:42:14,418 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:14,418 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34551. Reason: nanny-close
2023-11-29 06:42:14,418 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42661'. Reason: nanny-close
2023-11-29 06:42:14,418 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:14,419 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43744; closing.
2023-11-29 06:42:14,419 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:14,419 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45637. Reason: nanny-close
2023-11-29 06:42:14,419 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38567', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240134.4192557')
2023-11-29 06:42:14,419 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38385'. Reason: nanny-close
2023-11-29 06:42:14,419 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:14,419 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:14,419 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42449. Reason: nanny-close
2023-11-29 06:42:14,420 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45215'. Reason: nanny-close
2023-11-29 06:42:14,420 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39079. Reason: nanny-close
2023-11-29 06:42:14,420 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:14,420 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:14,420 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43670; closing.
2023-11-29 06:42:14,420 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33801'. Reason: nanny-close
2023-11-29 06:42:14,421 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:14,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:14,421 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:14,421 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41527', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240134.4215207')
2023-11-29 06:42:14,421 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33845. Reason: nanny-close
2023-11-29 06:42:14,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43682; closing.
2023-11-29 06:42:14,422 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:14,422 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34551', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240134.4222598')
2023-11-29 06:42:14,422 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40093. Reason: nanny-close
2023-11-29 06:42:14,422 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:14,422 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:14,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43710; closing.
2023-11-29 06:42:14,423 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:14,423 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43696; closing.
2023-11-29 06:42:14,423 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:14,423 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45637', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240134.4238453')
2023-11-29 06:42:14,423 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:14,424 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:14,424 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:14,424 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42449', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240134.4241395')
2023-11-29 06:42:14,424 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:14,424 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43724; closing.
2023-11-29 06:42:14,425 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39079', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240134.4250398')
2023-11-29 06:42:14,425 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43732; closing.
2023-11-29 06:42:14,425 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:14,426 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33845', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240134.4259489')
2023-11-29 06:42:14,426 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:14,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43758; closing.
2023-11-29 06:42:14,426 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40093', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240134.4267743')
2023-11-29 06:42:14,426 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:16,383 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:42:16,384 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:42:16,384 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:42:16,387 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-29 06:42:16,387 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-11-29 06:42:18,768 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:18,772 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34027 instead
  warnings.warn(
2023-11-29 06:42:18,776 - distributed.scheduler - INFO - State start
2023-11-29 06:42:18,797 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:18,798 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-29 06:42:18,799 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34027/status
2023-11-29 06:42:18,799 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:42:18,834 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42781', status: init, memory: 0, processing: 0>
2023-11-29 06:42:18,850 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42781
2023-11-29 06:42:18,850 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44142
2023-11-29 06:42:18,852 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43049', status: init, memory: 0, processing: 0>
2023-11-29 06:42:18,853 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43049
2023-11-29 06:42:18,853 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44148
2023-11-29 06:42:18,868 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42631', status: init, memory: 0, processing: 0>
2023-11-29 06:42:18,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42631
2023-11-29 06:42:18,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44172
2023-11-29 06:42:18,869 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34339', status: init, memory: 0, processing: 0>
2023-11-29 06:42:18,870 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34339
2023-11-29 06:42:18,870 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44158
2023-11-29 06:42:18,891 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35031', status: init, memory: 0, processing: 0>
2023-11-29 06:42:18,892 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35031
2023-11-29 06:42:18,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44178
2023-11-29 06:42:18,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35759', status: init, memory: 0, processing: 0>
2023-11-29 06:42:18,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35759
2023-11-29 06:42:18,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44176
2023-11-29 06:42:18,897 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44172; closing.
2023-11-29 06:42:18,897 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42631', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240138.897586')
2023-11-29 06:42:18,898 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44142; closing.
2023-11-29 06:42:18,899 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42781', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240138.8999045')
2023-11-29 06:42:18,900 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44148; closing.
2023-11-29 06:42:18,900 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44158; closing.
2023-11-29 06:42:18,900 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44142>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44142>: Stream is closed
2023-11-29 06:42:18,903 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43049', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240138.9030364')
2023-11-29 06:42:18,903 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34339', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240138.903441')
2023-11-29 06:42:18,907 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44176; closing.
2023-11-29 06:42:18,907 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35759', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240138.9074223')
2023-11-29 06:42:18,936 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41895', status: init, memory: 0, processing: 0>
2023-11-29 06:42:18,937 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41895
2023-11-29 06:42:18,937 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44192
2023-11-29 06:42:18,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44192; closing.
2023-11-29 06:42:18,946 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41895', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240138.9468987')
2023-11-29 06:42:18,947 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44178; closing.
2023-11-29 06:42:18,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35031', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240138.9479265')
2023-11-29 06:42:18,948 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:18,961 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36179', status: init, memory: 0, processing: 0>
2023-11-29 06:42:18,961 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36179
2023-11-29 06:42:18,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44206
2023-11-29 06:42:18,969 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36911'
2023-11-29 06:42:18,983 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37657'
2023-11-29 06:42:18,997 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46671'
2023-11-29 06:42:19,003 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44206; closing.
2023-11-29 06:42:19,003 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36179', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240139.0035512')
2023-11-29 06:42:19,003 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:19,012 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44607'
2023-11-29 06:42:19,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36257'
2023-11-29 06:42:19,024 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35673'
2023-11-29 06:42:19,033 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40513'
2023-11-29 06:42:19,043 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45261'
2023-11-29 06:42:20,420 - distributed.scheduler - INFO - Receive client connection: Client-6fe32039-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:20,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58638
2023-11-29 06:42:20,665 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:20,665 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:20,669 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:20,787 - distributed.scheduler - INFO - Receive client connection: Client-723124d4-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:20,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58656
2023-11-29 06:42:20,916 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:20,917 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:20,921 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:20,940 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:20,940 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:20,941 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:20,941 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:20,942 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:20,942 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:20,944 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:20,945 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:20,945 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:20,946 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:20,947 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:20,949 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:20,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:20,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:20,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:20,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:20,959 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:20,959 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:22,681 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33741
2023-11-29 06:42:22,684 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33741
2023-11-29 06:42:22,684 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38581
2023-11-29 06:42:22,684 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:22,684 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:22,685 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:22,685 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:22,685 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wnvszp2s
2023-11-29 06:42:22,686 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4f9a8cf-71b4-450d-93b6-8c39a4246bcf
2023-11-29 06:42:23,036 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7517e4f8-a933-47ec-a42e-e8bc2d98d5bd
2023-11-29 06:42:23,037 - distributed.worker - INFO - Starting Worker plugin PreImport-dab5a738-ad7d-4b82-af72-af6ab1e6b16b
2023-11-29 06:42:23,037 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:23,074 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33741', status: init, memory: 0, processing: 0>
2023-11-29 06:42:23,075 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33741
2023-11-29 06:42:23,075 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58672
2023-11-29 06:42:23,076 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:23,080 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:23,080 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:23,081 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:23,828 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46509
2023-11-29 06:42:23,829 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46509
2023-11-29 06:42:23,829 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34127
2023-11-29 06:42:23,829 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:23,829 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:23,829 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:23,829 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:23,830 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bhvfiye2
2023-11-29 06:42:23,828 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44571
2023-11-29 06:42:23,830 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44571
2023-11-29 06:42:23,830 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34025
2023-11-29 06:42:23,830 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:23,830 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:23,830 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f4533792-bb2a-41c1-85b2-cf26ff3acc4d
2023-11-29 06:42:23,830 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:23,830 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:23,830 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0l24xthj
2023-11-29 06:42:23,832 - distributed.worker - INFO - Starting Worker plugin RMMSetup-524cdae0-2df3-4534-a5de-cc94d2ff86e8
2023-11-29 06:42:23,934 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37297
2023-11-29 06:42:23,936 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37297
2023-11-29 06:42:23,936 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41291
2023-11-29 06:42:23,936 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:23,936 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44051
2023-11-29 06:42:23,936 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:23,936 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44051
2023-11-29 06:42:23,936 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39355
2023-11-29 06:42:23,936 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:23,937 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:23,935 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46119
2023-11-29 06:42:23,937 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:23,937 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46119
2023-11-29 06:42:23,937 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:23,937 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-60z4ty8a
2023-11-29 06:42:23,937 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41263
2023-11-29 06:42:23,937 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:23,937 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:23,937 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:23,937 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:23,937 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wjunmdf6
2023-11-29 06:42:23,937 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:23,937 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:23,937 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y33s_rwc
2023-11-29 06:42:23,937 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-096f5450-d262-438c-b3c7-c1ad178d3286
2023-11-29 06:42:23,938 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e15d4635-7c0e-401e-98d5-13110d9c306d
2023-11-29 06:42:23,938 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a62de69b-0355-4491-a98c-ad75b0c93aa2
2023-11-29 06:42:23,939 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1012cd65-baae-4add-9627-4c0313c7e991
2023-11-29 06:42:23,938 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33817
2023-11-29 06:42:23,941 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33817
2023-11-29 06:42:23,941 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45613
2023-11-29 06:42:23,941 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:23,941 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:23,941 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:23,941 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:23,941 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0c_0haux
2023-11-29 06:42:23,939 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44795
2023-11-29 06:42:23,941 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44795
2023-11-29 06:42:23,942 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34885
2023-11-29 06:42:23,942 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:23,942 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:23,942 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:23,942 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:42:23,942 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ibgka2d6
2023-11-29 06:42:23,942 - distributed.worker - INFO - Starting Worker plugin RMMSetup-343d6ff3-9c47-4daa-97b7-bf634b4297d0
2023-11-29 06:42:23,943 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8746ba3-e802-4523-91b6-a22672f577e0
2023-11-29 06:42:23,973 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-295ea3a8-b946-40fe-8f7c-ca9cfe003b2d
2023-11-29 06:42:23,973 - distributed.worker - INFO - Starting Worker plugin PreImport-9adc59f2-efd1-4349-a656-57aa596e1788
2023-11-29 06:42:23,974 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:23,977 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c95c80dc-669b-4a32-aa14-b704ed209e81
2023-11-29 06:42:23,977 - distributed.worker - INFO - Starting Worker plugin PreImport-207e84d9-62ee-4b0a-9d61-715e3e4f0ea6
2023-11-29 06:42:23,978 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,009 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46509', status: init, memory: 0, processing: 0>
2023-11-29 06:42:24,009 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46509
2023-11-29 06:42:24,009 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58762
2023-11-29 06:42:24,010 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:24,011 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44571', status: init, memory: 0, processing: 0>
2023-11-29 06:42:24,012 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44571
2023-11-29 06:42:24,012 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58752
2023-11-29 06:42:24,013 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:24,014 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:24,014 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:24,018 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:24,018 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,020 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:24,056 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5618e932-ffb2-4b11-b3c6-d03ed8a0cc02
2023-11-29 06:42:24,056 - distributed.worker - INFO - Starting Worker plugin PreImport-ff5a1ef1-4b90-4b61-bd5f-689baa34f2ca
2023-11-29 06:42:24,056 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a616f6c-b18f-4fba-bc7b-bc07a980e059
2023-11-29 06:42:24,063 - distributed.worker - INFO - Starting Worker plugin PreImport-54862882-79cf-4170-b595-76e0829e6930
2023-11-29 06:42:24,063 - distributed.worker - INFO - Starting Worker plugin PreImport-6b3dc25e-598f-472a-82a9-1e40e74b2f2c
2023-11-29 06:42:24,063 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,064 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,067 - distributed.worker - INFO - Starting Worker plugin PreImport-43ca4659-b4ed-4389-aa8b-1385479c2db8
2023-11-29 06:42:24,067 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8afb75c2-e8ef-43a0-af76-3796d076b22a
2023-11-29 06:42:24,067 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8224b849-108a-46b4-98da-baa077b6ed17
2023-11-29 06:42:24,068 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,069 - distributed.worker - INFO - Starting Worker plugin PreImport-cafb5390-2d74-40d0-a2ea-e5743c46863a
2023-11-29 06:42:24,069 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,090 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46119', status: init, memory: 0, processing: 0>
2023-11-29 06:42:24,091 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46119
2023-11-29 06:42:24,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58768
2023-11-29 06:42:24,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:24,096 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37297', status: init, memory: 0, processing: 0>
2023-11-29 06:42:24,097 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37297
2023-11-29 06:42:24,097 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58772
2023-11-29 06:42:24,098 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:24,098 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33817', status: init, memory: 0, processing: 0>
2023-11-29 06:42:24,099 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:24,099 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,099 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33817
2023-11-29 06:42:24,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58800
2023-11-29 06:42:24,100 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:24,100 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44051', status: init, memory: 0, processing: 0>
2023-11-29 06:42:24,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:24,101 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44051
2023-11-29 06:42:24,101 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58784
2023-11-29 06:42:24,102 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:24,102 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,103 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:24,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:24,107 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44795', status: init, memory: 0, processing: 0>
2023-11-29 06:42:24,107 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:24,108 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,108 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44795
2023-11-29 06:42:24,108 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58814
2023-11-29 06:42:24,109 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:24,109 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,109 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:24,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:24,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:24,113 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:24,113 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:24,114 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:24,124 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:24,124 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:24,124 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:24,124 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:24,124 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:24,124 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:24,124 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:24,125 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:42:24,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:24,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:24,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:24,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:24,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:24,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:24,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:24,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:24,134 - distributed.scheduler - INFO - Remove client Client-6fe32039-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:24,135 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58638; closing.
2023-11-29 06:42:24,135 - distributed.scheduler - INFO - Remove client Client-6fe32039-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:24,135 - distributed.scheduler - INFO - Close client connection: Client-6fe32039-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:24,136 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36911'. Reason: nanny-close
2023-11-29 06:42:24,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37657'. Reason: nanny-close
2023-11-29 06:42:24,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46671'. Reason: nanny-close
2023-11-29 06:42:24,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44607'. Reason: nanny-close
2023-11-29 06:42:24,138 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:24,138 - distributed.scheduler - INFO - Remove client Client-723124d4-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:24,138 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58656; closing.
2023-11-29 06:42:24,138 - distributed.scheduler - INFO - Remove client Client-723124d4-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:24,139 - distributed.scheduler - INFO - Close client connection: Client-723124d4-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:24,139 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36257'. Reason: nanny-close
2023-11-29 06:42:24,139 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:24,139 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44571. Reason: nanny-close
2023-11-29 06:42:24,139 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35673'. Reason: nanny-close
2023-11-29 06:42:24,140 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40513'. Reason: nanny-close
2023-11-29 06:42:24,140 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33741. Reason: nanny-close
2023-11-29 06:42:24,140 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45261'. Reason: nanny-close
2023-11-29 06:42:24,140 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:24,141 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46509. Reason: nanny-close
2023-11-29 06:42:24,141 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58752; closing.
2023-11-29 06:42:24,141 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:24,142 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44571', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240144.14212')
2023-11-29 06:42:24,142 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:24,143 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:24,143 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:24,143 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58672; closing.
2023-11-29 06:42:24,144 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:24,144 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58762; closing.
2023-11-29 06:42:24,144 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:24,144 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33741', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240144.1448152')
2023-11-29 06:42:24,145 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46509', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240144.1451948')
2023-11-29 06:42:24,147 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:24,148 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:24,148 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46119. Reason: nanny-close
2023-11-29 06:42:24,149 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37297. Reason: nanny-close
2023-11-29 06:42:24,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:24,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:24,151 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58768; closing.
2023-11-29 06:42:24,151 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:24,151 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:24,151 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44051. Reason: nanny-close
2023-11-29 06:42:24,151 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46119', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240144.1514292')
2023-11-29 06:42:24,151 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58772; closing.
2023-11-29 06:42:24,151 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44795. Reason: nanny-close
2023-11-29 06:42:24,152 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37297', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240144.1520689')
2023-11-29 06:42:24,152 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:24,153 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:24,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:24,153 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58784; closing.
2023-11-29 06:42:24,153 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:24,153 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44051', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240144.1535199')
2023-11-29 06:42:24,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:24,154 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33817. Reason: nanny-close
2023-11-29 06:42:24,154 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58814; closing.
2023-11-29 06:42:24,154 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:24,155 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44795', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240144.1550224')
2023-11-29 06:42:24,155 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:24,156 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58800; closing.
2023-11-29 06:42:24,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:24,156 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33817', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240144.1567242')
2023-11-29 06:42:24,156 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:24,158 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:25,806 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:42:25,806 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:42:25,807 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:42:25,809 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-29 06:42:25,810 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-11-29 06:42:28,019 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:28,023 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40233 instead
  warnings.warn(
2023-11-29 06:42:28,029 - distributed.scheduler - INFO - State start
2023-11-29 06:42:28,051 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:28,053 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-29 06:42:28,054 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40233/status
2023-11-29 06:42:28,054 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:42:28,406 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46617', status: init, memory: 0, processing: 0>
2023-11-29 06:42:28,419 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46617
2023-11-29 06:42:28,420 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58930
2023-11-29 06:42:28,421 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32861', status: init, memory: 0, processing: 0>
2023-11-29 06:42:28,422 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32861
2023-11-29 06:42:28,422 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58916
2023-11-29 06:42:28,423 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40501', status: init, memory: 0, processing: 0>
2023-11-29 06:42:28,423 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40501
2023-11-29 06:42:28,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58934
2023-11-29 06:42:28,445 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34853'
2023-11-29 06:42:28,462 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58916; closing.
2023-11-29 06:42:28,462 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32861', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240148.4627488')
2023-11-29 06:42:28,463 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58930; closing.
2023-11-29 06:42:28,464 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58934; closing.
2023-11-29 06:42:28,464 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46617', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240148.464597')
2023-11-29 06:42:28,465 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40501', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240148.4650857')
2023-11-29 06:42:28,465 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:28,545 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36757', status: init, memory: 0, processing: 0>
2023-11-29 06:42:28,546 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36757
2023-11-29 06:42:28,546 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58958
2023-11-29 06:42:28,554 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36643', status: init, memory: 0, processing: 0>
2023-11-29 06:42:28,554 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36643
2023-11-29 06:42:28,554 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58988
2023-11-29 06:42:28,556 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39301', status: init, memory: 0, processing: 0>
2023-11-29 06:42:28,556 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39301
2023-11-29 06:42:28,557 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58960
2023-11-29 06:42:28,561 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46547', status: init, memory: 0, processing: 0>
2023-11-29 06:42:28,561 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46547
2023-11-29 06:42:28,561 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58948
2023-11-29 06:42:28,562 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58958; closing.
2023-11-29 06:42:28,563 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240148.5632374')
2023-11-29 06:42:28,565 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44687', status: init, memory: 0, processing: 0>
2023-11-29 06:42:28,566 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44687
2023-11-29 06:42:28,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58976
2023-11-29 06:42:28,613 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58960; closing.
2023-11-29 06:42:28,613 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39301', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240148.6138756')
2023-11-29 06:42:28,614 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58988; closing.
2023-11-29 06:42:28,614 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36643', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240148.6148806')
2023-11-29 06:42:28,616 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58988>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-29 06:42:28,618 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58948; closing.
2023-11-29 06:42:28,618 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46547', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240148.6185727')
2023-11-29 06:42:28,618 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58976; closing.
2023-11-29 06:42:28,619 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44687', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240148.6192482')
2023-11-29 06:42:28,619 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:29,640 - distributed.scheduler - INFO - Receive client connection: Client-757ffb40-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:29,641 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58994
2023-11-29 06:42:29,989 - distributed.scheduler - INFO - Receive client connection: Client-77ade20d-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:29,990 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48600
2023-11-29 06:42:30,162 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:30,162 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:30,726 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:31,677 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45035
2023-11-29 06:42:31,678 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45035
2023-11-29 06:42:31,678 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-11-29 06:42:31,678 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:31,678 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:31,678 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:31,679 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-29 06:42:31,679 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7xbj9odh
2023-11-29 06:42:31,679 - distributed.worker - INFO - Starting Worker plugin PreImport-b7d50744-63de-46e6-98ba-a47f1d54f8b6
2023-11-29 06:42:31,679 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d7d6ac6d-52dc-4922-93a5-d8d1f93f9b0a
2023-11-29 06:42:31,679 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01779a3f-f98f-4911-8ddb-2f54f1940561
2023-11-29 06:42:31,680 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:31,709 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45035', status: init, memory: 0, processing: 0>
2023-11-29 06:42:31,710 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45035
2023-11-29 06:42:31,710 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48620
2023-11-29 06:42:31,711 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:31,712 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:31,712 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:31,713 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:31,767 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:31,770 - distributed.scheduler - INFO - Remove client Client-757ffb40-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:31,770 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58994; closing.
2023-11-29 06:42:31,770 - distributed.scheduler - INFO - Remove client Client-757ffb40-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:31,770 - distributed.scheduler - INFO - Close client connection: Client-757ffb40-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:31,772 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34853'. Reason: nanny-close
2023-11-29 06:42:31,772 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:31,773 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45035. Reason: nanny-close
2023-11-29 06:42:31,775 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:31,775 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48620; closing.
2023-11-29 06:42:31,775 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45035', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240151.7756994')
2023-11-29 06:42:31,775 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:31,776 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:32,988 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:42:32,989 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:42:32,990 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:42:32,992 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-29 06:42:32,993 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-11-29 06:42:36,892 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:36,896 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40991 instead
  warnings.warn(
2023-11-29 06:42:36,900 - distributed.scheduler - INFO - State start
2023-11-29 06:42:36,921 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:36,922 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-29 06:42:36,923 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40991/status
2023-11-29 06:42:36,923 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:42:36,960 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39797', status: init, memory: 0, processing: 0>
2023-11-29 06:42:36,973 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39797
2023-11-29 06:42:36,973 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49572
2023-11-29 06:42:37,010 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49572; closing.
2023-11-29 06:42:37,011 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39797', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240157.0110896')
2023-11-29 06:42:37,011 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:37,018 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44893', status: init, memory: 0, processing: 0>
2023-11-29 06:42:37,018 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44893
2023-11-29 06:42:37,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49580
2023-11-29 06:42:37,030 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37827', status: init, memory: 0, processing: 0>
2023-11-29 06:42:37,031 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37827
2023-11-29 06:42:37,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49588
2023-11-29 06:42:37,055 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49580; closing.
2023-11-29 06:42:37,056 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44893', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240157.0561624')
2023-11-29 06:42:37,056 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49588; closing.
2023-11-29 06:42:37,057 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240157.0572655')
2023-11-29 06:42:37,057 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:37,091 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34139'
2023-11-29 06:42:37,149 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37113', status: init, memory: 0, processing: 0>
2023-11-29 06:42:37,149 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37113
2023-11-29 06:42:37,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49608
2023-11-29 06:42:37,162 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49608; closing.
2023-11-29 06:42:37,162 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37113', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240157.1628735')
2023-11-29 06:42:37,163 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:37,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36205', status: init, memory: 0, processing: 0>
2023-11-29 06:42:37,218 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36205
2023-11-29 06:42:37,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49622
2023-11-29 06:42:37,265 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49622; closing.
2023-11-29 06:42:37,265 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36205', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240157.26559')
2023-11-29 06:42:37,266 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:37,307 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43743', status: init, memory: 0, processing: 0>
2023-11-29 06:42:37,307 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43743
2023-11-29 06:42:37,308 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49628
2023-11-29 06:42:37,317 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49628; closing.
2023-11-29 06:42:37,317 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43743', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240157.3175788')
2023-11-29 06:42:37,317 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:37,480 - distributed.scheduler - INFO - Receive client connection: Client-7abc61e9-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:37,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49644
2023-11-29 06:42:38,704 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:38,705 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:39,198 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:40,092 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35063
2023-11-29 06:42:40,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35063
2023-11-29 06:42:40,093 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45233
2023-11-29 06:42:40,093 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:42:40,093 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:40,093 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:40,093 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-29 06:42:40,093 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gjmrbabz
2023-11-29 06:42:40,094 - distributed.worker - INFO - Starting Worker plugin PreImport-765f262e-96c3-4bfe-8c18-b174a5d8bdb5
2023-11-29 06:42:40,095 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25bd6f47-621e-4e75-a876-b9a9f1e84c99
2023-11-29 06:42:40,095 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2887d7b5-f9ec-461a-b848-245d2881c8ef
2023-11-29 06:42:40,096 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:40,124 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35063', status: init, memory: 0, processing: 0>
2023-11-29 06:42:40,125 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35063
2023-11-29 06:42:40,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40532
2023-11-29 06:42:40,126 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:40,127 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:42:40,127 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:40,129 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:42:40,227 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:40,229 - distributed.scheduler - INFO - Remove client Client-7abc61e9-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:40,229 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49644; closing.
2023-11-29 06:42:40,230 - distributed.scheduler - INFO - Remove client Client-7abc61e9-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:40,230 - distributed.scheduler - INFO - Close client connection: Client-7abc61e9-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:40,231 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34139'. Reason: nanny-close
2023-11-29 06:42:40,231 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:40,233 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35063. Reason: nanny-close
2023-11-29 06:42:40,234 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40532; closing.
2023-11-29 06:42:40,234 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:42:40,235 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35063', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240160.235052')
2023-11-29 06:42:40,235 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:40,236 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:41,397 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:42:41,398 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:42:41,398 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:42:41,399 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-29 06:42:41,400 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-11-29 06:42:43,467 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:43,471 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34401 instead
  warnings.warn(
2023-11-29 06:42:43,475 - distributed.scheduler - INFO - State start
2023-11-29 06:42:43,496 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:43,497 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-29 06:42:43,497 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34401/status
2023-11-29 06:42:43,497 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:42:46,299 - distributed.scheduler - INFO - Receive client connection: Client-81669226-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:46,311 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40776
2023-11-29 06:42:47,254 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:40750'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40750>: Stream is closed
2023-11-29 06:42:47,607 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:42:47,607 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:42:47,608 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:42:47,609 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-29 06:42:47,609 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-11-29 06:42:49,768 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:49,772 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37253 instead
  warnings.warn(
2023-11-29 06:42:49,776 - distributed.scheduler - INFO - State start
2023-11-29 06:42:49,796 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:49,797 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-29 06:42:49,798 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37253/status
2023-11-29 06:42:49,798 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:42:49,947 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43451'
2023-11-29 06:42:50,989 - distributed.scheduler - INFO - Receive client connection: Client-827c51fc-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:51,001 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57666
2023-11-29 06:42:51,543 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:51,543 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:51,547 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:52,651 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43585
2023-11-29 06:42:52,652 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43585
2023-11-29 06:42:52,652 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41447
2023-11-29 06:42:52,652 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-29 06:42:52,652 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:52,652 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:42:52,652 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-29 06:42:52,652 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-pxglqzw6
2023-11-29 06:42:52,653 - distributed.worker - INFO - Starting Worker plugin PreImport-b1ff48b3-dddd-4716-9c49-f8c737b6910b
2023-11-29 06:42:52,653 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28022e12-97ba-4606-8cc5-a2c8060bbf4b
2023-11-29 06:42:52,653 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3af09e86-cce3-4b94-beb1-28b657dc74a3
2023-11-29 06:42:52,654 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:52,681 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43585', status: init, memory: 0, processing: 0>
2023-11-29 06:42:52,682 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43585
2023-11-29 06:42:52,682 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57688
2023-11-29 06:42:52,683 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:42:52,684 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-29 06:42:52,684 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:42:52,687 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-29 06:42:52,731 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:42:52,733 - distributed.scheduler - INFO - Remove client Client-827c51fc-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:52,734 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57666; closing.
2023-11-29 06:42:52,734 - distributed.scheduler - INFO - Remove client Client-827c51fc-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:52,734 - distributed.scheduler - INFO - Close client connection: Client-827c51fc-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:52,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43451'. Reason: nanny-close
2023-11-29 06:42:52,735 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:42:52,736 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43585. Reason: nanny-close
2023-11-29 06:42:52,738 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57688; closing.
2023-11-29 06:42:52,738 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-29 06:42:52,738 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43585', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240172.7387204')
2023-11-29 06:42:52,739 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:42:52,740 - distributed.nanny - INFO - Worker closed
2023-11-29 06:42:53,801 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:42:53,802 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:42:53,802 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:42:53,803 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-29 06:42:53,803 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-11-29 06:42:55,937 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:55,941 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37795 instead
  warnings.warn(
2023-11-29 06:42:55,945 - distributed.scheduler - INFO - State start
2023-11-29 06:42:55,964 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:42:55,965 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-29 06:42:55,966 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37795/status
2023-11-29 06:42:55,966 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:42:56,106 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44387'
2023-11-29 06:42:56,119 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44417'
2023-11-29 06:42:56,135 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46185'
2023-11-29 06:42:56,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36091'
2023-11-29 06:42:56,145 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40059'
2023-11-29 06:42:56,154 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38363'
2023-11-29 06:42:56,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36245'
2023-11-29 06:42:56,172 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42267'
2023-11-29 06:42:56,220 - distributed.scheduler - INFO - Receive client connection: Client-8650c6bf-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:42:56,235 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47236
2023-11-29 06:42:56,457 - distributed.scheduler - INFO - Receive client connection: Client-861d2a1a-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:42:56,458 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47280
2023-11-29 06:42:58,055 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:58,055 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:58,056 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:58,056 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:58,059 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:58,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:58,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:58,060 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:58,064 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:58,068 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:58,068 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:58,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:58,069 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:58,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:58,069 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:58,069 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:58,069 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:58,071 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:42:58,071 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:42:58,072 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:58,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:58,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:58,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:42:58,075 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:43:00,953 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37363
2023-11-29 06:43:00,953 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37363
2023-11-29 06:43:00,953 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41215
2023-11-29 06:43:00,953 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:43:00,954 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:00,954 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:43:00,954 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:43:00,954 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4t04m0hc
2023-11-29 06:43:00,954 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3118d1c9-d679-43ec-a471-abf743036531
2023-11-29 06:43:00,956 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43989
2023-11-29 06:43:00,956 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43989
2023-11-29 06:43:00,957 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37861
2023-11-29 06:43:00,957 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:43:00,957 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:00,957 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:43:00,957 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:43:00,957 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_rxy7qyo
2023-11-29 06:43:00,957 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-043bb4cd-3d1a-4ee8-bc33-a003c1b04f55
2023-11-29 06:43:00,958 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8a7d3e4-3940-4d14-b5c4-96f02aaf305f
2023-11-29 06:43:00,966 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40737
2023-11-29 06:43:00,967 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40737
2023-11-29 06:43:00,967 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39345
2023-11-29 06:43:00,967 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:43:00,967 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:00,967 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:43:00,967 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:43:00,967 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-71fdi1j7
2023-11-29 06:43:00,967 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35955
2023-11-29 06:43:00,968 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35955
2023-11-29 06:43:00,967 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46633
2023-11-29 06:43:00,968 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37015
2023-11-29 06:43:00,968 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46633
2023-11-29 06:43:00,968 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:43:00,968 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e16c95f-8692-443e-9633-fb3154534207
2023-11-29 06:43:00,968 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34795
2023-11-29 06:43:00,968 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:00,968 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:43:00,968 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:00,968 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:43:00,968 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:43:00,968 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:43:00,968 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2e964h_o
2023-11-29 06:43:00,968 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:43:00,968 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vrbtn4lk
2023-11-29 06:43:00,969 - distributed.worker - INFO - Starting Worker plugin RMMSetup-89f31342-74e1-4df0-a9e0-9f83cf584c9b
2023-11-29 06:43:00,969 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-68f12578-f1d3-4fbb-b5e7-9cd7b60e5e9e
2023-11-29 06:43:00,969 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eeb671ad-2710-4d70-a0f1-6bb92a4af028
2023-11-29 06:43:00,970 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40629
2023-11-29 06:43:00,971 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40629
2023-11-29 06:43:00,971 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43973
2023-11-29 06:43:00,971 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:43:00,971 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:00,971 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:43:00,971 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:43:00,971 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n79snalx
2023-11-29 06:43:00,972 - distributed.worker - INFO - Starting Worker plugin PreImport-6b39c1eb-4860-4f70-8a81-5cd6a05b76e6
2023-11-29 06:43:00,972 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de11ae81-f202-4b41-965c-3d9525c31c81
2023-11-29 06:43:00,974 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a1c6ca36-8dbc-4f72-a412-16d5b489bad3
2023-11-29 06:43:00,980 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40767
2023-11-29 06:43:00,981 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40767
2023-11-29 06:43:00,981 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45485
2023-11-29 06:43:00,981 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:43:00,981 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:00,981 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:43:00,982 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:43:00,982 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hutc6g2f
2023-11-29 06:43:00,982 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b992b6ed-f5e8-4002-8a21-ec96d459fe76
2023-11-29 06:43:00,984 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37111
2023-11-29 06:43:00,985 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37111
2023-11-29 06:43:00,985 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42791
2023-11-29 06:43:00,985 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:43:00,985 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:00,985 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:43:00,986 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-29 06:43:00,986 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yi1r262a
2023-11-29 06:43:00,986 - distributed.worker - INFO - Starting Worker plugin PreImport-79c2722e-1c6a-41e8-b08c-db0dfd860938
2023-11-29 06:43:00,986 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cfd924b4-db55-4d1e-93ec-b08aeb932bbe
2023-11-29 06:43:01,072 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34239', status: init, memory: 0, processing: 0>
2023-11-29 06:43:01,073 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34239
2023-11-29 06:43:01,073 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44472
2023-11-29 06:43:01,111 - distributed.worker - INFO - Starting Worker plugin PreImport-a8220540-b15c-4933-b3eb-f9153b47d3fe
2023-11-29 06:43:01,111 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,122 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,122 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3bbc58c-9c10-4117-b4df-0978542f63f9
2023-11-29 06:43:01,123 - distributed.worker - INFO - Starting Worker plugin PreImport-82bb8d20-2ac8-4260-8735-5b8f62f0f609
2023-11-29 06:43:01,123 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,131 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c41a037b-4e14-41bc-bab4-2b347fd209d3
2023-11-29 06:43:01,132 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4866dceb-c1d8-490f-90dd-b663408523ca
2023-11-29 06:43:01,132 - distributed.worker - INFO - Starting Worker plugin PreImport-7dd4bf30-c03a-445a-902b-dd9a00d45800
2023-11-29 06:43:01,132 - distributed.worker - INFO - Starting Worker plugin PreImport-50817ab0-1459-4004-a27d-e8e5fea9b8ff
2023-11-29 06:43:01,132 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,132 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,134 - distributed.worker - INFO - Starting Worker plugin PreImport-842a6088-73a8-44db-95db-070e78915ea1
2023-11-29 06:43:01,134 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,140 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff4df6f4-e6d6-43dc-b786-2ae06cc455f1
2023-11-29 06:43:01,142 - distributed.worker - INFO - Starting Worker plugin PreImport-5424cfb4-9f44-40ab-ac82-5bc0e6e338aa
2023-11-29 06:43:01,142 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,148 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43989', status: init, memory: 0, processing: 0>
2023-11-29 06:43:01,149 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43989
2023-11-29 06:43:01,149 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44484
2023-11-29 06:43:01,150 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:43:01,152 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25076e24-46e4-428b-b1cf-a50469335df2
2023-11-29 06:43:01,153 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,154 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40629', status: init, memory: 0, processing: 0>
2023-11-29 06:43:01,155 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40629
2023-11-29 06:43:01,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44504
2023-11-29 06:43:01,155 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37363', status: init, memory: 0, processing: 0>
2023-11-29 06:43:01,156 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37363
2023-11-29 06:43:01,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44488
2023-11-29 06:43:01,156 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:43:01,157 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:43:01,157 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,157 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:43:01,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:43:01,161 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:43:01,161 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,163 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:43:01,163 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:43:01,163 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46633', status: init, memory: 0, processing: 0>
2023-11-29 06:43:01,164 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46633
2023-11-29 06:43:01,164 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44506
2023-11-29 06:43:01,165 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:43:01,165 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:43:01,169 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:43:01,169 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,170 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35955', status: init, memory: 0, processing: 0>
2023-11-29 06:43:01,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:43:01,170 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35955
2023-11-29 06:43:01,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44516
2023-11-29 06:43:01,171 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:43:01,174 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40737', status: init, memory: 0, processing: 0>
2023-11-29 06:43:01,175 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:43:01,175 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,175 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40737
2023-11-29 06:43:01,175 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44532
2023-11-29 06:43:01,176 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:43:01,176 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:43:01,180 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37111', status: init, memory: 0, processing: 0>
2023-11-29 06:43:01,181 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37111
2023-11-29 06:43:01,181 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44550
2023-11-29 06:43:01,181 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40767', status: init, memory: 0, processing: 0>
2023-11-29 06:43:01,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:43:01,182 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40767
2023-11-29 06:43:01,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44548
2023-11-29 06:43:01,182 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:43:01,182 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:43:01,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:43:01,185 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:43:01,185 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:43:01,189 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:43:01,189 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:01,190 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:43:06,263 - distributed.scheduler - INFO - Remove client Client-8650c6bf-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:43:06,263 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47236; closing.
2023-11-29 06:43:06,263 - distributed.scheduler - INFO - Remove client Client-8650c6bf-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:43:06,264 - distributed.scheduler - INFO - Close client connection: Client-8650c6bf-8e82-11ee-b66a-d8c49764f6bb
2023-11-29 06:43:06,271 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44472; closing.
2023-11-29 06:43:06,271 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34239', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240186.2718441')
2023-11-29 06:43:06,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:43:06,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:43:06,368 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:43:06,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:43:06,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:43:06,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:43:06,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:43:06,369 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-29 06:43:06,382 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:43:06,382 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:43:06,382 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:43:06,382 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:43:06,382 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:43:06,382 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:43:06,383 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:43:06,383 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:43:06,387 - distributed.scheduler - INFO - Remove client Client-861d2a1a-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:43:06,387 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47280; closing.
2023-11-29 06:43:06,387 - distributed.scheduler - INFO - Remove client Client-861d2a1a-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:43:06,388 - distributed.scheduler - INFO - Close client connection: Client-861d2a1a-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:43:06,389 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44387'. Reason: nanny-close
2023-11-29 06:43:06,389 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:43:06,390 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44417'. Reason: nanny-close
2023-11-29 06:43:06,390 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:43:06,390 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37111. Reason: nanny-close
2023-11-29 06:43:06,391 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46185'. Reason: nanny-close
2023-11-29 06:43:06,391 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:43:06,391 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46633. Reason: nanny-close
2023-11-29 06:43:06,391 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36091'. Reason: nanny-close
2023-11-29 06:43:06,392 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:43:06,392 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40767. Reason: nanny-close
2023-11-29 06:43:06,392 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40059'. Reason: nanny-close
2023-11-29 06:43:06,392 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:43:06,392 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44550; closing.
2023-11-29 06:43:06,392 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:43:06,392 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40737. Reason: nanny-close
2023-11-29 06:43:06,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38363'. Reason: nanny-close
2023-11-29 06:43:06,393 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37111', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240186.3930757')
2023-11-29 06:43:06,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:43:06,393 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35955. Reason: nanny-close
2023-11-29 06:43:06,393 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:43:06,393 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36245'. Reason: nanny-close
2023-11-29 06:43:06,393 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:43:06,393 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37363. Reason: nanny-close
2023-11-29 06:43:06,394 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42267'. Reason: nanny-close
2023-11-29 06:43:06,394 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:43:06,394 - distributed.nanny - INFO - Worker closed
2023-11-29 06:43:06,394 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:43:06,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40629. Reason: nanny-close
2023-11-29 06:43:06,394 - distributed.nanny - INFO - Worker closed
2023-11-29 06:43:06,394 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44506; closing.
2023-11-29 06:43:06,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:43:06,395 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43989. Reason: nanny-close
2023-11-29 06:43:06,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:43:06,395 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44548; closing.
2023-11-29 06:43:06,395 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46633', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240186.3958478')
2023-11-29 06:43:06,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:43:06,396 - distributed.nanny - INFO - Worker closed
2023-11-29 06:43:06,396 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40767', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240186.396808')
2023-11-29 06:43:06,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:43:06,397 - distributed.nanny - INFO - Worker closed
2023-11-29 06:43:06,397 - distributed.nanny - INFO - Worker closed
2023-11-29 06:43:06,397 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44532; closing.
2023-11-29 06:43:06,397 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:43:06,397 - distributed.nanny - INFO - Worker closed
2023-11-29 06:43:06,398 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40737', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240186.398224')
2023-11-29 06:43:06,398 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44516; closing.
2023-11-29 06:43:06,398 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44488; closing.
2023-11-29 06:43:06,398 - distributed.nanny - INFO - Worker closed
2023-11-29 06:43:06,399 - distributed.nanny - INFO - Worker closed
2023-11-29 06:43:06,399 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35955', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240186.3994553')
2023-11-29 06:43:06,399 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37363', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240186.399847')
2023-11-29 06:43:06,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44504; closing.
2023-11-29 06:43:06,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44484; closing.
2023-11-29 06:43:06,400 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40629', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240186.4008558')
2023-11-29 06:43:06,401 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43989', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240186.4013672')
2023-11-29 06:43:06,401 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:43:07,856 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:43:07,856 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:43:07,857 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:43:07,858 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-29 06:43:07,858 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-11-29 06:43:09,946 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:43:09,950 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44083 instead
  warnings.warn(
2023-11-29 06:43:09,954 - distributed.scheduler - INFO - State start
2023-11-29 06:43:09,974 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:43:09,975 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2023-11-29 06:43:09,975 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:43:09,976 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 616, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 3951, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 810, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 573, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 624, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2023-11-29 06:43:10,158 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43467'
2023-11-29 06:43:11,623 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:43:11,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:43:11,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:43:12,659 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46727
2023-11-29 06:43:12,659 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46727
2023-11-29 06:43:12,659 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33137
2023-11-29 06:43:12,659 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:43:12,660 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:12,660 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:43:12,660 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-29 06:43:12,660 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fjnt4uvv
2023-11-29 06:43:12,660 - distributed.worker - INFO - Starting Worker plugin PreImport-c005d36c-b3cc-47ee-a456-3436417eaadf
2023-11-29 06:43:12,660 - distributed.worker - INFO - Starting Worker plugin RMMSetup-319e9a59-8ae5-4d0e-b180-889422b9ed66
2023-11-29 06:43:12,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fd6dc2c-6b4e-409d-9247-39849f7d4c20
2023-11-29 06:43:12,758 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:12,784 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:43:12,784 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:43:12,784 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:12,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:43:12,802 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:43:12,812 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43467'. Reason: nanny-close
2023-11-29 06:43:12,813 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:43:12,814 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46727. Reason: nanny-close
2023-11-29 06:43:12,815 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:43:12,817 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-11-29 06:43:15,609 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:43:15,613 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38029 instead
  warnings.warn(
2023-11-29 06:43:15,616 - distributed.scheduler - INFO - State start
2023-11-29 06:43:15,648 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-29 06:43:15,649 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-29 06:43:15,650 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38029/status
2023-11-29 06:43:15,650 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-29 06:43:15,782 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35117'
2023-11-29 06:43:15,886 - distributed.scheduler - INFO - Receive client connection: Client-91e3324e-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:43:15,901 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58878
2023-11-29 06:43:17,346 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-29 06:43:17,346 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-29 06:43:17,351 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-29 06:43:18,307 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39919
2023-11-29 06:43:18,308 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39919
2023-11-29 06:43:18,308 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33595
2023-11-29 06:43:18,308 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-29 06:43:18,308 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:18,308 - distributed.worker - INFO -               Threads:                          1
2023-11-29 06:43:18,308 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-29 06:43:18,308 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u819ufk5
2023-11-29 06:43:18,309 - distributed.worker - INFO - Starting Worker plugin PreImport-db7a6b22-016d-4df6-bc02-53b0c33fa5cf
2023-11-29 06:43:18,309 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c3d95d5-8dd2-49ec-8187-1936f4efb22f
2023-11-29 06:43:18,414 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d8915c5e-4569-48ef-9860-ec9f0586a0fe
2023-11-29 06:43:18,414 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:18,443 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39919', status: init, memory: 0, processing: 0>
2023-11-29 06:43:18,444 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39919
2023-11-29 06:43:18,444 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58892
2023-11-29 06:43:18,446 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-29 06:43:18,446 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-29 06:43:18,446 - distributed.worker - INFO - -------------------------------------------------
2023-11-29 06:43:18,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-29 06:43:18,450 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-11-29 06:43:18,454 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-29 06:43:18,457 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:43:18,458 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-29 06:43:18,461 - distributed.scheduler - INFO - Remove client Client-91e3324e-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:43:18,461 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58878; closing.
2023-11-29 06:43:18,461 - distributed.scheduler - INFO - Remove client Client-91e3324e-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:43:18,461 - distributed.scheduler - INFO - Close client connection: Client-91e3324e-8e82-11ee-b4a5-d8c49764f6bb
2023-11-29 06:43:18,463 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35117'. Reason: nanny-close
2023-11-29 06:43:18,463 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-29 06:43:18,465 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39919. Reason: nanny-close
2023-11-29 06:43:18,466 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-29 06:43:18,466 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58892; closing.
2023-11-29 06:43:18,467 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39919', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1701240198.4669306')
2023-11-29 06:43:18,467 - distributed.scheduler - INFO - Lost all workers
2023-11-29 06:43:18,468 - distributed.nanny - INFO - Worker closed
2023-11-29 06:43:19,529 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-29 06:43:19,529 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-29 06:43:19,529 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-29 06:43:19,530 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-29 06:43:19,531 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39697 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33449 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34303 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44899 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41343 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44757 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38661 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38087 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42779 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44263 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44533 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33989 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37049 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42245 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46071 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46495 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36307 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36875 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37827 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43405 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42843 instead
  warnings.warn(
2023-11-29 06:49:12,888 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-c1495e9d-4fdc-43a4-bd33-44622fa7c800
Function:  _run_coroutine_on_worker
args:      (276238617251793947591216169215727325401, <function shuffle_task at 0x7f9ef908b280>, ('explicit-comms-shuffle-958d4c306596076fb29ab08d1fbd55c1', {0: {('explicit-comms-shuffle-660b1370f6c47f932dbc02bf54c4ffc7', 0)}, 1: set(), 2: set()}, {0: {0}, 1: set(), 2: set()}, ['_partitions'], 1, False, 1, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:93: 2 cudaErrorMemoryAllocation out of memory')"

/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
