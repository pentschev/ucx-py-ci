============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-8.0.0, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.5
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-02-10 06:50:35,541 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:50:35,544 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:50:35,547 - distributed.scheduler - INFO - State start
2024-02-10 06:50:35,568 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:50:35,569 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-10 06:50:35,569 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:50:35,569 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:50:35,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41767'
2024-02-10 06:50:35,637 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37017'
2024-02-10 06:50:35,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34149'
2024-02-10 06:50:35,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39775'
2024-02-10 06:50:37,021 - distributed.scheduler - INFO - Receive client connection: Client-b056729b-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:37,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37852
2024-02-10 06:50:37,137 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:37,137 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:37,141 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:37,141 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38351
2024-02-10 06:50:37,142 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38351
2024-02-10 06:50:37,142 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46651
2024-02-10 06:50:37,142 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-10 06:50:37,142 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,142 - distributed.worker - INFO -               Threads:                          4
2024-02-10 06:50:37,142 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-10 06:50:37,142 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-patajqgi
2024-02-10 06:50:37,142 - distributed.worker - INFO - Starting Worker plugin PreImport-7bdc1356-304d-41e7-b9fb-bddf190eb54a
2024-02-10 06:50:37,142 - distributed.worker - INFO - Starting Worker plugin RMMSetup-82c37307-1bbc-4af6-836d-98a1c9db7ac9
2024-02-10 06:50:37,142 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d31b17b-71c8-435e-8347-1f2e911e946b
2024-02-10 06:50:37,142 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,171 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:37,171 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:37,172 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:37,172 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:37,175 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:37,175 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:37,176 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42127
2024-02-10 06:50:37,176 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39623
2024-02-10 06:50:37,176 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39623
2024-02-10 06:50:37,176 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42127
2024-02-10 06:50:37,176 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44791
2024-02-10 06:50:37,176 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44623
2024-02-10 06:50:37,176 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-10 06:50:37,176 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-10 06:50:37,176 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,176 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,176 - distributed.worker - INFO -               Threads:                          4
2024-02-10 06:50:37,176 - distributed.worker - INFO -               Threads:                          4
2024-02-10 06:50:37,176 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-10 06:50:37,176 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-10 06:50:37,176 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-723qnioy
2024-02-10 06:50:37,176 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-2hir_d49
2024-02-10 06:50:37,176 - distributed.worker - INFO - Starting Worker plugin PreImport-d29ed505-f9ca-4a48-aec4-033778b0d3f8
2024-02-10 06:50:37,176 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bcf89155-9fab-4362-ab21-6f56b573b4aa
2024-02-10 06:50:37,176 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86d1bc22-4081-4947-822f-41fcdce01007
2024-02-10 06:50:37,176 - distributed.worker - INFO - Starting Worker plugin PreImport-66cbc72f-8d7b-4bee-9dbd-e469a0bdc0cd
2024-02-10 06:50:37,176 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce6fdb33-71ec-4903-8804-381736e5ec71
2024-02-10 06:50:37,176 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,176 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e25ce22f-222b-42c9-b54d-7af6c8db11db
2024-02-10 06:50:37,177 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:37,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:37,188 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:37,189 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42353
2024-02-10 06:50:37,189 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42353
2024-02-10 06:50:37,189 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35619
2024-02-10 06:50:37,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-10 06:50:37,189 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,189 - distributed.worker - INFO -               Threads:                          4
2024-02-10 06:50:37,189 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-10 06:50:37,189 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-geh3ypes
2024-02-10 06:50:37,190 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-71539c0a-b2e3-4067-80be-dcf4cdbd8399
2024-02-10 06:50:37,190 - distributed.worker - INFO - Starting Worker plugin PreImport-176bb051-7616-441b-a53b-5a574ed8cb8b
2024-02-10 06:50:37,190 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a3029c5f-88c7-4ac7-9a40-7d734b370f10
2024-02-10 06:50:37,190 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,192 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38351', status: init, memory: 0, processing: 0>
2024-02-10 06:50:37,193 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38351
2024-02-10 06:50:37,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37882
2024-02-10 06:50:37,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:37,195 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-10 06:50:37,195 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-10 06:50:37,254 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39623', status: init, memory: 0, processing: 0>
2024-02-10 06:50:37,254 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39623
2024-02-10 06:50:37,255 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37892
2024-02-10 06:50:37,255 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:37,256 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-10 06:50:37,256 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-10 06:50:37,263 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42127', status: init, memory: 0, processing: 0>
2024-02-10 06:50:37,264 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42127
2024-02-10 06:50:37,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37896
2024-02-10 06:50:37,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:37,266 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-10 06:50:37,266 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,267 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-10 06:50:37,274 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42353', status: init, memory: 0, processing: 0>
2024-02-10 06:50:37,274 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42353
2024-02-10 06:50:37,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37910
2024-02-10 06:50:37,275 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:37,276 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-10 06:50:37,276 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:37,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-10 06:50:37,348 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-10 06:50:37,348 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-10 06:50:37,349 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-10 06:50:37,349 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-10 06:50:37,353 - distributed.scheduler - INFO - Remove client Client-b056729b-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:37,353 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37852; closing.
2024-02-10 06:50:37,354 - distributed.scheduler - INFO - Remove client Client-b056729b-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:37,354 - distributed.scheduler - INFO - Close client connection: Client-b056729b-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:37,355 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41767'. Reason: nanny-close
2024-02-10 06:50:37,355 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:37,356 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37017'. Reason: nanny-close
2024-02-10 06:50:37,356 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:37,356 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34149'. Reason: nanny-close
2024-02-10 06:50:37,356 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42353. Reason: nanny-close
2024-02-10 06:50:37,356 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:37,357 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39775'. Reason: nanny-close
2024-02-10 06:50:37,357 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38351. Reason: nanny-close
2024-02-10 06:50:37,357 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:37,357 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39623. Reason: nanny-close
2024-02-10 06:50:37,358 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42127. Reason: nanny-close
2024-02-10 06:50:37,358 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-10 06:50:37,358 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37910; closing.
2024-02-10 06:50:37,359 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-10 06:50:37,359 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37892; closing.
2024-02-10 06:50:37,359 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-10 06:50:37,359 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42353', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547837.3595474')
2024-02-10 06:50:37,359 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:37,360 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39623', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547837.3601918')
2024-02-10 06:50:37,360 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-10 06:50:37,360 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:37,360 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:37,361 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37896; closing.
2024-02-10 06:50:37,361 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37882; closing.
2024-02-10 06:50:37,361 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:37,362 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42127', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547837.3619606')
2024-02-10 06:50:37,362 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38351', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547837.362355')
2024-02-10 06:50:37,362 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:50:37,870 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:50:37,870 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:50:37,871 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:50:37,872 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-10 06:50:37,873 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-02-10 06:50:39,683 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:50:39,688 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:50:39,691 - distributed.scheduler - INFO - State start
2024-02-10 06:50:39,710 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:50:39,711 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:50:39,712 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:50:39,712 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:50:39,772 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45349'
2024-02-10 06:50:39,784 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45479'
2024-02-10 06:50:39,798 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39999'
2024-02-10 06:50:39,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43047'
2024-02-10 06:50:39,808 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44655'
2024-02-10 06:50:39,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46727'
2024-02-10 06:50:39,824 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45597'
2024-02-10 06:50:39,832 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38175'
2024-02-10 06:50:40,048 - distributed.scheduler - INFO - Receive client connection: Client-b2cf446e-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:40,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39018
2024-02-10 06:50:41,397 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:41,397 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:41,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:41,402 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36127
2024-02-10 06:50:41,402 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36127
2024-02-10 06:50:41,402 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34563
2024-02-10 06:50:41,402 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:41,402 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:41,402 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:41,402 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:41,402 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cwzzzkl3
2024-02-10 06:50:41,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc29c03e-4fc7-4df9-a8b5-7c77f7f33161
2024-02-10 06:50:41,636 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:41,636 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:41,637 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:41,638 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:41,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:41,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:41,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:41,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:41,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:41,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:41,640 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:41,640 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:41,640 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:41,641 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:41,641 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:41,641 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36079
2024-02-10 06:50:41,641 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36079
2024-02-10 06:50:41,641 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45705
2024-02-10 06:50:41,641 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:41,641 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:41,641 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:41,641 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:41,641 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iehaai_p
2024-02-10 06:50:41,642 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fbe2761d-25e9-4441-9d11-6afce625283d
2024-02-10 06:50:41,642 - distributed.worker - INFO - Starting Worker plugin PreImport-76f4a310-ea39-430b-b09c-cfe4432a5978
2024-02-10 06:50:41,642 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1dd20905-f331-4e6c-9f32-8c24e3c0fc53
2024-02-10 06:50:41,643 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:41,643 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:41,644 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:41,644 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36057
2024-02-10 06:50:41,644 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:41,644 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36057
2024-02-10 06:50:41,644 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39501
2024-02-10 06:50:41,644 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:41,644 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:41,644 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:41,644 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:41,644 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wdhlj87t
2024-02-10 06:50:41,644 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38817
2024-02-10 06:50:41,644 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-913cb428-f8d5-43d7-8bbe-a7db03b05c73
2024-02-10 06:50:41,644 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38817
2024-02-10 06:50:41,644 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44189
2024-02-10 06:50:41,644 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32823
2024-02-10 06:50:41,644 - distributed.worker - INFO - Starting Worker plugin PreImport-08672baa-5f4e-477a-9c99-c9b5a9907bc1
2024-02-10 06:50:41,644 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44189
2024-02-10 06:50:41,644 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:41,644 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37953
2024-02-10 06:50:41,644 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43123
2024-02-10 06:50:41,644 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:41,644 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae306563-73c3-4f86-965f-16e7369dc964
2024-02-10 06:50:41,645 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37953
2024-02-10 06:50:41,645 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:41,645 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:41,645 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33793
2024-02-10 06:50:41,645 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:41,645 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:41,645 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:41,645 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:41,645 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6o711c3e
2024-02-10 06:50:41,645 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:41,645 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:41,645 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:41,645 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8m0rz6r2
2024-02-10 06:50:41,645 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:41,645 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-osrllmtu
2024-02-10 06:50:41,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1fd5f6c-ea1c-47a1-b413-9275690d4eac
2024-02-10 06:50:41,645 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-541c2a4c-a42a-4055-babf-d5bb669efcd9
2024-02-10 06:50:41,645 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:41,645 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a2b040f2-b229-441b-9278-bb1f0a8eb406
2024-02-10 06:50:41,645 - distributed.worker - INFO - Starting Worker plugin PreImport-6dcc4cc6-4af8-4def-8a2f-0c08f38f7e63
2024-02-10 06:50:41,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c90f5bae-4a09-4981-9dbe-84c3cf52326e
2024-02-10 06:50:41,645 - distributed.worker - INFO - Starting Worker plugin PreImport-88afab1e-e0b0-4851-8f53-cfe627d50360
2024-02-10 06:50:41,645 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0f51011e-12b0-4f6d-8ace-d60f20ecd326
2024-02-10 06:50:41,645 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:41,646 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39655
2024-02-10 06:50:41,646 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39655
2024-02-10 06:50:41,646 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36125
2024-02-10 06:50:41,646 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:41,646 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:41,646 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:41,646 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:41,646 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nwg3wf6l
2024-02-10 06:50:41,646 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5e2fb30c-9eac-4b80-8a49-2fa0dbc26283
2024-02-10 06:50:41,646 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38873
2024-02-10 06:50:41,647 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38873
2024-02-10 06:50:41,647 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39645
2024-02-10 06:50:41,647 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:41,647 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:41,647 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:41,647 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:41,647 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wj74mvrv
2024-02-10 06:50:41,647 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57e80b5a-0354-4975-b646-c5417d8c0e3a
2024-02-10 06:50:41,878 - distributed.worker - INFO - Starting Worker plugin PreImport-137859bc-c1f3-42a8-9c15-48158c3e6ccf
2024-02-10 06:50:41,879 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-037be558-e24b-40e6-ac04-5409b77515a1
2024-02-10 06:50:41,880 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:41,909 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36127', status: init, memory: 0, processing: 0>
2024-02-10 06:50:41,910 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36127
2024-02-10 06:50:41,910 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39036
2024-02-10 06:50:41,911 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:41,912 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:41,912 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:41,914 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:43,327 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,335 - distributed.worker - INFO - Starting Worker plugin PreImport-60be6d0b-7a3b-468b-ad27-e36b74c32407
2024-02-10 06:50:43,336 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a0009700-e7a3-4dcf-a619-6de18aff24af
2024-02-10 06:50:43,337 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,347 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,350 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36057', status: init, memory: 0, processing: 0>
2024-02-10 06:50:43,351 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36057
2024-02-10 06:50:43,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39066
2024-02-10 06:50:43,351 - distributed.worker - INFO - Starting Worker plugin PreImport-e772d4f6-1e69-459c-b30f-f289b565f636
2024-02-10 06:50:43,352 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-21bec5dd-f4dc-463f-bb6c-b243b4a65715
2024-02-10 06:50:43,352 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:43,353 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,353 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:43,353 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:43,355 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,358 - distributed.worker - INFO - Starting Worker plugin PreImport-4db5d73d-6f08-4544-9dd9-62ffe3d7c986
2024-02-10 06:50:43,359 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9954bb7d-f70b-4718-bdf4-a939753b249d
2024-02-10 06:50:43,359 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,362 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,363 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38817', status: init, memory: 0, processing: 0>
2024-02-10 06:50:43,363 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38817
2024-02-10 06:50:43,363 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39082
2024-02-10 06:50:43,364 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:43,365 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:43,365 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,366 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:43,380 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36079', status: init, memory: 0, processing: 0>
2024-02-10 06:50:43,381 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36079
2024-02-10 06:50:43,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39084
2024-02-10 06:50:43,382 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:43,383 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39655', status: init, memory: 0, processing: 0>
2024-02-10 06:50:43,383 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:43,383 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,383 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39655
2024-02-10 06:50:43,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39086
2024-02-10 06:50:43,384 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38873', status: init, memory: 0, processing: 0>
2024-02-10 06:50:43,385 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38873
2024-02-10 06:50:43,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39096
2024-02-10 06:50:43,385 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:43,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:43,386 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:43,386 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:43,386 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,386 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:43,387 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,387 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44189', status: init, memory: 0, processing: 0>
2024-02-10 06:50:43,388 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:43,388 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44189
2024-02-10 06:50:43,388 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39098
2024-02-10 06:50:43,388 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:43,388 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37953', status: init, memory: 0, processing: 0>
2024-02-10 06:50:43,389 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:43,389 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37953
2024-02-10 06:50:43,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39090
2024-02-10 06:50:43,390 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:43,390 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,390 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:43,391 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:43,391 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:43,391 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:43,393 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:43,464 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:43,464 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:43,464 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:43,465 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:43,465 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:43,465 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:43,465 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:43,465 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:43,469 - distributed.scheduler - INFO - Remove client Client-b2cf446e-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:43,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39018; closing.
2024-02-10 06:50:43,469 - distributed.scheduler - INFO - Remove client Client-b2cf446e-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:43,470 - distributed.scheduler - INFO - Close client connection: Client-b2cf446e-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:43,471 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45349'. Reason: nanny-close
2024-02-10 06:50:43,471 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:43,471 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45479'. Reason: nanny-close
2024-02-10 06:50:43,472 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:43,472 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39999'. Reason: nanny-close
2024-02-10 06:50:43,472 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36127. Reason: nanny-close
2024-02-10 06:50:43,472 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:43,473 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43047'. Reason: nanny-close
2024-02-10 06:50:43,473 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36079. Reason: nanny-close
2024-02-10 06:50:43,473 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:43,473 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44655'. Reason: nanny-close
2024-02-10 06:50:43,473 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36057. Reason: nanny-close
2024-02-10 06:50:43,474 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:43,474 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46727'. Reason: nanny-close
2024-02-10 06:50:43,474 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38873. Reason: nanny-close
2024-02-10 06:50:43,474 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:43,474 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45597'. Reason: nanny-close
2024-02-10 06:50:43,474 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:43,474 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39655. Reason: nanny-close
2024-02-10 06:50:43,474 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:43,475 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39036; closing.
2024-02-10 06:50:43,475 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38175'. Reason: nanny-close
2024-02-10 06:50:43,475 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37953. Reason: nanny-close
2024-02-10 06:50:43,475 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36127', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547843.4752822')
2024-02-10 06:50:43,475 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:43,475 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:43,475 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38817. Reason: nanny-close
2024-02-10 06:50:43,475 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:43,476 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:43,476 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44189. Reason: nanny-close
2024-02-10 06:50:43,476 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:43,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39096; closing.
2024-02-10 06:50:43,477 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:43,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39084; closing.
2024-02-10 06:50:43,477 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:43,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39066; closing.
2024-02-10 06:50:43,477 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:43,477 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:43,478 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:43,478 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38873', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547843.4782944')
2024-02-10 06:50:43,478 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:43,478 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36079', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547843.4787486')
2024-02-10 06:50:43,478 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:43,479 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36057', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547843.479122')
2024-02-10 06:50:43,479 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:43,480 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:43,480 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:43,480 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39086; closing.
2024-02-10 06:50:43,480 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39090; closing.
2024-02-10 06:50:43,480 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:43,481 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39655', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547843.4810612')
2024-02-10 06:50:43,481 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37953', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547843.4813583')
2024-02-10 06:50:43,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39082; closing.
2024-02-10 06:50:43,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39098; closing.
2024-02-10 06:50:43,482 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38817', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547843.4822488')
2024-02-10 06:50:43,482 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44189', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547843.4826853')
2024-02-10 06:50:43,482 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:50:44,236 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:50:44,236 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:50:44,237 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:50:44,237 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:50:44,238 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-02-10 06:50:46,064 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:50:46,067 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:50:46,071 - distributed.scheduler - INFO - State start
2024-02-10 06:50:46,090 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:50:46,091 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:50:46,092 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:50:46,092 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:50:46,120 - distributed.scheduler - INFO - Receive client connection: Client-b69bd45d-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:46,133 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39178
2024-02-10 06:50:46,219 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45601'
2024-02-10 06:50:46,231 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32907'
2024-02-10 06:50:46,240 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46251'
2024-02-10 06:50:46,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32837'
2024-02-10 06:50:46,254 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38713'
2024-02-10 06:50:46,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42877'
2024-02-10 06:50:46,270 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37043'
2024-02-10 06:50:46,279 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33127'
2024-02-10 06:50:47,850 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:47,850 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:47,854 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:47,854 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43301
2024-02-10 06:50:47,854 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43301
2024-02-10 06:50:47,854 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37563
2024-02-10 06:50:47,854 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:47,854 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:47,855 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:47,855 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:47,855 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ar7_9fa5
2024-02-10 06:50:47,855 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e139c0be-ef71-445c-bd14-bd068796557f
2024-02-10 06:50:47,855 - distributed.worker - INFO - Starting Worker plugin PreImport-85b0ed21-0b8b-4a57-8405-5b7325f6b011
2024-02-10 06:50:47,855 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3613dd39-29e5-4fc9-b7ca-17e13600ac57
2024-02-10 06:50:47,879 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:47,880 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:47,883 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:47,884 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36385
2024-02-10 06:50:47,884 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36385
2024-02-10 06:50:47,884 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44185
2024-02-10 06:50:47,884 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:47,884 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:47,884 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:47,884 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:47,884 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9jbofo1v
2024-02-10 06:50:47,884 - distributed.worker - INFO - Starting Worker plugin PreImport-9d1327bd-b603-47ab-b84c-3b3891a54c29
2024-02-10 06:50:47,884 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79ffc7b6-6c95-4767-843b-aa085c441d60
2024-02-10 06:50:48,100 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:48,100 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:48,101 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:48,101 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:48,104 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:48,105 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:48,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:48,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:48,105 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46121
2024-02-10 06:50:48,105 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46121
2024-02-10 06:50:48,105 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42807
2024-02-10 06:50:48,106 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:48,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:48,106 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:48,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:48,106 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43923
2024-02-10 06:50:48,106 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:48,106 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43923
2024-02-10 06:50:48,106 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:48,106 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44575
2024-02-10 06:50:48,106 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jmm2wmwp
2024-02-10 06:50:48,106 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:48,106 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:48,106 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:48,106 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:48,106 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7515f1ae-6e20-46b3-b8a7-18df9da51a9a
2024-02-10 06:50:48,106 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iutvbssd
2024-02-10 06:50:48,106 - distributed.worker - INFO - Starting Worker plugin PreImport-97be8f1c-5e7a-4438-be14-b6f6e298ca7e
2024-02-10 06:50:48,106 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de80b3f7-7383-4df7-b3ce-ada01b3ac2b8
2024-02-10 06:50:48,106 - distributed.worker - INFO - Starting Worker plugin RMMSetup-63fe57d1-cc21-41a8-8b3e-6118abbdbdea
2024-02-10 06:50:48,107 - distributed.worker - INFO - Starting Worker plugin PreImport-cf0c9d46-373b-443d-9dfd-99a43942e849
2024-02-10 06:50:48,108 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9b2dde4-bdbf-4d83-b2da-3f9184c0c1da
2024-02-10 06:50:48,108 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:48,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:48,110 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:48,110 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:48,110 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:48,110 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39103
2024-02-10 06:50:48,110 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:48,110 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39103
2024-02-10 06:50:48,111 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43741
2024-02-10 06:50:48,111 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:48,111 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44515
2024-02-10 06:50:48,111 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:48,111 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:48,111 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44515
2024-02-10 06:50:48,111 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:48,111 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37631
2024-02-10 06:50:48,111 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fpay7ilt
2024-02-10 06:50:48,111 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:48,111 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:48,111 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:48,111 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:48,111 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b3abf83-bf5b-416e-82e4-420794415810
2024-02-10 06:50:48,111 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tyjn8ezj
2024-02-10 06:50:48,111 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f5db35b-a31e-47c1-a443-ecc8333a0215
2024-02-10 06:50:48,112 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:48,113 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35619
2024-02-10 06:50:48,113 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35619
2024-02-10 06:50:48,113 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41697
2024-02-10 06:50:48,113 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:48,113 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:48,113 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:48,113 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:48,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o_1k5vuv
2024-02-10 06:50:48,114 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-316f92d7-ac15-45b2-9be9-b3d53e524a62
2024-02-10 06:50:48,114 - distributed.worker - INFO - Starting Worker plugin PreImport-152ecc0c-86a2-42af-988c-94f5264de422
2024-02-10 06:50:48,114 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c767f918-63f0-41e6-b097-08d6e8229d55
2024-02-10 06:50:48,115 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:48,116 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37267
2024-02-10 06:50:48,116 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37267
2024-02-10 06:50:48,116 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40129
2024-02-10 06:50:48,116 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:48,116 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:48,116 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:48,116 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:48,116 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-brqx8d72
2024-02-10 06:50:48,116 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e28ac3aa-d4a1-4576-b865-458f5297c789
2024-02-10 06:50:48,117 - distributed.worker - INFO - Starting Worker plugin PreImport-429b29db-b6da-474b-83f2-1510e3105d8a
2024-02-10 06:50:48,118 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cfc73c03-2b49-401f-81c1-7d85b9c13f11
2024-02-10 06:50:48,427 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:48,452 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43301', status: init, memory: 0, processing: 0>
2024-02-10 06:50:48,453 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43301
2024-02-10 06:50:48,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39258
2024-02-10 06:50:48,454 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:48,456 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:48,456 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:48,457 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:49,596 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ee11a0f-97e4-49e5-a15e-5a4aa15c14e1
2024-02-10 06:50:49,598 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,624 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36385', status: init, memory: 0, processing: 0>
2024-02-10 06:50:49,624 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36385
2024-02-10 06:50:49,625 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39274
2024-02-10 06:50:49,626 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:49,627 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:49,627 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,629 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:49,727 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,728 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,745 - distributed.worker - INFO - Starting Worker plugin PreImport-4923bf0a-a594-4586-bf9d-34fefa1397f1
2024-02-10 06:50:49,745 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96c06530-f556-4532-93f5-3f2ff5759d48
2024-02-10 06:50:49,745 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,751 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46121', status: init, memory: 0, processing: 0>
2024-02-10 06:50:49,751 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46121
2024-02-10 06:50:49,751 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39278
2024-02-10 06:50:49,752 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43923', status: init, memory: 0, processing: 0>
2024-02-10 06:50:49,752 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:49,752 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43923
2024-02-10 06:50:49,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39280
2024-02-10 06:50:49,753 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:49,753 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,754 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:49,755 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:49,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:49,755 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,756 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:49,769 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44515', status: init, memory: 0, processing: 0>
2024-02-10 06:50:49,769 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44515
2024-02-10 06:50:49,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39290
2024-02-10 06:50:49,770 - distributed.worker - INFO - Starting Worker plugin PreImport-fbca4207-8e23-4216-bf71-343e77ad26ba
2024-02-10 06:50:49,770 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:49,771 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb4f1607-a588-457b-a580-a522fad21730
2024-02-10 06:50:49,771 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:49,771 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,772 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:49,779 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,791 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,804 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39103', status: init, memory: 0, processing: 0>
2024-02-10 06:50:49,805 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39103
2024-02-10 06:50:49,805 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39296
2024-02-10 06:50:49,806 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:49,807 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:49,808 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,808 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35619', status: init, memory: 0, processing: 0>
2024-02-10 06:50:49,809 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35619
2024-02-10 06:50:49,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39300
2024-02-10 06:50:49,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:49,810 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:49,811 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:49,812 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:49,822 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37267', status: init, memory: 0, processing: 0>
2024-02-10 06:50:49,823 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37267
2024-02-10 06:50:49,823 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39306
2024-02-10 06:50:49,824 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:49,825 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:49,825 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:49,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:49,895 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:49,895 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:49,896 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:49,896 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:49,896 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:49,896 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:49,896 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:49,896 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:49,900 - distributed.scheduler - INFO - Remove client Client-b69bd45d-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:49,901 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39178; closing.
2024-02-10 06:50:49,901 - distributed.scheduler - INFO - Remove client Client-b69bd45d-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:49,901 - distributed.scheduler - INFO - Close client connection: Client-b69bd45d-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:49,902 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45601'. Reason: nanny-close
2024-02-10 06:50:49,902 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:49,903 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32907'. Reason: nanny-close
2024-02-10 06:50:49,903 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:49,903 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46251'. Reason: nanny-close
2024-02-10 06:50:49,904 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36385. Reason: nanny-close
2024-02-10 06:50:49,904 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:49,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32837'. Reason: nanny-close
2024-02-10 06:50:49,904 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37267. Reason: nanny-close
2024-02-10 06:50:49,904 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:49,904 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38713'. Reason: nanny-close
2024-02-10 06:50:49,905 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43923. Reason: nanny-close
2024-02-10 06:50:49,905 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:49,905 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46121. Reason: nanny-close
2024-02-10 06:50:49,905 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42877'. Reason: nanny-close
2024-02-10 06:50:49,905 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:49,906 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39274; closing.
2024-02-10 06:50:49,906 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:49,906 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37043'. Reason: nanny-close
2024-02-10 06:50:49,906 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35619. Reason: nanny-close
2024-02-10 06:50:49,906 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36385', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547849.9065697')
2024-02-10 06:50:49,906 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:49,906 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:49,907 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39103. Reason: nanny-close
2024-02-10 06:50:49,907 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33127'. Reason: nanny-close
2024-02-10 06:50:49,907 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:49,907 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:49,907 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43301. Reason: nanny-close
2024-02-10 06:50:49,908 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:49,908 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:49,908 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44515. Reason: nanny-close
2024-02-10 06:50:49,908 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:49,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39278; closing.
2024-02-10 06:50:49,908 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39306; closing.
2024-02-10 06:50:49,909 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:49,909 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:49,909 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46121', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547849.9097342')
2024-02-10 06:50:49,909 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:49,910 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:49,910 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37267', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547849.9101372')
2024-02-10 06:50:49,910 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39280; closing.
2024-02-10 06:50:49,910 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:49,911 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39300; closing.
2024-02-10 06:50:49,911 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:49,911 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43923', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547849.9114063')
2024-02-10 06:50:49,911 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:49,912 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:49,912 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:49,912 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35619', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547849.9126163')
2024-02-10 06:50:49,913 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39296; closing.
2024-02-10 06:50:49,913 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39290; closing.
2024-02-10 06:50:49,913 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39258; closing.
2024-02-10 06:50:49,913 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:49,913 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39103', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547849.9139247')
2024-02-10 06:50:49,914 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44515', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547849.9142952')
2024-02-10 06:50:49,914 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43301', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547849.9146256')
2024-02-10 06:50:49,914 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:50:50,717 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:50:50,718 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:50:50,718 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:50:50,719 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:50:50,719 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-02-10 06:50:52,563 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:50:52,567 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:50:52,570 - distributed.scheduler - INFO - State start
2024-02-10 06:50:52,590 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:50:52,590 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:50:52,591 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:50:52,591 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:50:52,668 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39015'
2024-02-10 06:50:52,679 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36129'
2024-02-10 06:50:52,689 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42511'
2024-02-10 06:50:52,701 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46387'
2024-02-10 06:50:52,703 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34601'
2024-02-10 06:50:52,711 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42939'
2024-02-10 06:50:52,719 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34427'
2024-02-10 06:50:52,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40617'
2024-02-10 06:50:54,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:54,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:54,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:54,340 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38625
2024-02-10 06:50:54,340 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38625
2024-02-10 06:50:54,340 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36187
2024-02-10 06:50:54,340 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:54,340 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:54,341 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:54,341 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:54,341 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mh2d3j2t
2024-02-10 06:50:54,341 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59aa79ac-51dd-427e-b719-8b38d18cd268
2024-02-10 06:50:54,341 - distributed.worker - INFO - Starting Worker plugin PreImport-3aa8c91f-9ad1-4288-8807-70c9017d382a
2024-02-10 06:50:54,341 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d0bff52-050a-4da0-86c7-2e2f74fca51f
2024-02-10 06:50:54,383 - distributed.scheduler - INFO - Receive client connection: Client-ba78c4ba-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:54,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:54,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:54,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:54,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:54,392 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:54,392 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:54,395 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41206
2024-02-10 06:50:54,395 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:54,396 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:54,396 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:54,396 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37951
2024-02-10 06:50:54,396 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37951
2024-02-10 06:50:54,396 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40025
2024-02-10 06:50:54,396 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37233
2024-02-10 06:50:54,397 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:54,397 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40025
2024-02-10 06:50:54,397 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:54,397 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34719
2024-02-10 06:50:54,397 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:54,397 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:54,397 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:54,397 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:54,397 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:54,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8pwh1fcw
2024-02-10 06:50:54,397 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:54,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9sqct15r
2024-02-10 06:50:54,397 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e42f7495-3550-4cdd-948c-4dfffb1b44b0
2024-02-10 06:50:54,397 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5dc65648-57c8-4375-902e-95b9891445c4
2024-02-10 06:50:54,397 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40865
2024-02-10 06:50:54,397 - distributed.worker - INFO - Starting Worker plugin PreImport-646dd6ac-f574-4fcc-b851-eb3960839eff
2024-02-10 06:50:54,397 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40865
2024-02-10 06:50:54,397 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33805
2024-02-10 06:50:54,397 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5e263f8-f901-4e3b-80db-b47f529181ed
2024-02-10 06:50:54,397 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:54,397 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:54,397 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:54,398 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:54,398 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-splyiith
2024-02-10 06:50:54,398 - distributed.worker - INFO - Starting Worker plugin PreImport-79d0687d-8057-4ae2-b29f-b4fa36e40c2d
2024-02-10 06:50:54,398 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c965f8db-5b89-41d8-a173-958ac59d1a8d
2024-02-10 06:50:54,398 - distributed.worker - INFO - Starting Worker plugin PreImport-6562d6d6-9b32-42b1-a0da-fb22ada5547b
2024-02-10 06:50:54,398 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f082e14d-b1d0-4b48-8201-6cb560a05379
2024-02-10 06:50:54,399 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cb61e2b-b299-448f-92d3-ef5335918ef0
2024-02-10 06:50:54,400 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:54,400 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:54,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:54,405 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42203
2024-02-10 06:50:54,405 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42203
2024-02-10 06:50:54,405 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45127
2024-02-10 06:50:54,405 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:54,405 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:54,405 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:54,405 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:54,405 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q279sqoe
2024-02-10 06:50:54,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42cc935a-d569-463c-a353-e13ffed81eb5
2024-02-10 06:50:54,550 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:54,550 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:54,551 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:54,551 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:54,554 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:54,555 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38875
2024-02-10 06:50:54,555 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38875
2024-02-10 06:50:54,555 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41395
2024-02-10 06:50:54,555 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:54,555 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:54,555 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:54,556 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:54,556 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0ci8p139
2024-02-10 06:50:54,556 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e80abadf-0e66-4a75-8c87-860c3a4f3bbb
2024-02-10 06:50:54,556 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:54,557 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43389
2024-02-10 06:50:54,557 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43389
2024-02-10 06:50:54,557 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45179
2024-02-10 06:50:54,557 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:54,557 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:54,557 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:54,557 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:54,558 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3rtymqst
2024-02-10 06:50:54,558 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eeda44b7-2f0b-404a-ac6d-494a616909e8
2024-02-10 06:50:54,558 - distributed.worker - INFO - Starting Worker plugin PreImport-1b3a8276-7538-4741-a565-de05c6c5b1a2
2024-02-10 06:50:54,558 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73312f0e-5506-4ead-8433-5713007de894
2024-02-10 06:50:54,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:50:54,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:50:54,587 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:50:54,588 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38385
2024-02-10 06:50:54,588 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38385
2024-02-10 06:50:54,588 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32965
2024-02-10 06:50:54,588 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:50:54,588 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:54,588 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:50:54,588 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:50:54,588 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vofg21le
2024-02-10 06:50:54,589 - distributed.worker - INFO - Starting Worker plugin RMMSetup-196f5f2d-0b87-4f86-bf5c-2142b8f50ced
2024-02-10 06:50:54,810 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:54,837 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38625', status: init, memory: 0, processing: 0>
2024-02-10 06:50:54,838 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38625
2024-02-10 06:50:54,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41224
2024-02-10 06:50:54,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:54,840 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:54,840 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:54,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:56,402 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,408 - distributed.worker - INFO - Starting Worker plugin PreImport-fc23e0b5-35de-47aa-8e25-6fb3ada8eea9
2024-02-10 06:50:56,408 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-24004814-c750-4531-9e2c-5d1eeb9839c4
2024-02-10 06:50:56,409 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,426 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40025', status: init, memory: 0, processing: 0>
2024-02-10 06:50:56,426 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40025
2024-02-10 06:50:56,426 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41246
2024-02-10 06:50:56,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:56,428 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:56,428 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,430 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:56,431 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42203', status: init, memory: 0, processing: 0>
2024-02-10 06:50:56,432 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42203
2024-02-10 06:50:56,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41256
2024-02-10 06:50:56,433 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:56,433 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:56,433 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,435 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:56,436 - distributed.worker - INFO - Starting Worker plugin PreImport-2f10bc42-0347-4bb4-a640-0717d1581b12
2024-02-10 06:50:56,437 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d781e53-9235-4fa6-8359-3fd881a7997e
2024-02-10 06:50:56,438 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,438 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,440 - distributed.worker - INFO - Starting Worker plugin PreImport-c1344a77-e3d9-4127-9346-ca03e5b2a1b8
2024-02-10 06:50:56,441 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-338aab59-62cd-4927-b55f-27fbec33f3b2
2024-02-10 06:50:56,442 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,445 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,449 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,462 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38875', status: init, memory: 0, processing: 0>
2024-02-10 06:50:56,463 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38875
2024-02-10 06:50:56,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41292
2024-02-10 06:50:56,464 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:56,465 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:56,465 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,465 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43389', status: init, memory: 0, processing: 0>
2024-02-10 06:50:56,466 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43389
2024-02-10 06:50:56,466 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41272
2024-02-10 06:50:56,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:56,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:56,468 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:56,468 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:56,475 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38385', status: init, memory: 0, processing: 0>
2024-02-10 06:50:56,475 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38385
2024-02-10 06:50:56,476 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41276
2024-02-10 06:50:56,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:56,478 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:56,479 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:56,481 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40865', status: init, memory: 0, processing: 0>
2024-02-10 06:50:56,481 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40865
2024-02-10 06:50:56,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41296
2024-02-10 06:50:56,482 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37951', status: init, memory: 0, processing: 0>
2024-02-10 06:50:56,483 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37951
2024-02-10 06:50:56,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:56,483 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41300
2024-02-10 06:50:56,484 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:56,484 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,484 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:50:56,485 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:50:56,485 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:50:56,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:56,487 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:50:56,562 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:56,562 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:56,562 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:56,562 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:56,563 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:56,563 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:56,563 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:56,563 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:50:56,573 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:50:56,573 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:50:56,573 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:50:56,573 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:50:56,573 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:50:56,573 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:50:56,573 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:50:56,574 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:50:56,581 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:50:56,582 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:50:56,585 - distributed.scheduler - INFO - Remove client Client-ba78c4ba-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:56,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41206; closing.
2024-02-10 06:50:56,585 - distributed.scheduler - INFO - Remove client Client-ba78c4ba-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:56,586 - distributed.scheduler - INFO - Close client connection: Client-ba78c4ba-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:56,586 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39015'. Reason: nanny-close
2024-02-10 06:50:56,587 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:56,587 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36129'. Reason: nanny-close
2024-02-10 06:50:56,588 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:56,588 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42511'. Reason: nanny-close
2024-02-10 06:50:56,588 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40865. Reason: nanny-close
2024-02-10 06:50:56,588 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:56,589 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46387'. Reason: nanny-close
2024-02-10 06:50:56,589 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37951. Reason: nanny-close
2024-02-10 06:50:56,589 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:56,589 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34601'. Reason: nanny-close
2024-02-10 06:50:56,589 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40025. Reason: nanny-close
2024-02-10 06:50:56,589 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:56,589 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42939'. Reason: nanny-close
2024-02-10 06:50:56,590 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38875. Reason: nanny-close
2024-02-10 06:50:56,590 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:56,590 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34427'. Reason: nanny-close
2024-02-10 06:50:56,590 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:56,590 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38385. Reason: nanny-close
2024-02-10 06:50:56,590 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40617'. Reason: nanny-close
2024-02-10 06:50:56,590 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:56,590 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41296; closing.
2024-02-10 06:50:56,591 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:50:56,591 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38625. Reason: nanny-close
2024-02-10 06:50:56,591 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40865', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547856.591223')
2024-02-10 06:50:56,591 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42203. Reason: nanny-close
2024-02-10 06:50:56,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:56,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:56,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:56,591 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43389. Reason: nanny-close
2024-02-10 06:50:56,592 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41292; closing.
2024-02-10 06:50:56,593 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:56,593 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:56,593 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:56,593 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:56,593 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:56,593 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:56,593 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547856.5935013')
2024-02-10 06:50:56,593 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:56,593 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41300; closing.
2024-02-10 06:50:56,594 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41246; closing.
2024-02-10 06:50:56,594 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:50:56,594 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:56,594 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:56,595 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:56,597 - distributed.nanny - INFO - Worker closed
2024-02-10 06:50:56,596 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41292>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41292>: Stream is closed
2024-02-10 06:50:56,598 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37951', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547856.5987175')
2024-02-10 06:50:56,599 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40025', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547856.5992208')
2024-02-10 06:50:56,599 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41224; closing.
2024-02-10 06:50:56,600 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38625', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547856.600481')
2024-02-10 06:50:56,600 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41256; closing.
2024-02-10 06:50:56,601 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41276; closing.
2024-02-10 06:50:56,601 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41272; closing.
2024-02-10 06:50:56,601 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42203', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547856.6015718')
2024-02-10 06:50:56,602 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38385', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547856.6020632')
2024-02-10 06:50:56,602 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43389', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547856.6025264')
2024-02-10 06:50:56,602 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:50:57,452 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:50:57,452 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:50:57,453 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:50:57,454 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:50:57,455 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-02-10 06:50:59,253 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:50:59,257 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:50:59,260 - distributed.scheduler - INFO - State start
2024-02-10 06:50:59,280 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:50:59,281 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:50:59,281 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:50:59,281 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:50:59,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44637'
2024-02-10 06:50:59,446 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35573'
2024-02-10 06:50:59,451 - distributed.scheduler - INFO - Receive client connection: Client-be7cfdcc-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:50:59,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44147'
2024-02-10 06:50:59,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41416
2024-02-10 06:50:59,469 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38365'
2024-02-10 06:50:59,471 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37199'
2024-02-10 06:50:59,479 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33189'
2024-02-10 06:50:59,487 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36029'
2024-02-10 06:50:59,495 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40651'
2024-02-10 06:51:01,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:01,214 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:01,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:01,214 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:01,218 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:01,218 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:01,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38889
2024-02-10 06:51:01,219 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38889
2024-02-10 06:51:01,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42753
2024-02-10 06:51:01,219 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42753
2024-02-10 06:51:01,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44191
2024-02-10 06:51:01,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:01,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35183
2024-02-10 06:51:01,219 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:01,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:01,219 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:01,219 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:01,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:01,220 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:01,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-se2w4lcn
2024-02-10 06:51:01,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:01,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3n5o7jvv
2024-02-10 06:51:01,220 - distributed.worker - INFO - Starting Worker plugin PreImport-5b6308dc-0bfe-4f8e-ab77-2c794fecffab
2024-02-10 06:51:01,220 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a372db78-5d54-4723-9441-816b69ab2258
2024-02-10 06:51:01,220 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f70459ec-f594-4236-8150-364b8efe479a
2024-02-10 06:51:01,222 - distributed.worker - INFO - Starting Worker plugin PreImport-3a022b2a-ac79-42e5-9fcd-41b395158cde
2024-02-10 06:51:01,222 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2414245-d07e-4979-b76e-e5dbbfa19699
2024-02-10 06:51:01,223 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9380c05-5fac-471a-9209-fabe81786f3e
2024-02-10 06:51:01,228 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:01,228 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:01,232 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:01,233 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33755
2024-02-10 06:51:01,233 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33755
2024-02-10 06:51:01,233 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46613
2024-02-10 06:51:01,233 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:01,233 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:01,233 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:01,233 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:01,233 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v_p9xum5
2024-02-10 06:51:01,234 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec466e86-6a9a-4b64-8a09-ef042f525b7d
2024-02-10 06:51:01,239 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:01,240 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:01,244 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:01,245 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40405
2024-02-10 06:51:01,245 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40405
2024-02-10 06:51:01,245 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43425
2024-02-10 06:51:01,245 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:01,245 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:01,245 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:01,245 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:01,245 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-48k9ra0d
2024-02-10 06:51:01,245 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8635239f-6263-486c-a289-d563106649e1
2024-02-10 06:51:01,260 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:01,260 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:01,264 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:01,265 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42007
2024-02-10 06:51:01,265 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42007
2024-02-10 06:51:01,266 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41013
2024-02-10 06:51:01,266 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:01,266 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:01,266 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:01,266 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:01,266 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:01,266 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:01,266 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6xce6ldz
2024-02-10 06:51:01,266 - distributed.worker - INFO - Starting Worker plugin RMMSetup-330dac55-273c-4a77-a61b-00bc45f3b099
2024-02-10 06:51:01,270 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:01,271 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36007
2024-02-10 06:51:01,271 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36007
2024-02-10 06:51:01,271 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40407
2024-02-10 06:51:01,271 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:01,271 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:01,271 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:01,271 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:01,271 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u32z7gbv
2024-02-10 06:51:01,271 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d97353b4-d287-491b-a122-9feea6cfd63f
2024-02-10 06:51:01,271 - distributed.worker - INFO - Starting Worker plugin PreImport-855f3a95-73ff-48c1-ac89-ea1160ccc043
2024-02-10 06:51:01,272 - distributed.worker - INFO - Starting Worker plugin RMMSetup-daaed34a-b18f-4c64-ae8f-f15acfba7bb7
2024-02-10 06:51:01,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:01,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:01,284 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:01,285 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36347
2024-02-10 06:51:01,285 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36347
2024-02-10 06:51:01,285 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35873
2024-02-10 06:51:01,285 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:01,285 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:01,285 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:01,285 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:01,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n3fe6x9l
2024-02-10 06:51:01,285 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-302789e6-18ec-4f1a-b3d3-5f1e8b8841e0
2024-02-10 06:51:01,285 - distributed.worker - INFO - Starting Worker plugin PreImport-3d6cf8a1-1888-41c5-a6f8-4500a0e54850
2024-02-10 06:51:01,286 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b07a69fa-0a17-4ef2-918b-22a7bdb53c65
2024-02-10 06:51:01,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:01,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:01,361 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:01,362 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42923
2024-02-10 06:51:01,362 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42923
2024-02-10 06:51:01,362 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43159
2024-02-10 06:51:01,362 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:01,362 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:01,362 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:01,362 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:01,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1v762gjc
2024-02-10 06:51:01,363 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90f7c9d3-3c11-49fb-a9cf-8af4df4bafda
2024-02-10 06:51:01,363 - distributed.worker - INFO - Starting Worker plugin PreImport-31207801-1ac4-4892-a5cd-d9887684eb97
2024-02-10 06:51:01,363 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d80b045-aa0f-4061-9287-47dd58151329
2024-02-10 06:51:03,309 - distributed.worker - INFO - Starting Worker plugin PreImport-15239e98-c9ca-4571-958e-4d92e82f7668
2024-02-10 06:51:03,309 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,309 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-876ba5d0-22fe-4104-a188-33e19c4cadfb
2024-02-10 06:51:03,310 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,333 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,336 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33755', status: init, memory: 0, processing: 0>
2024-02-10 06:51:03,337 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33755
2024-02-10 06:51:03,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49676
2024-02-10 06:51:03,338 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:03,338 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38889', status: init, memory: 0, processing: 0>
2024-02-10 06:51:03,339 - distributed.worker - INFO - Starting Worker plugin PreImport-aed705bf-79b4-4406-acc0-b08e6cef005b
2024-02-10 06:51:03,339 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:03,339 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,339 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc90b125-e30f-40cb-ac8b-3340d09c74a2
2024-02-10 06:51:03,339 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38889
2024-02-10 06:51:03,339 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49684
2024-02-10 06:51:03,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:03,341 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:03,341 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,342 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:03,342 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:03,346 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,356 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36007', status: init, memory: 0, processing: 0>
2024-02-10 06:51:03,356 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36007
2024-02-10 06:51:03,356 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49692
2024-02-10 06:51:03,357 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:03,358 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:03,358 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:03,362 - distributed.worker - INFO - Starting Worker plugin PreImport-054d8e6a-2bb0-4bb8-82e1-4e00aa6f669f
2024-02-10 06:51:03,363 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65b57d8e-b41b-432a-926e-573e6f2ce0c6
2024-02-10 06:51:03,363 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,370 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,375 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,378 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40405', status: init, memory: 0, processing: 0>
2024-02-10 06:51:03,379 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40405
2024-02-10 06:51:03,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49702
2024-02-10 06:51:03,380 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42753', status: init, memory: 0, processing: 0>
2024-02-10 06:51:03,381 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:03,381 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42753
2024-02-10 06:51:03,381 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49708
2024-02-10 06:51:03,382 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:03,382 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,382 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:03,384 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:03,384 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:03,386 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:03,387 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42007', status: init, memory: 0, processing: 0>
2024-02-10 06:51:03,387 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42007
2024-02-10 06:51:03,387 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49724
2024-02-10 06:51:03,388 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:03,389 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:03,389 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,390 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:03,400 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42923', status: init, memory: 0, processing: 0>
2024-02-10 06:51:03,400 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42923
2024-02-10 06:51:03,400 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49736
2024-02-10 06:51:03,401 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:03,402 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:03,402 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,403 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:03,404 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36347', status: init, memory: 0, processing: 0>
2024-02-10 06:51:03,405 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36347
2024-02-10 06:51:03,405 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49726
2024-02-10 06:51:03,406 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:03,407 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:03,407 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:03,409 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:03,450 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:03,450 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:03,450 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:03,450 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:03,451 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:03,451 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:03,451 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:03,451 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:03,461 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:51:03,461 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:51:03,461 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:51:03,461 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:51:03,461 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:51:03,461 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:51:03,462 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:51:03,462 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:51:03,469 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:03,471 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:03,473 - distributed.scheduler - INFO - Remove client Client-be7cfdcc-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:03,473 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41416; closing.
2024-02-10 06:51:03,473 - distributed.scheduler - INFO - Remove client Client-be7cfdcc-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:03,473 - distributed.scheduler - INFO - Close client connection: Client-be7cfdcc-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:03,474 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44637'. Reason: nanny-close
2024-02-10 06:51:03,475 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:03,475 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35573'. Reason: nanny-close
2024-02-10 06:51:03,476 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:03,476 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44147'. Reason: nanny-close
2024-02-10 06:51:03,476 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38889. Reason: nanny-close
2024-02-10 06:51:03,476 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:03,476 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38365'. Reason: nanny-close
2024-02-10 06:51:03,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:03,477 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42753. Reason: nanny-close
2024-02-10 06:51:03,477 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37199'. Reason: nanny-close
2024-02-10 06:51:03,477 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36007. Reason: nanny-close
2024-02-10 06:51:03,477 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:03,477 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33189'. Reason: nanny-close
2024-02-10 06:51:03,477 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33755. Reason: nanny-close
2024-02-10 06:51:03,478 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:03,478 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36029'. Reason: nanny-close
2024-02-10 06:51:03,478 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40405. Reason: nanny-close
2024-02-10 06:51:03,478 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:03,478 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40651'. Reason: nanny-close
2024-02-10 06:51:03,478 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:03,478 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:03,479 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36347. Reason: nanny-close
2024-02-10 06:51:03,479 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49684; closing.
2024-02-10 06:51:03,479 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42007. Reason: nanny-close
2024-02-10 06:51:03,479 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:03,479 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547863.4793293')
2024-02-10 06:51:03,479 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:03,479 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:03,479 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42923. Reason: nanny-close
2024-02-10 06:51:03,479 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49692; closing.
2024-02-10 06:51:03,480 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36007', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547863.48038')
2024-02-10 06:51:03,480 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:03,480 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:03,480 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:03,480 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:03,480 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:03,481 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:03,481 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:03,481 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:03,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49708; closing.
2024-02-10 06:51:03,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49676; closing.
2024-02-10 06:51:03,482 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:03,482 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:03,483 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:03,483 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:03,482 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49692>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-10 06:51:03,484 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42753', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547863.4841428')
2024-02-10 06:51:03,484 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33755', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547863.4845724')
2024-02-10 06:51:03,485 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49702; closing.
2024-02-10 06:51:03,485 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49724; closing.
2024-02-10 06:51:03,485 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40405', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547863.485901')
2024-02-10 06:51:03,486 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42007', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547863.4862735')
2024-02-10 06:51:03,486 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49726; closing.
2024-02-10 06:51:03,486 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49736; closing.
2024-02-10 06:51:03,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36347', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547863.4873354')
2024-02-10 06:51:03,487 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42923', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547863.487747')
2024-02-10 06:51:03,487 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:51:04,390 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:51:04,390 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:51:04,391 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:51:04,392 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:51:04,392 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-02-10 06:51:06,409 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:06,413 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:51:06,416 - distributed.scheduler - INFO - State start
2024-02-10 06:51:06,436 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:06,437 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:51:06,438 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:51:06,438 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:51:06,441 - distributed.scheduler - INFO - Receive client connection: Client-c2f0f679-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:51:06,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49900
2024-02-10 06:51:06,550 - distributed.scheduler - INFO - Receive client connection: Client-c2a56435-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:06,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49916
2024-02-10 06:51:06,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37443'
2024-02-10 06:51:06,576 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43947'
2024-02-10 06:51:06,585 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38773'
2024-02-10 06:51:06,599 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35719'
2024-02-10 06:51:06,601 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34539'
2024-02-10 06:51:06,609 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44443'
2024-02-10 06:51:06,619 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46361'
2024-02-10 06:51:06,627 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45255'
2024-02-10 06:51:08,412 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:08,412 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:08,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:08,413 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:08,413 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:08,414 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:08,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:08,415 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:08,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:08,417 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:08,418 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44905
2024-02-10 06:51:08,418 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44905
2024-02-10 06:51:08,418 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35109
2024-02-10 06:51:08,418 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34973
2024-02-10 06:51:08,418 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:08,418 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34973
2024-02-10 06:51:08,418 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:08,418 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:08,418 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40465
2024-02-10 06:51:08,418 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:08,418 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:08,418 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:08,418 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:08,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c707uk2p
2024-02-10 06:51:08,418 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:08,418 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:08,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fuu23vq5
2024-02-10 06:51:08,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ddd16e88-fffd-45d2-bb49-748f422e2df0
2024-02-10 06:51:08,419 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8234eb47-b62e-463f-b5bc-dfcf05073947
2024-02-10 06:51:08,419 - distributed.worker - INFO - Starting Worker plugin PreImport-e5deb094-ebff-46a0-8c26-f3aad51172b8
2024-02-10 06:51:08,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed4e18fe-530f-4784-b82e-cad9c7ec94c1
2024-02-10 06:51:08,419 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39275
2024-02-10 06:51:08,419 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39275
2024-02-10 06:51:08,419 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:08,419 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43657
2024-02-10 06:51:08,419 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:08,419 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:08,419 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:08,419 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:08,419 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qqxjxk0s
2024-02-10 06:51:08,419 - distributed.worker - INFO - Starting Worker plugin PreImport-667f385e-0bbd-4581-a53a-8620a84bf0c7
2024-02-10 06:51:08,420 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a2b47504-35d2-4b60-9dc6-77b438036954
2024-02-10 06:51:08,420 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2d0c420a-5604-4a9b-96b2-0a2416e67445
2024-02-10 06:51:08,420 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44551
2024-02-10 06:51:08,420 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44551
2024-02-10 06:51:08,420 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37411
2024-02-10 06:51:08,420 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:08,420 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:08,420 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:08,420 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:08,420 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n7znm3us
2024-02-10 06:51:08,420 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09ac2c1f-f4f7-4244-bb13-f722a2f4c1f1
2024-02-10 06:51:08,421 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:08,421 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:08,425 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:08,426 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46211
2024-02-10 06:51:08,426 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46211
2024-02-10 06:51:08,426 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46503
2024-02-10 06:51:08,426 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:08,426 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:08,426 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:08,426 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:08,426 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yhiyjxg0
2024-02-10 06:51:08,426 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b1b09f4-2e4c-4250-bc68-45650436e6fd
2024-02-10 06:51:08,427 - distributed.worker - INFO - Starting Worker plugin PreImport-9900fe39-7d61-4848-8715-0f67e728c190
2024-02-10 06:51:08,427 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dad7016f-0bcb-4aec-8828-8549a971ca51
2024-02-10 06:51:08,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:08,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:08,430 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:08,430 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:08,432 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:08,433 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44395
2024-02-10 06:51:08,433 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44395
2024-02-10 06:51:08,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37509
2024-02-10 06:51:08,433 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:08,433 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:08,433 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:08,433 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:08,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9hq2ymyo
2024-02-10 06:51:08,434 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3b7374d-2046-47a8-a7e5-a9f2b3f060d1
2024-02-10 06:51:08,434 - distributed.worker - INFO - Starting Worker plugin PreImport-004cbb37-43e5-4f18-8b14-22562e6af2ae
2024-02-10 06:51:08,434 - distributed.worker - INFO - Starting Worker plugin RMMSetup-40f7f1bf-4f46-4c04-ad22-84523b057d83
2024-02-10 06:51:08,434 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:08,435 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43517
2024-02-10 06:51:08,435 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43517
2024-02-10 06:51:08,435 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44085
2024-02-10 06:51:08,435 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:08,436 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:08,436 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:08,436 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:08,436 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iu32mkju
2024-02-10 06:51:08,436 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b1988c45-059a-432d-aae6-4f49423aab1e
2024-02-10 06:51:08,436 - distributed.worker - INFO - Starting Worker plugin PreImport-3f80744c-ada5-418b-bde9-f3dd10973e74
2024-02-10 06:51:08,436 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d2f7109-560a-47b7-9897-ece1fdd2e23b
2024-02-10 06:51:08,515 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:08,516 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:08,520 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:08,520 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41991
2024-02-10 06:51:08,521 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41991
2024-02-10 06:51:08,521 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45939
2024-02-10 06:51:08,521 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:08,521 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:08,521 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:08,521 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:08,521 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wyr45jfr
2024-02-10 06:51:08,521 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a65e663d-0471-4791-bef1-5fa9d628fe09
2024-02-10 06:51:10,568 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,590 - distributed.worker - INFO - Starting Worker plugin PreImport-7b7d33c9-ce40-4e84-ab77-97035d769c81
2024-02-10 06:51:10,591 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f02fe02-9259-43aa-9ba0-6e945e333d2a
2024-02-10 06:51:10,591 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34973', status: init, memory: 0, processing: 0>
2024-02-10 06:51:10,592 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34973
2024-02-10 06:51:10,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36636
2024-02-10 06:51:10,592 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,593 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:10,594 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:10,594 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,595 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:10,623 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44905', status: init, memory: 0, processing: 0>
2024-02-10 06:51:10,624 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44905
2024-02-10 06:51:10,624 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36652
2024-02-10 06:51:10,625 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:10,626 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,627 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:10,627 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,627 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,629 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:10,643 - distributed.worker - INFO - Starting Worker plugin PreImport-f952842d-2830-441a-bed0-b22737ae1bec
2024-02-10 06:51:10,644 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ca9e5412-c6c3-4558-a327-e8a2017407d4
2024-02-10 06:51:10,645 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,648 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43517', status: init, memory: 0, processing: 0>
2024-02-10 06:51:10,649 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43517
2024-02-10 06:51:10,649 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36666
2024-02-10 06:51:10,650 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:10,650 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:10,651 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,651 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,652 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:10,656 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,661 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46211', status: init, memory: 0, processing: 0>
2024-02-10 06:51:10,661 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46211
2024-02-10 06:51:10,662 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36680
2024-02-10 06:51:10,663 - distributed.worker - INFO - Starting Worker plugin PreImport-8228d603-c421-4e1a-940b-7a7c33aa397d
2024-02-10 06:51:10,663 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:10,663 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa02f5a7-2297-4c97-8aef-c2dad4eed231
2024-02-10 06:51:10,664 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:10,664 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,665 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:10,673 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39275', status: init, memory: 0, processing: 0>
2024-02-10 06:51:10,673 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39275
2024-02-10 06:51:10,673 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36684
2024-02-10 06:51:10,674 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:10,675 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:10,675 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,675 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44551', status: init, memory: 0, processing: 0>
2024-02-10 06:51:10,676 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44551
2024-02-10 06:51:10,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36682
2024-02-10 06:51:10,677 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:10,677 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:10,678 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:10,678 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,680 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:10,687 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44395', status: init, memory: 0, processing: 0>
2024-02-10 06:51:10,688 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44395
2024-02-10 06:51:10,688 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36696
2024-02-10 06:51:10,689 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:10,690 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:10,690 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,692 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:10,700 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41991', status: init, memory: 0, processing: 0>
2024-02-10 06:51:10,701 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41991
2024-02-10 06:51:10,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36702
2024-02-10 06:51:10,703 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:10,704 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:10,704 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:10,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:10,722 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,722 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,722 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,723 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,723 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,723 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,723 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,723 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,725 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,726 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:10,731 - distributed.scheduler - INFO - Remove client Client-c2a56435-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:10,731 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49916; closing.
2024-02-10 06:51:10,731 - distributed.scheduler - INFO - Remove client Client-c2a56435-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:10,732 - distributed.scheduler - INFO - Close client connection: Client-c2a56435-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:10,733 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37443'. Reason: nanny-close
2024-02-10 06:51:10,733 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:10,734 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43947'. Reason: nanny-close
2024-02-10 06:51:10,734 - distributed.scheduler - INFO - Remove client Client-c2f0f679-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:51:10,734 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49900; closing.
2024-02-10 06:51:10,734 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:10,734 - distributed.scheduler - INFO - Remove client Client-c2f0f679-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:51:10,734 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38773'. Reason: nanny-close
2024-02-10 06:51:10,734 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39275. Reason: nanny-close
2024-02-10 06:51:10,734 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:10,734 - distributed.scheduler - INFO - Close client connection: Client-c2f0f679-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:51:10,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35719'. Reason: nanny-close
2024-02-10 06:51:10,735 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34973. Reason: nanny-close
2024-02-10 06:51:10,735 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:10,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34539'. Reason: nanny-close
2024-02-10 06:51:10,735 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44395. Reason: nanny-close
2024-02-10 06:51:10,735 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:10,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44443'. Reason: nanny-close
2024-02-10 06:51:10,736 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44905. Reason: nanny-close
2024-02-10 06:51:10,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:10,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46361'. Reason: nanny-close
2024-02-10 06:51:10,736 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:10,736 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36684; closing.
2024-02-10 06:51:10,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:10,736 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44551. Reason: nanny-close
2024-02-10 06:51:10,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45255'. Reason: nanny-close
2024-02-10 06:51:10,736 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39275', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547870.736756')
2024-02-10 06:51:10,736 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43517. Reason: nanny-close
2024-02-10 06:51:10,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:10,737 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:10,737 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41991. Reason: nanny-close
2024-02-10 06:51:10,737 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46211. Reason: nanny-close
2024-02-10 06:51:10,737 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:10,737 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:10,738 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:10,738 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:10,738 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36636; closing.
2024-02-10 06:51:10,738 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:10,739 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:10,739 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36696; closing.
2024-02-10 06:51:10,739 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34973', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547870.7395656')
2024-02-10 06:51:10,739 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:10,739 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36652; closing.
2024-02-10 06:51:10,740 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:10,740 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:10,740 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:10,740 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:10,740 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44395', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547870.740584')
2024-02-10 06:51:10,740 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:10,740 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44905', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547870.7409155')
2024-02-10 06:51:10,741 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36666; closing.
2024-02-10 06:51:10,741 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36682; closing.
2024-02-10 06:51:10,741 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:10,742 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43517', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547870.7421398')
2024-02-10 06:51:10,742 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:10,742 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44551', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547870.7424278')
2024-02-10 06:51:10,742 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36702; closing.
2024-02-10 06:51:10,743 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36680; closing.
2024-02-10 06:51:10,743 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41991', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547870.7434552')
2024-02-10 06:51:10,743 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46211', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547870.7438674')
2024-02-10 06:51:10,744 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:51:12,550 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:51:12,551 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:51:12,551 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:51:12,553 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:51:12,554 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-02-10 06:51:14,514 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:14,518 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:51:14,521 - distributed.scheduler - INFO - State start
2024-02-10 06:51:14,543 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:14,544 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:51:14,544 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:51:14,545 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:51:14,573 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37379'
2024-02-10 06:51:14,832 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37955', status: init, memory: 0, processing: 0>
2024-02-10 06:51:14,844 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37955
2024-02-10 06:51:14,844 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37390
2024-02-10 06:51:14,863 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35649', status: init, memory: 0, processing: 0>
2024-02-10 06:51:14,863 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35649
2024-02-10 06:51:14,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37404
2024-02-10 06:51:14,887 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37404; closing.
2024-02-10 06:51:14,888 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35649', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547874.8878996')
2024-02-10 06:51:14,888 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37390; closing.
2024-02-10 06:51:14,889 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37955', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547874.8890035')
2024-02-10 06:51:14,889 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:51:15,885 - distributed.scheduler - INFO - Receive client connection: Client-c94b06df-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:51:15,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37418
2024-02-10 06:51:16,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:16,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:16,580 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:16,581 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35933
2024-02-10 06:51:16,581 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35933
2024-02-10 06:51:16,581 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-02-10 06:51:16,581 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:16,581 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:16,581 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:16,581 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-10 06:51:16,581 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dn508v_b
2024-02-10 06:51:16,581 - distributed.worker - INFO - Starting Worker plugin PreImport-5763f61c-626e-4532-8726-4c1d7f3a201e
2024-02-10 06:51:16,581 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ade5017b-ee2c-4475-b57f-a364d8d5a173
2024-02-10 06:51:16,582 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1bcab8de-0af4-4f4f-b4d8-f7e6d0aa0d27
2024-02-10 06:51:16,582 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:16,628 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35933', status: init, memory: 0, processing: 0>
2024-02-10 06:51:16,629 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35933
2024-02-10 06:51:16,629 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37444
2024-02-10 06:51:16,630 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:16,630 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:16,630 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:16,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:18,743 - distributed.scheduler - INFO - Receive client connection: Client-c78c9ffe-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:18,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37506
2024-02-10 06:51:18,750 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:18,757 - distributed.scheduler - INFO - Remove client Client-c78c9ffe-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:18,757 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37506; closing.
2024-02-10 06:51:18,757 - distributed.scheduler - INFO - Remove client Client-c78c9ffe-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:18,757 - distributed.scheduler - INFO - Close client connection: Client-c78c9ffe-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:18,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37379'. Reason: nanny-close
2024-02-10 06:51:18,758 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:18,759 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35933. Reason: nanny-close
2024-02-10 06:51:18,761 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:18,761 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37444; closing.
2024-02-10 06:51:18,762 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35933', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547878.7622159')
2024-02-10 06:51:18,762 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:51:18,763 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:19,273 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:51:19,273 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:51:19,274 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:51:19,276 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:51:19,277 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-02-10 06:51:23,111 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:23,115 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:51:23,118 - distributed.scheduler - INFO - State start
2024-02-10 06:51:23,138 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:23,139 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:51:23,140 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:51:23,140 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:51:23,188 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34465', status: init, memory: 0, processing: 0>
2024-02-10 06:51:23,198 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34465
2024-02-10 06:51:23,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46384
2024-02-10 06:51:23,201 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37747'
2024-02-10 06:51:23,251 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46384; closing.
2024-02-10 06:51:23,252 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34465', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547883.252095')
2024-02-10 06:51:23,252 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:51:23,329 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40387', status: init, memory: 0, processing: 0>
2024-02-10 06:51:23,330 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40387
2024-02-10 06:51:23,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46398
2024-02-10 06:51:23,352 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46398; closing.
2024-02-10 06:51:23,352 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40387', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547883.352784')
2024-02-10 06:51:23,353 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:51:24,244 - distributed.scheduler - INFO - Receive client connection: Client-c94b06df-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:51:24,245 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46406
2024-02-10 06:51:24,715 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:24,716 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:25,170 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:25,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34749
2024-02-10 06:51:25,171 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34749
2024-02-10 06:51:25,171 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42509
2024-02-10 06:51:25,171 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:25,171 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:25,172 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:25,172 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-10 06:51:25,172 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bmi1tvpm
2024-02-10 06:51:25,172 - distributed.worker - INFO - Starting Worker plugin PreImport-676bf712-1eae-4914-88f1-ba5f239803d7
2024-02-10 06:51:25,173 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af744cf6-6c89-4b75-a8b9-f7cb87da0e74
2024-02-10 06:51:25,173 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a30e11e7-4977-431f-8fd0-2b1db5a89da0
2024-02-10 06:51:25,173 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:25,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34749', status: init, memory: 0, processing: 0>
2024-02-10 06:51:25,218 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34749
2024-02-10 06:51:25,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46430
2024-02-10 06:51:25,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:25,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:25,219 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:25,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:26,751 - distributed.scheduler - INFO - Receive client connection: Client-ccab1273-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:26,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46450
2024-02-10 06:51:26,760 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:26,764 - distributed.scheduler - INFO - Remove client Client-ccab1273-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:26,765 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46450; closing.
2024-02-10 06:51:26,765 - distributed.scheduler - INFO - Remove client Client-ccab1273-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:26,765 - distributed.scheduler - INFO - Close client connection: Client-ccab1273-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:26,766 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37747'. Reason: nanny-close
2024-02-10 06:51:26,767 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:26,768 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34749. Reason: nanny-close
2024-02-10 06:51:26,771 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:26,771 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46430; closing.
2024-02-10 06:51:26,771 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34749', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547886.771878')
2024-02-10 06:51:26,772 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:51:26,773 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:27,281 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:51:27,282 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:51:27,282 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:51:27,283 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:51:27,284 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-02-10 06:51:29,121 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:29,125 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:51:29,127 - distributed.scheduler - INFO - State start
2024-02-10 06:51:29,148 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:29,149 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:51:29,150 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:51:29,150 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:51:29,200 - distributed.scheduler - INFO - Receive client connection: Client-c94b06df-c7e0-11ee-8170-d8c49764f6bb
2024-02-10 06:51:29,210 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46694
2024-02-10 06:51:31,152 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:46708'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46708>: Stream is closed
2024-02-10 06:51:31,383 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:51:31,384 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:51:31,384 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:51:31,385 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:51:31,386 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-02-10 06:51:33,261 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:33,265 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-10 06:51:33,268 - distributed.scheduler - INFO - State start
2024-02-10 06:51:33,288 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:33,289 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-10 06:51:33,289 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-10 06:51:33,289 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:51:33,358 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40917'
2024-02-10 06:51:34,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:34,763 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:34,766 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:34,767 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43189
2024-02-10 06:51:34,767 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43189
2024-02-10 06:51:34,767 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35627
2024-02-10 06:51:34,767 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-10 06:51:34,767 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:34,767 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:34,767 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-10 06:51:34,767 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-c08p07ym
2024-02-10 06:51:34,768 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96ad39f4-e4a5-45e8-a972-cab8cf803e4a
2024-02-10 06:51:34,768 - distributed.worker - INFO - Starting Worker plugin PreImport-4fcc1cf5-a813-4f6d-9966-ae796c3dad96
2024-02-10 06:51:34,768 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec879e52-90cb-445f-94d4-adff2b1f8c19
2024-02-10 06:51:34,768 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:34,813 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43189', status: init, memory: 0, processing: 0>
2024-02-10 06:51:34,824 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43189
2024-02-10 06:51:34,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54298
2024-02-10 06:51:34,824 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:34,825 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-10 06:51:34,825 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:34,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-10 06:51:35,631 - distributed.scheduler - INFO - Receive client connection: Client-d2b94489-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:35,632 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54312
2024-02-10 06:51:35,638 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:35,644 - distributed.scheduler - INFO - Remove client Client-d2b94489-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:35,644 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54312; closing.
2024-02-10 06:51:35,645 - distributed.scheduler - INFO - Remove client Client-d2b94489-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:35,645 - distributed.scheduler - INFO - Close client connection: Client-d2b94489-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:35,646 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40917'. Reason: nanny-close
2024-02-10 06:51:35,646 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:35,647 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43189. Reason: nanny-close
2024-02-10 06:51:35,648 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-10 06:51:35,648 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54298; closing.
2024-02-10 06:51:35,649 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43189', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547895.6490338')
2024-02-10 06:51:35,649 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:51:35,650 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:36,161 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:51:36,161 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:51:36,161 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:51:36,162 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-10 06:51:36,163 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-02-10 06:51:38,089 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:38,094 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35067 instead
  warnings.warn(
2024-02-10 06:51:38,098 - distributed.scheduler - INFO - State start
2024-02-10 06:51:38,120 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:38,121 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-10 06:51:38,121 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35067/status
2024-02-10 06:51:38,122 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-10 06:51:38,336 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45747'
2024-02-10 06:51:38,346 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43643'
2024-02-10 06:51:38,356 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41331'
2024-02-10 06:51:38,368 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42967'
2024-02-10 06:51:38,371 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39883'
2024-02-10 06:51:38,379 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43319'
2024-02-10 06:51:38,387 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36453'
2024-02-10 06:51:38,395 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37679'
2024-02-10 06:51:39,787 - distributed.scheduler - INFO - Receive client connection: Client-d591af08-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:39,801 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58598
2024-02-10 06:51:40,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:40,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:40,210 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:40,210 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:40,213 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:40,214 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41635
2024-02-10 06:51:40,214 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41635
2024-02-10 06:51:40,214 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43429
2024-02-10 06:51:40,214 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:40,214 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:40,214 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:40,214 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:40,214 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t2zf33g2
2024-02-10 06:51:40,214 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:40,215 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b459d867-0c9f-4940-8396-c2637a9c1c26
2024-02-10 06:51:40,215 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36723
2024-02-10 06:51:40,215 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36723
2024-02-10 06:51:40,215 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38941
2024-02-10 06:51:40,215 - distributed.worker - INFO - Starting Worker plugin PreImport-41fb9271-5dce-431a-affc-83edcd395642
2024-02-10 06:51:40,216 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:40,216 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:40,216 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:40,216 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7451eb49-30b9-4820-b7ed-f9e3756d7f9b
2024-02-10 06:51:40,216 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:40,216 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iymvcmhi
2024-02-10 06:51:40,216 - distributed.worker - INFO - Starting Worker plugin PreImport-57ab5bef-6e0b-4f76-bff0-82bb451eaaf3
2024-02-10 06:51:40,216 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3bc724c3-497b-4152-abe8-454161114d1a
2024-02-10 06:51:40,219 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d144c728-242b-4390-8aa4-5ca423824c2f
2024-02-10 06:51:40,242 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:40,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:40,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:40,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45023
2024-02-10 06:51:40,248 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45023
2024-02-10 06:51:40,248 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39299
2024-02-10 06:51:40,248 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:40,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:40,248 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:40,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:40,248 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:40,248 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:40,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ddlc8kuu
2024-02-10 06:51:40,249 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3eada148-78b1-46e5-99c9-f9581b8c97b6
2024-02-10 06:51:40,253 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:40,254 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43159
2024-02-10 06:51:40,254 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43159
2024-02-10 06:51:40,254 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40459
2024-02-10 06:51:40,254 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:40,254 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:40,254 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:40,254 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:40,254 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yn_23zlc
2024-02-10 06:51:40,254 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d9947d8-315e-4dd1-b4d9-eda249d76954
2024-02-10 06:51:40,254 - distributed.worker - INFO - Starting Worker plugin PreImport-ed88577f-527d-4802-9a7e-41596640f937
2024-02-10 06:51:40,255 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0415ac67-9202-4744-949f-e6901e8af6f1
2024-02-10 06:51:40,274 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:40,274 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:40,279 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:40,280 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34657
2024-02-10 06:51:40,280 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34657
2024-02-10 06:51:40,280 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45609
2024-02-10 06:51:40,280 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:40,280 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:40,280 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:40,280 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:40,280 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e8944uvm
2024-02-10 06:51:40,280 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cfa3e382-07f3-426a-8013-20bea76c5d3d
2024-02-10 06:51:40,305 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:40,305 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:40,309 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:40,310 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41599
2024-02-10 06:51:40,310 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41599
2024-02-10 06:51:40,310 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44919
2024-02-10 06:51:40,310 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:40,311 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:40,311 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:40,311 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:40,311 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1ea45054
2024-02-10 06:51:40,311 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54c035f7-530d-4804-9497-5f27b688f9f2
2024-02-10 06:51:40,311 - distributed.worker - INFO - Starting Worker plugin PreImport-1e480315-36e2-4131-b61c-b4b69a9ce085
2024-02-10 06:51:40,311 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2c8670b9-c167-4b60-a585-dc5831c1e72b
2024-02-10 06:51:40,315 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:40,316 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:40,320 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:40,321 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34323
2024-02-10 06:51:40,321 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34323
2024-02-10 06:51:40,321 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43723
2024-02-10 06:51:40,321 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:40,321 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:40,321 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:40,321 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:40,321 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0f3ks6qy
2024-02-10 06:51:40,321 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eeb70eff-77ec-46f2-98ac-46f3cc0c39bf
2024-02-10 06:51:40,322 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:40,322 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:40,326 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:40,327 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34531
2024-02-10 06:51:40,327 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34531
2024-02-10 06:51:40,327 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38981
2024-02-10 06:51:40,327 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:40,327 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:40,327 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:40,328 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-10 06:51:40,328 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k7ow480t
2024-02-10 06:51:40,328 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57af7509-c9d8-4133-9f97-debb407911d9
2024-02-10 06:51:40,328 - distributed.worker - INFO - Starting Worker plugin PreImport-54e53b82-f748-4a2c-a73f-6e39d578d6cd
2024-02-10 06:51:40,328 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f3d88a3-b9f8-4131-b540-65f9066eea2a
2024-02-10 06:51:41,025 - distributed.scheduler - INFO - Receive client connection: Client-d8472267-c7e0-11ee-833f-d8c49764f6bb
2024-02-10 06:51:41,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36366
2024-02-10 06:51:42,009 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,038 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36723', status: init, memory: 0, processing: 0>
2024-02-10 06:51:42,039 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36723
2024-02-10 06:51:42,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36392
2024-02-10 06:51:42,041 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:42,042 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:42,042 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:42,140 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,150 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,156 - distributed.worker - INFO - Starting Worker plugin PreImport-b7e8f854-6b20-4f70-9f4e-53aadd48a651
2024-02-10 06:51:42,157 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64f33bbe-92bc-4e9a-b2c3-cfcfebe95ada
2024-02-10 06:51:42,158 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,165 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43159', status: init, memory: 0, processing: 0>
2024-02-10 06:51:42,166 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43159
2024-02-10 06:51:42,166 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36396
2024-02-10 06:51:42,167 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:42,168 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:42,168 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:42,181 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45023', status: init, memory: 0, processing: 0>
2024-02-10 06:51:42,181 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45023
2024-02-10 06:51:42,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36412
2024-02-10 06:51:42,182 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41635', status: init, memory: 0, processing: 0>
2024-02-10 06:51:42,182 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:42,183 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41635
2024-02-10 06:51:42,183 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36400
2024-02-10 06:51:42,183 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:42,183 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,184 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:42,185 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:42,186 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:42,186 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:42,195 - distributed.worker - INFO - Starting Worker plugin PreImport-7c4744b3-b778-4c42-907e-c26aaa9b4639
2024-02-10 06:51:42,196 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82d4eba2-d317-410d-ba36-56afcfb31e7d
2024-02-10 06:51:42,196 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,207 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,216 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34657', status: init, memory: 0, processing: 0>
2024-02-10 06:51:42,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34657
2024-02-10 06:51:42,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36424
2024-02-10 06:51:42,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:42,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:42,219 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:42,230 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41599', status: init, memory: 0, processing: 0>
2024-02-10 06:51:42,230 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41599
2024-02-10 06:51:42,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36436
2024-02-10 06:51:42,231 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:42,232 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:42,232 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:42,233 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,234 - distributed.worker - INFO - Starting Worker plugin PreImport-05a209ef-a864-4586-b1f4-425e79135485
2024-02-10 06:51:42,235 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96fed36b-cebd-433c-9407-7aca6a7463fa
2024-02-10 06:51:42,238 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,265 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34531', status: init, memory: 0, processing: 0>
2024-02-10 06:51:42,265 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34531
2024-02-10 06:51:42,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36440
2024-02-10 06:51:42,267 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:42,268 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:42,268 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,270 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34323', status: init, memory: 0, processing: 0>
2024-02-10 06:51:42,270 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34323
2024-02-10 06:51:42,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:42,271 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36450
2024-02-10 06:51:42,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:42,273 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:42,273 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:42,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:42,299 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,299 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,299 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,300 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,300 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,300 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,300 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,300 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,303 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,304 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-10 06:51:42,308 - distributed.scheduler - INFO - Remove client Client-d8472267-c7e0-11ee-833f-d8c49764f6bb
2024-02-10 06:51:42,308 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36366; closing.
2024-02-10 06:51:42,308 - distributed.scheduler - INFO - Remove client Client-d8472267-c7e0-11ee-833f-d8c49764f6bb
2024-02-10 06:51:42,309 - distributed.scheduler - INFO - Close client connection: Client-d8472267-c7e0-11ee-833f-d8c49764f6bb
2024-02-10 06:51:42,317 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:42,317 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:42,317 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:42,317 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:42,317 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:42,318 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:42,318 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:42,318 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-10 06:51:42,322 - distributed.scheduler - INFO - Remove client Client-d591af08-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:42,322 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58598; closing.
2024-02-10 06:51:42,322 - distributed.scheduler - INFO - Remove client Client-d591af08-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:42,323 - distributed.scheduler - INFO - Close client connection: Client-d591af08-c7e0-11ee-9a4b-d8c49764f6bb
2024-02-10 06:51:42,324 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45747'. Reason: nanny-close
2024-02-10 06:51:42,324 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:42,324 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43643'. Reason: nanny-close
2024-02-10 06:51:42,325 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:42,325 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41331'. Reason: nanny-close
2024-02-10 06:51:42,325 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:42,325 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36723. Reason: nanny-close
2024-02-10 06:51:42,326 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42967'. Reason: nanny-close
2024-02-10 06:51:42,326 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41635. Reason: nanny-close
2024-02-10 06:51:42,326 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:42,326 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39883'. Reason: nanny-close
2024-02-10 06:51:42,326 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43159. Reason: nanny-close
2024-02-10 06:51:42,327 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43319'. Reason: nanny-close
2024-02-10 06:51:42,327 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:42,327 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34657. Reason: nanny-close
2024-02-10 06:51:42,327 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36453'. Reason: nanny-close
2024-02-10 06:51:42,327 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:42,327 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37679'. Reason: nanny-close
2024-02-10 06:51:42,328 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:42,328 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34531. Reason: nanny-close
2024-02-10 06:51:42,328 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45023. Reason: nanny-close
2024-02-10 06:51:42,328 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:42,328 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:42,328 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36392; closing.
2024-02-10 06:51:42,328 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41599. Reason: nanny-close
2024-02-10 06:51:42,329 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36396; closing.
2024-02-10 06:51:42,329 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:42,329 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:42,329 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36723', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547902.3293502')
2024-02-10 06:51:42,329 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:42,330 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43159', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547902.3300564')
2024-02-10 06:51:42,330 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:42,330 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34323. Reason: nanny-close
2024-02-10 06:51:42,330 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:42,330 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:42,330 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:42,331 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:42,331 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:42,331 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36424; closing.
2024-02-10 06:51:42,331 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:42,331 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:42,332 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:42,332 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34657', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547902.33225')
2024-02-10 06:51:42,332 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36400; closing.
2024-02-10 06:51:42,333 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:42,333 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:42,333 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36412; closing.
2024-02-10 06:51:42,334 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547902.333983')
2024-02-10 06:51:42,334 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36440; closing.
2024-02-10 06:51:42,334 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36436; closing.
2024-02-10 06:51:42,335 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547902.3349993')
2024-02-10 06:51:42,335 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34531', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547902.3355408')
2024-02-10 06:51:42,335 - distributed.nanny - INFO - Worker closed
2024-02-10 06:51:42,335 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41599', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547902.335924')
2024-02-10 06:51:42,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36450; closing.
2024-02-10 06:51:42,336 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34323', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707547902.3366547')
2024-02-10 06:51:42,336 - distributed.scheduler - INFO - Lost all workers
2024-02-10 06:51:42,971 - distributed.scheduler - INFO - Receive client connection: Client-d9700df9-c7e0-11ee-833f-d8c49764f6bb
2024-02-10 06:51:42,971 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36452
2024-02-10 06:51:43,139 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-10 06:51:43,140 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-10 06:51:43,140 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:51:43,142 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-10 06:51:43,142 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-02-10 06:51:45,336 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40413'
2024-02-10 06:51:45,400 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:45,405 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45201 instead
  warnings.warn(
2024-02-10 06:51:45,409 - distributed.scheduler - INFO - State start
2024-02-10 06:51:45,431 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:45,432 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-10 06:51:45,433 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:51:45,434 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-10 06:51:46,905 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40413'. Reason: nanny-close
2024-02-10 06:51:47,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:47,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:47,380 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:47,381 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36945
2024-02-10 06:51:47,381 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36945
2024-02-10 06:51:47,381 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40711
2024-02-10 06:51:47,381 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:47,381 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:47,381 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:47,381 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-10 06:51:47,381 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n841sk6a
2024-02-10 06:51:47,381 - distributed.worker - INFO - Starting Worker plugin PreImport-402d2e13-20fa-494a-ac79-da38af04ab37
2024-02-10 06:51:47,382 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a370c19e-35f9-48fb-9444-b043dc8373ab
2024-02-10 06:51:48,440 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58c36720-ae8f-493e-8d12-fb1109aea119
2024-02-10 06:51:48,441 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:48,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:48,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:48,556 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:48,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:48,574 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:48,575 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36945. Reason: nanny-close
2024-02-10 06:51:48,578 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:48,580 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-02-10 06:51:51,349 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:51,353 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37287 instead
  warnings.warn(
2024-02-10 06:51:51,357 - distributed.scheduler - INFO - State start
2024-02-10 06:51:51,380 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-10 06:51:51,381 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-10 06:51:51,381 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-10 06:51:51,382 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-10 06:51:51,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39671'
2024-02-10 06:51:53,063 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-10 06:51:53,063 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-10 06:51:53,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-10 06:51:53,067 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35407
2024-02-10 06:51:53,067 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35407
2024-02-10 06:51:53,068 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43833
2024-02-10 06:51:53,068 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-10 06:51:53,068 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:53,068 - distributed.worker - INFO -               Threads:                          1
2024-02-10 06:51:53,068 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-10 06:51:53,068 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cxj7tc2m
2024-02-10 06:51:53,068 - distributed.worker - INFO - Starting Worker plugin PreImport-5f97e1ce-45b2-4257-bbc5-8ad0d8104632
2024-02-10 06:51:53,068 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d174effb-d89c-434d-85d5-fb8c6a5aadbc
2024-02-10 06:51:53,540 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d199b7f-ccd4-4e27-904f-f3ffb0499757
2024-02-10 06:51:53,540 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:53,623 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-10 06:51:53,623 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-10 06:51:53,623 - distributed.worker - INFO - -------------------------------------------------
2024-02-10 06:51:53,624 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-10 06:51:53,650 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-02-10 06:51:53,662 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-10 06:51:53,674 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39671'. Reason: nanny-close
2024-02-10 06:51:53,674 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-10 06:51:53,675 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35407. Reason: nanny-close
2024-02-10 06:51:53,677 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-10 06:51:53,680 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34409 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37923 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41397 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40329 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 51 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
