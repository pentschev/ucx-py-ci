============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-8.1.1, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.5.post1
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-03-12 07:03:45,539 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:03:45,544 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40603 instead
  warnings.warn(
2024-03-12 07:03:45,548 - distributed.scheduler - INFO - State start
2024-03-12 07:03:45,570 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:03:45,571 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-12 07:03:45,572 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40603/status
2024-03-12 07:03:45,572 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:03:45,581 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41893'
2024-03-12 07:03:45,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34673'
2024-03-12 07:03:45,605 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36669'
2024-03-12 07:03:45,620 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44475'
2024-03-12 07:03:47,179 - distributed.scheduler - INFO - Receive client connection: Client-a9d86138-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:03:47,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57604
2024-03-12 07:03:47,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:47,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:47,364 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:47,364 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:47,366 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:47,367 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40287
2024-03-12 07:03:47,367 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40287
2024-03-12 07:03:47,367 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44505
2024-03-12 07:03:47,367 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-12 07:03:47,367 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,367 - distributed.worker - INFO -               Threads:                          4
2024-03-12 07:03:47,367 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-12 07:03:47,367 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-6um4fmpn
2024-03-12 07:03:47,368 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-56353904-0a42-43a5-8c79-1854a4c12e1e
2024-03-12 07:03:47,368 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85f14974-8472-41e0-9e3b-dba896507645
2024-03-12 07:03:47,368 - distributed.worker - INFO - Starting Worker plugin PreImport-001566a8-6871-418b-a331-327d75fd0d68
2024-03-12 07:03:47,368 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,368 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:47,369 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45827
2024-03-12 07:03:47,369 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45827
2024-03-12 07:03:47,369 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35281
2024-03-12 07:03:47,369 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-12 07:03:47,370 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,370 - distributed.worker - INFO -               Threads:                          4
2024-03-12 07:03:47,370 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-12 07:03:47,370 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-fox2ixg4
2024-03-12 07:03:47,370 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:47,370 - distributed.worker - INFO - Starting Worker plugin PreImport-75daa0d0-ee17-4e0d-85b4-cf1e0565b295
2024-03-12 07:03:47,370 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:47,370 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5bdc44c5-a60c-42b8-a31c-ed9d30ca5ff2
2024-03-12 07:03:47,370 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef423f9b-dd17-4c91-8268-808201ef2542
2024-03-12 07:03:47,370 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,371 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:47,371 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:47,374 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:47,375 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41369
2024-03-12 07:03:47,375 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41369
2024-03-12 07:03:47,375 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41899
2024-03-12 07:03:47,375 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-12 07:03:47,375 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,375 - distributed.worker - INFO -               Threads:                          4
2024-03-12 07:03:47,375 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-12 07:03:47,375 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-c_0hknfv
2024-03-12 07:03:47,376 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:47,376 - distributed.worker - INFO - Starting Worker plugin PreImport-02155e1a-bd61-4e52-b6ad-2b2ad70198ee
2024-03-12 07:03:47,376 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fcdcf960-1b00-43d9-9a63-4d4c1be438f1
2024-03-12 07:03:47,376 - distributed.worker - INFO - Starting Worker plugin RMMSetup-052ce812-2656-4132-8cdd-3cf3703a2d92
2024-03-12 07:03:47,376 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,376 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42495
2024-03-12 07:03:47,376 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42495
2024-03-12 07:03:47,377 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32837
2024-03-12 07:03:47,377 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-12 07:03:47,377 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,377 - distributed.worker - INFO -               Threads:                          4
2024-03-12 07:03:47,377 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-12 07:03:47,377 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-bghesokn
2024-03-12 07:03:47,377 - distributed.worker - INFO - Starting Worker plugin PreImport-58daed70-73e7-4770-9514-bfcf391499e4
2024-03-12 07:03:47,377 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79f2b392-6365-417b-af2c-d2174e52a25b
2024-03-12 07:03:47,377 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-07724382-8709-42d6-93a0-0c8d53de3053
2024-03-12 07:03:47,379 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,481 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40287', status: init, memory: 0, processing: 0>
2024-03-12 07:03:47,482 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40287
2024-03-12 07:03:47,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57622
2024-03-12 07:03:47,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:47,483 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-12 07:03:47,483 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-12 07:03:47,486 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45827', status: init, memory: 0, processing: 0>
2024-03-12 07:03:47,487 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45827
2024-03-12 07:03:47,487 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57628
2024-03-12 07:03:47,488 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:47,488 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-12 07:03:47,488 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-12 07:03:47,507 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41369', status: init, memory: 0, processing: 0>
2024-03-12 07:03:47,508 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41369
2024-03-12 07:03:47,508 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57632
2024-03-12 07:03:47,509 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:47,510 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-12 07:03:47,510 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,511 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-12 07:03:47,512 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42495', status: init, memory: 0, processing: 0>
2024-03-12 07:03:47,512 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42495
2024-03-12 07:03:47,513 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57642
2024-03-12 07:03:47,514 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:47,515 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-12 07:03:47,515 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:47,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-12 07:03:47,605 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-12 07:03:47,605 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-12 07:03:47,605 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-12 07:03:47,605 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-12 07:03:47,610 - distributed.scheduler - INFO - Remove client Client-a9d86138-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:03:47,610 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57604; closing.
2024-03-12 07:03:47,610 - distributed.scheduler - INFO - Remove client Client-a9d86138-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:03:47,611 - distributed.scheduler - INFO - Close client connection: Client-a9d86138-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:03:47,612 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41893'. Reason: nanny-close
2024-03-12 07:03:47,613 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:47,613 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34673'. Reason: nanny-close
2024-03-12 07:03:47,614 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:47,614 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36669'. Reason: nanny-close
2024-03-12 07:03:47,614 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42495. Reason: nanny-close
2024-03-12 07:03:47,614 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:47,614 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44475'. Reason: nanny-close
2024-03-12 07:03:47,614 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45827. Reason: nanny-close
2024-03-12 07:03:47,615 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:47,615 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41369. Reason: nanny-close
2024-03-12 07:03:47,615 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40287. Reason: nanny-close
2024-03-12 07:03:47,616 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57642; closing.
2024-03-12 07:03:47,616 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-12 07:03:47,616 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-12 07:03:47,616 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42495', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227027.6167152')
2024-03-12 07:03:47,617 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-12 07:03:47,617 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-12 07:03:47,617 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57632; closing.
2024-03-12 07:03:47,618 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57628; closing.
2024-03-12 07:03:47,618 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:47,618 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:47,618 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41369', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227027.6185272')
2024-03-12 07:03:47,618 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:47,618 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57622; closing.
2024-03-12 07:03:47,618 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:47,619 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227027.6190457')
2024-03-12 07:03:47,619 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:57632>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:57632>: Stream is closed
2024-03-12 07:03:47,621 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40287', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227027.6210487')
2024-03-12 07:03:47,621 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:03:48,377 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:03:48,377 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:03:48,378 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:03:48,379 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-12 07:03:48,379 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-03-12 07:03:50,512 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:03:50,517 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38451 instead
  warnings.warn(
2024-03-12 07:03:50,521 - distributed.scheduler - INFO - State start
2024-03-12 07:03:50,546 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:03:50,547 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:03:50,548 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38451/status
2024-03-12 07:03:50,548 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:03:50,778 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40875'
2024-03-12 07:03:50,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35787'
2024-03-12 07:03:50,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37419'
2024-03-12 07:03:50,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41909'
2024-03-12 07:03:50,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36179'
2024-03-12 07:03:50,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35311'
2024-03-12 07:03:50,833 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35601'
2024-03-12 07:03:50,842 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35457'
2024-03-12 07:03:51,583 - distributed.scheduler - INFO - Receive client connection: Client-acdad94c-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:03:51,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59618
2024-03-12 07:03:52,697 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:52,698 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:52,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:52,701 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:52,702 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:52,703 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38289
2024-03-12 07:03:52,703 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38289
2024-03-12 07:03:52,703 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33567
2024-03-12 07:03:52,703 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:03:52,703 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:52,703 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:03:52,703 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:03:52,703 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ri00yjwb
2024-03-12 07:03:52,703 - distributed.worker - INFO - Starting Worker plugin PreImport-4834b884-1186-4fdb-ba28-618d26155992
2024-03-12 07:03:52,703 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59c401fb-7dfd-4e13-b687-a6e31976144f
2024-03-12 07:03:52,704 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64385679-c6f6-4749-a27d-bd6944e2e550
2024-03-12 07:03:52,708 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:52,709 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37663
2024-03-12 07:03:52,709 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37663
2024-03-12 07:03:52,709 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33409
2024-03-12 07:03:52,709 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:03:52,709 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:52,709 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:03:52,709 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:03:52,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j6olzoqt
2024-03-12 07:03:52,710 - distributed.worker - INFO - Starting Worker plugin PreImport-d115465e-5f1c-4136-9a51-b4a3c53256a8
2024-03-12 07:03:52,710 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-30fbc033-9ae6-41e2-8f71-e6f1f19124f2
2024-03-12 07:03:52,719 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8ad590a-6fbd-448f-a9db-b6344541c926
2024-03-12 07:03:52,740 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:52,740 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:52,745 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:52,746 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35595
2024-03-12 07:03:52,746 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35595
2024-03-12 07:03:52,746 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43249
2024-03-12 07:03:52,746 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:03:52,746 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:52,746 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:03:52,746 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:03:52,746 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qs9j25z7
2024-03-12 07:03:52,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-937552f9-19a3-49b2-8f91-b53001aa0878
2024-03-12 07:03:52,746 - distributed.worker - INFO - Starting Worker plugin PreImport-8ad33d88-ecbb-4f70-9d9a-00b2d2d569a7
2024-03-12 07:03:52,746 - distributed.worker - INFO - Starting Worker plugin RMMSetup-179506dc-7c61-4058-9484-fba3c9db4d49
2024-03-12 07:03:52,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:52,761 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:52,765 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:52,766 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44781
2024-03-12 07:03:52,766 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44781
2024-03-12 07:03:52,766 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39623
2024-03-12 07:03:52,766 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:03:52,766 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:52,766 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:03:52,766 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:03:52,766 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k92qfjyf
2024-03-12 07:03:52,767 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d2cb7c3-f5b5-4591-9d54-21dbfdbb107e
2024-03-12 07:03:52,770 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:52,770 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:52,775 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:52,775 - distributed.worker - INFO - Starting Worker plugin PreImport-16dbc511-31f4-41f8-a1a8-e345166db604
2024-03-12 07:03:52,776 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33477
2024-03-12 07:03:52,776 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33477
2024-03-12 07:03:52,776 - distributed.worker - INFO - Starting Worker plugin RMMSetup-020ac9c0-59bb-4e87-97f0-294e9a8f192b
2024-03-12 07:03:52,776 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34767
2024-03-12 07:03:52,776 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:03:52,776 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:52,776 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:03:52,776 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:03:52,776 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ezsdune6
2024-03-12 07:03:52,776 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef198faa-3d31-42f0-8ee3-82f6a2a9d694
2024-03-12 07:03:52,789 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:52,789 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:52,793 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:52,794 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45867
2024-03-12 07:03:52,794 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45867
2024-03-12 07:03:52,794 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43161
2024-03-12 07:03:52,794 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:03:52,794 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:52,794 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:03:52,795 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:03:52,795 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rhp_nkpm
2024-03-12 07:03:52,795 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-084516b2-d4bf-45da-8fb0-ea583966f3a2
2024-03-12 07:03:52,796 - distributed.worker - INFO - Starting Worker plugin PreImport-b491734d-0a66-47f3-8a5e-2ebae0b7d5ef
2024-03-12 07:03:52,796 - distributed.worker - INFO - Starting Worker plugin RMMSetup-718ba98c-7e0c-4f97-ab3a-a5f7ed5988be
2024-03-12 07:03:52,817 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:52,817 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:52,822 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:52,823 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32783
2024-03-12 07:03:52,823 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32783
2024-03-12 07:03:52,823 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37585
2024-03-12 07:03:52,823 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:03:52,823 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:52,823 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:03:52,823 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:03:52,823 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dskgl4iu
2024-03-12 07:03:52,823 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea59a736-8861-4cf4-8e98-5c0320fe2426
2024-03-12 07:03:52,826 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:03:52,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:03:52,831 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:03:52,832 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41615
2024-03-12 07:03:52,832 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41615
2024-03-12 07:03:52,832 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38453
2024-03-12 07:03:52,832 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:03:52,832 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:52,832 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:03:52,832 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:03:52,832 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hyfpf_9m
2024-03-12 07:03:52,832 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2d6a24b-c0fd-4253-a6f3-51a8bde5b319
2024-03-12 07:03:54,719 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,752 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,753 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38289', status: init, memory: 0, processing: 0>
2024-03-12 07:03:54,754 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38289
2024-03-12 07:03:54,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59636
2024-03-12 07:03:54,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:54,756 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:03:54,756 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,758 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:03:54,775 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35595', status: init, memory: 0, processing: 0>
2024-03-12 07:03:54,775 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35595
2024-03-12 07:03:54,775 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59640
2024-03-12 07:03:54,776 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:54,777 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:03:54,777 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:03:54,821 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,855 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37663', status: init, memory: 0, processing: 0>
2024-03-12 07:03:54,856 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37663
2024-03-12 07:03:54,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59644
2024-03-12 07:03:54,857 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:54,858 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:03:54,859 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:03:54,865 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,897 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44781', status: init, memory: 0, processing: 0>
2024-03-12 07:03:54,897 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44781
2024-03-12 07:03:54,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59646
2024-03-12 07:03:54,898 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:54,900 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:03:54,900 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,901 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:03:54,917 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-785a27e4-5f39-4101-83c1-4798783a080c
2024-03-12 07:03:54,918 - distributed.worker - INFO - Starting Worker plugin PreImport-ff29f21f-9965-4361-90d5-419ad61992a5
2024-03-12 07:03:54,918 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,940 - distributed.worker - INFO - Starting Worker plugin PreImport-ce094511-246e-46e3-91e5-d6e3917997b4
2024-03-12 07:03:54,941 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fc4fe4cd-becf-4eda-bfa8-0970f568c2b7
2024-03-12 07:03:54,941 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,942 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33477', status: init, memory: 0, processing: 0>
2024-03-12 07:03:54,942 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33477
2024-03-12 07:03:54,943 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59650
2024-03-12 07:03:54,943 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:54,944 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:03:54,944 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,946 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:03:54,950 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,957 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dfb12379-69db-436f-96cb-6637f18a2da3
2024-03-12 07:03:54,957 - distributed.worker - INFO - Starting Worker plugin PreImport-8b9e529b-7f63-4b81-aa62-c2cc77f9c5af
2024-03-12 07:03:54,958 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,964 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32783', status: init, memory: 0, processing: 0>
2024-03-12 07:03:54,965 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32783
2024-03-12 07:03:54,966 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59660
2024-03-12 07:03:54,966 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:54,967 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:03:54,967 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:03:54,982 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41615', status: init, memory: 0, processing: 0>
2024-03-12 07:03:54,983 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41615
2024-03-12 07:03:54,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59682
2024-03-12 07:03:54,984 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45867', status: init, memory: 0, processing: 0>
2024-03-12 07:03:54,984 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:54,984 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45867
2024-03-12 07:03:54,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59668
2024-03-12 07:03:54,985 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:03:54,985 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,985 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:03:54,986 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:03:54,986 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:03:54,986 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:03:54,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:03:55,181 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:03:55,181 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:03:55,181 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:03:55,182 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:03:55,182 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:03:55,182 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:03:55,182 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:03:55,182 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:03:55,187 - distributed.scheduler - INFO - Remove client Client-acdad94c-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:03:55,187 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59618; closing.
2024-03-12 07:03:55,188 - distributed.scheduler - INFO - Remove client Client-acdad94c-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:03:55,188 - distributed.scheduler - INFO - Close client connection: Client-acdad94c-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:03:55,189 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40875'. Reason: nanny-close
2024-03-12 07:03:55,190 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:55,190 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35787'. Reason: nanny-close
2024-03-12 07:03:55,191 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:55,191 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37419'. Reason: nanny-close
2024-03-12 07:03:55,192 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38289. Reason: nanny-close
2024-03-12 07:03:55,192 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:55,192 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41909'. Reason: nanny-close
2024-03-12 07:03:55,192 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37663. Reason: nanny-close
2024-03-12 07:03:55,192 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:55,192 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36179'. Reason: nanny-close
2024-03-12 07:03:55,192 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35595. Reason: nanny-close
2024-03-12 07:03:55,193 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:55,193 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35311'. Reason: nanny-close
2024-03-12 07:03:55,193 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32783. Reason: nanny-close
2024-03-12 07:03:55,193 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:55,193 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35601'. Reason: nanny-close
2024-03-12 07:03:55,194 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45867. Reason: nanny-close
2024-03-12 07:03:55,194 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:55,194 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35457'. Reason: nanny-close
2024-03-12 07:03:55,194 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:03:55,194 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:03:55,194 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44781. Reason: nanny-close
2024-03-12 07:03:55,194 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59636; closing.
2024-03-12 07:03:55,194 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:03:55,194 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41615. Reason: nanny-close
2024-03-12 07:03:55,195 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:03:55,195 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38289', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227035.1950183')
2024-03-12 07:03:55,195 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:03:55,195 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33477. Reason: nanny-close
2024-03-12 07:03:55,196 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59640; closing.
2024-03-12 07:03:55,196 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:55,196 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:55,196 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:55,196 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:03:55,196 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:03:55,197 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:55,197 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:03:55,197 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35595', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227035.1975076')
2024-03-12 07:03:55,197 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:03:55,197 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59644; closing.
2024-03-12 07:03:55,198 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59660; closing.
2024-03-12 07:03:55,198 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:55,198 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:55,199 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:55,199 - distributed.nanny - INFO - Worker closed
2024-03-12 07:03:55,199 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37663', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227035.1992357')
2024-03-12 07:03:55,199 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32783', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227035.1996768')
2024-03-12 07:03:55,200 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59668; closing.
2024-03-12 07:03:55,200 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59682; closing.
2024-03-12 07:03:55,200 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59646; closing.
2024-03-12 07:03:55,201 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227035.200999')
2024-03-12 07:03:55,201 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41615', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227035.2013657')
2024-03-12 07:03:55,201 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44781', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227035.201667')
2024-03-12 07:03:55,202 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59650; closing.
2024-03-12 07:03:55,202 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227035.2024832')
2024-03-12 07:03:55,202 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:03:56,055 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:03:56,056 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:03:56,056 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:03:56,057 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:03:56,058 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-03-12 07:03:58,257 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:03:58,262 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-12 07:03:58,266 - distributed.scheduler - INFO - State start
2024-03-12 07:03:58,287 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:03:58,288 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:03:58,288 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-12 07:03:58,289 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:03:58,363 - distributed.scheduler - INFO - Receive client connection: Client-b17411d3-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:03:58,373 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59780
2024-03-12 07:03:58,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43735'
2024-03-12 07:03:58,475 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45745'
2024-03-12 07:03:58,484 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34395'
2024-03-12 07:03:58,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44153'
2024-03-12 07:03:58,501 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43597'
2024-03-12 07:03:58,509 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37137'
2024-03-12 07:03:58,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46819'
2024-03-12 07:03:58,528 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33769'
2024-03-12 07:04:00,253 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:00,253 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:00,257 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:00,258 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38881
2024-03-12 07:04:00,258 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38881
2024-03-12 07:04:00,258 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41025
2024-03-12 07:04:00,258 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:00,258 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,258 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:00,259 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:00,259 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5oco7tta
2024-03-12 07:04:00,259 - distributed.worker - INFO - Starting Worker plugin PreImport-9097403c-bc68-4172-832e-73f9298c26ba
2024-03-12 07:04:00,259 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2884f871-1843-4944-becf-ed8c2cf86c3b
2024-03-12 07:04:00,259 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c473ecf-b290-4626-bca3-d62b4d1613d3
2024-03-12 07:04:00,271 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:00,271 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:00,276 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:00,277 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36771
2024-03-12 07:04:00,277 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36771
2024-03-12 07:04:00,277 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37809
2024-03-12 07:04:00,277 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:00,277 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,277 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:00,277 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:00,277 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ky1agolk
2024-03-12 07:04:00,277 - distributed.worker - INFO - Starting Worker plugin PreImport-bc37f1e5-29b9-4665-aaee-6495a400b314
2024-03-12 07:04:00,278 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc4dc657-867a-469f-9b7c-fb7df2be8241
2024-03-12 07:04:00,278 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d78763ed-9a62-47ab-939c-992f01b401a3
2024-03-12 07:04:00,489 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:00,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:00,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:00,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:00,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:00,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:00,494 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:00,494 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:00,494 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:00,495 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44715
2024-03-12 07:04:00,495 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44715
2024-03-12 07:04:00,495 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46269
2024-03-12 07:04:00,495 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:00,495 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,495 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:00,495 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36025
2024-03-12 07:04:00,495 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:00,495 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36025
2024-03-12 07:04:00,495 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35929
2024-03-12 07:04:00,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nfxq4s8f
2024-03-12 07:04:00,495 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46533
2024-03-12 07:04:00,495 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35929
2024-03-12 07:04:00,495 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:00,495 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44679
2024-03-12 07:04:00,495 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,495 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:00,495 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:00,495 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,495 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f5ccee2-f8f5-4b67-aec1-5875b6f85bba
2024-03-12 07:04:00,495 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:00,495 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:00,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2nht9hbp
2024-03-12 07:04:00,495 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:00,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zkc_39s7
2024-03-12 07:04:00,496 - distributed.worker - INFO - Starting Worker plugin PreImport-6ad29eba-f83d-4018-8d17-35fc10677cb9
2024-03-12 07:04:00,496 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24d2ac4c-b8ea-4f85-b9d2-8e6efba8c074
2024-03-12 07:04:00,496 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-420747e1-5800-43c0-b914-e134c64f1c0f
2024-03-12 07:04:00,496 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f33308f0-2f8d-4119-8494-a8b0499dc2bf
2024-03-12 07:04:00,699 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:00,699 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:00,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:00,701 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:00,701 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:00,701 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:00,704 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:00,705 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35915
2024-03-12 07:04:00,705 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35915
2024-03-12 07:04:00,705 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41435
2024-03-12 07:04:00,705 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:00,705 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,705 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:00,705 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:00,705 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ptkjwji3
2024-03-12 07:04:00,705 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:00,705 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ba81075-7721-4ff0-96be-5180b1d2d27d
2024-03-12 07:04:00,706 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:00,706 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46665
2024-03-12 07:04:00,706 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46665
2024-03-12 07:04:00,706 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40755
2024-03-12 07:04:00,706 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:00,706 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,706 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:00,706 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:00,706 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nm1evb7v
2024-03-12 07:04:00,706 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32789
2024-03-12 07:04:00,707 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32789
2024-03-12 07:04:00,707 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33571
2024-03-12 07:04:00,707 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8156c684-5f6e-49c4-89d7-914b3564630d
2024-03-12 07:04:00,707 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:00,707 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,707 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:00,707 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:00,707 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o6jqbj2x
2024-03-12 07:04:00,707 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8b85b1b7-8dc5-4907-947a-909ee6f984c1
2024-03-12 07:04:00,797 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,819 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38881', status: init, memory: 0, processing: 0>
2024-03-12 07:04:00,820 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38881
2024-03-12 07:04:00,820 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33284
2024-03-12 07:04:00,821 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:00,822 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:00,822 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:00,892 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,912 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36771', status: init, memory: 0, processing: 0>
2024-03-12 07:04:00,913 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36771
2024-03-12 07:04:00,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33300
2024-03-12 07:04:00,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:00,915 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:00,915 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:00,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:02,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab122d56-5b8c-425a-9cc4-a757d314ded6
2024-03-12 07:04:02,172 - distributed.worker - INFO - Starting Worker plugin PreImport-46116e72-634c-42fd-9ab5-49ecd10ffd34
2024-03-12 07:04:02,173 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,205 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44715', status: init, memory: 0, processing: 0>
2024-03-12 07:04:02,206 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44715
2024-03-12 07:04:02,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33312
2024-03-12 07:04:02,208 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:02,209 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:02,209 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:02,273 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d8b87831-9655-496e-b5f5-7e54895b3fce
2024-03-12 07:04:02,274 - distributed.worker - INFO - Starting Worker plugin PreImport-4d559d98-2936-496a-a02f-5ae3cbd96d6f
2024-03-12 07:04:02,274 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,297 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35929', status: init, memory: 0, processing: 0>
2024-03-12 07:04:02,297 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35929
2024-03-12 07:04:02,297 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33322
2024-03-12 07:04:02,298 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:02,299 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:02,299 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,300 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:02,338 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22cdd9c2-8954-4a3c-b82e-f551e7176b72
2024-03-12 07:04:02,344 - distributed.worker - INFO - Starting Worker plugin PreImport-3184ed3a-ec66-4f9b-87da-32ddc77eb1bd
2024-03-12 07:04:02,345 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,360 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36025', status: init, memory: 0, processing: 0>
2024-03-12 07:04:02,361 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36025
2024-03-12 07:04:02,361 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33338
2024-03-12 07:04:02,362 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:02,362 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:02,362 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:02,376 - distributed.worker - INFO - Starting Worker plugin PreImport-3a8a5a70-800e-4752-8943-89380f91ca9e
2024-03-12 07:04:02,376 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1803cac3-00be-469d-aaf9-a934c0529e50
2024-03-12 07:04:02,376 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32789', status: init, memory: 0, processing: 0>
2024-03-12 07:04:02,376 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,377 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32789
2024-03-12 07:04:02,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33344
2024-03-12 07:04:02,378 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:02,380 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:02,380 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,382 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:02,390 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e38136e3-f9c8-4fe1-94bd-3bb0aaaa0748
2024-03-12 07:04:02,391 - distributed.worker - INFO - Starting Worker plugin PreImport-ebcfcfac-4f36-4c73-a9c1-d367d78bc738
2024-03-12 07:04:02,391 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,399 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35915', status: init, memory: 0, processing: 0>
2024-03-12 07:04:02,399 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35915
2024-03-12 07:04:02,400 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33346
2024-03-12 07:04:02,400 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:02,401 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:02,401 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:02,412 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46665', status: init, memory: 0, processing: 0>
2024-03-12 07:04:02,412 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46665
2024-03-12 07:04:02,412 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33348
2024-03-12 07:04:02,413 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:02,414 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:02,414 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:02,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:02,460 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:02,461 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:02,461 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:02,461 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:02,461 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:02,461 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:02,461 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:02,461 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:02,466 - distributed.scheduler - INFO - Remove client Client-b17411d3-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:02,466 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59780; closing.
2024-03-12 07:04:02,466 - distributed.scheduler - INFO - Remove client Client-b17411d3-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:02,467 - distributed.scheduler - INFO - Close client connection: Client-b17411d3-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:02,468 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43735'. Reason: nanny-close
2024-03-12 07:04:02,468 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:02,469 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45745'. Reason: nanny-close
2024-03-12 07:04:02,470 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:02,471 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36025. Reason: nanny-close
2024-03-12 07:04:02,471 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34395'. Reason: nanny-close
2024-03-12 07:04:02,471 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:02,471 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44153'. Reason: nanny-close
2024-03-12 07:04:02,471 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35929. Reason: nanny-close
2024-03-12 07:04:02,472 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:02,472 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43597'. Reason: nanny-close
2024-03-12 07:04:02,472 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44715. Reason: nanny-close
2024-03-12 07:04:02,472 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:02,472 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37137'. Reason: nanny-close
2024-03-12 07:04:02,472 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:02,472 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:02,472 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32789. Reason: nanny-close
2024-03-12 07:04:02,473 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33338; closing.
2024-03-12 07:04:02,473 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46819'. Reason: nanny-close
2024-03-12 07:04:02,473 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35915. Reason: nanny-close
2024-03-12 07:04:02,473 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:02,473 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36025', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227042.473261')
2024-03-12 07:04:02,473 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33769'. Reason: nanny-close
2024-03-12 07:04:02,473 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:02,473 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46665. Reason: nanny-close
2024-03-12 07:04:02,473 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:02,474 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:02,474 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38881. Reason: nanny-close
2024-03-12 07:04:02,474 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:02,475 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:02,475 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33322; closing.
2024-03-12 07:04:02,475 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:02,475 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36771. Reason: nanny-close
2024-03-12 07:04:02,475 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:02,476 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:02,476 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35929', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227042.4760451')
2024-03-12 07:04:02,476 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33312; closing.
2024-03-12 07:04:02,476 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:02,476 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:02,477 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:02,477 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227042.4771585')
2024-03-12 07:04:02,477 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:02,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33346; closing.
2024-03-12 07:04:02,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33344; closing.
2024-03-12 07:04:02,477 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:02,478 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:02,478 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:02,478 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35915', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227042.478648')
2024-03-12 07:04:02,479 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32789', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227042.4789789')
2024-03-12 07:04:02,479 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:02,479 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33348; closing.
2024-03-12 07:04:02,479 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33284; closing.
2024-03-12 07:04:02,480 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46665', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227042.4802322')
2024-03-12 07:04:02,480 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227042.4806437')
2024-03-12 07:04:02,481 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33300; closing.
2024-03-12 07:04:02,481 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36771', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227042.4815254')
2024-03-12 07:04:02,481 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:04:03,434 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:04:03,434 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:04:03,434 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:04:03,436 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:04:03,436 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-03-12 07:04:05,478 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:05,483 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-12 07:04:05,486 - distributed.scheduler - INFO - State start
2024-03-12 07:04:05,508 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:05,509 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:04:05,509 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-12 07:04:05,510 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:04:05,753 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33847'
2024-03-12 07:04:05,763 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33779'
2024-03-12 07:04:05,778 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38599'
2024-03-12 07:04:05,780 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37137'
2024-03-12 07:04:05,790 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43893'
2024-03-12 07:04:05,798 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44035'
2024-03-12 07:04:05,806 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44421'
2024-03-12 07:04:05,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42177'
2024-03-12 07:04:07,691 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:07,692 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:07,696 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:07,697 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39607
2024-03-12 07:04:07,697 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39607
2024-03-12 07:04:07,697 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40073
2024-03-12 07:04:07,697 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:07,697 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:07,697 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:07,697 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:07,697 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n8fy99we
2024-03-12 07:04:07,698 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d61e2309-754a-4f3a-9a93-13ebcfffbeba
2024-03-12 07:04:07,698 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7da99524-d767-4f48-9528-7168f0901eec
2024-03-12 07:04:07,709 - distributed.scheduler - INFO - Receive client connection: Client-b5cc3f23-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:07,721 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33518
2024-03-12 07:04:07,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:07,763 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:07,765 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:07,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:07,768 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:07,768 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39311
2024-03-12 07:04:07,769 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39311
2024-03-12 07:04:07,769 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39983
2024-03-12 07:04:07,769 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:07,769 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:07,769 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:07,769 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:07,769 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fl3p7fln
2024-03-12 07:04:07,769 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1ea6939b-1d0b-4095-92e9-1b42b2eca15b
2024-03-12 07:04:07,770 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:07,771 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39613
2024-03-12 07:04:07,771 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39613
2024-03-12 07:04:07,771 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41863
2024-03-12 07:04:07,771 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:07,771 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:07,771 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:07,771 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:07,771 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-42inkyom
2024-03-12 07:04:07,772 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-843837fb-b34c-4a4b-a26e-bd8dc72bc3bc
2024-03-12 07:04:07,774 - distributed.worker - INFO - Starting Worker plugin PreImport-4a7123d4-513e-4354-badd-bf2165f0bcb8
2024-03-12 07:04:07,774 - distributed.worker - INFO - Starting Worker plugin RMMSetup-366d11df-d450-4cb9-a529-15bce44bc39f
2024-03-12 07:04:07,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:07,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:07,781 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:07,781 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:07,781 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:07,782 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37813
2024-03-12 07:04:07,782 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37813
2024-03-12 07:04:07,782 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40925
2024-03-12 07:04:07,782 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:07,782 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:07,782 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:07,782 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:07,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-90eodbhu
2024-03-12 07:04:07,782 - distributed.worker - INFO - Starting Worker plugin RMMSetup-160cd394-5d24-4da9-be31-b82cde230cff
2024-03-12 07:04:07,786 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:07,787 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33877
2024-03-12 07:04:07,787 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33877
2024-03-12 07:04:07,787 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46705
2024-03-12 07:04:07,787 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:07,787 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:07,787 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:07,787 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:07,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wnymf296
2024-03-12 07:04:07,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7b11ed71-58d9-4ff1-a2d3-def487fe2d70
2024-03-12 07:04:07,788 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:07,788 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:07,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:07,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:07,792 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:07,793 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40857
2024-03-12 07:04:07,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40857
2024-03-12 07:04:07,793 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34209
2024-03-12 07:04:07,793 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:07,794 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:07,794 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:07,794 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:07,794 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mq7h1zz1
2024-03-12 07:04:07,794 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5541311f-0576-4494-89de-4be6c9b23fc7
2024-03-12 07:04:07,794 - distributed.worker - INFO - Starting Worker plugin PreImport-020a17c3-c7e8-4f30-b814-3f1b57d3c1dd
2024-03-12 07:04:07,794 - distributed.worker - INFO - Starting Worker plugin RMMSetup-99454618-0eba-418c-8e81-3ba7958af4ec
2024-03-12 07:04:07,796 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:07,796 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:07,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:07,797 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44525
2024-03-12 07:04:07,797 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44525
2024-03-12 07:04:07,797 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45387
2024-03-12 07:04:07,797 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:07,797 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:07,797 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:07,797 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:07,797 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c86dh8xt
2024-03-12 07:04:07,798 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9392e6b9-4a82-484d-b772-1d1d88992a5d
2024-03-12 07:04:07,801 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:07,802 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45145
2024-03-12 07:04:07,802 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45145
2024-03-12 07:04:07,802 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33595
2024-03-12 07:04:07,802 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:07,802 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:07,802 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:07,802 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:07,802 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vekrq4er
2024-03-12 07:04:07,802 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81577b49-3aeb-4d14-bb9e-59e4c38e7df4
2024-03-12 07:04:09,787 - distributed.worker - INFO - Starting Worker plugin PreImport-a4d6b73c-d48f-4947-abc6-b4b7e8bffd14
2024-03-12 07:04:09,789 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,820 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39607', status: init, memory: 0, processing: 0>
2024-03-12 07:04:09,821 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39607
2024-03-12 07:04:09,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33536
2024-03-12 07:04:09,823 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:09,824 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:09,824 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,826 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:09,830 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95e8382e-1654-448f-b349-216fc2335c72
2024-03-12 07:04:09,830 - distributed.worker - INFO - Starting Worker plugin PreImport-d4c45e5d-4f46-453c-a43f-fe439994fce9
2024-03-12 07:04:09,832 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,863 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39311', status: init, memory: 0, processing: 0>
2024-03-12 07:04:09,863 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39311
2024-03-12 07:04:09,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33546
2024-03-12 07:04:09,865 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:09,866 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:09,866 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:09,877 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4564d041-55d0-4447-b8f0-04181ba2fd02
2024-03-12 07:04:09,877 - distributed.worker - INFO - Starting Worker plugin PreImport-3c931640-3838-4e96-9a5e-0a46144d8ed7
2024-03-12 07:04:09,878 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,907 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80dc4277-b0b0-424c-88c6-b79501e94b85
2024-03-12 07:04:09,908 - distributed.worker - INFO - Starting Worker plugin PreImport-533eb5a1-d835-4bcb-978e-9d0c7f0a6ef6
2024-03-12 07:04:09,908 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,910 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a9551f1-226e-4e66-92ae-79015135baea
2024-03-12 07:04:09,910 - distributed.worker - INFO - Starting Worker plugin PreImport-5e885351-78f6-4ff5-9704-82d18f40071f
2024-03-12 07:04:09,911 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,916 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45145', status: init, memory: 0, processing: 0>
2024-03-12 07:04:09,916 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45145
2024-03-12 07:04:09,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33554
2024-03-12 07:04:09,918 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:09,919 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:09,919 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,921 - distributed.worker - INFO - Starting Worker plugin PreImport-9e2f9525-9fe4-49c7-8cc0-67960811e654
2024-03-12 07:04:09,921 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:09,921 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,921 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d2b592a-36f3-495c-b084-d438ea04b72e
2024-03-12 07:04:09,922 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,925 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,934 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37813', status: init, memory: 0, processing: 0>
2024-03-12 07:04:09,934 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37813
2024-03-12 07:04:09,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57820
2024-03-12 07:04:09,935 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33877', status: init, memory: 0, processing: 0>
2024-03-12 07:04:09,935 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:09,936 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33877
2024-03-12 07:04:09,936 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57834
2024-03-12 07:04:09,936 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:09,936 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,937 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:09,938 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:09,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:09,938 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,939 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:09,944 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44525', status: init, memory: 0, processing: 0>
2024-03-12 07:04:09,945 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44525
2024-03-12 07:04:09,945 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57840
2024-03-12 07:04:09,946 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40857', status: init, memory: 0, processing: 0>
2024-03-12 07:04:09,946 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:09,946 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40857
2024-03-12 07:04:09,946 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57838
2024-03-12 07:04:09,946 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:09,947 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,947 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:09,948 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:09,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:09,948 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,949 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:09,957 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39613', status: init, memory: 0, processing: 0>
2024-03-12 07:04:09,957 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39613
2024-03-12 07:04:09,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57854
2024-03-12 07:04:09,959 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:09,960 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:09,960 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:09,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:09,987 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:09,987 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:09,987 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:09,988 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:09,988 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:09,988 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:09,988 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:09,989 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:10,000 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:10,000 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:10,000 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:10,000 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:10,000 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:10,000 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:10,000 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:10,001 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:10,010 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:10,013 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:10,015 - distributed.scheduler - INFO - Remove client Client-b5cc3f23-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:10,016 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33518; closing.
2024-03-12 07:04:10,016 - distributed.scheduler - INFO - Remove client Client-b5cc3f23-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:10,016 - distributed.scheduler - INFO - Close client connection: Client-b5cc3f23-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:10,017 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33847'. Reason: nanny-close
2024-03-12 07:04:10,018 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:10,018 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33779'. Reason: nanny-close
2024-03-12 07:04:10,019 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:10,019 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38599'. Reason: nanny-close
2024-03-12 07:04:10,020 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39607. Reason: nanny-close
2024-03-12 07:04:10,020 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:10,020 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37137'. Reason: nanny-close
2024-03-12 07:04:10,020 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39613. Reason: nanny-close
2024-03-12 07:04:10,020 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:10,021 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43893'. Reason: nanny-close
2024-03-12 07:04:10,021 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40857. Reason: nanny-close
2024-03-12 07:04:10,021 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:10,021 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44035'. Reason: nanny-close
2024-03-12 07:04:10,021 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44525. Reason: nanny-close
2024-03-12 07:04:10,022 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:10,022 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44421'. Reason: nanny-close
2024-03-12 07:04:10,022 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45145. Reason: nanny-close
2024-03-12 07:04:10,022 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:10,022 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42177'. Reason: nanny-close
2024-03-12 07:04:10,023 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33536; closing.
2024-03-12 07:04:10,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:10,023 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:10,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:10,023 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39311. Reason: nanny-close
2024-03-12 07:04:10,023 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33877. Reason: nanny-close
2024-03-12 07:04:10,023 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39607', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227050.0233352')
2024-03-12 07:04:10,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:10,023 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:10,023 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37813. Reason: nanny-close
2024-03-12 07:04:10,023 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57838; closing.
2024-03-12 07:04:10,024 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40857', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227050.024423')
2024-03-12 07:04:10,024 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:10,025 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:10,025 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:10,025 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:10,025 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:10,025 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:10,025 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57840; closing.
2024-03-12 07:04:10,025 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:10,026 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57854; closing.
2024-03-12 07:04:10,026 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:10,027 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:10,027 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:10,027 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:10,026 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:57838>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-12 07:04:10,028 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44525', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227050.0287936')
2024-03-12 07:04:10,029 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39613', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227050.0292091')
2024-03-12 07:04:10,029 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33554; closing.
2024-03-12 07:04:10,030 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:10,030 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45145', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227050.0303454')
2024-03-12 07:04:10,030 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57834; closing.
2024-03-12 07:04:10,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57820; closing.
2024-03-12 07:04:10,031 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33546; closing.
2024-03-12 07:04:10,031 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33877', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227050.0316129')
2024-03-12 07:04:10,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37813', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227050.0320299')
2024-03-12 07:04:10,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39311', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227050.0323925')
2024-03-12 07:04:10,032 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:04:10,883 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:04:10,883 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:04:10,884 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:04:10,885 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:04:10,885 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-03-12 07:04:12,985 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:12,989 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-12 07:04:12,993 - distributed.scheduler - INFO - State start
2024-03-12 07:04:13,025 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:13,026 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:04:13,026 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-12 07:04:13,027 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:04:13,143 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44207'
2024-03-12 07:04:13,153 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39117'
2024-03-12 07:04:13,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41817'
2024-03-12 07:04:13,169 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35203'
2024-03-12 07:04:13,178 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37603'
2024-03-12 07:04:13,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46787'
2024-03-12 07:04:13,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44139'
2024-03-12 07:04:13,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35725'
2024-03-12 07:04:13,313 - distributed.scheduler - INFO - Receive client connection: Client-ba3dc9a1-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:13,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58018
2024-03-12 07:04:15,029 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:15,029 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:15,034 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:15,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35653
2024-03-12 07:04:15,035 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35653
2024-03-12 07:04:15,035 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36221
2024-03-12 07:04:15,035 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:15,035 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:15,035 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:15,035 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:15,035 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qstjop7e
2024-03-12 07:04:15,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d36e294-8a93-4d5b-a330-419dd3801caf
2024-03-12 07:04:15,285 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:15,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:15,290 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:15,290 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:15,290 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:15,291 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34807
2024-03-12 07:04:15,291 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34807
2024-03-12 07:04:15,291 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36585
2024-03-12 07:04:15,291 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:15,291 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:15,291 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:15,291 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:15,291 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dx8fjvu5
2024-03-12 07:04:15,291 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2dd25f65-555e-4c89-9ef5-78c0f6ef0e15
2024-03-12 07:04:15,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:15,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:15,297 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:15,298 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37605
2024-03-12 07:04:15,298 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37605
2024-03-12 07:04:15,298 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44537
2024-03-12 07:04:15,298 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:15,298 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:15,298 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:15,298 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:15,299 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-26c20295
2024-03-12 07:04:15,299 - distributed.worker - INFO - Starting Worker plugin PreImport-7eb7ec44-b22f-4281-86ae-a3b8400061a5
2024-03-12 07:04:15,299 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98a08433-c521-4d9f-9839-419eeaaa422c
2024-03-12 07:04:15,300 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8297a87-c915-493e-88ff-130c9a9636b1
2024-03-12 07:04:15,300 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:15,301 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40019
2024-03-12 07:04:15,301 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40019
2024-03-12 07:04:15,301 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43749
2024-03-12 07:04:15,301 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:15,301 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:15,302 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:15,302 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:15,301 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:15,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3tnubzmf
2024-03-12 07:04:15,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:15,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:15,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:15,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:15,302 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:15,302 - distributed.worker - INFO - Starting Worker plugin PreImport-7eb2a82a-3a55-4fce-b7d5-527bb59c2978
2024-03-12 07:04:15,302 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9bb79a60-f14c-46c7-b531-01c8c525b7fa
2024-03-12 07:04:15,303 - distributed.worker - INFO - Starting Worker plugin RMMSetup-678290b6-ef94-4a38-a1fa-02dd7958c3c5
2024-03-12 07:04:15,305 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:15,305 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:15,308 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:15,308 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:15,308 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:15,309 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35253
2024-03-12 07:04:15,309 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44509
2024-03-12 07:04:15,309 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44509
2024-03-12 07:04:15,309 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35253
2024-03-12 07:04:15,309 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39381
2024-03-12 07:04:15,309 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39909
2024-03-12 07:04:15,309 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45839
2024-03-12 07:04:15,309 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:15,309 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:15,309 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45839
2024-03-12 07:04:15,309 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:15,309 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:15,309 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:15,309 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:15,309 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39389
2024-03-12 07:04:15,310 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:15,310 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:15,310 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:15,310 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:15,310 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z3rvkh3x
2024-03-12 07:04:15,310 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zt5r2kjx
2024-03-12 07:04:15,310 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:15,310 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:15,310 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lgg4qy1h
2024-03-12 07:04:15,310 - distributed.worker - INFO - Starting Worker plugin RMMSetup-923409d5-6c38-4e40-a820-eea0ea3284af
2024-03-12 07:04:15,310 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f956cd35-f05b-4cbf-8677-25bb97ff0f04
2024-03-12 07:04:15,310 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2c25c27-d8d0-48b5-80f4-08876abc6acd
2024-03-12 07:04:15,320 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:15,323 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41659
2024-03-12 07:04:15,323 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41659
2024-03-12 07:04:15,323 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34865
2024-03-12 07:04:15,323 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:15,323 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:15,323 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:15,324 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:15,324 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0mtha_0i
2024-03-12 07:04:15,325 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8abfc09c-c362-4df5-9f9b-e11d4fcf0794
2024-03-12 07:04:15,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f3fe44f3-c182-4042-931c-69c2a7d91bbb
2024-03-12 07:04:15,575 - distributed.worker - INFO - Starting Worker plugin PreImport-0271cf1c-add7-4e69-83fe-0e56442ed695
2024-03-12 07:04:15,576 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:15,601 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35653', status: init, memory: 0, processing: 0>
2024-03-12 07:04:15,602 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35653
2024-03-12 07:04:15,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58036
2024-03-12 07:04:15,603 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:15,604 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:15,604 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:15,606 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:17,293 - distributed.worker - INFO - Starting Worker plugin PreImport-8850b369-c03f-41ce-a165-c6a1dd9250b3
2024-03-12 07:04:17,293 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-48c9d6d4-40b7-48a4-8b80-67fb52224fd0
2024-03-12 07:04:17,294 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,316 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34807', status: init, memory: 0, processing: 0>
2024-03-12 07:04:17,317 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34807
2024-03-12 07:04:17,317 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58042
2024-03-12 07:04:17,318 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:17,318 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:17,318 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,320 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:17,394 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,401 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-558278ce-b33d-4733-a2f5-82e066e4bcea
2024-03-12 07:04:17,402 - distributed.worker - INFO - Starting Worker plugin PreImport-9383d93e-ea0b-4e7a-bc3e-a49606662037
2024-03-12 07:04:17,403 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,409 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55e2554f-c24a-4fa7-beaf-5737cce34b67
2024-03-12 07:04:17,409 - distributed.worker - INFO - Starting Worker plugin PreImport-c7f1a24c-32ec-4e39-8898-b457a0127c47
2024-03-12 07:04:17,409 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,424 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40019', status: init, memory: 0, processing: 0>
2024-03-12 07:04:17,424 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40019
2024-03-12 07:04:17,424 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58044
2024-03-12 07:04:17,425 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,426 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:17,427 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:17,427 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,428 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35253', status: init, memory: 0, processing: 0>
2024-03-12 07:04:17,428 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35253
2024-03-12 07:04:17,428 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58060
2024-03-12 07:04:17,429 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:17,429 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:17,430 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:17,430 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,431 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:17,439 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9897cc8a-6ec6-411f-83d9-750580d3a6dc
2024-03-12 07:04:17,442 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-826e61f0-99cf-4721-b338-f6c4d7a6febc
2024-03-12 07:04:17,443 - distributed.worker - INFO - Starting Worker plugin PreImport-af3ce174-dca1-4ffd-8f2d-db0db95fee6a
2024-03-12 07:04:17,443 - distributed.worker - INFO - Starting Worker plugin PreImport-2fafb296-a8dd-4f6a-9549-d6a406f1fad4
2024-03-12 07:04:17,443 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,447 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45839', status: init, memory: 0, processing: 0>
2024-03-12 07:04:17,448 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45839
2024-03-12 07:04:17,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58072
2024-03-12 07:04:17,447 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,450 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:17,451 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37605', status: init, memory: 0, processing: 0>
2024-03-12 07:04:17,451 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:17,451 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,452 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37605
2024-03-12 07:04:17,452 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58074
2024-03-12 07:04:17,453 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:17,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:17,454 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:17,454 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,456 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:17,473 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41659', status: init, memory: 0, processing: 0>
2024-03-12 07:04:17,474 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41659
2024-03-12 07:04:17,474 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58096
2024-03-12 07:04:17,475 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:17,476 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:17,477 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,478 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:17,481 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44509', status: init, memory: 0, processing: 0>
2024-03-12 07:04:17,481 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44509
2024-03-12 07:04:17,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58080
2024-03-12 07:04:17,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:17,484 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:17,484 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:17,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:17,499 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:17,499 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:17,500 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:17,500 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:17,500 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:17,501 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:17,501 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:17,502 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:17,513 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:17,513 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:17,513 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:17,513 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:17,513 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:17,513 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:17,513 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:17,514 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:04:17,521 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:17,523 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:17,525 - distributed.scheduler - INFO - Remove client Client-ba3dc9a1-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:17,525 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58018; closing.
2024-03-12 07:04:17,525 - distributed.scheduler - INFO - Remove client Client-ba3dc9a1-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:17,526 - distributed.scheduler - INFO - Close client connection: Client-ba3dc9a1-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:17,527 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44207'. Reason: nanny-close
2024-03-12 07:04:17,527 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:17,527 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39117'. Reason: nanny-close
2024-03-12 07:04:17,528 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:17,528 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41817'. Reason: nanny-close
2024-03-12 07:04:17,529 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37605. Reason: nanny-close
2024-03-12 07:04:17,529 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:17,529 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35203'. Reason: nanny-close
2024-03-12 07:04:17,529 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:17,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37603'. Reason: nanny-close
2024-03-12 07:04:17,530 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35653. Reason: nanny-close
2024-03-12 07:04:17,530 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40019. Reason: nanny-close
2024-03-12 07:04:17,530 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:17,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46787'. Reason: nanny-close
2024-03-12 07:04:17,530 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34807. Reason: nanny-close
2024-03-12 07:04:17,530 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44139'. Reason: nanny-close
2024-03-12 07:04:17,531 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:17,531 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35725'. Reason: nanny-close
2024-03-12 07:04:17,531 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:17,531 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58074; closing.
2024-03-12 07:04:17,531 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45839. Reason: nanny-close
2024-03-12 07:04:17,531 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:17,531 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37605', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227057.5318818')
2024-03-12 07:04:17,532 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41659. Reason: nanny-close
2024-03-12 07:04:17,532 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:17,532 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35253. Reason: nanny-close
2024-03-12 07:04:17,532 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:17,532 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:17,533 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:17,533 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:17,533 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:17,534 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58042; closing.
2024-03-12 07:04:17,534 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:17,534 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58036; closing.
2024-03-12 07:04:17,534 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58044; closing.
2024-03-12 07:04:17,534 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:17,534 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:17,535 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227057.5350606')
2024-03-12 07:04:17,535 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:17,535 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44509. Reason: nanny-close
2024-03-12 07:04:17,535 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35653', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227057.5354447')
2024-03-12 07:04:17,535 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:17,535 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40019', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227057.535814')
2024-03-12 07:04:17,536 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:17,536 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:17,536 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58072; closing.
2024-03-12 07:04:17,536 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58096; closing.
2024-03-12 07:04:17,537 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:17,537 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45839', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227057.5377996')
2024-03-12 07:04:17,538 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:17,538 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41659', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227057.5382316')
2024-03-12 07:04:17,538 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58060; closing.
2024-03-12 07:04:17,539 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35253', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227057.5391133')
2024-03-12 07:04:17,539 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:17,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58080; closing.
2024-03-12 07:04:17,539 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44509', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227057.5398784')
2024-03-12 07:04:17,540 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:04:18,442 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:04:18,443 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:04:18,443 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:04:18,445 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:04:18,445 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-03-12 07:04:20,577 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:20,581 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-12 07:04:20,584 - distributed.scheduler - INFO - State start
2024-03-12 07:04:20,609 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:20,610 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:04:20,611 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-12 07:04:20,611 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:04:20,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44275'
2024-03-12 07:04:20,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42483'
2024-03-12 07:04:20,776 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38133'
2024-03-12 07:04:20,785 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46669'
2024-03-12 07:04:20,801 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42763'
2024-03-12 07:04:20,805 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45411'
2024-03-12 07:04:20,815 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39297'
2024-03-12 07:04:20,826 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36077'
2024-03-12 07:04:20,923 - distributed.scheduler - INFO - Receive client connection: Client-bec59d04-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:20,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34474
2024-03-12 07:04:22,715 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:22,715 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:22,715 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:22,715 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:22,719 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:22,719 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:22,720 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44421
2024-03-12 07:04:22,720 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44421
2024-03-12 07:04:22,720 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36691
2024-03-12 07:04:22,720 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45895
2024-03-12 07:04:22,720 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:22,720 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36691
2024-03-12 07:04:22,720 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:22,720 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36333
2024-03-12 07:04:22,720 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:22,720 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:22,720 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:22,720 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:22,720 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m51n5uo7
2024-03-12 07:04:22,720 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:22,720 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:22,721 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-56u59peq
2024-03-12 07:04:22,721 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65f0f9b4-ba07-4d64-b652-15eefec7d647
2024-03-12 07:04:22,721 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7080c446-155c-4a64-87bf-fd6140141696
2024-03-12 07:04:22,916 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:22,916 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:22,922 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:22,924 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43251
2024-03-12 07:04:22,924 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43251
2024-03-12 07:04:22,924 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33333
2024-03-12 07:04:22,924 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:22,924 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:22,924 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:22,924 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:22,924 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d5irwhk9
2024-03-12 07:04:22,925 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55dc3c41-1961-4643-a391-ce58cf1885ba
2024-03-12 07:04:22,939 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:22,939 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:22,941 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:22,942 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:22,943 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:22,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:22,944 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:22,945 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34623
2024-03-12 07:04:22,945 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34623
2024-03-12 07:04:22,945 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43001
2024-03-12 07:04:22,945 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:22,945 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:22,945 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:22,945 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:22,945 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7r_7qe14
2024-03-12 07:04:22,946 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bae36e60-4479-4937-b819-b374955b4ae0
2024-03-12 07:04:22,946 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:22,946 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:22,946 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:22,947 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33381
2024-03-12 07:04:22,947 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33381
2024-03-12 07:04:22,947 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43301
2024-03-12 07:04:22,947 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:22,947 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:22,947 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:22,948 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:22,948 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7uwscxcr
2024-03-12 07:04:22,948 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46175e5d-17a2-49c2-abea-8ec438a077d4
2024-03-12 07:04:22,948 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:22,948 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:22,948 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:22,949 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42167
2024-03-12 07:04:22,949 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42167
2024-03-12 07:04:22,949 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39943
2024-03-12 07:04:22,949 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:22,949 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:22,949 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:22,949 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:22,949 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wywhotim
2024-03-12 07:04:22,949 - distributed.worker - INFO - Starting Worker plugin RMMSetup-faa85448-2090-4e2a-a2a8-6ef5c4e62cad
2024-03-12 07:04:22,951 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:22,952 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36805
2024-03-12 07:04:22,952 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36805
2024-03-12 07:04:22,952 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43035
2024-03-12 07:04:22,952 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:22,952 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:22,952 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:22,952 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:22,952 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h_p5bpdt
2024-03-12 07:04:22,952 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a58339b3-32fc-489b-92eb-4ebd81baec91
2024-03-12 07:04:22,952 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:22,953 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43663
2024-03-12 07:04:22,953 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43663
2024-03-12 07:04:22,953 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38067
2024-03-12 07:04:22,953 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:22,953 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:22,953 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:22,954 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:22,954 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7sogn722
2024-03-12 07:04:22,954 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5232925e-27a3-4572-a3ac-fce6d07d8125
2024-03-12 07:04:24,990 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a771c8cf-f763-4d70-82c7-ebb539767582
2024-03-12 07:04:24,990 - distributed.worker - INFO - Starting Worker plugin PreImport-2de3f3d5-86e9-484b-b82b-3855b51fcee7
2024-03-12 07:04:24,991 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,011 - distributed.worker - INFO - Starting Worker plugin PreImport-4261f837-667e-44d0-b659-c62d127c4114
2024-03-12 07:04:25,012 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b083713d-5163-478d-8577-cbbd4e4fa927
2024-03-12 07:04:25,012 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36691', status: init, memory: 0, processing: 0>
2024-03-12 07:04:25,013 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36691
2024-03-12 07:04:25,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34490
2024-03-12 07:04:25,013 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,014 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:25,014 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:25,014 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,016 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:25,049 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44421', status: init, memory: 0, processing: 0>
2024-03-12 07:04:25,050 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44421
2024-03-12 07:04:25,050 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34502
2024-03-12 07:04:25,051 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:25,053 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:25,053 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:25,143 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-55a2991d-3344-4778-a3b3-e5ecfadc5db1
2024-03-12 07:04:25,144 - distributed.worker - INFO - Starting Worker plugin PreImport-754d7bb7-5b83-48ad-860b-fe55818ee5a5
2024-03-12 07:04:25,145 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,150 - distributed.worker - INFO - Starting Worker plugin PreImport-fe875198-ffce-4b72-a60d-4df5785aa4da
2024-03-12 07:04:25,150 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-022a3d85-afc3-4553-8ddb-6b5137f5c513
2024-03-12 07:04:25,151 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,155 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d97a001-58a0-4a06-8783-99c3531ae20c
2024-03-12 07:04:25,156 - distributed.worker - INFO - Starting Worker plugin PreImport-620e0d93-2114-4825-8a56-a840eb9a505b
2024-03-12 07:04:25,157 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,167 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49a4c0e3-b99b-4d24-b734-56f37dc0f5a7
2024-03-12 07:04:25,168 - distributed.worker - INFO - Starting Worker plugin PreImport-0304e68f-d7db-4fd5-8971-0a21f07f7525
2024-03-12 07:04:25,168 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,168 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-90faf026-469c-4eb0-aa39-1e0349aab8f5
2024-03-12 07:04:25,169 - distributed.worker - INFO - Starting Worker plugin PreImport-b7018ec5-621e-4032-aa2d-acda18069f6e
2024-03-12 07:04:25,169 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,172 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36805', status: init, memory: 0, processing: 0>
2024-03-12 07:04:25,172 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36805
2024-03-12 07:04:25,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34522
2024-03-12 07:04:25,173 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7683928b-d08b-4413-8de8-d92d040b925c
2024-03-12 07:04:25,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:25,174 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:25,174 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,174 - distributed.worker - INFO - Starting Worker plugin PreImport-361ce073-6ba7-4a35-a1ed-64722cebfe8a
2024-03-12 07:04:25,175 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:25,175 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,181 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42167', status: init, memory: 0, processing: 0>
2024-03-12 07:04:25,182 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42167
2024-03-12 07:04:25,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34526
2024-03-12 07:04:25,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:25,183 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34623', status: init, memory: 0, processing: 0>
2024-03-12 07:04:25,184 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:25,184 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,184 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34623
2024-03-12 07:04:25,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34516
2024-03-12 07:04:25,185 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:25,185 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:25,187 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:25,187 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:25,190 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43663', status: init, memory: 0, processing: 0>
2024-03-12 07:04:25,190 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43663
2024-03-12 07:04:25,190 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34544
2024-03-12 07:04:25,191 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:25,191 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:25,191 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:25,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43251', status: init, memory: 0, processing: 0>
2024-03-12 07:04:25,208 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43251
2024-03-12 07:04:25,208 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34538
2024-03-12 07:04:25,210 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:25,211 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33381', status: init, memory: 0, processing: 0>
2024-03-12 07:04:25,211 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33381
2024-03-12 07:04:25,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34552
2024-03-12 07:04:25,211 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:25,211 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,213 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:25,213 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:25,214 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:25,214 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:25,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:25,292 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:25,292 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:25,292 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:25,293 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:25,293 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:25,293 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:25,293 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:25,294 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:25,297 - distributed.scheduler - INFO - Remove client Client-bec59d04-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:25,297 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34474; closing.
2024-03-12 07:04:25,298 - distributed.scheduler - INFO - Remove client Client-bec59d04-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:25,298 - distributed.scheduler - INFO - Close client connection: Client-bec59d04-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:25,299 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44275'. Reason: nanny-close
2024-03-12 07:04:25,299 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:25,300 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42483'. Reason: nanny-close
2024-03-12 07:04:25,300 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:25,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38133'. Reason: nanny-close
2024-03-12 07:04:25,301 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44421. Reason: nanny-close
2024-03-12 07:04:25,301 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:25,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46669'. Reason: nanny-close
2024-03-12 07:04:25,301 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:25,301 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43251. Reason: nanny-close
2024-03-12 07:04:25,302 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42763'. Reason: nanny-close
2024-03-12 07:04:25,302 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36691. Reason: nanny-close
2024-03-12 07:04:25,302 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:25,302 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45411'. Reason: nanny-close
2024-03-12 07:04:25,302 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36805. Reason: nanny-close
2024-03-12 07:04:25,302 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:25,303 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39297'. Reason: nanny-close
2024-03-12 07:04:25,303 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:25,303 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34623. Reason: nanny-close
2024-03-12 07:04:25,303 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36077'. Reason: nanny-close
2024-03-12 07:04:25,303 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:25,303 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33381. Reason: nanny-close
2024-03-12 07:04:25,303 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:25,303 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43663. Reason: nanny-close
2024-03-12 07:04:25,303 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:25,304 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34502; closing.
2024-03-12 07:04:25,304 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34490; closing.
2024-03-12 07:04:25,304 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:25,304 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44421', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227065.3044086')
2024-03-12 07:04:25,304 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:25,304 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42167. Reason: nanny-close
2024-03-12 07:04:25,305 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36691', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227065.3049443')
2024-03-12 07:04:25,305 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:25,305 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:25,305 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:25,305 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34522; closing.
2024-03-12 07:04:25,305 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:25,305 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:25,306 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34538; closing.
2024-03-12 07:04:25,306 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:25,306 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:25,306 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:25,307 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:25,307 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36805', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227065.3070245')
2024-03-12 07:04:25,307 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43251', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227065.307391')
2024-03-12 07:04:25,307 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:25,307 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:25,308 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:25,308 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34516; closing.
2024-03-12 07:04:25,309 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34544; closing.
2024-03-12 07:04:25,309 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34552; closing.
2024-03-12 07:04:25,309 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34526; closing.
2024-03-12 07:04:25,309 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34623', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227065.3096786')
2024-03-12 07:04:25,310 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43663', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227065.3100636')
2024-03-12 07:04:25,310 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33381', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227065.3104033')
2024-03-12 07:04:25,310 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42167', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227065.310709')
2024-03-12 07:04:25,310 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:04:26,165 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:04:26,165 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:04:26,165 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:04:26,166 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:04:26,167 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-03-12 07:04:28,031 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:28,034 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-12 07:04:28,037 - distributed.scheduler - INFO - State start
2024-03-12 07:04:28,060 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:28,060 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:04:28,061 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-12 07:04:28,061 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:04:28,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41117'
2024-03-12 07:04:29,582 - distributed.scheduler - INFO - Receive client connection: Client-c34fa844-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:29,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34620
2024-03-12 07:04:29,632 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:29,632 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:30,108 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:30,108 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33369
2024-03-12 07:04:30,108 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33369
2024-03-12 07:04:30,108 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-03-12 07:04:30,109 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:30,109 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:30,109 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:30,109 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-12 07:04:30,109 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rlevezh4
2024-03-12 07:04:30,109 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70f584b8-ba51-48b3-898a-69e8ac5df07b
2024-03-12 07:04:30,109 - distributed.worker - INFO - Starting Worker plugin PreImport-8ead78f6-92d5-4a0a-85e5-4714cb08245c
2024-03-12 07:04:30,109 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54724604-aea4-456d-b897-8eb4bc53437f
2024-03-12 07:04:30,109 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:30,188 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33369', status: init, memory: 0, processing: 0>
2024-03-12 07:04:30,189 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33369
2024-03-12 07:04:30,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42624
2024-03-12 07:04:30,190 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:30,191 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:30,191 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:30,192 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:30,206 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:30,209 - distributed.scheduler - INFO - Remove client Client-c34fa844-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:30,209 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34620; closing.
2024-03-12 07:04:30,209 - distributed.scheduler - INFO - Remove client Client-c34fa844-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:30,210 - distributed.scheduler - INFO - Close client connection: Client-c34fa844-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:30,211 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41117'. Reason: nanny-close
2024-03-12 07:04:30,211 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:30,212 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33369. Reason: nanny-close
2024-03-12 07:04:30,214 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:30,214 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42624; closing.
2024-03-12 07:04:30,215 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33369', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227070.2149386')
2024-03-12 07:04:30,215 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:04:30,215 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:30,926 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:04:30,926 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:04:30,926 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:04:30,927 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:04:30,928 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-03-12 07:04:34,968 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:34,972 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-12 07:04:34,977 - distributed.scheduler - INFO - State start
2024-03-12 07:04:34,999 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:35,000 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:04:35,000 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-12 07:04:35,001 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:04:35,196 - distributed.scheduler - INFO - Receive client connection: Client-c75b2f02-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:35,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42704
2024-03-12 07:04:35,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41755'
2024-03-12 07:04:36,887 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:36,887 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:37,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:37,438 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40321
2024-03-12 07:04:37,438 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40321
2024-03-12 07:04:37,439 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36901
2024-03-12 07:04:37,439 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:37,439 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:37,439 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:37,439 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-12 07:04:37,439 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dorjavs6
2024-03-12 07:04:37,440 - distributed.worker - INFO - Starting Worker plugin PreImport-a2b03f50-55a9-48d4-8093-5ce0a66e9ad6
2024-03-12 07:04:37,441 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-871852f7-1807-4860-860d-507da89171d3
2024-03-12 07:04:37,441 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f495654c-9b43-48c6-928d-7f1b210e75aa
2024-03-12 07:04:37,442 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:37,510 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40321', status: init, memory: 0, processing: 0>
2024-03-12 07:04:37,511 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40321
2024-03-12 07:04:37,511 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42724
2024-03-12 07:04:37,513 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:37,514 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:37,514 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:37,515 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:37,553 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:37,557 - distributed.scheduler - INFO - Remove client Client-c75b2f02-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:37,557 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42704; closing.
2024-03-12 07:04:37,557 - distributed.scheduler - INFO - Remove client Client-c75b2f02-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:37,558 - distributed.scheduler - INFO - Close client connection: Client-c75b2f02-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:37,559 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41755'. Reason: nanny-close
2024-03-12 07:04:37,559 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:37,561 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40321. Reason: nanny-close
2024-03-12 07:04:37,563 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42724; closing.
2024-03-12 07:04:37,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:37,564 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40321', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227077.5642061')
2024-03-12 07:04:37,564 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:04:37,566 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:38,274 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:04:38,274 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:04:38,275 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:04:38,276 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:04:38,276 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-03-12 07:04:40,489 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:40,494 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-12 07:04:40,498 - distributed.scheduler - INFO - State start
2024-03-12 07:04:40,520 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:40,521 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:04:40,522 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-12 07:04:40,522 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:04:43,222 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:45280'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45280>: Stream is closed
2024-03-12 07:04:43,502 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:04:43,503 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:04:43,503 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:04:43,504 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:04:43,504 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-03-12 07:04:45,670 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:45,675 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-12 07:04:45,678 - distributed.scheduler - INFO - State start
2024-03-12 07:04:45,699 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:45,700 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-12 07:04:45,701 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-12 07:04:45,701 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:04:45,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41727'
2024-03-12 07:04:47,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:47,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:47,531 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:47,531 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38641
2024-03-12 07:04:47,532 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38641
2024-03-12 07:04:47,532 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43249
2024-03-12 07:04:47,532 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-12 07:04:47,532 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:47,532 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:47,532 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-12 07:04:47,532 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-wf866f7w
2024-03-12 07:04:47,532 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee19183a-7c97-4d35-b39e-c40849ae875c
2024-03-12 07:04:47,532 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cfbf6356-4740-47b5-824c-6a3a2b6eaabf
2024-03-12 07:04:47,532 - distributed.worker - INFO - Starting Worker plugin PreImport-647045a8-f404-4797-9d6f-3b8d65212e1c
2024-03-12 07:04:47,532 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:47,577 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38641', status: init, memory: 0, processing: 0>
2024-03-12 07:04:47,590 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38641
2024-03-12 07:04:47,590 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48194
2024-03-12 07:04:47,590 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:47,591 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-12 07:04:47,591 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:47,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-12 07:04:48,339 - distributed.scheduler - INFO - Receive client connection: Client-cdbd5f6e-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:48,339 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48202
2024-03-12 07:04:48,347 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:48,352 - distributed.scheduler - INFO - Remove client Client-cdbd5f6e-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:48,352 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48202; closing.
2024-03-12 07:04:48,353 - distributed.scheduler - INFO - Remove client Client-cdbd5f6e-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:48,353 - distributed.scheduler - INFO - Close client connection: Client-cdbd5f6e-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:48,354 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41727'. Reason: nanny-close
2024-03-12 07:04:48,354 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:48,356 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38641. Reason: nanny-close
2024-03-12 07:04:48,357 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48194; closing.
2024-03-12 07:04:48,357 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-12 07:04:48,358 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38641', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227088.3579495')
2024-03-12 07:04:48,358 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:04:48,359 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:48,919 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:04:48,919 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:04:48,920 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:04:48,921 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-12 07:04:48,921 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-03-12 07:04:51,092 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:51,097 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42753 instead
  warnings.warn(
2024-03-12 07:04:51,100 - distributed.scheduler - INFO - State start
2024-03-12 07:04:51,121 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:04:51,122 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:04:51,123 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42753/status
2024-03-12 07:04:51,123 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:04:51,249 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41673'
2024-03-12 07:04:51,261 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38095'
2024-03-12 07:04:51,274 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43299'
2024-03-12 07:04:51,285 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39269'
2024-03-12 07:04:51,289 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41315'
2024-03-12 07:04:51,302 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43411'
2024-03-12 07:04:51,312 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35827'
2024-03-12 07:04:51,321 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41433'
2024-03-12 07:04:53,125 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:53,125 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:53,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:53,125 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:53,129 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:53,129 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:53,130 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42699
2024-03-12 07:04:53,130 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45437
2024-03-12 07:04:53,130 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42699
2024-03-12 07:04:53,130 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45437
2024-03-12 07:04:53,130 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45347
2024-03-12 07:04:53,130 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33337
2024-03-12 07:04:53,130 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:53,130 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:53,130 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:53,130 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:53,130 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:53,130 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:53,130 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:53,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-17dqi74z
2024-03-12 07:04:53,130 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:53,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-opaku31n
2024-03-12 07:04:53,131 - distributed.worker - INFO - Starting Worker plugin PreImport-70b65d56-0a6a-4d67-8152-91f7ca995537
2024-03-12 07:04:53,131 - distributed.worker - INFO - Starting Worker plugin PreImport-3505967a-0fba-4a29-acb5-858f4930a1ec
2024-03-12 07:04:53,131 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fdbaaf51-32bb-41e5-a9cd-acadaa4d9b10
2024-03-12 07:04:53,131 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-049d8603-8ca9-4259-81b0-5c646f8e30ce
2024-03-12 07:04:53,131 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c7ac01b0-9e12-479d-919f-d07a4795436d
2024-03-12 07:04:53,132 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8095bf8-eb72-4d3e-b04c-286ac4a313bc
2024-03-12 07:04:53,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:53,214 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:53,215 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:53,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:53,216 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:53,216 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:53,219 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:53,220 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:53,220 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38691
2024-03-12 07:04:53,220 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38691
2024-03-12 07:04:53,220 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45863
2024-03-12 07:04:53,220 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:53,220 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:53,220 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:53,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:53,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sjiddzzj
2024-03-12 07:04:53,220 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5b6b273-0c7a-44da-b12b-dd84c2cc8818
2024-03-12 07:04:53,220 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38125
2024-03-12 07:04:53,220 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38125
2024-03-12 07:04:53,220 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:53,220 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38461
2024-03-12 07:04:53,221 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:53,221 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:53,221 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:53,221 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:53,221 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-do2avwr9
2024-03-12 07:04:53,221 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d5e678c-283e-4795-88f4-73809273dbf4
2024-03-12 07:04:53,221 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39665
2024-03-12 07:04:53,221 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39665
2024-03-12 07:04:53,221 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41951
2024-03-12 07:04:53,222 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:53,222 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:53,222 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:53,222 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:53,222 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f5g5ef38
2024-03-12 07:04:53,222 - distributed.worker - INFO - Starting Worker plugin RMMSetup-33a2be9a-9704-47b7-891f-6b309079c701
2024-03-12 07:04:53,405 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:53,405 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:53,410 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:53,411 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34843
2024-03-12 07:04:53,411 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34843
2024-03-12 07:04:53,411 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43559
2024-03-12 07:04:53,411 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:53,411 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:53,411 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:53,411 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:53,411 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jzcinw7p
2024-03-12 07:04:53,411 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-38c59ac7-43cc-4ead-a896-7f9157ac570a
2024-03-12 07:04:53,411 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3837b852-640c-434b-abe6-96cd8ed782f0
2024-03-12 07:04:53,583 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:53,583 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:53,584 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:04:53,584 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:04:53,587 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:53,588 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44495
2024-03-12 07:04:53,589 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44495
2024-03-12 07:04:53,589 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34029
2024-03-12 07:04:53,589 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:53,589 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:53,589 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:53,589 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:53,589 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d8ah2_di
2024-03-12 07:04:53,589 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dff6bc8d-4c70-4f06-89dc-c81dc67c94e3
2024-03-12 07:04:53,592 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:04:53,593 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36987
2024-03-12 07:04:53,593 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36987
2024-03-12 07:04:53,593 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45417
2024-03-12 07:04:53,593 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:04:53,593 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:53,593 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:04:53,594 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-12 07:04:53,594 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fv7_xj2s
2024-03-12 07:04:53,594 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f5685a27-440e-4f87-981c-bebecb395500
2024-03-12 07:04:55,008 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,032 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,039 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45437', status: init, memory: 0, processing: 0>
2024-03-12 07:04:55,050 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45437
2024-03-12 07:04:55,050 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56752
2024-03-12 07:04:55,052 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:55,053 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:55,053 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:55,063 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42699', status: init, memory: 0, processing: 0>
2024-03-12 07:04:55,064 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42699
2024-03-12 07:04:55,064 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56768
2024-03-12 07:04:55,065 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:55,066 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:55,066 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:55,206 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-123f78bf-41cf-4008-832f-e5b017dedd6e
2024-03-12 07:04:55,207 - distributed.worker - INFO - Starting Worker plugin PreImport-14ae606a-8574-4087-9c8a-2ae51c031fb8
2024-03-12 07:04:55,209 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,239 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c30aaf8-0e67-4471-b7f2-4be4a4b2bce1
2024-03-12 07:04:55,240 - distributed.worker - INFO - Starting Worker plugin PreImport-776aea4c-f4df-4e6c-a2a9-2dbfed5d0489
2024-03-12 07:04:55,240 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,246 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39665', status: init, memory: 0, processing: 0>
2024-03-12 07:04:55,246 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39665
2024-03-12 07:04:55,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56776
2024-03-12 07:04:55,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:55,250 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:55,250 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,252 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:55,256 - distributed.worker - INFO - Starting Worker plugin PreImport-f4fe640f-3498-40b8-b368-64395c4edf34
2024-03-12 07:04:55,257 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,262 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38691', status: init, memory: 0, processing: 0>
2024-03-12 07:04:55,262 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38691
2024-03-12 07:04:55,262 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56778
2024-03-12 07:04:55,264 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:55,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:55,264 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:55,280 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34843', status: init, memory: 0, processing: 0>
2024-03-12 07:04:55,280 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34843
2024-03-12 07:04:55,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56792
2024-03-12 07:04:55,281 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:55,282 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:55,282 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,282 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-910c0b87-419a-4775-aa66-ecaa1866130b
2024-03-12 07:04:55,282 - distributed.worker - INFO - Starting Worker plugin PreImport-7a959c6e-e644-4401-a099-95a7683f440b
2024-03-12 07:04:55,283 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,284 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:55,285 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d885fea-b068-473c-8587-b28236913f9a
2024-03-12 07:04:55,286 - distributed.worker - INFO - Starting Worker plugin PreImport-5f33030f-0286-4ceb-b66e-f2d9ef33c59b
2024-03-12 07:04:55,288 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,305 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44495', status: init, memory: 0, processing: 0>
2024-03-12 07:04:55,305 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44495
2024-03-12 07:04:55,305 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56798
2024-03-12 07:04:55,306 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:55,307 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:55,307 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,308 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:55,314 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f310d98-e0f7-4c6d-8a6c-16926ca815e6
2024-03-12 07:04:55,315 - distributed.worker - INFO - Starting Worker plugin PreImport-a47a179c-ea39-490a-b210-662b746d4b21
2024-03-12 07:04:55,315 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,325 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38125', status: init, memory: 0, processing: 0>
2024-03-12 07:04:55,326 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38125
2024-03-12 07:04:55,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56802
2024-03-12 07:04:55,328 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:55,329 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:55,329 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:55,336 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36987', status: init, memory: 0, processing: 0>
2024-03-12 07:04:55,336 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36987
2024-03-12 07:04:55,337 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56814
2024-03-12 07:04:55,337 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:04:55,338 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:04:55,338 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:04:55,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:04:57,302 - distributed.scheduler - INFO - Receive client connection: Client-d0f1dc7a-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:57,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56890
2024-03-12 07:04:57,314 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:57,314 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:57,314 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:57,314 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:57,314 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:57,315 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:57,315 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:57,315 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-12 07:04:57,328 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:57,328 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:57,328 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:57,328 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:57,328 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:57,328 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:57,329 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:57,329 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:04:57,332 - distributed.scheduler - INFO - Remove client Client-d0f1dc7a-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:57,333 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56890; closing.
2024-03-12 07:04:57,333 - distributed.scheduler - INFO - Remove client Client-d0f1dc7a-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:57,334 - distributed.scheduler - INFO - Close client connection: Client-d0f1dc7a-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:04:57,334 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41673'. Reason: nanny-close
2024-03-12 07:04:57,335 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:57,335 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38095'. Reason: nanny-close
2024-03-12 07:04:57,337 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:57,338 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43299'. Reason: nanny-close
2024-03-12 07:04:57,338 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45437. Reason: nanny-close
2024-03-12 07:04:57,338 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:57,338 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39269'. Reason: nanny-close
2024-03-12 07:04:57,338 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42699. Reason: nanny-close
2024-03-12 07:04:57,338 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:57,339 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41315'. Reason: nanny-close
2024-03-12 07:04:57,339 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38691. Reason: nanny-close
2024-03-12 07:04:57,339 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:57,339 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43411'. Reason: nanny-close
2024-03-12 07:04:57,339 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34843. Reason: nanny-close
2024-03-12 07:04:57,339 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:57,340 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35827'. Reason: nanny-close
2024-03-12 07:04:57,340 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38125. Reason: nanny-close
2024-03-12 07:04:57,340 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:57,340 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:57,340 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56752; closing.
2024-03-12 07:04:57,340 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41433'. Reason: nanny-close
2024-03-12 07:04:57,340 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39665. Reason: nanny-close
2024-03-12 07:04:57,340 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:04:57,341 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45437', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227097.3409605')
2024-03-12 07:04:57,341 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:57,341 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44495. Reason: nanny-close
2024-03-12 07:04:57,341 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:57,341 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:57,342 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56792; closing.
2024-03-12 07:04:57,342 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:57,342 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:57,342 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:57,342 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:57,343 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:57,343 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:57,343 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:57,343 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34843', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227097.3435159')
2024-03-12 07:04:57,343 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36987. Reason: nanny-close
2024-03-12 07:04:57,343 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56768; closing.
2024-03-12 07:04:57,344 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56778; closing.
2024-03-12 07:04:57,344 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:57,344 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:57,345 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:57,345 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56792>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56792>: Stream is closed
2024-03-12 07:04:57,345 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:56878 remote=tcp://127.0.0.1:9369>: Stream is closed
2024-03-12 07:04:57,347 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42699', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227097.3473072')
2024-03-12 07:04:57,347 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:04:57,347 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38691', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227097.3478136')
2024-03-12 07:04:57,348 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56802; closing.
2024-03-12 07:04:57,348 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56776; closing.
2024-03-12 07:04:57,348 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56798; closing.
2024-03-12 07:04:57,349 - distributed.nanny - INFO - Worker closed
2024-03-12 07:04:57,349 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38125', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227097.3491488')
2024-03-12 07:04:57,349 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39665', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227097.3495257')
2024-03-12 07:04:57,350 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44495', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227097.3499317')
2024-03-12 07:04:57,351 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56814; closing.
2024-03-12 07:04:57,351 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36987', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227097.351607')
2024-03-12 07:04:57,351 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:04:57,352 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56814>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-12 07:04:58,250 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:04:58,250 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:04:58,251 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:04:58,252 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:04:58,252 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-03-12 07:05:00,430 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:05:00,435 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-12 07:05:00,438 - distributed.scheduler - INFO - State start
2024-03-12 07:05:00,462 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:05:00,463 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:05:00,464 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-12 07:05:00,464 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:05:00,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36069'
2024-03-12 07:05:00,739 - distributed.scheduler - INFO - Receive client connection: Client-d67c94ae-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:05:00,751 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59940
2024-03-12 07:05:02,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:05:02,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:05:02,328 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:05:02,329 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43837
2024-03-12 07:05:02,329 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43837
2024-03-12 07:05:02,329 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41825
2024-03-12 07:05:02,329 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:05:02,329 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:05:02,329 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:05:02,329 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-12 07:05:02,329 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jgnfwjlj
2024-03-12 07:05:02,330 - distributed.worker - INFO - Starting Worker plugin PreImport-326fbf75-1417-4274-b3fa-b98d3681c96e
2024-03-12 07:05:02,330 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65a19d85-f134-40dc-a1d6-4e1661767e85
2024-03-12 07:05:02,330 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f13ad02d-b4da-4764-b30e-d76c5f05d971
2024-03-12 07:05:02,642 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:05:02,734 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43837', status: init, memory: 0, processing: 0>
2024-03-12 07:05:02,736 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43837
2024-03-12 07:05:02,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59952
2024-03-12 07:05:02,737 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:05:02,738 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:05:02,738 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:05:02,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:05:02,748 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:05:02,753 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:05:02,754 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:05:02,757 - distributed.scheduler - INFO - Remove client Client-d67c94ae-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:05:02,758 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59940; closing.
2024-03-12 07:05:02,758 - distributed.scheduler - INFO - Remove client Client-d67c94ae-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:05:02,758 - distributed.scheduler - INFO - Close client connection: Client-d67c94ae-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:05:02,759 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36069'. Reason: nanny-close
2024-03-12 07:05:02,760 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:05:02,761 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43837. Reason: nanny-close
2024-03-12 07:05:02,763 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:05:02,763 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59952; closing.
2024-03-12 07:05:02,763 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43837', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227102.7638526')
2024-03-12 07:05:02,764 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:05:02,765 - distributed.nanny - INFO - Worker closed
2024-03-12 07:05:03,425 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:05:03,425 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:05:03,426 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:05:03,427 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:05:03,427 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-03-12 07:05:05,623 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:05:05,628 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-12 07:05:05,631 - distributed.scheduler - INFO - State start
2024-03-12 07:05:05,652 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-12 07:05:05,653 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-12 07:05:05,653 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-12 07:05:05,654 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-12 07:05:05,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40577'
2024-03-12 07:05:06,744 - distributed.scheduler - INFO - Receive client connection: Client-d9925c97-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:05:06,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60082
2024-03-12 07:05:07,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-12 07:05:07,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-12 07:05:07,341 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-12 07:05:07,342 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38051
2024-03-12 07:05:07,342 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38051
2024-03-12 07:05:07,342 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45545
2024-03-12 07:05:07,342 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-12 07:05:07,342 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:05:07,342 - distributed.worker - INFO -               Threads:                          1
2024-03-12 07:05:07,342 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-12 07:05:07,342 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rjmh4is9
2024-03-12 07:05:07,342 - distributed.worker - INFO - Starting Worker plugin PreImport-0989e08e-e00c-4571-9c74-f07959245307
2024-03-12 07:05:07,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-99b0d0df-5dfa-4fbf-8d2b-da071046a1ec
2024-03-12 07:05:07,343 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8cf427f-fbf6-4714-8d30-7b612bfb42f3
2024-03-12 07:05:07,623 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:05:07,699 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38051', status: init, memory: 0, processing: 0>
2024-03-12 07:05:07,700 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38051
2024-03-12 07:05:07,700 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60092
2024-03-12 07:05:07,701 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-12 07:05:07,702 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-12 07:05:07,702 - distributed.worker - INFO - -------------------------------------------------
2024-03-12 07:05:07,703 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-12 07:05:07,785 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-03-12 07:05:07,790 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-03-12 07:05:07,794 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:05:07,796 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-12 07:05:07,798 - distributed.scheduler - INFO - Remove client Client-d9925c97-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:05:07,799 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60082; closing.
2024-03-12 07:05:07,799 - distributed.scheduler - INFO - Remove client Client-d9925c97-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:05:07,799 - distributed.scheduler - INFO - Close client connection: Client-d9925c97-e03e-11ee-9029-d8c49764f6bb
2024-03-12 07:05:07,800 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40577'. Reason: nanny-close
2024-03-12 07:05:07,801 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-12 07:05:07,802 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38051. Reason: nanny-close
2024-03-12 07:05:07,804 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-12 07:05:07,804 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60092; closing.
2024-03-12 07:05:07,805 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38051', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1710227107.805074')
2024-03-12 07:05:07,805 - distributed.scheduler - INFO - Lost all workers
2024-03-12 07:05:07,805 - distributed.nanny - INFO - Worker closed
2024-03-12 07:05:08,466 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-12 07:05:08,466 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-12 07:05:08,467 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-12 07:05:08,468 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-12 07:05:08,468 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46099 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35247 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] SKIPPED (could ...)
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40659 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42717 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36783 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43225 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-683' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34151 instead
  warnings.warn(
2024-03-12 07:06:55,215 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-11:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 37, in _test_local_cluster
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42617 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42587 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46339 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43749 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45355 instead
  warnings.warn(
2024-03-12 07:10:08,730 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:45076 remote=tcp://127.0.0.1:41291>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33255 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44597 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37865 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39663 instead
  warnings.warn(
[1710227476.782332] [dgx13:62158:0]            sock.c:481  UCX  ERROR bind(fd=130 addr=0.0.0.0:36580) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41699 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42945 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33331 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44541 instead
  warnings.warn(
2024-03-12 07:12:55,747 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-25:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39613 instead
  warnings.warn(
2024-03-12 07:12:58,407 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-26:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38483 instead
  warnings.warn(
2024-03-12 07:13:01,166 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41813 instead
  warnings.warn(
2024-03-12 07:13:05,104 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34877 instead
  warnings.warn(
2024-03-12 07:13:08,818 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-29:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36745 instead
  warnings.warn(
2024-03-12 07:13:12,001 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-30:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41899 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43207 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] 2024-03-12 07:14:05,988 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-6bb7a130-cd18-43a7-9f5e-176fbd374e83
Function:  _run_coroutine_on_worker
args:      (84744365621294653009420071291786938015, <function shuffle_task at 0x7eff009c2430>, ('explicit-comms-shuffle-9982a4a7d2d746970fa3230b0f164d0e', {0: {('from_pandas-cb393ec4ea697ffa65fe88980b15e9d6', 0)}}, {0: {0}}, ['key'], 1, False, 1, 1))
kwargs:    {}
Exception: "RuntimeError('Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory')"

Process SpawnProcess-34:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 153, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 379, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/usr/src/dask-cuda/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/usr/src/dask-cuda/dask_cuda/explicit_comms/dataframe/shuffle.py", line 376, in shuffle_task
    partitions = create_partitions(
  File "/usr/src/dask-cuda/dask_cuda/explicit_comms/dataframe/shuffle.py", line 240, in create_partitions
    partition_dataframe(
  File "/usr/src/dask-cuda/dask_cuda/explicit_comms/dataframe/shuffle.py", line 192, in partition_dataframe
    df.partition_by_hash(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 4863, in partition_by_hash
    cols = [*self._index._columns, *self._columns]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/index.py", line 847, in _columns
    return self._as_int_index()._columns
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/index.py", line 558, in _as_int_index
    return cudf.Index._from_data(self._data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/index.py", line 345, in _data
    {self.name: self._values}
  File "/opt/conda/envs/gdf/lib/python3.9/functools.py", line 993, in __get__
    val = self.func(instance)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/index.py", line 308, in _values
    return column.as_column(self._range, dtype=self.dtype)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1763, in as_column
    as_device_scalar(arbitrary.start, dtype=cudf.dtype("int64")),
  File "scalar.pyx", line 344, in cudf._lib.scalar.as_device_scalar
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/scalar.py", line 156, in device_value
    self._device_value = cudf._lib.scalar.DeviceScalar(
  File "scalar.pyx", line 141, in cudf._lib.scalar.DeviceScalar.__init__
  File "table.pyx", line 107, in cudf._lib.pylibcudf.table.Table.from_arrow
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40777 instead
  warnings.warn(
2024-03-12 07:14:17,878 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-12 07:14:17,885 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:46681'.
2024-03-12 07:14:17,886 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:46681'. Shutting down.
2024-03-12 07:14:17,889 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f78b6d9f5b0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-12 07:14:19,500 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-12 07:14:19,508 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:40429'.
2024-03-12 07:14:19,509 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:40429'. Shutting down.
2024-03-12 07:14:19,512 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fb71529c5b0>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-12 07:14:19,892 - distributed.nanny - ERROR - Worker process died unexpectedly
2024-03-12 07:14:21,516 - distributed.nanny - ERROR - Worker process died unexpectedly
