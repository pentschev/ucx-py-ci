============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-10 06:33:01,324 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:01,329 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45595 instead
  warnings.warn(
2024-01-10 06:33:01,333 - distributed.scheduler - INFO - State start
2024-01-10 06:33:01,356 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:01,357 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-10 06:33:01,358 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45595/status
2024-01-10 06:33:01,358 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:33:01,465 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34697'
2024-01-10 06:33:01,487 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42237'
2024-01-10 06:33:01,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33737'
2024-01-10 06:33:01,499 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38791'
2024-01-10 06:33:01,952 - distributed.scheduler - INFO - Receive client connection: Client-19002010-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:01,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51892
2024-01-10 06:33:03,292 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:03,292 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:03,292 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:03,292 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:03,296 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:03,296 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:03,297 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35409
2024-01-10 06:33:03,297 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37931
2024-01-10 06:33:03,297 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35409
2024-01-10 06:33:03,297 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37931
2024-01-10 06:33:03,297 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40507
2024-01-10 06:33:03,297 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37417
2024-01-10 06:33:03,297 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:33:03,297 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:33:03,297 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,297 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,297 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:33:03,297 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:33:03,297 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:33:03,297 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:33:03,297 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-zzhae_nl
2024-01-10 06:33:03,297 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-9uvajwbk
2024-01-10 06:33:03,298 - distributed.worker - INFO - Starting Worker plugin RMMSetup-947d94c9-b477-4edf-b60e-ddac23f7a296
2024-01-10 06:33:03,298 - distributed.worker - INFO - Starting Worker plugin RMMSetup-00c13455-6cde-4230-b3bd-48f3120acb02
2024-01-10 06:33:03,298 - distributed.worker - INFO - Starting Worker plugin PreImport-015121d1-818c-4f0d-b819-884317a55e11
2024-01-10 06:33:03,298 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9bc7791-a4c0-43ff-982e-603b47843a08
2024-01-10 06:33:03,298 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ffc1e3c-3a97-4b64-a710-369fa1c89e4b
2024-01-10 06:33:03,299 - distributed.worker - INFO - Starting Worker plugin PreImport-dc2fdebe-eec7-413f-a193-835f2c6f13f6
2024-01-10 06:33:03,299 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,300 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:03,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:03,304 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:03,305 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39399
2024-01-10 06:33:03,305 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39399
2024-01-10 06:33:03,305 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45545
2024-01-10 06:33:03,305 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:33:03,305 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:03,305 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,305 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:03,305 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:33:03,306 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:33:03,306 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-c6s3p_fk
2024-01-10 06:33:03,306 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7306e075-89c4-48ec-833f-6ea3bc955d90
2024-01-10 06:33:03,306 - distributed.worker - INFO - Starting Worker plugin PreImport-60ff94f6-a87f-4384-bbd8-c3ff03a527c6
2024-01-10 06:33:03,306 - distributed.worker - INFO - Starting Worker plugin RMMSetup-221b1ee4-f589-48ec-b865-e358e289ec79
2024-01-10 06:33:03,307 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,309 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:03,310 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36093
2024-01-10 06:33:03,310 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36093
2024-01-10 06:33:03,310 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38451
2024-01-10 06:33:03,310 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:33:03,311 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,311 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:33:03,311 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:33:03,311 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-jgpxkuyg
2024-01-10 06:33:03,311 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fdfa8834-aa17-412a-8f53-982aad22525c
2024-01-10 06:33:03,315 - distributed.worker - INFO - Starting Worker plugin PreImport-6bd03620-9c05-4d51-915e-ac72e38f5b09
2024-01-10 06:33:03,316 - distributed.worker - INFO - Starting Worker plugin RMMSetup-595d5047-5ebf-4649-99e2-124ca928d9a8
2024-01-10 06:33:03,316 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,448 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35409', status: init, memory: 0, processing: 0>
2024-01-10 06:33:03,450 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35409
2024-01-10 06:33:03,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51908
2024-01-10 06:33:03,451 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:03,451 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:33:03,451 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:33:03,461 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37931', status: init, memory: 0, processing: 0>
2024-01-10 06:33:03,461 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37931
2024-01-10 06:33:03,461 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51898
2024-01-10 06:33:03,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:03,463 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:33:03,463 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:33:03,469 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39399', status: init, memory: 0, processing: 0>
2024-01-10 06:33:03,469 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39399
2024-01-10 06:33:03,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51910
2024-01-10 06:33:03,470 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:03,471 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:33:03,471 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,472 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:33:03,474 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36093', status: init, memory: 0, processing: 0>
2024-01-10 06:33:03,474 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36093
2024-01-10 06:33:03,474 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51916
2024-01-10 06:33:03,475 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:03,476 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:33:03,476 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:03,477 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:33:03,505 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:33:03,505 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:33:03,505 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:33:03,505 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:33:03,510 - distributed.scheduler - INFO - Remove client Client-19002010-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:03,511 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51892; closing.
2024-01-10 06:33:03,511 - distributed.scheduler - INFO - Remove client Client-19002010-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:03,511 - distributed.scheduler - INFO - Close client connection: Client-19002010-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:03,512 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34697'. Reason: nanny-close
2024-01-10 06:33:03,512 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:03,513 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42237'. Reason: nanny-close
2024-01-10 06:33:03,513 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:03,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33737'. Reason: nanny-close
2024-01-10 06:33:03,514 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37931. Reason: nanny-close
2024-01-10 06:33:03,514 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:03,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38791'. Reason: nanny-close
2024-01-10 06:33:03,514 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36093. Reason: nanny-close
2024-01-10 06:33:03,514 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:03,515 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35409. Reason: nanny-close
2024-01-10 06:33:03,515 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39399. Reason: nanny-close
2024-01-10 06:33:03,516 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:33:03,516 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:33:03,516 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51898; closing.
2024-01-10 06:33:03,516 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37931', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868383.516676')
2024-01-10 06:33:03,517 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:33:03,517 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51916; closing.
2024-01-10 06:33:03,517 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:33:03,517 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:03,517 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:03,518 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36093', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868383.517944')
2024-01-10 06:33:03,518 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51910; closing.
2024-01-10 06:33:03,518 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:03,518 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39399', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868383.5187953')
2024-01-10 06:33:03,518 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:03,519 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51908; closing.
2024-01-10 06:33:03,519 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:51916>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:33:03,520 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:51910>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:51910>: Stream is closed
2024-01-10 06:33:03,521 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35409', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868383.5213304')
2024-01-10 06:33:03,521 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:33:04,278 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:33:04,278 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:33:04,279 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:33:04,280 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-10 06:33:04,280 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-10 06:33:06,492 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:06,497 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:33:06,500 - distributed.scheduler - INFO - State start
2024-01-10 06:33:06,523 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:06,524 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:33:06,524 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:33:06,525 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:33:06,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41143'
2024-01-10 06:33:06,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33349'
2024-01-10 06:33:06,766 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37349'
2024-01-10 06:33:06,768 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42319'
2024-01-10 06:33:06,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44617'
2024-01-10 06:33:06,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46733'
2024-01-10 06:33:06,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33101'
2024-01-10 06:33:06,806 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37495'
2024-01-10 06:33:08,030 - distributed.scheduler - INFO - Receive client connection: Client-1c1ae6f8-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:08,047 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57850
2024-01-10 06:33:08,610 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:08,610 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:08,614 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:08,615 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33753
2024-01-10 06:33:08,615 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33753
2024-01-10 06:33:08,615 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39951
2024-01-10 06:33:08,616 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:08,616 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:08,616 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:08,616 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:08,616 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2fq__ub9
2024-01-10 06:33:08,616 - distributed.worker - INFO - Starting Worker plugin PreImport-f2432bc2-d143-4f6e-9ff4-63d14db1dcb9
2024-01-10 06:33:08,616 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32672146-526e-4efa-9059-a785e10f7bc8
2024-01-10 06:33:08,616 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4466b058-2529-42a0-b059-9429f2b0a4dc
2024-01-10 06:33:08,652 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:08,652 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:08,656 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:08,657 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32933
2024-01-10 06:33:08,657 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32933
2024-01-10 06:33:08,657 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43829
2024-01-10 06:33:08,657 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:08,657 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:08,657 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:08,657 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:08,657 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-75d37gou
2024-01-10 06:33:08,658 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae4bc890-3994-4b41-be74-4feb8a2d636d
2024-01-10 06:33:08,659 - distributed.worker - INFO - Starting Worker plugin PreImport-c57512ce-b326-4d33-b205-a5308ec066d2
2024-01-10 06:33:08,660 - distributed.worker - INFO - Starting Worker plugin RMMSetup-004b73ee-1f99-4f5d-b881-494228817816
2024-01-10 06:33:08,854 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:08,854 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:08,858 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:08,859 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34773
2024-01-10 06:33:08,859 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34773
2024-01-10 06:33:08,860 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42431
2024-01-10 06:33:08,860 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:08,860 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:08,860 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:08,860 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:08,860 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hix0w5ul
2024-01-10 06:33:08,860 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9b10b7c-65d6-4e28-8209-d4c785f59afd
2024-01-10 06:33:08,874 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:08,874 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:08,878 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:08,879 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40861
2024-01-10 06:33:08,879 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40861
2024-01-10 06:33:08,879 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35841
2024-01-10 06:33:08,879 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:08,879 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:08,879 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:08,879 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:08,879 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:08,880 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:08,880 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:08,881 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:08,881 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rdfyigz2
2024-01-10 06:33:08,881 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9bffb68b-007f-4403-a8dc-0127f7a0656a
2024-01-10 06:33:08,882 - distributed.worker - INFO - Starting Worker plugin PreImport-b2b2ae20-480d-4b4e-8732-178936b9379e
2024-01-10 06:33:08,882 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2932536-ea92-4af5-b0ac-8ff0299e971f
2024-01-10 06:33:08,884 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:08,885 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:08,885 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36077
2024-01-10 06:33:08,885 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36077
2024-01-10 06:33:08,885 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33905
2024-01-10 06:33:08,885 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:08,886 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:08,886 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:08,886 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:08,886 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4p62r_b5
2024-01-10 06:33:08,886 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6d6cc61d-1ece-4806-8ddf-216e9aecb5bc
2024-01-10 06:33:08,886 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42361
2024-01-10 06:33:08,886 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42361
2024-01-10 06:33:08,886 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41521
2024-01-10 06:33:08,886 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:08,886 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:08,886 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:08,886 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:08,886 - distributed.worker - INFO - Starting Worker plugin PreImport-f05084c6-6ec1-4e17-bf48-586e2c9576d9
2024-01-10 06:33:08,887 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4qiafn4w
2024-01-10 06:33:08,887 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87ef67ab-df13-4f20-8f4c-eea67f939bc6
2024-01-10 06:33:08,887 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d90a36ff-64c1-43dd-a390-bebc25316c40
2024-01-10 06:33:08,887 - distributed.worker - INFO - Starting Worker plugin PreImport-a5d7e3ec-9cce-4cf9-ba85-9364197ad8c7
2024-01-10 06:33:08,887 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2787b368-1005-4744-b00f-c7ab1093d8b7
2024-01-10 06:33:08,888 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:08,888 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:08,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:08,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:08,893 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:08,894 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38857
2024-01-10 06:33:08,894 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38857
2024-01-10 06:33:08,894 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38027
2024-01-10 06:33:08,894 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:08,894 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:08,894 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:08,894 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:08,894 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f202fzh_
2024-01-10 06:33:08,895 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b1516e6-b817-4147-ad13-6b81890b26d6
2024-01-10 06:33:08,895 - distributed.worker - INFO - Starting Worker plugin PreImport-173c5241-aa5d-4570-863e-9d023512496f
2024-01-10 06:33:08,895 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0006eac8-5cb9-4709-8601-5d24e903da24
2024-01-10 06:33:08,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:08,897 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34353
2024-01-10 06:33:08,898 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34353
2024-01-10 06:33:08,898 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33735
2024-01-10 06:33:08,898 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:08,898 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:08,898 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:08,898 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:08,898 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nnoyjzrq
2024-01-10 06:33:08,898 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50dbb8e4-f2de-4ffa-b275-d67ea2f398eb
2024-01-10 06:33:09,167 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:09,191 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33753', status: init, memory: 0, processing: 0>
2024-01-10 06:33:09,193 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33753
2024-01-10 06:33:09,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57880
2024-01-10 06:33:09,194 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:09,195 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:09,195 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:09,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:10,751 - distributed.worker - INFO - Starting Worker plugin PreImport-ce054a86-9fae-4c05-bbb7-fc70802c6c38
2024-01-10 06:33:10,752 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54e8bab2-0004-4a23-b63f-a29711fea568
2024-01-10 06:33:10,752 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,775 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,777 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34773', status: init, memory: 0, processing: 0>
2024-01-10 06:33:10,778 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34773
2024-01-10 06:33:10,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60350
2024-01-10 06:33:10,779 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:10,780 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:10,780 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:10,782 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,786 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,797 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,805 - distributed.worker - INFO - Starting Worker plugin PreImport-fd1b0a58-0028-4560-9582-783038b1c3ba
2024-01-10 06:33:10,806 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c892783-6a81-4202-bdab-d206233f113d
2024-01-10 06:33:10,806 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,806 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,809 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38857', status: init, memory: 0, processing: 0>
2024-01-10 06:33:10,810 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38857
2024-01-10 06:33:10,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60376
2024-01-10 06:33:10,811 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:10,811 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40861', status: init, memory: 0, processing: 0>
2024-01-10 06:33:10,812 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:10,812 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40861
2024-01-10 06:33:10,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60356
2024-01-10 06:33:10,812 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:10,814 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:10,815 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:10,815 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,816 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36077', status: init, memory: 0, processing: 0>
2024-01-10 06:33:10,816 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36077
2024-01-10 06:33:10,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60360
2024-01-10 06:33:10,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:10,818 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:10,819 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:10,819 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:10,830 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34353', status: init, memory: 0, processing: 0>
2024-01-10 06:33:10,831 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34353
2024-01-10 06:33:10,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60394
2024-01-10 06:33:10,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:10,832 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:10,832 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,832 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32933', status: init, memory: 0, processing: 0>
2024-01-10 06:33:10,833 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32933
2024-01-10 06:33:10,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60384
2024-01-10 06:33:10,834 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:10,834 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:10,835 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42361', status: init, memory: 0, processing: 0>
2024-01-10 06:33:10,836 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42361
2024-01-10 06:33:10,836 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60408
2024-01-10 06:33:10,836 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:10,836 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,837 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:10,838 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:10,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:10,838 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:10,839 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:10,935 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:10,935 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:10,935 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:10,935 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:10,935 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:10,936 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:10,936 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:10,936 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:10,940 - distributed.scheduler - INFO - Remove client Client-1c1ae6f8-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:10,941 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57850; closing.
2024-01-10 06:33:10,941 - distributed.scheduler - INFO - Remove client Client-1c1ae6f8-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:10,941 - distributed.scheduler - INFO - Close client connection: Client-1c1ae6f8-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:10,942 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41143'. Reason: nanny-close
2024-01-10 06:33:10,943 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:10,943 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33349'. Reason: nanny-close
2024-01-10 06:33:10,944 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:10,944 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37349'. Reason: nanny-close
2024-01-10 06:33:10,944 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33753. Reason: nanny-close
2024-01-10 06:33:10,944 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:10,944 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42319'. Reason: nanny-close
2024-01-10 06:33:10,945 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:10,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32933. Reason: nanny-close
2024-01-10 06:33:10,945 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44617'. Reason: nanny-close
2024-01-10 06:33:10,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34773. Reason: nanny-close
2024-01-10 06:33:10,945 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:10,945 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46733'. Reason: nanny-close
2024-01-10 06:33:10,945 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38857. Reason: nanny-close
2024-01-10 06:33:10,946 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:10,946 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:10,946 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33101'. Reason: nanny-close
2024-01-10 06:33:10,946 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36077. Reason: nanny-close
2024-01-10 06:33:10,946 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57880; closing.
2024-01-10 06:33:10,947 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:10,947 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33753', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868390.9472797')
2024-01-10 06:33:10,947 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37495'. Reason: nanny-close
2024-01-10 06:33:10,947 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40861. Reason: nanny-close
2024-01-10 06:33:10,947 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:10,947 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:10,947 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60384; closing.
2024-01-10 06:33:10,947 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:10,947 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34353. Reason: nanny-close
2024-01-10 06:33:10,948 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:10,948 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32933', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868390.9483752')
2024-01-10 06:33:10,948 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:10,949 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:10,949 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42361. Reason: nanny-close
2024-01-10 06:33:10,949 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:10,949 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60376; closing.
2024-01-10 06:33:10,949 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:10,949 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60350; closing.
2024-01-10 06:33:10,949 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:10,950 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:10,950 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:10,950 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:10,951 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:10,951 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:10,950 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60384>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:33:10,952 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38857', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868390.95205')
2024-01-10 06:33:10,952 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:10,952 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34773', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868390.9524367')
2024-01-10 06:33:10,952 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60360; closing.
2024-01-10 06:33:10,952 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:10,953 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868390.9534762')
2024-01-10 06:33:10,953 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60356; closing.
2024-01-10 06:33:10,954 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60394; closing.
2024-01-10 06:33:10,954 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40861', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868390.9544811')
2024-01-10 06:33:10,954 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34353', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868390.954933')
2024-01-10 06:33:10,955 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60408; closing.
2024-01-10 06:33:10,955 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42361', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868390.9556887')
2024-01-10 06:33:10,955 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:33:10,956 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:60408>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:33:11,959 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:33:11,960 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:33:11,960 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:33:11,961 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:33:11,962 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-10 06:33:14,213 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:14,218 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:33:14,221 - distributed.scheduler - INFO - State start
2024-01-10 06:33:14,559 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:14,561 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:33:14,562 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:33:14,563 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:33:14,702 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35883'
2024-01-10 06:33:14,717 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41859'
2024-01-10 06:33:14,726 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46757'
2024-01-10 06:33:14,742 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41291'
2024-01-10 06:33:14,745 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34825'
2024-01-10 06:33:14,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42877'
2024-01-10 06:33:14,762 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46569'
2024-01-10 06:33:14,772 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43993'
2024-01-10 06:33:16,592 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:16,592 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:16,592 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:16,592 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:16,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:16,596 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:16,597 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36643
2024-01-10 06:33:16,597 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41485
2024-01-10 06:33:16,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36643
2024-01-10 06:33:16,597 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41485
2024-01-10 06:33:16,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41767
2024-01-10 06:33:16,597 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35601
2024-01-10 06:33:16,597 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:16,597 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:16,597 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:16,598 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:16,598 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:16,598 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:16,598 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:16,598 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:16,598 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wy80t9k7
2024-01-10 06:33:16,598 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ic1ugjfd
2024-01-10 06:33:16,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f073a315-a5bb-4cec-81d3-439c98c1951c
2024-01-10 06:33:16,598 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c603c77-b37c-4d5e-b1e4-a3be910726db
2024-01-10 06:33:16,633 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:16,633 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:16,638 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:16,638 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36399
2024-01-10 06:33:16,638 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36399
2024-01-10 06:33:16,639 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36893
2024-01-10 06:33:16,639 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:16,639 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:16,639 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:16,639 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:16,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h0c9_u58
2024-01-10 06:33:16,639 - distributed.worker - INFO - Starting Worker plugin PreImport-3169db44-80ac-4dbd-9c62-5502aa163b3f
2024-01-10 06:33:16,639 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a97a509e-e228-4f9d-bf40-f24e5eb241b2
2024-01-10 06:33:16,639 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2bb2a0ef-f906-423a-b853-0ef854056a7b
2024-01-10 06:33:16,678 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:16,678 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:16,681 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:16,681 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:16,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:16,683 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42807
2024-01-10 06:33:16,683 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42807
2024-01-10 06:33:16,683 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39367
2024-01-10 06:33:16,683 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:16,683 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:16,683 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:16,683 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:16,683 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-esj9bkc0
2024-01-10 06:33:16,683 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8f9cc5ab-5296-4461-b782-fe15ba58a683
2024-01-10 06:33:16,685 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:16,686 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39251
2024-01-10 06:33:16,686 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39251
2024-01-10 06:33:16,686 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43605
2024-01-10 06:33:16,686 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:16,686 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:16,686 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:16,687 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:16,687 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_dbp9h68
2024-01-10 06:33:16,687 - distributed.worker - INFO - Starting Worker plugin PreImport-e2706567-fc09-40e5-b08f-365eb1a2ef24
2024-01-10 06:33:16,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f0719f8e-76ff-412c-a9af-196bdddf652a
2024-01-10 06:33:16,687 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee1e8b18-71e8-43a1-ac97-0f8ba7e4e854
2024-01-10 06:33:16,688 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:16,688 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:16,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:16,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:16,692 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:16,693 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:16,693 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44517
2024-01-10 06:33:16,693 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44517
2024-01-10 06:33:16,693 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45753
2024-01-10 06:33:16,693 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:16,694 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:16,694 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:16,694 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:16,694 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-joff9_dd
2024-01-10 06:33:16,694 - distributed.worker - INFO - Starting Worker plugin RMMSetup-91098a84-cc2d-47e8-95f7-8c3842d24e8b
2024-01-10 06:33:16,694 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40827
2024-01-10 06:33:16,694 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40827
2024-01-10 06:33:16,694 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46167
2024-01-10 06:33:16,694 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:16,694 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:16,694 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:16,695 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:16,695 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4kaay24w
2024-01-10 06:33:16,695 - distributed.worker - INFO - Starting Worker plugin RMMSetup-de9ced2e-9707-402e-99f8-992b707568ef
2024-01-10 06:33:17,060 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:17,060 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:17,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:17,067 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44003
2024-01-10 06:33:17,067 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44003
2024-01-10 06:33:17,067 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37131
2024-01-10 06:33:17,067 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:17,067 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:17,067 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:17,067 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:17,067 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m33e1ny0
2024-01-10 06:33:17,068 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9173916e-b804-4500-a200-892d677fe179
2024-01-10 06:33:18,204 - distributed.scheduler - INFO - Receive client connection: Client-20ba4492-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:18,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60596
2024-01-10 06:33:18,447 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af2ccab6-7f78-4a3e-ada1-e906de6e100e
2024-01-10 06:33:18,447 - distributed.worker - INFO - Starting Worker plugin PreImport-e4b42c50-f8bd-45f0-b13c-821de9c9f11c
2024-01-10 06:33:18,448 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,474 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea103d3f-0523-4476-9dc5-03db0c8296bd
2024-01-10 06:33:18,475 - distributed.worker - INFO - Starting Worker plugin PreImport-16a85b4f-d870-429d-96d5-594346a4d872
2024-01-10 06:33:18,477 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,485 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41485', status: init, memory: 0, processing: 0>
2024-01-10 06:33:18,486 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41485
2024-01-10 06:33:18,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60622
2024-01-10 06:33:18,488 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:18,490 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:18,490 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:18,519 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36643', status: init, memory: 0, processing: 0>
2024-01-10 06:33:18,520 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36643
2024-01-10 06:33:18,520 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60634
2024-01-10 06:33:18,522 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:18,523 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:18,523 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,526 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:18,563 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,597 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36399', status: init, memory: 0, processing: 0>
2024-01-10 06:33:18,598 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36399
2024-01-10 06:33:18,598 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60644
2024-01-10 06:33:18,599 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:18,600 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:18,600 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:18,637 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,648 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f80767ee-57f5-4cd2-9f80-38fac4119094
2024-01-10 06:33:18,649 - distributed.worker - INFO - Starting Worker plugin PreImport-95db98b0-c6f6-437b-9624-881759e2325d
2024-01-10 06:33:18,649 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,664 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-431e73a9-53d2-4f95-8734-f50594a7abfc
2024-01-10 06:33:18,665 - distributed.worker - INFO - Starting Worker plugin PreImport-8949a6fb-e6bc-4278-8718-1eb6b7b3c818
2024-01-10 06:33:18,665 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,665 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8f4af961-1a5b-465f-b521-cbb26b853666
2024-01-10 06:33:18,667 - distributed.worker - INFO - Starting Worker plugin PreImport-4bf26b25-ff59-4a51-a8c9-ab5025022aca
2024-01-10 06:33:18,668 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,671 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b38931b8-c157-4c84-be24-bb6b5b010fa3
2024-01-10 06:33:18,672 - distributed.worker - INFO - Starting Worker plugin PreImport-1ca010aa-c1bb-4e50-a466-d190d37401fc
2024-01-10 06:33:18,672 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,674 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39251', status: init, memory: 0, processing: 0>
2024-01-10 06:33:18,674 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39251
2024-01-10 06:33:18,674 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60654
2024-01-10 06:33:18,676 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40827', status: init, memory: 0, processing: 0>
2024-01-10 06:33:18,676 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:18,676 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40827
2024-01-10 06:33:18,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60670
2024-01-10 06:33:18,677 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:18,677 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,677 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:18,678 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:18,678 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,679 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:18,680 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:18,688 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44517', status: init, memory: 0, processing: 0>
2024-01-10 06:33:18,689 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44517
2024-01-10 06:33:18,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60674
2024-01-10 06:33:18,690 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:18,691 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:18,691 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,692 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:18,694 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44003', status: init, memory: 0, processing: 0>
2024-01-10 06:33:18,694 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44003
2024-01-10 06:33:18,694 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60692
2024-01-10 06:33:18,695 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42807', status: init, memory: 0, processing: 0>
2024-01-10 06:33:18,695 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:18,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42807
2024-01-10 06:33:18,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60676
2024-01-10 06:33:18,696 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:18,696 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:18,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:18,697 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:18,698 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:18,699 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:18,740 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:18,740 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:18,740 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:18,741 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:18,741 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:18,741 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:18,741 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:18,741 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:18,745 - distributed.scheduler - INFO - Remove client Client-20ba4492-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:18,746 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60596; closing.
2024-01-10 06:33:18,746 - distributed.scheduler - INFO - Remove client Client-20ba4492-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:18,746 - distributed.scheduler - INFO - Close client connection: Client-20ba4492-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:18,747 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35883'. Reason: nanny-close
2024-01-10 06:33:18,748 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:18,748 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41859'. Reason: nanny-close
2024-01-10 06:33:18,748 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:18,749 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46757'. Reason: nanny-close
2024-01-10 06:33:18,749 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:18,749 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41485. Reason: nanny-close
2024-01-10 06:33:18,749 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41291'. Reason: nanny-close
2024-01-10 06:33:18,750 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:18,750 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36643. Reason: nanny-close
2024-01-10 06:33:18,750 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34825'. Reason: nanny-close
2024-01-10 06:33:18,750 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42807. Reason: nanny-close
2024-01-10 06:33:18,750 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:18,750 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42877'. Reason: nanny-close
2024-01-10 06:33:18,750 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44517. Reason: nanny-close
2024-01-10 06:33:18,751 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:18,751 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46569'. Reason: nanny-close
2024-01-10 06:33:18,751 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:18,751 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36399. Reason: nanny-close
2024-01-10 06:33:18,751 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43993'. Reason: nanny-close
2024-01-10 06:33:18,751 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:18,751 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39251. Reason: nanny-close
2024-01-10 06:33:18,752 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:18,752 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44003. Reason: nanny-close
2024-01-10 06:33:18,752 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60676; closing.
2024-01-10 06:33:18,752 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868398.7524886')
2024-01-10 06:33:18,752 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:18,752 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40827. Reason: nanny-close
2024-01-10 06:33:18,752 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:18,753 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:18,753 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:18,753 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:18,753 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:18,754 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:18,754 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:18,754 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60674; closing.
2024-01-10 06:33:18,754 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60622; closing.
2024-01-10 06:33:18,755 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:18,755 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:18,755 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:18,755 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44517', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868398.755901')
2024-01-10 06:33:18,755 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:18,756 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:18,756 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:18,756 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60644; closing.
2024-01-10 06:33:18,756 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41485', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868398.7564821')
2024-01-10 06:33:18,756 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:18,756 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60692; closing.
2024-01-10 06:33:18,757 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60654; closing.
2024-01-10 06:33:18,757 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60634; closing.
2024-01-10 06:33:18,757 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36399', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868398.7576766')
2024-01-10 06:33:18,758 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44003', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868398.7581081')
2024-01-10 06:33:18,758 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39251', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868398.75846')
2024-01-10 06:33:18,758 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36643', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868398.7588148')
2024-01-10 06:33:18,759 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60670; closing.
2024-01-10 06:33:18,760 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868398.7599676')
2024-01-10 06:33:18,760 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:33:19,613 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:33:19,614 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:33:19,614 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:33:19,615 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:33:19,615 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-10 06:33:21,822 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:21,826 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:33:21,830 - distributed.scheduler - INFO - State start
2024-01-10 06:33:21,852 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:21,853 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:33:21,854 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:33:21,854 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:33:22,033 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43921'
2024-01-10 06:33:22,047 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46441'
2024-01-10 06:33:22,060 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37769'
2024-01-10 06:33:22,071 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46109'
2024-01-10 06:33:22,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39765'
2024-01-10 06:33:22,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43023'
2024-01-10 06:33:22,094 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43787'
2024-01-10 06:33:22,104 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43955'
2024-01-10 06:33:23,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:23,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:23,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:23,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:23,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:23,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:23,954 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37619
2024-01-10 06:33:23,954 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35469
2024-01-10 06:33:23,954 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35469
2024-01-10 06:33:23,954 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37619
2024-01-10 06:33:23,954 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44671
2024-01-10 06:33:23,954 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40695
2024-01-10 06:33:23,954 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:23,954 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:23,954 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:23,954 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:23,955 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:23,955 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:23,955 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:23,955 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:23,955 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tq6arabq
2024-01-10 06:33:23,955 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4eae7ral
2024-01-10 06:33:23,955 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-10bad01e-277b-4823-85b5-1ac53a98802c
2024-01-10 06:33:23,955 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6cd0ac52-3b85-416f-83bf-947418afb2e2
2024-01-10 06:33:23,955 - distributed.worker - INFO - Starting Worker plugin PreImport-e3d39bdf-5b65-4b68-a1ff-5f8c524ed274
2024-01-10 06:33:23,956 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dfa9602a-f14d-4ad3-87f4-232c62af1465
2024-01-10 06:33:23,959 - distributed.worker - INFO - Starting Worker plugin PreImport-2a99d6a1-4547-4901-bf58-3530bccdab7b
2024-01-10 06:33:23,959 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f14dcb63-816d-41c4-a628-29527fa3285d
2024-01-10 06:33:23,998 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:23,998 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:24,004 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:24,005 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40249
2024-01-10 06:33:24,005 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40249
2024-01-10 06:33:24,005 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40553
2024-01-10 06:33:24,005 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:24,005 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:24,005 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:24,005 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:24,005 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z1tox989
2024-01-10 06:33:24,006 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-33fa3477-ccd4-477d-aa31-8c75ecaee969
2024-01-10 06:33:24,006 - distributed.worker - INFO - Starting Worker plugin PreImport-03133271-8e31-458b-ba07-9efc9833dd21
2024-01-10 06:33:24,006 - distributed.worker - INFO - Starting Worker plugin RMMSetup-52279fe0-6d31-4b37-a504-b70f763d5c0b
2024-01-10 06:33:24,032 - distributed.scheduler - INFO - Receive client connection: Client-25339d8c-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:24,035 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:24,035 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:24,036 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:24,036 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:24,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:24,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:24,040 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:24,041 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38213
2024-01-10 06:33:24,041 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38213
2024-01-10 06:33:24,041 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44337
2024-01-10 06:33:24,041 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:24,041 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:24,041 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:24,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:24,041 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:24,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:24,041 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jhugk2m5
2024-01-10 06:33:24,041 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a7b70bbb-ecb2-49af-bbbb-32b6e3852431
2024-01-10 06:33:24,041 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:24,042 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42503
2024-01-10 06:33:24,042 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42503
2024-01-10 06:33:24,042 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43935
2024-01-10 06:33:24,042 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:24,043 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:24,043 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:24,043 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:24,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-09l3h1o0
2024-01-10 06:33:24,043 - distributed.worker - INFO - Starting Worker plugin PreImport-628a7c88-bb19-4961-937a-90c9769c6159
2024-01-10 06:33:24,043 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9f8d00c-76c2-43af-a609-8e56c15650e2
2024-01-10 06:33:24,043 - distributed.worker - INFO - Starting Worker plugin RMMSetup-408cb1a6-4029-48ef-8043-744884678005
2024-01-10 06:33:24,043 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:24,043 - distributed.worker - INFO - Starting Worker plugin PreImport-9d6d044f-3a92-429e-adca-63353c547987
2024-01-10 06:33:24,044 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76d4640c-7459-427c-8c39-9bc44a5f1cf7
2024-01-10 06:33:24,044 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46233
2024-01-10 06:33:24,044 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46233
2024-01-10 06:33:24,044 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33215
2024-01-10 06:33:24,044 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:24,044 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:24,044 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:24,045 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:24,045 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1z9uh_ge
2024-01-10 06:33:24,045 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-905c7c21-2429-4399-a036-39d3092d3b3a
2024-01-10 06:33:24,045 - distributed.worker - INFO - Starting Worker plugin PreImport-6852bccc-d985-4ca5-af85-06e181ef0630
2024-01-10 06:33:24,045 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd7fc809-96ec-427e-8ce3-34ad5768e94d
2024-01-10 06:33:24,045 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:24,046 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34265
2024-01-10 06:33:24,046 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34265
2024-01-10 06:33:24,046 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45401
2024-01-10 06:33:24,047 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:24,047 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:24,047 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:24,047 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:24,047 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-amlnqjwn
2024-01-10 06:33:24,047 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53302
2024-01-10 06:33:24,047 - distributed.worker - INFO - Starting Worker plugin RMMSetup-899ad002-c7c9-497b-93a0-acbb65a79d86
2024-01-10 06:33:24,053 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:24,053 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:24,058 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:24,059 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36505
2024-01-10 06:33:24,059 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36505
2024-01-10 06:33:24,059 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39111
2024-01-10 06:33:24,059 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:24,059 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:24,059 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:24,059 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:24,059 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mtir428l
2024-01-10 06:33:24,059 - distributed.worker - INFO - Starting Worker plugin RMMSetup-290ad6a0-b3cc-4105-9e23-62d11f39ccdd
2024-01-10 06:33:26,025 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,061 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37619', status: init, memory: 0, processing: 0>
2024-01-10 06:33:26,063 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37619
2024-01-10 06:33:26,063 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53316
2024-01-10 06:33:26,064 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:26,065 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:26,065 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:26,101 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,122 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,124 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,136 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35469', status: init, memory: 0, processing: 0>
2024-01-10 06:33:26,137 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35469
2024-01-10 06:33:26,137 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53330
2024-01-10 06:33:26,139 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:26,140 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:26,140 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:26,151 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46233', status: init, memory: 0, processing: 0>
2024-01-10 06:33:26,151 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46233
2024-01-10 06:33:26,151 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53350
2024-01-10 06:33:26,152 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:26,153 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:26,153 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:26,156 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40249', status: init, memory: 0, processing: 0>
2024-01-10 06:33:26,156 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40249
2024-01-10 06:33:26,157 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53334
2024-01-10 06:33:26,158 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:26,157 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,159 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:26,159 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,161 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:26,162 - distributed.worker - INFO - Starting Worker plugin PreImport-cce21b0c-e3bc-44b8-88ab-f62eade30cd0
2024-01-10 06:33:26,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bde4b438-196f-4f7d-9e5e-b2fd3c13320a
2024-01-10 06:33:26,164 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,174 - distributed.worker - INFO - Starting Worker plugin PreImport-ccb1795c-6616-4513-a75e-f180c6d0b426
2024-01-10 06:33:26,175 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-499fdbc5-6109-43ba-957a-1397a78793fc
2024-01-10 06:33:26,176 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,188 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,193 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38213', status: init, memory: 0, processing: 0>
2024-01-10 06:33:26,194 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38213
2024-01-10 06:33:26,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53352
2024-01-10 06:33:26,195 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34265', status: init, memory: 0, processing: 0>
2024-01-10 06:33:26,195 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:26,196 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34265
2024-01-10 06:33:26,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53368
2024-01-10 06:33:26,196 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:26,196 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,197 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:26,197 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:26,198 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,198 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:26,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:26,199 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36505', status: init, memory: 0, processing: 0>
2024-01-10 06:33:26,200 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36505
2024-01-10 06:33:26,200 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53380
2024-01-10 06:33:26,201 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:26,202 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:26,202 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:26,220 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42503', status: init, memory: 0, processing: 0>
2024-01-10 06:33:26,220 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42503
2024-01-10 06:33:26,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53392
2024-01-10 06:33:26,222 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:26,223 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:26,223 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:26,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:26,296 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:26,297 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:26,297 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:26,297 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:26,297 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:26,297 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:26,298 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:26,298 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:26,309 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:26,309 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:26,309 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:26,309 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:26,309 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:26,310 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:26,310 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:26,310 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:26,318 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:26,320 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:26,322 - distributed.scheduler - INFO - Remove client Client-25339d8c-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:26,322 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53302; closing.
2024-01-10 06:33:26,323 - distributed.scheduler - INFO - Remove client Client-25339d8c-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:26,323 - distributed.scheduler - INFO - Close client connection: Client-25339d8c-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:26,324 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43921'. Reason: nanny-close
2024-01-10 06:33:26,324 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:26,325 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46441'. Reason: nanny-close
2024-01-10 06:33:26,325 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:26,326 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37769'. Reason: nanny-close
2024-01-10 06:33:26,326 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:26,326 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46109'. Reason: nanny-close
2024-01-10 06:33:26,326 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42503. Reason: nanny-close
2024-01-10 06:33:26,326 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35469. Reason: nanny-close
2024-01-10 06:33:26,326 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:26,327 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39765'. Reason: nanny-close
2024-01-10 06:33:26,327 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36505. Reason: nanny-close
2024-01-10 06:33:26,327 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:26,327 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43023'. Reason: nanny-close
2024-01-10 06:33:26,327 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40249. Reason: nanny-close
2024-01-10 06:33:26,327 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:26,328 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43787'. Reason: nanny-close
2024-01-10 06:33:26,328 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37619. Reason: nanny-close
2024-01-10 06:33:26,328 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:26,328 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43955'. Reason: nanny-close
2024-01-10 06:33:26,328 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:26,328 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38213. Reason: nanny-close
2024-01-10 06:33:26,329 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53380; closing.
2024-01-10 06:33:26,329 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:26,329 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34265. Reason: nanny-close
2024-01-10 06:33:26,329 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:26,329 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36505', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868406.329333')
2024-01-10 06:33:26,329 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46233. Reason: nanny-close
2024-01-10 06:33:26,329 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:26,330 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:26,330 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53334; closing.
2024-01-10 06:33:26,330 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:26,330 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53392; closing.
2024-01-10 06:33:26,330 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:26,331 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:26,331 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:26,331 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:26,331 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:26,331 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40249', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868406.3318932')
2024-01-10 06:33:26,332 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:26,332 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:26,332 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42503', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868406.3322663')
2024-01-10 06:33:26,332 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:26,332 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53330; closing.
2024-01-10 06:33:26,333 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:26,333 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:26,333 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:26,333 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53334>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53334>: Stream is closed
2024-01-10 06:33:26,335 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53316; closing.
2024-01-10 06:33:26,335 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35469', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868406.3358252')
2024-01-10 06:33:26,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53368; closing.
2024-01-10 06:33:26,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53352; closing.
2024-01-10 06:33:26,336 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37619', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868406.3369274')
2024-01-10 06:33:26,337 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34265', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868406.3373313')
2024-01-10 06:33:26,337 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38213', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868406.3376107')
2024-01-10 06:33:26,337 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53350; closing.
2024-01-10 06:33:26,338 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46233', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868406.3384585')
2024-01-10 06:33:26,338 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:33:27,290 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:33:27,291 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:33:27,291 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:33:27,292 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:33:27,293 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-10 06:33:29,480 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:29,484 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:33:29,488 - distributed.scheduler - INFO - State start
2024-01-10 06:33:29,511 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:29,512 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:33:29,513 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:33:29,513 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:33:29,700 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33675'
2024-01-10 06:33:29,715 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43341'
2024-01-10 06:33:29,731 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38511'
2024-01-10 06:33:29,733 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33879'
2024-01-10 06:33:29,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35739'
2024-01-10 06:33:29,750 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46031'
2024-01-10 06:33:29,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42529'
2024-01-10 06:33:29,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44603'
2024-01-10 06:33:31,605 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:31,605 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:31,609 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:31,610 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37551
2024-01-10 06:33:31,610 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37551
2024-01-10 06:33:31,610 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33235
2024-01-10 06:33:31,610 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:31,611 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:31,611 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:31,611 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:31,611 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-52cwx5x7
2024-01-10 06:33:31,611 - distributed.worker - INFO - Starting Worker plugin PreImport-9f2b178c-7ea1-47bd-84c6-05f695352489
2024-01-10 06:33:31,611 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9552b1e4-f346-4d70-b556-df56c6348df6
2024-01-10 06:33:31,611 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad6077dd-730d-493f-b8c8-02ee26790048
2024-01-10 06:33:31,616 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:31,616 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:31,616 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:31,617 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:31,620 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:31,621 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34961
2024-01-10 06:33:31,621 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34961
2024-01-10 06:33:31,621 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42825
2024-01-10 06:33:31,621 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:31,621 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:31,621 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:31,621 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:31,621 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:31,621 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lkxd5mfo
2024-01-10 06:33:31,621 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d87be50a-9b2d-41fe-b53b-3765e7d9eab4
2024-01-10 06:33:31,621 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34249
2024-01-10 06:33:31,622 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34249
2024-01-10 06:33:31,622 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38613
2024-01-10 06:33:31,622 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:31,622 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:31,622 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:31,622 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:31,622 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-24mve5_j
2024-01-10 06:33:31,622 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a274c93-cf6c-4b1b-a70e-726594c1208e
2024-01-10 06:33:31,623 - distributed.worker - INFO - Starting Worker plugin PreImport-ef2557ea-23f3-4e00-8328-2d58619c4c3f
2024-01-10 06:33:31,624 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c526323-74c0-4d9e-86de-5fafba753169
2024-01-10 06:33:31,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:31,648 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:31,652 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:31,653 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32795
2024-01-10 06:33:31,653 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32795
2024-01-10 06:33:31,653 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38129
2024-01-10 06:33:31,653 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:31,653 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:31,653 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:31,653 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:31,653 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vlyuom6w
2024-01-10 06:33:31,654 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-000e0892-de8b-492f-ac3c-fc0ca0afbb65
2024-01-10 06:33:31,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:31,654 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:31,654 - distributed.worker - INFO - Starting Worker plugin PreImport-8723c453-4e18-4b93-84fa-04dbf8fde954
2024-01-10 06:33:31,654 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c71c508-b095-4e57-87df-b9bd576650aa
2024-01-10 06:33:31,658 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:31,659 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39633
2024-01-10 06:33:31,659 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39633
2024-01-10 06:33:31,659 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46293
2024-01-10 06:33:31,659 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:31,659 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:31,659 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:31,659 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:31,660 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-349jcrzq
2024-01-10 06:33:31,660 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-24246aa2-adcc-4305-9b28-e5dbf17ac5b2
2024-01-10 06:33:31,663 - distributed.worker - INFO - Starting Worker plugin PreImport-1a1827d0-0ff0-46e1-b3e1-1878bc64dbce
2024-01-10 06:33:31,664 - distributed.worker - INFO - Starting Worker plugin RMMSetup-253f6567-90d5-46b1-b032-1aeb7cf30b01
2024-01-10 06:33:31,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:31,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:31,680 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:31,681 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37307
2024-01-10 06:33:31,681 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37307
2024-01-10 06:33:31,681 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33017
2024-01-10 06:33:31,681 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:31,681 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:31,681 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:31,681 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:31,681 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:31,681 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3lqy6ect
2024-01-10 06:33:31,681 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:31,682 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e17b7897-ff3f-4764-85b5-02650dbf6ddf
2024-01-10 06:33:31,683 - distributed.worker - INFO - Starting Worker plugin PreImport-e0d41906-ef58-476e-9a0a-74225d7bdb13
2024-01-10 06:33:31,684 - distributed.worker - INFO - Starting Worker plugin RMMSetup-64d0eabd-11ab-499f-a926-8e0c3dd466b1
2024-01-10 06:33:31,686 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:31,686 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39289
2024-01-10 06:33:31,687 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39289
2024-01-10 06:33:31,687 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37237
2024-01-10 06:33:31,687 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:31,687 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:31,687 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:31,687 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:31,687 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kafasnrw
2024-01-10 06:33:31,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-002bb40a-3124-4057-93ef-34675d39bcbf
2024-01-10 06:33:31,687 - distributed.worker - INFO - Starting Worker plugin PreImport-4a35a412-f7ba-401d-9e32-e34eb55d20e3
2024-01-10 06:33:31,687 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8e7430c-25a4-4a6d-abd1-51c9ff0c23b6
2024-01-10 06:33:31,692 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:31,692 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:31,696 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:31,697 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36209
2024-01-10 06:33:31,697 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36209
2024-01-10 06:33:31,697 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43569
2024-01-10 06:33:31,698 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:31,698 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:31,698 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:31,698 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:31,698 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b1fypa0_
2024-01-10 06:33:31,698 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4522bc21-c2c6-413c-826b-7e9fc789d465
2024-01-10 06:33:31,849 - distributed.scheduler - INFO - Receive client connection: Client-29c898bf-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:31,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33456
2024-01-10 06:33:33,778 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,810 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37551', status: init, memory: 0, processing: 0>
2024-01-10 06:33:33,812 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37551
2024-01-10 06:33:33,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33474
2024-01-10 06:33:33,813 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:33,814 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:33,814 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:33,874 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,893 - distributed.worker - INFO - Starting Worker plugin PreImport-44f4c10c-dcf4-40ae-8418-65c8049a2e67
2024-01-10 06:33:33,894 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a044c79f-a5d9-4e49-b270-adfc079d2586
2024-01-10 06:33:33,894 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34249', status: init, memory: 0, processing: 0>
2024-01-10 06:33:33,910 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34249
2024-01-10 06:33:33,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33480
2024-01-10 06:33:33,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:33,913 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:33,913 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:33,921 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34961', status: init, memory: 0, processing: 0>
2024-01-10 06:33:33,922 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34961
2024-01-10 06:33:33,922 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33482
2024-01-10 06:33:33,923 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:33,924 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:33,924 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:33,927 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,930 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,936 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,937 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,953 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32795', status: init, memory: 0, processing: 0>
2024-01-10 06:33:33,954 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32795
2024-01-10 06:33:33,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33484
2024-01-10 06:33:33,955 - distributed.worker - INFO - Starting Worker plugin PreImport-94bf0a90-5a9c-4240-bf1d-63f01d825322
2024-01-10 06:33:33,955 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:33,956 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b01f228f-bcfb-48ab-801a-ecdc8298c82e
2024-01-10 06:33:33,956 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:33,956 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,956 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:33,964 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39289', status: init, memory: 0, processing: 0>
2024-01-10 06:33:33,965 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39289
2024-01-10 06:33:33,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33512
2024-01-10 06:33:33,966 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:33,967 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:33,967 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,968 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:33,969 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37307', status: init, memory: 0, processing: 0>
2024-01-10 06:33:33,969 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37307
2024-01-10 06:33:33,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33500
2024-01-10 06:33:33,970 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:33,971 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:33,972 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,973 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39633', status: init, memory: 0, processing: 0>
2024-01-10 06:33:33,974 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39633
2024-01-10 06:33:33,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33514
2024-01-10 06:33:33,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:33,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:33,976 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:33,976 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,978 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:33,979 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36209', status: init, memory: 0, processing: 0>
2024-01-10 06:33:33,980 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36209
2024-01-10 06:33:33,980 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33522
2024-01-10 06:33:33,981 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:33,981 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:33,981 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:33,983 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:34,059 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:34,059 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:34,059 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:34,059 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:34,059 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:34,060 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:34,060 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:34,060 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:34,071 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:34,071 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:34,071 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:34,072 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:34,072 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:34,072 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:34,072 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:34,072 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:33:34,081 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:34,082 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:34,085 - distributed.scheduler - INFO - Remove client Client-29c898bf-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:34,085 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33456; closing.
2024-01-10 06:33:34,085 - distributed.scheduler - INFO - Remove client Client-29c898bf-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:34,086 - distributed.scheduler - INFO - Close client connection: Client-29c898bf-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:34,087 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33675'. Reason: nanny-close
2024-01-10 06:33:34,087 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:34,087 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43341'. Reason: nanny-close
2024-01-10 06:33:34,088 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:34,088 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38511'. Reason: nanny-close
2024-01-10 06:33:34,088 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:34,088 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37551. Reason: nanny-close
2024-01-10 06:33:34,089 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33879'. Reason: nanny-close
2024-01-10 06:33:34,089 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:34,089 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34249. Reason: nanny-close
2024-01-10 06:33:34,089 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35739'. Reason: nanny-close
2024-01-10 06:33:34,089 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34961. Reason: nanny-close
2024-01-10 06:33:34,089 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:34,090 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46031'. Reason: nanny-close
2024-01-10 06:33:34,090 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32795. Reason: nanny-close
2024-01-10 06:33:34,090 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:34,090 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42529'. Reason: nanny-close
2024-01-10 06:33:34,090 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:34,090 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44603'. Reason: nanny-close
2024-01-10 06:33:34,090 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39633. Reason: nanny-close
2024-01-10 06:33:34,091 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:34,091 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37307. Reason: nanny-close
2024-01-10 06:33:34,091 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33474; closing.
2024-01-10 06:33:34,091 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:34,091 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36209. Reason: nanny-close
2024-01-10 06:33:34,091 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:34,091 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37551', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868414.0917614')
2024-01-10 06:33:34,092 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:34,092 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:34,092 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39289. Reason: nanny-close
2024-01-10 06:33:34,092 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33482; closing.
2024-01-10 06:33:34,092 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34961', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868414.0927846')
2024-01-10 06:33:34,092 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:34,093 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:34,093 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:34,093 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:34,093 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:34,093 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:34,094 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:34,094 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33480; closing.
2024-01-10 06:33:34,094 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:34,094 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33484; closing.
2024-01-10 06:33:34,095 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:34,095 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:34,095 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:34,095 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:34,094 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:33482>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:33:34,096 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34249', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868414.0967844')
2024-01-10 06:33:34,097 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32795', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868414.097188')
2024-01-10 06:33:34,097 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33514; closing.
2024-01-10 06:33:34,097 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33522; closing.
2024-01-10 06:33:34,098 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39633', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868414.0985255')
2024-01-10 06:33:34,098 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36209', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868414.0988796')
2024-01-10 06:33:34,099 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33500; closing.
2024-01-10 06:33:34,099 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33512; closing.
2024-01-10 06:33:34,100 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37307', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868414.0999346')
2024-01-10 06:33:34,100 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39289', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868414.1003225')
2024-01-10 06:33:34,100 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:33:35,103 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:33:35,104 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:33:35,104 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:33:35,106 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:33:35,107 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-10 06:33:37,315 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:37,318 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34749 instead
  warnings.warn(
2024-01-10 06:33:37,322 - distributed.scheduler - INFO - State start
2024-01-10 06:33:37,342 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:37,343 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:33:37,344 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34749/status
2024-01-10 06:33:37,344 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:33:37,530 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43101'
2024-01-10 06:33:37,543 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37071'
2024-01-10 06:33:37,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45171'
2024-01-10 06:33:37,566 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43615'
2024-01-10 06:33:37,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40529'
2024-01-10 06:33:37,577 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40167'
2024-01-10 06:33:37,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39705'
2024-01-10 06:33:37,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39651'
2024-01-10 06:33:38,870 - distributed.scheduler - INFO - Receive client connection: Client-2e89217c-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:38,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33664
2024-01-10 06:33:39,410 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:39,410 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:39,411 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:39,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:39,414 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:39,415 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46021
2024-01-10 06:33:39,415 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46021
2024-01-10 06:33:39,415 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38891
2024-01-10 06:33:39,415 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:39,415 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:39,416 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:39,416 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:39,416 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:39,416 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yv43at8z
2024-01-10 06:33:39,416 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f95aa61-6053-4136-bd66-7902d9693bc7
2024-01-10 06:33:39,416 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46125
2024-01-10 06:33:39,416 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46125
2024-01-10 06:33:39,416 - distributed.worker - INFO - Starting Worker plugin PreImport-4f24178d-9c4c-4681-84e8-31d862c33d72
2024-01-10 06:33:39,416 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42955
2024-01-10 06:33:39,416 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:39,416 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:39,416 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b7e06b13-bd16-4a14-9df6-1cca014f75b7
2024-01-10 06:33:39,416 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:39,417 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:39,417 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1oyrames
2024-01-10 06:33:39,417 - distributed.worker - INFO - Starting Worker plugin PreImport-b4c106fe-aebb-40f6-be85-caddf1be7589
2024-01-10 06:33:39,417 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b568ab6-00e3-4156-a999-88e8508c87a6
2024-01-10 06:33:39,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-298adae2-7dee-4d01-9b46-4fcf25daf3c8
2024-01-10 06:33:39,488 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:39,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:39,489 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:39,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:39,490 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:39,490 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:39,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:39,494 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44117
2024-01-10 06:33:39,494 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:39,494 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44117
2024-01-10 06:33:39,494 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40085
2024-01-10 06:33:39,494 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:39,494 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:39,494 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:39,494 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:39,494 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:39,494 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hhzvmwod
2024-01-10 06:33:39,495 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5089b9eb-293a-4911-8106-a7d56146a9ee
2024-01-10 06:33:39,495 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34219
2024-01-10 06:33:39,495 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34219
2024-01-10 06:33:39,495 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42067
2024-01-10 06:33:39,495 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37023
2024-01-10 06:33:39,495 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:39,495 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37023
2024-01-10 06:33:39,495 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:39,495 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33349
2024-01-10 06:33:39,495 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:39,495 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:39,495 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:39,495 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:39,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qoft9sk_
2024-01-10 06:33:39,495 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:39,495 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:39,496 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2ggic5rf
2024-01-10 06:33:39,496 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1e098ab-7d6e-4413-ac05-6c0dfdeead74
2024-01-10 06:33:39,496 - distributed.worker - INFO - Starting Worker plugin PreImport-ddfb6133-1d0e-411e-bb07-7cf8cdd528fa
2024-01-10 06:33:39,496 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-301023d3-9bd2-4e93-b039-c4a47af340db
2024-01-10 06:33:39,496 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ead448b6-13a5-4680-9c5d-5909913eb66f
2024-01-10 06:33:39,497 - distributed.worker - INFO - Starting Worker plugin PreImport-cda09f3f-29e8-4b28-ae5e-ed2318ba1d17
2024-01-10 06:33:39,497 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a66ca8c0-e8a4-42f6-b277-b947bfbbefcf
2024-01-10 06:33:39,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:39,509 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:39,512 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:39,513 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38331
2024-01-10 06:33:39,513 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38331
2024-01-10 06:33:39,513 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36385
2024-01-10 06:33:39,513 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:39,513 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:39,513 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:39,513 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:39,514 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-arojnilb
2024-01-10 06:33:39,514 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f34298b-63fe-4aaf-b0a5-655a35df6a07
2024-01-10 06:33:39,889 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:39,890 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:39,892 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:39,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:39,895 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:39,896 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38221
2024-01-10 06:33:39,896 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38221
2024-01-10 06:33:39,896 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40497
2024-01-10 06:33:39,896 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:39,896 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:39,896 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:39,896 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:39,896 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-359zvn2g
2024-01-10 06:33:39,896 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-30bcefaf-df48-449b-a5ff-50911643ee16
2024-01-10 06:33:39,897 - distributed.worker - INFO - Starting Worker plugin PreImport-855bbb0a-e850-44b9-8372-0a191a27d168
2024-01-10 06:33:39,897 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1de2c92d-1195-4522-8129-360b79f18447
2024-01-10 06:33:39,897 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:39,898 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34547
2024-01-10 06:33:39,898 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34547
2024-01-10 06:33:39,898 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45801
2024-01-10 06:33:39,898 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:39,898 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:39,898 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:39,898 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:33:39,898 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w820reg2
2024-01-10 06:33:39,899 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b3aee1ec-b20c-4f16-8e98-7eef8bc15560
2024-01-10 06:33:39,899 - distributed.worker - INFO - Starting Worker plugin PreImport-70ddce04-e9c5-43d6-bf17-f692ed9f9cbb
2024-01-10 06:33:39,900 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5b5a02a5-f83e-4089-9496-9b1fe239e627
2024-01-10 06:33:41,278 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,301 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,311 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46125', status: init, memory: 0, processing: 0>
2024-01-10 06:33:41,312 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46125
2024-01-10 06:33:41,312 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40836
2024-01-10 06:33:41,313 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:41,315 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:41,315 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,317 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:41,326 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46021', status: init, memory: 0, processing: 0>
2024-01-10 06:33:41,326 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46021
2024-01-10 06:33:41,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40840
2024-01-10 06:33:41,327 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:41,328 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:41,328 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:41,516 - distributed.worker - INFO - Starting Worker plugin PreImport-5543f9fe-6472-4488-a24f-04868b66e9b5
2024-01-10 06:33:41,516 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-721683ce-bf19-43fa-b396-16249414ce24
2024-01-10 06:33:41,516 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,537 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44117', status: init, memory: 0, processing: 0>
2024-01-10 06:33:41,538 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44117
2024-01-10 06:33:41,538 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40850
2024-01-10 06:33:41,539 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:41,542 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:41,542 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,543 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,544 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:41,545 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,559 - distributed.worker - INFO - Starting Worker plugin PreImport-f9988d24-14ad-4b9e-80ca-834cc28dd8d0
2024-01-10 06:33:41,560 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69a5bdee-bbd2-4c76-9726-f7bb9166de7a
2024-01-10 06:33:41,560 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,566 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34219', status: init, memory: 0, processing: 0>
2024-01-10 06:33:41,567 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34219
2024-01-10 06:33:41,567 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40856
2024-01-10 06:33:41,568 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:41,569 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:41,569 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,570 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:41,577 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,578 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37023', status: init, memory: 0, processing: 0>
2024-01-10 06:33:41,579 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37023
2024-01-10 06:33:41,579 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40862
2024-01-10 06:33:41,580 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,581 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:41,582 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:41,582 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,584 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38331', status: init, memory: 0, processing: 0>
2024-01-10 06:33:41,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:41,584 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38331
2024-01-10 06:33:41,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40874
2024-01-10 06:33:41,585 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:41,586 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:41,586 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,587 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:41,599 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38221', status: init, memory: 0, processing: 0>
2024-01-10 06:33:41,600 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38221
2024-01-10 06:33:41,600 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40876
2024-01-10 06:33:41,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:41,601 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:41,601 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,602 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:41,612 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34547', status: init, memory: 0, processing: 0>
2024-01-10 06:33:41,613 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34547
2024-01-10 06:33:41,613 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40886
2024-01-10 06:33:41,614 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:41,616 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:41,616 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:41,618 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:41,663 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:41,664 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:41,664 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:41,664 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:41,664 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:41,664 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:41,664 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:41,665 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:33:41,669 - distributed.scheduler - INFO - Remove client Client-2e89217c-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:41,669 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33664; closing.
2024-01-10 06:33:41,669 - distributed.scheduler - INFO - Remove client Client-2e89217c-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:41,669 - distributed.scheduler - INFO - Close client connection: Client-2e89217c-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:41,670 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43101'. Reason: nanny-close
2024-01-10 06:33:41,671 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:41,671 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37071'. Reason: nanny-close
2024-01-10 06:33:41,671 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:41,672 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45171'. Reason: nanny-close
2024-01-10 06:33:41,672 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:41,672 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46125. Reason: nanny-close
2024-01-10 06:33:41,672 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43615'. Reason: nanny-close
2024-01-10 06:33:41,672 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46021. Reason: nanny-close
2024-01-10 06:33:41,672 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:41,673 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40529'. Reason: nanny-close
2024-01-10 06:33:41,673 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38331. Reason: nanny-close
2024-01-10 06:33:41,673 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:41,673 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40167'. Reason: nanny-close
2024-01-10 06:33:41,673 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38221. Reason: nanny-close
2024-01-10 06:33:41,674 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:41,674 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39705'. Reason: nanny-close
2024-01-10 06:33:41,674 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:41,674 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34547. Reason: nanny-close
2024-01-10 06:33:41,674 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39651'. Reason: nanny-close
2024-01-10 06:33:41,675 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37023. Reason: nanny-close
2024-01-10 06:33:41,675 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:41,675 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:41,675 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40840; closing.
2024-01-10 06:33:41,675 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:41,675 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44117. Reason: nanny-close
2024-01-10 06:33:41,675 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:41,675 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:41,675 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46021', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868421.6757026')
2024-01-10 06:33:41,676 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34219. Reason: nanny-close
2024-01-10 06:33:41,676 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40874; closing.
2024-01-10 06:33:41,676 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40836; closing.
2024-01-10 06:33:41,676 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:41,676 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:41,676 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:41,677 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38331', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868421.676983')
2024-01-10 06:33:41,677 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:41,677 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:41,677 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:41,677 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46125', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868421.67744')
2024-01-10 06:33:41,677 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:41,677 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40876; closing.
2024-01-10 06:33:41,677 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:41,678 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38221', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868421.6789343')
2024-01-10 06:33:41,678 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:41,679 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:41,679 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:41,679 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:41,680 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40874>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:33:41,681 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40836>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:33:41,681 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40886; closing.
2024-01-10 06:33:41,681 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40862; closing.
2024-01-10 06:33:41,682 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40850; closing.
2024-01-10 06:33:41,682 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34547', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868421.682525')
2024-01-10 06:33:41,682 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868421.6828904')
2024-01-10 06:33:41,683 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44117', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868421.6833055')
2024-01-10 06:33:41,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40856; closing.
2024-01-10 06:33:41,686 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34219', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868421.6858895')
2024-01-10 06:33:41,686 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:33:42,887 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:33:42,887 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:33:42,888 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:33:42,889 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:33:42,889 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-10 06:33:44,987 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:44,992 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35393 instead
  warnings.warn(
2024-01-10 06:33:44,997 - distributed.scheduler - INFO - State start
2024-01-10 06:33:45,018 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:45,019 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:33:45,020 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35393/status
2024-01-10 06:33:45,020 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:33:45,150 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44123'
2024-01-10 06:33:45,480 - distributed.scheduler - INFO - Receive client connection: Client-3314e7de-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:45,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40976
2024-01-10 06:33:46,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:46,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:47,364 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:47,364 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44259
2024-01-10 06:33:47,365 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44259
2024-01-10 06:33:47,365 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-10 06:33:47,365 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:47,365 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:47,365 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:47,365 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:33:47,365 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uupdany1
2024-01-10 06:33:47,365 - distributed.worker - INFO - Starting Worker plugin PreImport-226d1a8b-e1f6-4801-a543-ea3ed5f8d03d
2024-01-10 06:33:47,365 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b79cb8eb-b9f4-42b4-8748-7b83645c54f1
2024-01-10 06:33:47,365 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72903e4f-c4bb-46b3-9969-2bf64aa88169
2024-01-10 06:33:47,366 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:47,425 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44259', status: init, memory: 0, processing: 0>
2024-01-10 06:33:47,426 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44259
2024-01-10 06:33:47,426 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40986
2024-01-10 06:33:47,428 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:47,429 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:47,429 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:47,431 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:47,432 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:47,435 - distributed.scheduler - INFO - Remove client Client-3314e7de-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:47,435 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40976; closing.
2024-01-10 06:33:47,435 - distributed.scheduler - INFO - Remove client Client-3314e7de-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:47,436 - distributed.scheduler - INFO - Close client connection: Client-3314e7de-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:47,437 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44123'. Reason: nanny-close
2024-01-10 06:33:47,480 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:47,481 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44259. Reason: nanny-close
2024-01-10 06:33:47,483 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:47,483 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40986; closing.
2024-01-10 06:33:47,484 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44259', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868427.48415')
2024-01-10 06:33:47,484 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:33:47,485 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:48,152 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:33:48,152 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:33:48,153 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:33:48,153 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:33:48,154 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-10 06:33:52,238 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:52,242 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:33:52,246 - distributed.scheduler - INFO - State start
2024-01-10 06:33:52,268 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:52,269 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:33:52,270 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:33:52,270 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:33:52,425 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37039'
2024-01-10 06:33:53,622 - distributed.scheduler - INFO - Receive client connection: Client-375b2357-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:53,636 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51526
2024-01-10 06:33:54,120 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:33:54,120 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:33:54,658 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:33:54,659 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42355
2024-01-10 06:33:54,660 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42355
2024-01-10 06:33:54,660 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43771
2024-01-10 06:33:54,660 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:33:54,660 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:54,660 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:33:54,660 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:33:54,660 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bct4rnym
2024-01-10 06:33:54,660 - distributed.worker - INFO - Starting Worker plugin PreImport-2896685e-59f8-459c-bdb0-2433af912722
2024-01-10 06:33:54,661 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4cc7304-6247-4e40-bf88-9ce440741a2d
2024-01-10 06:33:54,661 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d2b32f7-fc16-4f9d-af18-f5ed3918e96c
2024-01-10 06:33:54,662 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:54,721 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42355', status: init, memory: 0, processing: 0>
2024-01-10 06:33:54,722 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42355
2024-01-10 06:33:54,722 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51544
2024-01-10 06:33:54,723 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:33:54,724 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:33:54,724 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:33:54,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:33:54,760 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:33:54,763 - distributed.scheduler - INFO - Remove client Client-375b2357-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:54,763 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51526; closing.
2024-01-10 06:33:54,763 - distributed.scheduler - INFO - Remove client Client-375b2357-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:54,764 - distributed.scheduler - INFO - Close client connection: Client-375b2357-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:33:54,764 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37039'. Reason: nanny-close
2024-01-10 06:33:54,765 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:33:54,766 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42355. Reason: nanny-close
2024-01-10 06:33:54,768 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:33:54,768 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51544; closing.
2024-01-10 06:33:54,768 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42355', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868434.7688723')
2024-01-10 06:33:54,769 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:33:54,770 - distributed.nanny - INFO - Worker closed
2024-01-10 06:33:55,480 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:33:55,480 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:33:55,481 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:33:55,481 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:33:55,482 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-10 06:33:57,612 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:57,616 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:33:57,620 - distributed.scheduler - INFO - State start
2024-01-10 06:33:57,641 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:33:57,642 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:33:57,642 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:33:57,643 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:34:00,121 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:51556'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4428, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51556>: Stream is closed
2024-01-10 06:34:00,396 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:34:00,397 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:34:00,397 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:34:00,398 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:34:00,398 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-10 06:34:02,557 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:34:02,562 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:34:02,565 - distributed.scheduler - INFO - State start
2024-01-10 06:34:02,587 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:34:02,588 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-10 06:34:02,589 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:34:02,589 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:34:02,666 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33825'
2024-01-10 06:34:03,322 - distributed.scheduler - INFO - Receive client connection: Client-3d84a96b-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:03,336 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39756
2024-01-10 06:34:04,373 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:04,373 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:04,377 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:04,378 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38071
2024-01-10 06:34:04,378 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38071
2024-01-10 06:34:04,378 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39671
2024-01-10 06:34:04,378 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:34:04,378 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:04,378 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:04,378 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:34:04,378 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-rr6ay7o6
2024-01-10 06:34:04,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b7241b4-cb5d-4908-89cb-4c0e3cdbf56e
2024-01-10 06:34:04,379 - distributed.worker - INFO - Starting Worker plugin PreImport-7981f48c-44fc-403b-b2b6-2c7f1b998ada
2024-01-10 06:34:04,379 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8cb26093-495b-417c-a581-55ab96175aa5
2024-01-10 06:34:04,379 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:04,438 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38071', status: init, memory: 0, processing: 0>
2024-01-10 06:34:04,439 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38071
2024-01-10 06:34:04,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39770
2024-01-10 06:34:04,440 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:04,441 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:34:04,441 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:04,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:34:04,461 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:04,464 - distributed.scheduler - INFO - Remove client Client-3d84a96b-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:04,464 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39756; closing.
2024-01-10 06:34:04,464 - distributed.scheduler - INFO - Remove client Client-3d84a96b-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:04,464 - distributed.scheduler - INFO - Close client connection: Client-3d84a96b-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:04,465 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33825'. Reason: nanny-close
2024-01-10 06:34:04,466 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:04,467 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38071. Reason: nanny-close
2024-01-10 06:34:04,468 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:34:04,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39770; closing.
2024-01-10 06:34:04,469 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868444.4693112')
2024-01-10 06:34:04,469 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:34:04,470 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:05,031 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:34:05,031 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:34:05,031 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:34:05,032 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-10 06:34:05,033 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-10 06:34:07,329 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:34:07,334 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:34:07,337 - distributed.scheduler - INFO - State start
2024-01-10 06:34:07,360 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:34:07,362 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:34:07,362 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:34:07,363 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:34:07,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36489'
2024-01-10 06:34:07,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36405'
2024-01-10 06:34:07,520 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37073'
2024-01-10 06:34:07,523 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42905'
2024-01-10 06:34:07,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39607'
2024-01-10 06:34:07,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40597'
2024-01-10 06:34:07,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38533'
2024-01-10 06:34:07,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35809'
2024-01-10 06:34:08,778 - distributed.scheduler - INFO - Receive client connection: Client-4056188f-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:08,795 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35106
2024-01-10 06:34:09,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:09,383 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:09,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:09,383 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:09,387 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:09,387 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:09,388 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33247
2024-01-10 06:34:09,388 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33855
2024-01-10 06:34:09,388 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33247
2024-01-10 06:34:09,388 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33855
2024-01-10 06:34:09,388 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43539
2024-01-10 06:34:09,388 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46033
2024-01-10 06:34:09,388 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:34:09,388 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:34:09,388 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:09,388 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:09,388 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:09,388 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:09,388 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:34:09,388 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:34:09,388 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z76bfu8l
2024-01-10 06:34:09,388 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-34d_7ypr
2024-01-10 06:34:09,389 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a1f5655-d4d8-426d-995a-8b255887e0d5
2024-01-10 06:34:09,389 - distributed.worker - INFO - Starting Worker plugin PreImport-6eac991b-5bca-4264-b786-b6fa7696d295
2024-01-10 06:34:09,389 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c2a9935-85b4-4313-a020-b4d17d3e00cb
2024-01-10 06:34:09,389 - distributed.worker - INFO - Starting Worker plugin PreImport-c855d69d-2a25-4695-a0d0-e1fc3c1033a5
2024-01-10 06:34:09,389 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cfba249e-f4a7-4e2c-a945-3b130cdeedca
2024-01-10 06:34:09,390 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5754e58a-5c68-4518-8f80-f03aed0d4c06
2024-01-10 06:34:09,427 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:09,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:09,427 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:09,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:09,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:09,432 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:09,432 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36359
2024-01-10 06:34:09,432 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36359
2024-01-10 06:34:09,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38703
2024-01-10 06:34:09,433 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45891
2024-01-10 06:34:09,433 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45891
2024-01-10 06:34:09,433 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:34:09,433 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:09,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38375
2024-01-10 06:34:09,433 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:09,433 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:34:09,433 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:34:09,433 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:09,433 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:09,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m9mhi2cx
2024-01-10 06:34:09,433 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:34:09,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1hzbekeb
2024-01-10 06:34:09,433 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65a5e0b9-0c0e-4bd2-9680-d443297c2415
2024-01-10 06:34:09,433 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4544458a-447d-4eb2-aa85-6a535f9b8c58
2024-01-10 06:34:09,435 - distributed.worker - INFO - Starting Worker plugin PreImport-c49e4464-6ab5-4913-a8af-dfa5388a09ec
2024-01-10 06:34:09,435 - distributed.worker - INFO - Starting Worker plugin PreImport-74eb1360-9875-4fc3-855e-d1c921803b0b
2024-01-10 06:34:09,436 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4a64ab7-cba0-445c-92d6-152d24fd7e6b
2024-01-10 06:34:09,436 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11aeb2d2-abe6-4019-b46c-37fdf76fe0be
2024-01-10 06:34:09,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:09,453 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:09,458 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:09,459 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42621
2024-01-10 06:34:09,459 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42621
2024-01-10 06:34:09,459 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40635
2024-01-10 06:34:09,459 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:34:09,459 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:09,459 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:09,459 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:34:09,459 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9lqhl1z5
2024-01-10 06:34:09,459 - distributed.worker - INFO - Starting Worker plugin RMMSetup-184d5744-a59c-4ea5-9bf2-ca61aca5a16f
2024-01-10 06:34:09,464 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:09,464 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:09,468 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:09,469 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43255
2024-01-10 06:34:09,469 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43255
2024-01-10 06:34:09,469 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35739
2024-01-10 06:34:09,470 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:34:09,470 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:09,470 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:09,470 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:34:09,470 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-13_woqlz
2024-01-10 06:34:09,470 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a7f63956-ef6a-4915-834d-8dea47089db3
2024-01-10 06:34:09,470 - distributed.worker - INFO - Starting Worker plugin PreImport-75bb9f52-2184-4996-9ad5-0609c21a91a8
2024-01-10 06:34:09,470 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d881c1f7-cfb4-42f5-8034-1bcc03b301d2
2024-01-10 06:34:09,487 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:09,487 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:09,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:09,492 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35541
2024-01-10 06:34:09,492 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35541
2024-01-10 06:34:09,492 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36703
2024-01-10 06:34:09,492 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:34:09,492 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:09,492 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:09,492 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:34:09,492 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1j9g3ndu
2024-01-10 06:34:09,493 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9aa1efe0-40ae-4c9f-be9c-7d6d1f15a711
2024-01-10 06:34:09,494 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:09,494 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:09,498 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:09,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44035
2024-01-10 06:34:09,499 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44035
2024-01-10 06:34:09,499 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45595
2024-01-10 06:34:09,499 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:34:09,499 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:09,499 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:09,499 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:34:09,499 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oikcjz1c
2024-01-10 06:34:09,499 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-181cba11-6808-49c5-a02c-99f9009b3d97
2024-01-10 06:34:09,500 - distributed.worker - INFO - Starting Worker plugin PreImport-591d66e1-6d64-4979-93a6-b797fd82ec93
2024-01-10 06:34:09,500 - distributed.worker - INFO - Starting Worker plugin RMMSetup-92e1cad1-1e84-4765-8882-59b1c75340cc
2024-01-10 06:34:11,440 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,466 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,475 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45891', status: init, memory: 0, processing: 0>
2024-01-10 06:34:11,476 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45891
2024-01-10 06:34:11,476 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59964
2024-01-10 06:34:11,478 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:11,479 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:34:11,479 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:34:11,498 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,500 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33855', status: init, memory: 0, processing: 0>
2024-01-10 06:34:11,501 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33855
2024-01-10 06:34:11,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59972
2024-01-10 06:34:11,503 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:11,504 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:34:11,504 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,506 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:34:11,533 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33247', status: init, memory: 0, processing: 0>
2024-01-10 06:34:11,534 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33247
2024-01-10 06:34:11,534 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59982
2024-01-10 06:34:11,535 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:11,537 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:34:11,537 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,539 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:34:11,583 - distributed.worker - INFO - Starting Worker plugin PreImport-a3c867a3-9e83-4dbb-80b8-6c63a76672af
2024-01-10 06:34:11,584 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d82a38c2-166a-47c7-a1b9-70ad737941db
2024-01-10 06:34:11,585 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,597 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,606 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,608 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42621', status: init, memory: 0, processing: 0>
2024-01-10 06:34:11,609 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42621
2024-01-10 06:34:11,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59998
2024-01-10 06:34:11,610 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:11,611 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:34:11,611 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,612 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:34:11,621 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43255', status: init, memory: 0, processing: 0>
2024-01-10 06:34:11,622 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43255
2024-01-10 06:34:11,622 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60008
2024-01-10 06:34:11,623 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:11,623 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:34:11,624 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,624 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,625 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:34:11,628 - distributed.worker - INFO - Starting Worker plugin PreImport-e8b29237-796d-4858-a70f-0a549ad88c64
2024-01-10 06:34:11,628 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c1bd265-c0de-43cf-aa8d-d81266d4aca2
2024-01-10 06:34:11,629 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,640 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36359', status: init, memory: 0, processing: 0>
2024-01-10 06:34:11,640 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36359
2024-01-10 06:34:11,640 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60022
2024-01-10 06:34:11,642 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:11,643 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:34:11,643 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,645 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:34:11,649 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44035', status: init, memory: 0, processing: 0>
2024-01-10 06:34:11,649 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44035
2024-01-10 06:34:11,649 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60032
2024-01-10 06:34:11,650 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:11,651 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:34:11,651 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,652 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:34:11,654 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35541', status: init, memory: 0, processing: 0>
2024-01-10 06:34:11,654 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35541
2024-01-10 06:34:11,654 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60040
2024-01-10 06:34:11,655 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:11,656 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:34:11,656 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:11,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:34:11,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:34:11,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:34:11,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:34:11,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:34:11,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:34:11,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:34:11,714 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:34:11,715 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:34:11,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:11,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:11,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:11,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:11,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:11,728 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:11,729 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:11,729 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:11,733 - distributed.scheduler - INFO - Remove client Client-4056188f-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:11,733 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35106; closing.
2024-01-10 06:34:11,733 - distributed.scheduler - INFO - Remove client Client-4056188f-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:11,734 - distributed.scheduler - INFO - Close client connection: Client-4056188f-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:11,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36489'. Reason: nanny-close
2024-01-10 06:34:11,735 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:11,735 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36405'. Reason: nanny-close
2024-01-10 06:34:11,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:11,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37073'. Reason: nanny-close
2024-01-10 06:34:11,736 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33855. Reason: nanny-close
2024-01-10 06:34:11,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:11,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42905'. Reason: nanny-close
2024-01-10 06:34:11,737 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:11,737 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33247. Reason: nanny-close
2024-01-10 06:34:11,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39607'. Reason: nanny-close
2024-01-10 06:34:11,737 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42621. Reason: nanny-close
2024-01-10 06:34:11,737 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:11,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40597'. Reason: nanny-close
2024-01-10 06:34:11,738 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43255. Reason: nanny-close
2024-01-10 06:34:11,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:11,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38533'. Reason: nanny-close
2024-01-10 06:34:11,739 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45891. Reason: nanny-close
2024-01-10 06:34:11,739 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59972; closing.
2024-01-10 06:34:11,739 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:34:11,739 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:11,739 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35809'. Reason: nanny-close
2024-01-10 06:34:11,739 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33855', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868451.7395759')
2024-01-10 06:34:11,739 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36359. Reason: nanny-close
2024-01-10 06:34:11,739 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:34:11,739 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:11,740 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35541. Reason: nanny-close
2024-01-10 06:34:11,740 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:34:11,740 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:34:11,740 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44035. Reason: nanny-close
2024-01-10 06:34:11,741 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:11,741 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:11,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:34:11,741 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60008; closing.
2024-01-10 06:34:11,741 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59998; closing.
2024-01-10 06:34:11,742 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59982; closing.
2024-01-10 06:34:11,742 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:11,742 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:11,742 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:34:11,742 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:34:11,742 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43255', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868451.7426455')
2024-01-10 06:34:11,743 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42621', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868451.7430272')
2024-01-10 06:34:11,743 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:34:11,743 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:11,743 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59964; closing.
2024-01-10 06:34:11,743 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33247', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868451.7435465')
2024-01-10 06:34:11,743 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:11,744 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45891', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868451.7443452')
2024-01-10 06:34:11,744 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:11,744 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:11,744 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60022; closing.
2024-01-10 06:34:11,745 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36359', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868451.7453527')
2024-01-10 06:34:11,745 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60040; closing.
2024-01-10 06:34:11,745 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60032; closing.
2024-01-10 06:34:11,746 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35541', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868451.7462711')
2024-01-10 06:34:11,746 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44035', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868451.746653')
2024-01-10 06:34:11,746 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:34:12,701 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:34:12,701 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:34:12,702 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:34:12,703 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:34:12,703 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-10 06:34:14,804 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:34:14,809 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:34:14,812 - distributed.scheduler - INFO - State start
2024-01-10 06:34:14,834 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:34:14,835 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:34:14,835 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:34:14,836 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:34:14,969 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35529'
2024-01-10 06:34:17,060 - distributed.scheduler - INFO - Receive client connection: Client-44d8c2ea-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:17,073 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60104
2024-01-10 06:34:17,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:17,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:17,083 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:17,084 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36201
2024-01-10 06:34:17,084 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36201
2024-01-10 06:34:17,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38497
2024-01-10 06:34:17,084 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:34:17,084 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:17,085 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:17,085 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:34:17,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tsooydpk
2024-01-10 06:34:17,085 - distributed.worker - INFO - Starting Worker plugin PreImport-263d3221-bdd0-4952-85f2-0e916c73529a
2024-01-10 06:34:17,085 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9d8fe629-1a2e-497d-bf80-97b655d21298
2024-01-10 06:34:17,085 - distributed.worker - INFO - Starting Worker plugin RMMSetup-598ccf48-1a8a-49b4-b638-6df213fedf69
2024-01-10 06:34:17,514 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:17,590 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36201', status: init, memory: 0, processing: 0>
2024-01-10 06:34:17,592 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36201
2024-01-10 06:34:17,592 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60128
2024-01-10 06:34:17,593 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:17,594 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:34:17,594 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:17,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:34:17,629 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:34:17,633 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:17,635 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:17,637 - distributed.scheduler - INFO - Remove client Client-44d8c2ea-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:17,638 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60104; closing.
2024-01-10 06:34:17,638 - distributed.scheduler - INFO - Remove client Client-44d8c2ea-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:17,638 - distributed.scheduler - INFO - Close client connection: Client-44d8c2ea-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:17,639 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35529'. Reason: nanny-close
2024-01-10 06:34:17,640 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:17,641 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36201. Reason: nanny-close
2024-01-10 06:34:17,643 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:34:17,643 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60128; closing.
2024-01-10 06:34:17,643 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36201', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868457.6436212')
2024-01-10 06:34:17,643 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:34:17,644 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:18,254 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:34:18,255 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:34:18,255 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:34:18,256 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:34:18,257 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-10 06:34:20,299 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:34:20,303 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:34:20,307 - distributed.scheduler - INFO - State start
2024-01-10 06:34:20,329 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:34:20,330 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:34:20,330 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:34:20,331 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:34:20,471 - distributed.scheduler - INFO - Receive client connection: Client-48285b38-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:20,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54792
2024-01-10 06:34:20,559 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39265'
2024-01-10 06:34:22,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:34:22,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:34:22,228 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:34:22,229 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39267
2024-01-10 06:34:22,229 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39267
2024-01-10 06:34:22,229 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40343
2024-01-10 06:34:22,229 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:34:22,229 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:22,229 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:34:22,229 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:34:22,229 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-l23nt_ev
2024-01-10 06:34:22,230 - distributed.worker - INFO - Starting Worker plugin PreImport-c9cdaae3-9b92-44c3-9bd0-abf56da089ac
2024-01-10 06:34:22,230 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a5fc0d4d-ff35-4ce4-8cad-0d5edf8af2fe
2024-01-10 06:34:22,230 - distributed.worker - INFO - Starting Worker plugin RMMSetup-049da5e6-01c4-4fe4-b0b3-33b2cca43ac1
2024-01-10 06:34:22,544 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:22,617 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39267', status: init, memory: 0, processing: 0>
2024-01-10 06:34:22,618 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39267
2024-01-10 06:34:22,618 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54824
2024-01-10 06:34:22,619 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:34:22,620 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:34:22,620 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:34:22,621 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:34:22,648 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-10 06:34:22,652 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:34:22,656 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:22,657 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:34:22,660 - distributed.scheduler - INFO - Remove client Client-48285b38-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:22,660 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54792; closing.
2024-01-10 06:34:22,660 - distributed.scheduler - INFO - Remove client Client-48285b38-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:22,660 - distributed.scheduler - INFO - Close client connection: Client-48285b38-af82-11ee-b9a6-d8c49764f6bb
2024-01-10 06:34:22,661 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39265'. Reason: nanny-close
2024-01-10 06:34:22,662 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:34:22,663 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39267. Reason: nanny-close
2024-01-10 06:34:22,665 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:34:22,665 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54824; closing.
2024-01-10 06:34:22,665 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39267', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868462.6652925')
2024-01-10 06:34:22,665 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:34:22,666 - distributed.nanny - INFO - Worker closed
2024-01-10 06:34:23,527 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:34:23,527 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:34:23,528 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:34:23,529 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:34:23,529 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37705 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38505 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45823 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41327 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46107 instead
  warnings.warn(
2024-01-10 06:35:31,687 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:42877 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7f4b9c9ba740 error: Connection reset by remote peer')
2024-01-10 06:35:31,730 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 738, in wait
  File "libucxx.pyx", line 723, in wait_yield
  File "libucxx.pyx", line 718, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-10 06:35:31,732 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 738, in wait
  File "libucxx.pyx", line 723, in wait_yield
  File "libucxx.pyx", line 718, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
Process SpawnProcess-6:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 200, in _test_ucx_infiniband_nvlink
    assert all(client.run(check_ucx_options).values())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2998, in run
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2903, in _run
    raise exc
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7f4b9c9ba740 error: Connection reset by remote peer')
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33301 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33171 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42387 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34199 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35823 instead
  warnings.warn(
[1704868626.376155] [dgx13:70230:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:34234) failed: Address already in use
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34483 instead
  warnings.warn(
[1704868640.693485] [dgx13:70467:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:58300) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46333 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39563 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41887 instead
  warnings.warn(
[1704868667.874226] [dgx13:70928:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:41970) failed: Address already in use
[1704868669.690032] [dgx13:71017:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:48824) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41993 instead
  warnings.warn(
2024-01-10 06:38:08,294 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:58398 remote=tcp://127.0.0.1:43489>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33383 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46705 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33031 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45943 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36367 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33779 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35983 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40795 instead
  warnings.warn(
[1704868866.469268] [dgx13:74673:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:37416) failed: Address already in use
[1704868866.469322] [dgx13:74673:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:33602) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45351 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41445 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44609 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42161 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38155 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45665 instead
  warnings.warn(
[1704869024.801100] [dgx13:76884:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:34064) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40657 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43493 instead
  warnings.warn(
2024-01-10 06:45:22,397 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 738, in wait
  File "libucxx.pyx", line 723, in wait_yield
  File "libucxx.pyx", line 718, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-10 06:45:22,402 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 738, in wait
  File "libucxx.pyx", line 723, in wait_yield
  File "libucxx.pyx", line 718, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37189 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38781 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] [1704869252.334316] [dgx13:80335:0]            sock.c:481  UCX  ERROR bind(fd=125 addr=0.0.0.0:48374) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37757 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34997 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34989 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43443 instead
  warnings.warn(
[1704869345.294904] [dgx13:81804:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:52718) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42793 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42713 instead
  warnings.warn(
[1704869398.445969] [dgx13:82542:0]            sock.c:481  UCX  ERROR bind(fd=124 addr=0.0.0.0:44524) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40015 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42041 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35361 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46517 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42819 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38777 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35899 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39347 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39503 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38151 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40857 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41767 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44751 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44067 instead
  warnings.warn(
[1704869721.059176] [dgx13:86867:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:48434) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37051 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36805 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41689 instead
  warnings.warn(
[1704869757.781616] [dgx13:87314:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:36824) failed: Address already in use
[1704869762.023211] [dgx13:87305:0]            sock.c:481  UCX  ERROR bind(fd=155 addr=0.0.0.0:57228) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40477 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35305 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42895 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38857 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1704869895.348545] [dgx13:89085:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:48986) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] [1704869899.211008] [dgx13:63910:0]            sock.c:481  UCX  ERROR bind(fd=243 addr=0.0.0.0:41016) failed: Address already in use
[1704869899.562744] [dgx13:63910:0]            sock.c:481  UCX  ERROR bind(fd=254 addr=0.0.0.0:45418) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38507 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45827 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43419 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38389 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1704869973.618226] [dgx13:63910:1]            sock.c:481  UCX  ERROR bind(fd=244 addr=0.0.0.0:53672) failed: Address already in use
[1704869973.646194] [dgx13:63910:1]            sock.c:481  UCX  ERROR bind(fd=252 addr=0.0.0.0:37226) failed: Address already in use
[1704869978.922462] [dgx13:89892:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:42130) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] [1704869980.798872] [dgx13:63910:1]            sock.c:481  UCX  ERROR bind(fd=248 addr=0.0.0.0:32862) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] [1704869987.617683] [dgx13:63910:1]            sock.c:481  UCX  ERROR bind(fd=251 addr=0.0.0.0:41178) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-10 07:00:17,342 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:17,342 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:17,350 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:17,350 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:17,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:17,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:17,434 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:17,434 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:17,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:17,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:17,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:17,508 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:17,616 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:17,616 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:17,624 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:17,624 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:17,965 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:17,966 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36525
2024-01-10 07:00:17,966 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36525
2024-01-10 07:00:17,966 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45351
2024-01-10 07:00:17,966 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44667
2024-01-10 07:00:17,966 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:17,966 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:17,966 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wgvz6iii
2024-01-10 07:00:17,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0007cb5-966c-4602-8fac-d7e6aa2869ae
2024-01-10 07:00:17,967 - distributed.worker - INFO - Starting Worker plugin PreImport-9a2a942a-70d2-41db-a6ce-23901407acb7
2024-01-10 07:00:17,967 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80d0dd29-f181-432d-9257-9ba40bb81e41
2024-01-10 07:00:17,967 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:17,983 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:17,983 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46319
2024-01-10 07:00:17,984 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46319
2024-01-10 07:00:17,984 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42545
2024-01-10 07:00:17,984 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44667
2024-01-10 07:00:17,984 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:17,984 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:17,984 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-akp6_b3y
2024-01-10 07:00:17,984 - distributed.worker - INFO - Starting Worker plugin PreImport-f5ec5696-d3f2-439c-bd6f-c24482ac2d17
2024-01-10 07:00:17,984 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f46e25d8-bae9-4410-9edc-0a6b91c1d1c1
2024-01-10 07:00:17,984 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-993b2cba-26de-4ac0-9cbe-780056c5aa08
2024-01-10 07:00:17,986 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,022 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:18,023 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33621
2024-01-10 07:00:18,023 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33621
2024-01-10 07:00:18,023 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38523
2024-01-10 07:00:18,023 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,023 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,024 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:18,024 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ndzk_feu
2024-01-10 07:00:18,024 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2643c4d-ea55-4c07-aac7-854f8b31c7e8
2024-01-10 07:00:18,024 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5004f2c-9ea2-4ebe-aed6-39c7c65f5406
2024-01-10 07:00:18,025 - distributed.worker - INFO - Starting Worker plugin PreImport-b223a115-ea8d-48a0-b7d5-0b2ad0726046
2024-01-10 07:00:18,025 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,037 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:18,038 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,038 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44667
2024-01-10 07:00:18,080 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:18,081 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,081 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,082 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44667
2024-01-10 07:00:18,102 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:18,103 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,103 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,104 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44667
2024-01-10 07:00:18,106 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:18,107 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40819
2024-01-10 07:00:18,107 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40819
2024-01-10 07:00:18,107 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44233
2024-01-10 07:00:18,108 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,108 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,108 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:18,108 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-paqzw69z
2024-01-10 07:00:18,108 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a271c14c-20e7-4496-af16-febf4103e9dd
2024-01-10 07:00:18,108 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2273cfc2-a001-4450-b039-a2965eafd6c9
2024-01-10 07:00:18,108 - distributed.worker - INFO - Starting Worker plugin PreImport-ec8fb38b-2abc-44ac-a3b4-9a0983ec64e7
2024-01-10 07:00:18,108 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,157 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:18,158 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36435
2024-01-10 07:00:18,158 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36435
2024-01-10 07:00:18,158 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40049
2024-01-10 07:00:18,158 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,158 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,159 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:18,159 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tfg0yy4j
2024-01-10 07:00:18,159 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed58ae1d-dc0d-4214-b0ef-dc27fc18ea11
2024-01-10 07:00:18,159 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a01c70f-6bd0-4afd-beef-c92f5eb25609
2024-01-10 07:00:18,159 - distributed.worker - INFO - Starting Worker plugin PreImport-ebd220ec-6d51-4271-bf6e-2ff8cc861789
2024-01-10 07:00:18,160 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,166 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:18,167 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36527
2024-01-10 07:00:18,167 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36527
2024-01-10 07:00:18,168 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34779
2024-01-10 07:00:18,168 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,168 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,168 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:18,168 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m9x4resd
2024-01-10 07:00:18,168 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d3b7018-b53e-48b6-a7ca-4a663775cc0c
2024-01-10 07:00:18,170 - distributed.worker - INFO - Starting Worker plugin PreImport-8f8eedc8-e5e2-47a7-827f-20983fd2d06b
2024-01-10 07:00:18,170 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd9c53ad-383c-4d52-905a-6b7f042c0a39
2024-01-10 07:00:18,170 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,172 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:18,173 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,173 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,174 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44667
2024-01-10 07:00:18,241 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:18,242 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,242 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,243 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44667
2024-01-10 07:00:18,254 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:18,255 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,256 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44667
2024-01-10 07:00:18,273 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:18,274 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33145
2024-01-10 07:00:18,274 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33145
2024-01-10 07:00:18,274 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38503
2024-01-10 07:00:18,274 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,274 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,274 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:18,274 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qwjhkpyt
2024-01-10 07:00:18,275 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c7a87a8-a677-4913-99f9-b5ef111df0a4
2024-01-10 07:00:18,275 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-30fbb17b-06a4-4f66-88db-2abf91761480
2024-01-10 07:00:18,275 - distributed.worker - INFO - Starting Worker plugin PreImport-c601ffe1-58a3-48a8-953c-b5e8d724a2cd
2024-01-10 07:00:18,275 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,302 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:18,303 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36093
2024-01-10 07:00:18,304 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36093
2024-01-10 07:00:18,304 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44327
2024-01-10 07:00:18,304 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,304 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,304 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:18,304 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5v_o96s1
2024-01-10 07:00:18,304 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6ae7a32-f915-469c-86b6-645caaf936a1
2024-01-10 07:00:18,304 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5069864d-601f-4640-91e5-f54629420952
2024-01-10 07:00:18,304 - distributed.worker - INFO - Starting Worker plugin PreImport-12094e36-1e97-4d95-ac36-811c9ca14caa
2024-01-10 07:00:18,305 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:18,346 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,346 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,347 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44667
2024-01-10 07:00:18,374 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:18,375 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44667
2024-01-10 07:00:18,375 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:18,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44667
2024-01-10 07:00:18,391 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 07:00:18,392 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 07:00:18,392 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 07:00:18,392 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 07:00:18,392 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 07:00:18,392 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 07:00:18,392 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 07:00:18,392 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 07:00:18,398 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33621. Reason: nanny-close
2024-01-10 07:00:18,399 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46319. Reason: nanny-close
2024-01-10 07:00:18,399 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36525. Reason: nanny-close
2024-01-10 07:00:18,400 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40819. Reason: nanny-close
2024-01-10 07:00:18,400 - distributed.core - INFO - Connection to tcp://127.0.0.1:44667 has been closed.
2024-01-10 07:00:18,401 - distributed.core - INFO - Connection to tcp://127.0.0.1:44667 has been closed.
2024-01-10 07:00:18,401 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36435. Reason: nanny-close
2024-01-10 07:00:18,401 - distributed.core - INFO - Connection to tcp://127.0.0.1:44667 has been closed.
2024-01-10 07:00:18,401 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36527. Reason: nanny-close
2024-01-10 07:00:18,401 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36093. Reason: nanny-close
2024-01-10 07:00:18,401 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:18,402 - distributed.core - INFO - Connection to tcp://127.0.0.1:44667 has been closed.
2024-01-10 07:00:18,402 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:18,402 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33145. Reason: nanny-close
2024-01-10 07:00:18,402 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:18,403 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:18,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:44667 has been closed.
2024-01-10 07:00:18,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:44667 has been closed.
2024-01-10 07:00:18,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:44667 has been closed.
2024-01-10 07:00:18,404 - distributed.core - INFO - Connection to tcp://127.0.0.1:44667 has been closed.
2024-01-10 07:00:18,404 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:18,404 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:18,404 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:18,405 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-10 07:00:53,540 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:53,540 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:53,544 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:53,545 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46171
2024-01-10 07:00:53,545 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46171
2024-01-10 07:00:53,545 - distributed.worker - INFO -           Worker name:                          0
2024-01-10 07:00:53,546 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46675
2024-01-10 07:00:53,546 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42059
2024-01-10 07:00:53,546 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:53,546 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:53,546 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 07:00:53,546 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e7tx9vky
2024-01-10 07:00:53,546 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b2688e2-06e2-47e5-90f6-3ac99b500042
2024-01-10 07:00:53,546 - distributed.worker - INFO - Starting Worker plugin PreImport-baf9fc9d-da21-4665-b1ad-84f43315f4e0
2024-01-10 07:00:53,550 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-10 07:00:53,551 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c855bd20-18d7-475a-856d-3d7a36881c1e
2024-01-10 07:00:53,551 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46171. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-10 07:00:53,551 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-10 07:00:53,552 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-10 07:00:57,881 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:57,881 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:57,950 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:57,950 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:58,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:58,021 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:58,022 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:58,022 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:58,080 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:58,080 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:58,093 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:58,093 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:58,143 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:58,143 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:58,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 07:00:58,198 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 07:00:58,518 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:58,519 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36205
2024-01-10 07:00:58,519 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36205
2024-01-10 07:00:58,519 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38303
2024-01-10 07:00:58,519 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,519 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,519 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:58,519 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 07:00:58,519 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m4cht406
2024-01-10 07:00:58,519 - distributed.worker - INFO - Starting Worker plugin PreImport-da912a39-1bdd-4a89-a4ab-7beb10be4ca9
2024-01-10 07:00:58,519 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b7f3250-1d98-492c-983e-665ca6cbcf17
2024-01-10 07:00:58,520 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b40ce253-10d0-4560-9df0-16561719e34f
2024-01-10 07:00:58,520 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,585 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:58,586 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46133
2024-01-10 07:00:58,586 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46133
2024-01-10 07:00:58,586 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37115
2024-01-10 07:00:58,586 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,586 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,586 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:58,586 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 07:00:58,586 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0gcnzn79
2024-01-10 07:00:58,586 - distributed.worker - INFO - Starting Worker plugin RMMSetup-112e632a-364f-488d-a702-52819931a0ea
2024-01-10 07:00:58,587 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f0df0c9-af2d-49d3-8e12-c4226f7ebae0
2024-01-10 07:00:58,587 - distributed.worker - INFO - Starting Worker plugin PreImport-c5921d7b-010e-44b9-a9ee-82a347468fa1
2024-01-10 07:00:58,588 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,645 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:58,645 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40867
2024-01-10 07:00:58,645 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40867
2024-01-10 07:00:58,645 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40213
2024-01-10 07:00:58,646 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,646 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,646 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:58,646 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 07:00:58,646 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uvgotv8x
2024-01-10 07:00:58,646 - distributed.worker - INFO - Starting Worker plugin PreImport-068a408c-0911-47c3-ba66-608da9b9159d
2024-01-10 07:00:58,646 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-697782e8-bb8c-4960-aa5c-30b3d4e08475
2024-01-10 07:00:58,646 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6cb6171a-14c3-4b99-9f00-195439e05a26
2024-01-10 07:00:58,646 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,652 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:58,653 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36853
2024-01-10 07:00:58,653 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36853
2024-01-10 07:00:58,653 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39037
2024-01-10 07:00:58,653 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,653 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,653 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:58,653 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 07:00:58,653 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ebw9x6vg
2024-01-10 07:00:58,654 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0dc8434d-970d-4730-8d63-1e0001b48268
2024-01-10 07:00:58,654 - distributed.worker - INFO - Starting Worker plugin PreImport-112741e3-1223-4926-9897-53d916ff1404
2024-01-10 07:00:58,654 - distributed.worker - INFO - Starting Worker plugin RMMSetup-89d03544-af07-4737-9a7f-b385196704f0
2024-01-10 07:00:58,654 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,706 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:58,707 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32783
2024-01-10 07:00:58,707 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32783
2024-01-10 07:00:58,707 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39067
2024-01-10 07:00:58,707 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,707 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,707 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:58,707 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 07:00:58,707 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8flhlq3l
2024-01-10 07:00:58,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3e99a513-92c3-4cbf-9fd5-05d81fd149b9
2024-01-10 07:00:58,708 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9be4d5e0-dc24-49d6-890b-5ddc44d04e1d
2024-01-10 07:00:58,708 - distributed.worker - INFO - Starting Worker plugin PreImport-2a2c0d1d-2e85-428a-bfd4-bbcab0bd0065
2024-01-10 07:00:58,708 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,735 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:58,736 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42193
2024-01-10 07:00:58,736 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42193
2024-01-10 07:00:58,736 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39679
2024-01-10 07:00:58,736 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,736 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,736 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:58,736 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 07:00:58,736 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-55ljwcrt
2024-01-10 07:00:58,737 - distributed.worker - INFO - Starting Worker plugin PreImport-68b1486d-1294-4a8c-8406-869ade80431a
2024-01-10 07:00:58,737 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ed22e38-4f79-411e-b703-be92b13fc196
2024-01-10 07:00:58,737 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0ede8316-e9ab-472b-b052-e109b3268563
2024-01-10 07:00:58,737 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,764 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:58,765 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35233
2024-01-10 07:00:58,765 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35233
2024-01-10 07:00:58,765 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36993
2024-01-10 07:00:58,765 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,765 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,765 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:58,765 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 07:00:58,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pfmpkxi1
2024-01-10 07:00:58,766 - distributed.worker - INFO - Starting Worker plugin PreImport-ed58938e-fd62-4a87-ad53-0286e604187c
2024-01-10 07:00:58,766 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3074b0ad-a667-4e6c-9976-a6d60166c758
2024-01-10 07:00:58,766 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d89e04c-e0aa-4787-9eb6-166dc8bdf966
2024-01-10 07:00:58,766 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,775 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:58,776 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,776 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36011
2024-01-10 07:00:58,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:58,808 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,808 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36011
2024-01-10 07:00:58,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 07:00:58,848 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37957
2024-01-10 07:00:58,848 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37957
2024-01-10 07:00:58,849 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38021
2024-01-10 07:00:58,849 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,849 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,849 - distributed.worker - INFO -               Threads:                          1
2024-01-10 07:00:58,849 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 07:00:58,849 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iv2ehbl_
2024-01-10 07:00:58,849 - distributed.worker - INFO - Starting Worker plugin PreImport-bb222e78-e6e4-428a-a708-a03a2b0cf7bd
2024-01-10 07:00:58,849 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7b0ec7be-8ab2-451d-8c2d-2d597179a140
2024-01-10 07:00:58,849 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60c8ffb0-035b-493d-8217-f0e41d20d6dc
2024-01-10 07:00:58,850 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,889 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:58,890 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,890 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36011
2024-01-10 07:00:58,891 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:58,892 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,892 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,893 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36011
2024-01-10 07:00:58,925 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:58,926 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,926 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36011
2024-01-10 07:00:58,939 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:58,940 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,940 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,941 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36011
2024-01-10 07:00:58,946 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:58,946 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,946 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36011
2024-01-10 07:00:58,964 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 07:00:58,965 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36011
2024-01-10 07:00:58,965 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 07:00:58,966 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36011
2024-01-10 07:00:59,002 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36205. Reason: nanny-close
2024-01-10 07:00:59,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46133. Reason: nanny-close
2024-01-10 07:00:59,003 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40867. Reason: nanny-close
2024-01-10 07:00:59,004 - distributed.core - INFO - Connection to tcp://127.0.0.1:36011 has been closed.
2024-01-10 07:00:59,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36853. Reason: nanny-close
2024-01-10 07:00:59,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42193. Reason: nanny-close
2024-01-10 07:00:59,005 - distributed.core - INFO - Connection to tcp://127.0.0.1:36011 has been closed.
2024-01-10 07:00:59,005 - distributed.core - INFO - Connection to tcp://127.0.0.1:36011 has been closed.
2024-01-10 07:00:59,006 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:59,006 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37957. Reason: nanny-close
2024-01-10 07:00:59,006 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32783. Reason: nanny-close
2024-01-10 07:00:59,007 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:59,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:36011 has been closed.
2024-01-10 07:00:59,007 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:59,007 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35233. Reason: nanny-close
2024-01-10 07:00:59,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:36011 has been closed.
2024-01-10 07:00:59,008 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:59,008 - distributed.core - INFO - Connection to tcp://127.0.0.1:36011 has been closed.
2024-01-10 07:00:59,008 - distributed.core - INFO - Connection to tcp://127.0.0.1:36011 has been closed.
2024-01-10 07:00:59,008 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:59,009 - distributed.core - INFO - Connection to tcp://127.0.0.1:36011 has been closed.
2024-01-10 07:00:59,009 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:59,010 - distributed.nanny - INFO - Worker closed
2024-01-10 07:00:59,010 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-01-10 07:01:13,650 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-10 07:01:13,654 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk PASSED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-10 07:02:37,240 - distributed.worker - WARNING - RMM allocation of 1.00 MiB failed, spill-on-demand couldn't find any device memory to spill.
RMM allocs: 1.00 MiB, <ProxyManager dev_limit=25.60 GiB host_limit=0.98 TiB disk=0 B(0) host=0 B(0) dev=0 B(0)>, traceback:
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 937, in _bootstrap
    self._bootstrap_inner()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/threadpoolexecutor.py", line 57, in _worker
    task.run()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/_concurrent_futures_thread.py", line 65, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1541, in <lambda>
    executor, lambda: context.run(func, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2954, in apply_function
    msg = apply_function_simple(function, args, kwargs, time_delay)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2990, in apply_function_simple
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_proxify_host_file.py", line 467, in task
    rmm.DeviceBuffer(size=rmm_pool_size),  # Trigger OOM
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/proxify_host_file.py", line 617, in oom
    traceback.print_stack(file=f)


2024-01-10 07:02:37,466 - distributed.worker - WARNING - Compute Failed
Key:       task-5c302f54392a5ca98adb3c55c8357b0e
Function:  task
args:      ()
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_serializer PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[numpy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[cupy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_name PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[False] /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 9 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
