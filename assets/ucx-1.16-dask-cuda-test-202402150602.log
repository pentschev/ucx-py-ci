============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-8.0.0, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.5
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-02-15 06:58:35,863 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:58:35,867 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-15 06:58:35,871 - distributed.scheduler - INFO - State start
2024-02-15 06:58:35,896 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:58:35,897 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-15 06:58:35,898 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-15 06:58:35,898 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 06:58:35,984 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46867'
2024-02-15 06:58:36,003 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35009'
2024-02-15 06:58:36,006 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34967'
2024-02-15 06:58:36,013 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45853'
2024-02-15 06:58:37,507 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45979', status: init, memory: 0, processing: 0>
2024-02-15 06:58:37,524 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45979
2024-02-15 06:58:37,525 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58528
2024-02-15 06:58:37,537 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58528; closing.
2024-02-15 06:58:37,540 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45979', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980317.5395265')
2024-02-15 06:58:37,541 - distributed.scheduler - INFO - Lost all workers
2024-02-15 06:58:37,744 - distributed.scheduler - INFO - Receive client connection: Client-a277e89d-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:58:37,746 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58530
2024-02-15 06:58:37,841 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:37,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:37,846 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:37,847 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37479
2024-02-15 06:58:37,847 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37479
2024-02-15 06:58:37,847 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43975
2024-02-15 06:58:37,847 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-15 06:58:37,847 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:37,847 - distributed.worker - INFO -               Threads:                          4
2024-02-15 06:58:37,847 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-15 06:58:37,847 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-rg6gkg2g
2024-02-15 06:58:37,847 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25df0720-2d1f-48f5-b4d2-0314c43a123c
2024-02-15 06:58:37,847 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ae7316f-54cd-49ba-bccf-3bedfec247a5
2024-02-15 06:58:37,849 - distributed.worker - INFO - Starting Worker plugin PreImport-e837b520-25d0-486a-bb05-89848863f548
2024-02-15 06:58:37,849 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:37,905 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37479', status: init, memory: 0, processing: 0>
2024-02-15 06:58:37,906 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37479
2024-02-15 06:58:37,906 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58554
2024-02-15 06:58:37,907 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:37,908 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-15 06:58:37,908 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:37,909 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-15 06:58:37,935 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:37,935 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:37,939 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:37,940 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46471
2024-02-15 06:58:37,940 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46471
2024-02-15 06:58:37,940 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42073
2024-02-15 06:58:37,940 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-15 06:58:37,940 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:37,940 - distributed.worker - INFO -               Threads:                          4
2024-02-15 06:58:37,940 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-15 06:58:37,940 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-iol8lft_
2024-02-15 06:58:37,941 - distributed.worker - INFO - Starting Worker plugin RMMSetup-424d3ee1-ada7-49b8-bc02-7c8231993568
2024-02-15 06:58:37,941 - distributed.worker - INFO - Starting Worker plugin PreImport-28d63d87-66df-4f9b-a769-e9afc565ccdf
2024-02-15 06:58:37,941 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5fed814-3e2e-42ef-9416-a771e4998fa8
2024-02-15 06:58:37,941 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:37,942 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:37,942 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:37,946 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:37,947 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45473
2024-02-15 06:58:37,947 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45473
2024-02-15 06:58:37,947 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34073
2024-02-15 06:58:37,947 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:37,947 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-15 06:58:37,947 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:37,947 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:37,947 - distributed.worker - INFO -               Threads:                          4
2024-02-15 06:58:37,947 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-15 06:58:37,948 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-k8ab10ug
2024-02-15 06:58:37,948 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ef7d44e5-02ea-4518-9c39-d4ccab9e31be
2024-02-15 06:58:37,949 - distributed.worker - INFO - Starting Worker plugin PreImport-0a954f74-7cd2-45f5-8498-435558a08848
2024-02-15 06:58:37,949 - distributed.worker - INFO - Starting Worker plugin RMMSetup-190f5e4e-c8ce-46d1-86bb-ff7e00ec04a5
2024-02-15 06:58:37,949 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:37,952 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:37,952 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44569
2024-02-15 06:58:37,952 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44569
2024-02-15 06:58:37,953 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46005
2024-02-15 06:58:37,953 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-15 06:58:37,953 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:37,953 - distributed.worker - INFO -               Threads:                          4
2024-02-15 06:58:37,953 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-02-15 06:58:37,953 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-lmibo7dz
2024-02-15 06:58:37,953 - distributed.worker - INFO - Starting Worker plugin PreImport-a55fa14a-c0c3-4cd9-a812-1f85a24c9849
2024-02-15 06:58:37,953 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f2659fb-8a5d-4a2f-8fd7-9b4f46a92d75
2024-02-15 06:58:37,953 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8f4f800-26b8-425c-9e77-13879bdc105f
2024-02-15 06:58:37,953 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:38,040 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46471', status: init, memory: 0, processing: 0>
2024-02-15 06:58:38,041 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46471
2024-02-15 06:58:38,041 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58558
2024-02-15 06:58:38,042 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:38,043 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-15 06:58:38,043 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:38,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-15 06:58:38,060 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45473', status: init, memory: 0, processing: 0>
2024-02-15 06:58:38,061 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45473
2024-02-15 06:58:38,061 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58562
2024-02-15 06:58:38,062 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:38,062 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-15 06:58:38,062 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:38,064 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-15 06:58:38,065 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44569', status: init, memory: 0, processing: 0>
2024-02-15 06:58:38,066 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44569
2024-02-15 06:58:38,066 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58578
2024-02-15 06:58:38,067 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:38,067 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-15 06:58:38,067 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:38,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-15 06:58:38,168 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-15 06:58:38,168 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-15 06:58:38,168 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-15 06:58:38,168 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-02-15 06:58:38,175 - distributed.scheduler - INFO - Remove client Client-a277e89d-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:58:38,176 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58530; closing.
2024-02-15 06:58:38,176 - distributed.scheduler - INFO - Remove client Client-a277e89d-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:58:38,176 - distributed.scheduler - INFO - Close client connection: Client-a277e89d-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:58:38,177 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46867'. Reason: nanny-close
2024-02-15 06:58:38,178 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:58:38,178 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35009'. Reason: nanny-close
2024-02-15 06:58:38,179 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:58:38,179 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34967'. Reason: nanny-close
2024-02-15 06:58:38,179 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44569. Reason: nanny-close
2024-02-15 06:58:38,179 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:58:38,180 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45853'. Reason: nanny-close
2024-02-15 06:58:38,180 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45473. Reason: nanny-close
2024-02-15 06:58:38,180 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:58:38,180 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37479. Reason: nanny-close
2024-02-15 06:58:38,180 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46471. Reason: nanny-close
2024-02-15 06:58:38,181 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-15 06:58:38,181 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-15 06:58:38,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58554; closing.
2024-02-15 06:58:38,182 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-15 06:58:38,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58562; closing.
2024-02-15 06:58:38,182 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-15 06:58:38,183 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:38,183 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37479', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980318.1831014')
2024-02-15 06:58:38,183 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:38,183 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:38,183 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45473', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980318.1837492')
2024-02-15 06:58:38,184 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:38,184 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58558; closing.
2024-02-15 06:58:38,184 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58578; closing.
2024-02-15 06:58:38,185 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46471', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980318.185051')
2024-02-15 06:58:38,185 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44569', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980318.1855397')
2024-02-15 06:58:38,185 - distributed.scheduler - INFO - Lost all workers
2024-02-15 06:58:38,186 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:58578>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-15 06:58:38,190 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:58558>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-15 06:58:38,994 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 06:58:38,995 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 06:58:38,995 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 06:58:38,997 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-15 06:58:38,998 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-02-15 06:58:41,522 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:58:41,528 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44465 instead
  warnings.warn(
2024-02-15 06:58:41,533 - distributed.scheduler - INFO - State start
2024-02-15 06:58:41,685 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:58:41,686 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-15 06:58:41,687 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 06:58:41,688 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-15 06:58:41,832 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33571'
2024-02-15 06:58:41,844 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34253'
2024-02-15 06:58:41,855 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40183'
2024-02-15 06:58:41,871 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39033'
2024-02-15 06:58:41,874 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36817'
2024-02-15 06:58:41,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42349'
2024-02-15 06:58:41,892 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34793'
2024-02-15 06:58:41,903 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40305'
2024-02-15 06:58:43,790 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:43,790 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:43,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:43,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:43,793 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:43,793 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:43,794 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:43,795 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44337
2024-02-15 06:58:43,795 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44337
2024-02-15 06:58:43,795 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34139
2024-02-15 06:58:43,795 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:43,795 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:43,795 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:43,795 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:43,795 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:43,795 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-00x37kbo
2024-02-15 06:58:43,795 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7b7067e7-f1d3-4d78-ab4c-63223d635697
2024-02-15 06:58:43,796 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40327
2024-02-15 06:58:43,796 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40327
2024-02-15 06:58:43,796 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34357
2024-02-15 06:58:43,796 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:43,796 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:43,796 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:43,796 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:43,796 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-okth745i
2024-02-15 06:58:43,796 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96e084a2-acc1-4403-b4af-c0064e7a274f
2024-02-15 06:58:43,797 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:43,797 - distributed.worker - INFO - Starting Worker plugin PreImport-7907acf6-9566-41dc-b655-9a1fc4623741
2024-02-15 06:58:43,797 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2047f828-6dbb-4948-9f3c-88c84e2b4d89
2024-02-15 06:58:43,798 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36511
2024-02-15 06:58:43,798 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36511
2024-02-15 06:58:43,798 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46601
2024-02-15 06:58:43,798 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:43,798 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:43,798 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:43,798 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:43,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1vupy32o
2024-02-15 06:58:43,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb2de084-a9f7-40fe-b313-ac2801b6ebc9
2024-02-15 06:58:43,798 - distributed.worker - INFO - Starting Worker plugin PreImport-d544cd0a-622e-4774-a388-9562cffb16fd
2024-02-15 06:58:43,798 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cc0e644-b04f-41f4-957b-0c4dc717aafd
2024-02-15 06:58:43,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:43,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:43,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:43,807 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44309
2024-02-15 06:58:43,807 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44309
2024-02-15 06:58:43,807 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44211
2024-02-15 06:58:43,807 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:43,807 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:43,807 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:43,807 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:43,807 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ikod5qd
2024-02-15 06:58:43,807 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a40f6cd-7323-4128-ac19-dbdc6a83206b
2024-02-15 06:58:43,808 - distributed.worker - INFO - Starting Worker plugin PreImport-3e480aa3-64f9-4c08-a97d-b08941d91e07
2024-02-15 06:58:43,808 - distributed.worker - INFO - Starting Worker plugin RMMSetup-968604d7-9410-4104-97b3-079b4b8c991d
2024-02-15 06:58:43,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:43,957 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:43,962 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:43,963 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41619
2024-02-15 06:58:43,963 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41619
2024-02-15 06:58:43,963 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38153
2024-02-15 06:58:43,963 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:43,963 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:43,963 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:43,963 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:43,963 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wbwzwp8d
2024-02-15 06:58:43,963 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1b09230-24bd-4bfa-bb42-bb3e8f8d1703
2024-02-15 06:58:44,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:44,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:44,302 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:44,304 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43977
2024-02-15 06:58:44,304 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43977
2024-02-15 06:58:44,304 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44855
2024-02-15 06:58:44,304 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:44,304 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:44,304 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:44,304 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:44,304 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-75azgqii
2024-02-15 06:58:44,305 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a98cb918-a5bf-4959-921a-a759ca7fb600
2024-02-15 06:58:44,305 - distributed.worker - INFO - Starting Worker plugin PreImport-26ca0a5a-9f4a-485c-b0d7-5f66f191bbb6
2024-02-15 06:58:44,305 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e8c30d7f-a5e0-4253-ad2b-c2a546538ee9
2024-02-15 06:58:44,477 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:44,477 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:44,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:44,482 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:44,484 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:44,485 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44441
2024-02-15 06:58:44,485 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44441
2024-02-15 06:58:44,485 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37027
2024-02-15 06:58:44,485 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:44,486 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:44,486 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:44,486 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:44,486 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3pwydvcf
2024-02-15 06:58:44,486 - distributed.worker - INFO - Starting Worker plugin RMMSetup-98dace3d-f5bc-4243-b44d-09c55e07cbb6
2024-02-15 06:58:44,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:44,493 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38783
2024-02-15 06:58:44,493 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38783
2024-02-15 06:58:44,493 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34117
2024-02-15 06:58:44,493 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:44,493 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:44,493 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:44,493 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:44,493 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nzgff5w1
2024-02-15 06:58:44,494 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e094280-94e0-4112-ad6b-50886432ac6a
2024-02-15 06:58:44,494 - distributed.worker - INFO - Starting Worker plugin PreImport-1092452f-84c6-4ad2-a3f4-9e3999d5635d
2024-02-15 06:58:44,494 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85e2f2c5-4e42-41c4-8b2a-c818dd949b25
2024-02-15 06:58:46,105 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33571'. Reason: nanny-close
2024-02-15 06:58:46,105 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34253'. Reason: nanny-close
2024-02-15 06:58:46,105 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40183'. Reason: nanny-close
2024-02-15 06:58:46,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39033'. Reason: nanny-close
2024-02-15 06:58:46,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36817'. Reason: nanny-close
2024-02-15 06:58:46,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42349'. Reason: nanny-close
2024-02-15 06:58:46,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34793'. Reason: nanny-close
2024-02-15 06:58:46,106 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40305'. Reason: nanny-close
2024-02-15 06:58:48,561 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:48,575 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:48,638 - distributed.worker - INFO - Starting Worker plugin PreImport-e073d59a-2e90-4f61-80de-725003a90de5
2024-02-15 06:58:48,639 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7a7369dc-c361-46a4-a073-a6325286c8a2
2024-02-15 06:58:48,641 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:48,668 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:48,677 - distributed.worker - INFO - Starting Worker plugin PreImport-e3f746bd-c913-4b3c-b8d2-29994dfbf1ca
2024-02-15 06:58:48,678 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2b1e5959-d34e-42cf-bd5c-3918af3a5881
2024-02-15 06:58:48,678 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:48,689 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:48,698 - distributed.worker - INFO - Starting Worker plugin PreImport-84c11958-bf80-498d-ad10-f5878c06240e
2024-02-15 06:58:48,698 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be067598-eb8d-4613-9ba5-dce880f1ad1b
2024-02-15 06:58:48,704 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:48,710 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:50,785 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:50,786 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:50,786 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:50,787 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:50,822 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:58:50,824 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43977. Reason: nanny-close
2024-02-15 06:58:50,826 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:50,827 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:50,846 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:50,847 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:50,847 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:50,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:50,864 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:50,865 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:50,865 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:50,867 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:50,873 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:58:50,874 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:58:50,874 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44309. Reason: nanny-close
2024-02-15 06:58:50,874 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44441. Reason: nanny-close
2024-02-15 06:58:50,876 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:50,877 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:50,878 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:50,879 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:43146 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-02-15 06:58:51,149 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54326 parent=54129 started daemon>
2024-02-15 06:58:51,149 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54322 parent=54129 started daemon>
2024-02-15 06:58:51,149 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54318 parent=54129 started daemon>
2024-02-15 06:58:51,150 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54314 parent=54129 started daemon>
2024-02-15 06:58:51,150 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54309 parent=54129 started daemon>
2024-02-15 06:58:51,150 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54299 parent=54129 started daemon>
2024-02-15 06:58:51,150 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54295 parent=54129 started daemon>
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-02-15 06:58:54,106 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:58:54,111 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44467 instead
  warnings.warn(
2024-02-15 06:58:54,115 - distributed.scheduler - INFO - State start
2024-02-15 06:58:54,117 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-00x37kbo', purging
2024-02-15 06:58:54,117 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1vupy32o', purging
2024-02-15 06:58:54,118 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nzgff5w1', purging
2024-02-15 06:58:54,118 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-okth745i', purging
2024-02-15 06:58:54,118 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-wbwzwp8d', purging
2024-02-15 06:58:54,242 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:58:54,245 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-02-15 06:58:54,246 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 06:58:54,247 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-02-15 06:58:54,375 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43119'
2024-02-15 06:58:54,387 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40873'
2024-02-15 06:58:54,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46271'
2024-02-15 06:58:54,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38721'
2024-02-15 06:58:54,414 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33935'
2024-02-15 06:58:54,422 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43025'
2024-02-15 06:58:54,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45143'
2024-02-15 06:58:54,439 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44453'
2024-02-15 06:58:56,399 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:56,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:56,403 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:56,404 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41219
2024-02-15 06:58:56,404 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41219
2024-02-15 06:58:56,404 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34245
2024-02-15 06:58:56,404 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:56,404 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:56,404 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:56,405 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:56,405 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-38kwuwkc
2024-02-15 06:58:56,405 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77dccb41-fc8b-405c-8ccf-42d5ea058d34
2024-02-15 06:58:56,418 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:56,418 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:56,424 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:56,426 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35391
2024-02-15 06:58:56,426 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35391
2024-02-15 06:58:56,426 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45599
2024-02-15 06:58:56,426 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:56,426 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:56,426 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:56,426 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:56,426 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tdp5c3_1
2024-02-15 06:58:56,426 - distributed.worker - INFO - Starting Worker plugin RMMSetup-863b676a-517d-4416-83da-5d158f99865b
2024-02-15 06:58:56,445 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:56,445 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:56,452 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:56,453 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41195
2024-02-15 06:58:56,453 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41195
2024-02-15 06:58:56,453 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36673
2024-02-15 06:58:56,453 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:56,454 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:56,454 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:56,454 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:56,454 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e2epbsx1
2024-02-15 06:58:56,454 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe90f043-c5ef-435b-8868-5f9c011b7923
2024-02-15 06:58:56,454 - distributed.worker - INFO - Starting Worker plugin PreImport-a051b369-7e1d-49c9-8afa-1d6ad2abf706
2024-02-15 06:58:56,454 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e88f5849-9303-4b3c-9a5a-c1fdd76b135f
2024-02-15 06:58:56,459 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:56,459 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:56,463 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:56,463 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:56,466 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:56,467 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37907
2024-02-15 06:58:56,467 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37907
2024-02-15 06:58:56,467 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38363
2024-02-15 06:58:56,467 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:56,467 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:56,467 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:56,467 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:56,467 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-81i65rce
2024-02-15 06:58:56,468 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29eb4faa-96b0-44b4-a812-e9805de7b1a8
2024-02-15 06:58:56,468 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4b55678-8c1a-428f-8118-9447c4fef7dd
2024-02-15 06:58:56,469 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:56,470 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42473
2024-02-15 06:58:56,471 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42473
2024-02-15 06:58:56,471 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46487
2024-02-15 06:58:56,471 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:56,471 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:56,471 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:56,471 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:56,471 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zjjag42c
2024-02-15 06:58:56,471 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-afc87885-592e-4e19-bd9b-2fb36eedfcc7
2024-02-15 06:58:56,472 - distributed.worker - INFO - Starting Worker plugin PreImport-2031336d-0fcf-4316-9bf5-3c16e3ddbb0f
2024-02-15 06:58:56,472 - distributed.worker - INFO - Starting Worker plugin RMMSetup-439a7c1c-f0ae-4d81-9368-64f786261ce1
2024-02-15 06:58:56,508 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:56,508 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:56,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:56,510 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:56,513 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:56,514 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40807
2024-02-15 06:58:56,514 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40807
2024-02-15 06:58:56,514 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33817
2024-02-15 06:58:56,514 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:56,514 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:56,514 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:56,515 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:56,515 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9uey7rad
2024-02-15 06:58:56,515 - distributed.worker - INFO - Starting Worker plugin RMMSetup-edfda867-a3dc-415a-9ae4-56965443e686
2024-02-15 06:58:56,516 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:56,517 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35225
2024-02-15 06:58:56,517 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35225
2024-02-15 06:58:56,517 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35193
2024-02-15 06:58:56,517 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:56,517 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:56,517 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:56,518 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:56,518 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kzw3cafi
2024-02-15 06:58:56,518 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-62b8c62c-7471-40da-97a6-4b8a7a0d8aaa
2024-02-15 06:58:56,518 - distributed.worker - INFO - Starting Worker plugin PreImport-4a855396-4790-4a20-8d96-e6e9a063f4f0
2024-02-15 06:58:56,518 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf9afdb6-fdba-4d87-826f-01d875034db9
2024-02-15 06:58:56,685 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:58:56,685 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:58:56,690 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:58:56,691 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43281
2024-02-15 06:58:56,691 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43281
2024-02-15 06:58:56,691 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39633
2024-02-15 06:58:56,691 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:58:56,691 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:56,692 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:58:56,692 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:58:56,692 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cx4owhmq
2024-02-15 06:58:56,692 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d16b6b68-edca-4f12-8788-e4d841c58050
2024-02-15 06:58:56,693 - distributed.worker - INFO - Starting Worker plugin PreImport-37ccf700-bac3-4711-b721-d431ed760a63
2024-02-15 06:58:56,693 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc6714a2-b26e-4f53-97a0-dd16522ab56a
2024-02-15 06:58:58,101 - distributed.worker - INFO - Starting Worker plugin PreImport-ccafb135-ea46-4118-9140-83789df50e7d
2024-02-15 06:58:58,101 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a06e883-2beb-4b7e-8b98-32fee81dd0c1
2024-02-15 06:58:58,103 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:58,780 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:58,782 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:58,782 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:58,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:58,786 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40873'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:58,787 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:58,788 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41219. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:58,791 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:58,793 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:58,928 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:58,952 - distributed.worker - INFO - Starting Worker plugin PreImport-bed14711-ca4b-404b-944d-88006096181b
2024-02-15 06:58:58,952 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:58,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:58,953 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:58,953 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:58,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:58,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:58,977 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:58,977 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:58,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38721'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:58,978 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:58,978 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:58,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45143'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:58,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:58,979 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41195. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:58,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37907. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:58,981 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:58,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:58,983 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:58,983 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:58,985 - distributed.worker - INFO - Starting Worker plugin PreImport-7c38f35c-ea88-4dac-ac6c-1571fb045fb7
2024-02-15 06:58:58,985 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-506502e6-5818-459d-9589-af1f4cd57376
2024-02-15 06:58:58,987 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:59,023 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:59,024 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:59,024 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:59,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:59,041 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43119'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,041 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,042 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35391. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,045 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:59,046 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:59,085 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:59,117 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:59,124 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:59,125 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:59,125 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:59,127 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:59,129 - distributed.worker - INFO - Starting Worker plugin PreImport-770b8d69-4d57-46f3-a9c3-351cd06a9f5f
2024-02-15 06:58:59,130 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ccf18e7-7829-42b9-86ce-5c734d9b4ba1
2024-02-15 06:58:59,131 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:59,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33935'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,131 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,133 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42473. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:59,136 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:59,137 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:59,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:59,156 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:59,156 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:59,158 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:59,166 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:59,167 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:59,167 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:59,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:58:59,169 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:59,169 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:58:59,169 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:58:59,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:58:59,181 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46271'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,181 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,182 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40807. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,183 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43025'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,183 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,183 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44453'. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,184 - distributed.nanny - INFO - Nanny asking worker to close. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,184 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43281. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,184 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35225. Reason: failure-to-start-<class 'distributed.comm.core.CommClosedError'>
2024-02-15 06:58:59,185 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:59,187 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:59,187 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:59,188 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:58:59,189 - distributed.nanny - INFO - Worker closed
2024-02-15 06:58:59,190 - distributed.nanny - INFO - Worker closed
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 383, in start_unsafe
    await comm.write({"status": "ok"})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Nanny->Scheduler (registration) local=tcp://127.0.0.1:57918 remote=tcp://127.0.0.1:9369>: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-02-15 06:58:59,240 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54590 parent=54393 started daemon>
2024-02-15 06:58:59,241 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54586 parent=54393 started daemon>
2024-02-15 06:58:59,241 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54582 parent=54393 started daemon>
2024-02-15 06:58:59,241 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54578 parent=54393 started daemon>
2024-02-15 06:58:59,241 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54574 parent=54393 started daemon>
2024-02-15 06:58:59,242 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54568 parent=54393 started daemon>
2024-02-15 06:58:59,242 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=54559 parent=54393 started daemon>
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-02-15 06:59:39,936 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:59:39,941 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42711 instead
  warnings.warn(
2024-02-15 06:59:39,946 - distributed.scheduler - INFO - State start
2024-02-15 06:59:40,310 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:59:40,311 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-15 06:59:40,313 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42711/status
2024-02-15 06:59:40,313 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 06:59:40,572 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41509'
2024-02-15 06:59:40,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41779'
2024-02-15 06:59:40,603 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32773'
2024-02-15 06:59:40,612 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42959'
2024-02-15 06:59:40,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37457'
2024-02-15 06:59:40,633 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35175'
2024-02-15 06:59:40,645 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34109'
2024-02-15 06:59:40,655 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43419'
2024-02-15 06:59:42,718 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:42,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:42,725 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:42,726 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39973
2024-02-15 06:59:42,726 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39973
2024-02-15 06:59:42,726 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43397
2024-02-15 06:59:42,726 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:42,726 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:42,726 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:42,726 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:42,726 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9quaz1hw
2024-02-15 06:59:42,727 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-873a4b96-10c0-4fa7-9ff9-0fe94aa3f724
2024-02-15 06:59:42,727 - distributed.worker - INFO - Starting Worker plugin PreImport-409d54d9-7bf0-4ed2-a545-ccbae81d605f
2024-02-15 06:59:42,727 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d46621b-a10e-41fa-b269-0e31bdf17e7b
2024-02-15 06:59:42,727 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:42,728 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:42,734 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:42,736 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36559
2024-02-15 06:59:42,736 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36559
2024-02-15 06:59:42,736 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38307
2024-02-15 06:59:42,736 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:42,736 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:42,736 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:42,736 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:42,736 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j9zl_6s4
2024-02-15 06:59:42,736 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbb2f463-e36b-4586-af36-59a3b8a847ce
2024-02-15 06:59:42,737 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1a29cb5-aaa1-4bd1-a43e-ddfb76d00518
2024-02-15 06:59:42,751 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:42,751 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:42,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:42,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:42,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:42,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:42,755 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:42,755 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:42,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:42,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:42,758 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:42,759 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:42,759 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44667
2024-02-15 06:59:42,759 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44667
2024-02-15 06:59:42,759 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45005
2024-02-15 06:59:42,759 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:42,759 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:42,760 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:42,760 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:42,760 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7bkk5li_
2024-02-15 06:59:42,760 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1fcf10a1-2c65-4278-a1c8-8e571eb0f55e
2024-02-15 06:59:42,760 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44609
2024-02-15 06:59:42,760 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44609
2024-02-15 06:59:42,760 - distributed.worker - INFO - Starting Worker plugin PreImport-048c8c84-5301-435a-98ff-fb4236994e64
2024-02-15 06:59:42,760 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45263
2024-02-15 06:59:42,761 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:42,761 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:42,761 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9a05ab0f-010c-4fb1-b668-d35caf1805eb
2024-02-15 06:59:42,761 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:42,761 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:42,761 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zzk0rq6r
2024-02-15 06:59:42,761 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-216061e9-c2eb-47c1-9e14-b21d017f3831
2024-02-15 06:59:42,762 - distributed.worker - INFO - Starting Worker plugin PreImport-b1a3cb4d-8a91-431f-9779-d4117ef659a6
2024-02-15 06:59:42,762 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a979e6ba-40f1-4f75-85de-74b0ab57cfa7
2024-02-15 06:59:42,763 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:42,765 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40995
2024-02-15 06:59:42,765 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40995
2024-02-15 06:59:42,765 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40921
2024-02-15 06:59:42,765 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:42,765 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:42,765 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:42,765 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:42,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k8dtvn9i
2024-02-15 06:59:42,765 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86e23fac-6c26-4119-889c-a41f016f2b43
2024-02-15 06:59:42,766 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:42,769 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35975
2024-02-15 06:59:42,769 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:42,769 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:42,769 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35975
2024-02-15 06:59:42,769 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42381
2024-02-15 06:59:42,769 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:42,769 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:42,769 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:42,769 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:42,770 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:42,770 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vps322dp
2024-02-15 06:59:42,770 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b8209fad-aa8d-4872-b32e-82ead90d8c9c
2024-02-15 06:59:42,770 - distributed.worker - INFO - Starting Worker plugin PreImport-cfb8a833-09bc-45db-93ac-50b9b5ec1803
2024-02-15 06:59:42,771 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d6128f3d-cd39-49ef-afdf-a046af6e6ece
2024-02-15 06:59:42,772 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44221
2024-02-15 06:59:42,772 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44221
2024-02-15 06:59:42,772 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46557
2024-02-15 06:59:42,772 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:42,772 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:42,772 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:42,773 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:42,773 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ph_e3n52
2024-02-15 06:59:42,773 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61599c07-b426-4a66-aee1-7bc52dcdfbdf
2024-02-15 06:59:42,774 - distributed.worker - INFO - Starting Worker plugin PreImport-de9ac9c4-5ec0-43ab-aa16-b42f9a68da30
2024-02-15 06:59:42,774 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46eb9b6e-9ad6-4e55-9f37-14d08beddb58
2024-02-15 06:59:42,775 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:42,776 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35999
2024-02-15 06:59:42,777 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35999
2024-02-15 06:59:42,777 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37837
2024-02-15 06:59:42,777 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:42,777 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:42,777 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:42,777 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:42,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i5b3h6cy
2024-02-15 06:59:42,777 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a615f3c-435f-49b3-8c7f-33d94221a359
2024-02-15 06:59:44,352 - distributed.scheduler - INFO - Receive client connection: Client-c8a9e344-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:59:44,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50852
2024-02-15 06:59:47,098 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,121 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39973', status: init, memory: 0, processing: 0>
2024-02-15 06:59:47,123 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39973
2024-02-15 06:59:47,123 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50868
2024-02-15 06:59:47,123 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:47,124 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:47,124 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:47,168 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,190 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,195 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44667', status: init, memory: 0, processing: 0>
2024-02-15 06:59:47,199 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44667
2024-02-15 06:59:47,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50872
2024-02-15 06:59:47,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:47,201 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:47,201 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:47,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35975', status: init, memory: 0, processing: 0>
2024-02-15 06:59:47,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35975
2024-02-15 06:59:47,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50888
2024-02-15 06:59:47,219 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:47,220 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:47,220 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:47,221 - distributed.worker - INFO - Starting Worker plugin PreImport-9c0858e7-da63-4bce-90f9-e4d6f764c796
2024-02-15 06:59:47,222 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31f16c69-2687-48da-8dd8-e367de5f47d2
2024-02-15 06:59:47,221 - distributed.worker - INFO - Starting Worker plugin PreImport-499e7830-4213-4108-a111-e8c5dbd77515
2024-02-15 06:59:47,222 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,223 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,225 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44221', status: init, memory: 0, processing: 0>
2024-02-15 06:59:47,225 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44221
2024-02-15 06:59:47,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50894
2024-02-15 06:59:47,226 - distributed.worker - INFO - Starting Worker plugin PreImport-95a8d017-b8f8-4811-ba52-6bdf81c6165e
2024-02-15 06:59:47,226 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60639675-43a6-4c8e-a380-57d90dd8c597
2024-02-15 06:59:47,227 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:47,228 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,228 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:47,228 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,229 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:47,252 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40995', status: init, memory: 0, processing: 0>
2024-02-15 06:59:47,252 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40995
2024-02-15 06:59:47,252 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50906
2024-02-15 06:59:47,253 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:47,254 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:47,254 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,256 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:47,259 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36559', status: init, memory: 0, processing: 0>
2024-02-15 06:59:47,260 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36559
2024-02-15 06:59:47,260 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50914
2024-02-15 06:59:47,261 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:47,263 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:47,263 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:47,268 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35999', status: init, memory: 0, processing: 0>
2024-02-15 06:59:47,268 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35999
2024-02-15 06:59:47,268 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50930
2024-02-15 06:59:47,269 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44609', status: init, memory: 0, processing: 0>
2024-02-15 06:59:47,270 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44609
2024-02-15 06:59:47,270 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50938
2024-02-15 06:59:47,270 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:47,271 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:47,271 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:47,273 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:47,273 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:47,273 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:47,275 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:47,301 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 06:59:47,301 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 06:59:47,301 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 06:59:47,301 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 06:59:47,301 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 06:59:47,302 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 06:59:47,302 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 06:59:47,302 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 06:59:47,313 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:47,313 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:47,313 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:47,313 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:47,313 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:47,313 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:47,313 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:47,314 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:47,323 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:47,325 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:47,327 - distributed.scheduler - INFO - Remove client Client-c8a9e344-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:59:47,328 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50852; closing.
2024-02-15 06:59:47,328 - distributed.scheduler - INFO - Remove client Client-c8a9e344-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:59:47,328 - distributed.scheduler - INFO - Close client connection: Client-c8a9e344-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:59:47,329 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41509'. Reason: nanny-close
2024-02-15 06:59:47,330 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:47,330 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41779'. Reason: nanny-close
2024-02-15 06:59:47,331 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:47,331 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32773'. Reason: nanny-close
2024-02-15 06:59:47,331 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36559. Reason: nanny-close
2024-02-15 06:59:47,332 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:47,332 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42959'. Reason: nanny-close
2024-02-15 06:59:47,332 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44609. Reason: nanny-close
2024-02-15 06:59:47,332 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:47,333 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37457'. Reason: nanny-close
2024-02-15 06:59:47,333 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39973. Reason: nanny-close
2024-02-15 06:59:47,333 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:47,333 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35175'. Reason: nanny-close
2024-02-15 06:59:47,333 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:47,334 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44667. Reason: nanny-close
2024-02-15 06:59:47,334 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50914; closing.
2024-02-15 06:59:47,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:47,334 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34109'. Reason: nanny-close
2024-02-15 06:59:47,334 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44221. Reason: nanny-close
2024-02-15 06:59:47,334 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36559', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980387.3344557')
2024-02-15 06:59:47,334 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:47,334 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43419'. Reason: nanny-close
2024-02-15 06:59:47,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:47,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:47,335 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35999. Reason: nanny-close
2024-02-15 06:59:47,335 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:47,335 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35975. Reason: nanny-close
2024-02-15 06:59:47,336 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:47,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50938; closing.
2024-02-15 06:59:47,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50868; closing.
2024-02-15 06:59:47,336 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:47,336 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:47,336 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:47,336 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:47,336 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40995. Reason: nanny-close
2024-02-15 06:59:47,336 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44609', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980387.336869')
2024-02-15 06:59:47,337 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39973', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980387.3372257')
2024-02-15 06:59:47,337 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:47,337 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:47,338 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50872; closing.
2024-02-15 06:59:47,338 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50894; closing.
2024-02-15 06:59:47,338 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:47,338 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:47,338 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:47,338 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44667', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980387.3388648')
2024-02-15 06:59:47,339 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44221', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980387.3391488')
2024-02-15 06:59:47,339 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50930; closing.
2024-02-15 06:59:47,339 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:47,339 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:47,339 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:47,340 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35999', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980387.3401456')
2024-02-15 06:59:47,340 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50888; closing.
2024-02-15 06:59:47,340 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50906; closing.
2024-02-15 06:59:47,341 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35975', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980387.3410501')
2024-02-15 06:59:47,341 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40995', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980387.3414054')
2024-02-15 06:59:47,341 - distributed.scheduler - INFO - Lost all workers
2024-02-15 06:59:48,496 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 06:59:48,496 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 06:59:48,497 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 06:59:48,498 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-15 06:59:48,498 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-02-15 06:59:51,027 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:59:51,032 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-15 06:59:51,035 - distributed.scheduler - INFO - State start
2024-02-15 06:59:51,058 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:59:51,059 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-15 06:59:51,060 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-15 06:59:51,060 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 06:59:51,199 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43907'
2024-02-15 06:59:51,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40929'
2024-02-15 06:59:51,232 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44673'
2024-02-15 06:59:51,242 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44113'
2024-02-15 06:59:51,244 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37775'
2024-02-15 06:59:51,253 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42633'
2024-02-15 06:59:51,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33127'
2024-02-15 06:59:51,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40267'
2024-02-15 06:59:52,518 - distributed.scheduler - INFO - Receive client connection: Client-cf21d274-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:59:52,533 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33122
2024-02-15 06:59:53,119 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:53,119 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:53,123 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:53,124 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40349
2024-02-15 06:59:53,124 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40349
2024-02-15 06:59:53,124 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40623
2024-02-15 06:59:53,124 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:53,124 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:53,124 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:53,124 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:53,124 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1kwocow3
2024-02-15 06:59:53,125 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4251dd86-edda-4763-84f0-6ddabe5e58e6
2024-02-15 06:59:53,192 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:53,192 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:53,196 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:53,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:53,197 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:53,197 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45559
2024-02-15 06:59:53,197 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45559
2024-02-15 06:59:53,198 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45239
2024-02-15 06:59:53,198 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:53,198 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:53,198 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:53,198 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:53,198 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-piiwswp2
2024-02-15 06:59:53,198 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ebced0e-bba5-4c39-8c83-7cc3084f2630
2024-02-15 06:59:53,198 - distributed.worker - INFO - Starting Worker plugin PreImport-436272e9-4cb5-4155-a909-7364ec8a5b15
2024-02-15 06:59:53,198 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6efc304b-bf2a-420e-b90d-733883082171
2024-02-15 06:59:53,200 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:53,201 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40153
2024-02-15 06:59:53,201 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40153
2024-02-15 06:59:53,201 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36663
2024-02-15 06:59:53,201 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:53,201 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:53,201 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:53,201 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:53,202 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j3drvhnr
2024-02-15 06:59:53,202 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bcf8421c-b3cc-4261-8329-58b1bcdf5ad1
2024-02-15 06:59:53,415 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:53,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:53,422 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:53,423 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37491
2024-02-15 06:59:53,424 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37491
2024-02-15 06:59:53,424 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37239
2024-02-15 06:59:53,424 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:53,424 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:53,424 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:53,424 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:53,424 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-juz1r4hu
2024-02-15 06:59:53,424 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8454c58b-01fe-4e3f-b771-85d8ae664852
2024-02-15 06:59:53,424 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a36f2f1-e13d-4d88-ba01-ee9dd8ab131d
2024-02-15 06:59:53,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:53,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:53,434 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:53,434 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:53,434 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:53,434 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:53,434 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:53,435 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39019
2024-02-15 06:59:53,436 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39019
2024-02-15 06:59:53,436 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45231
2024-02-15 06:59:53,436 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:53,436 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:53,436 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:53,436 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:53,436 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ndcnbb7
2024-02-15 06:59:53,436 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d488c2de-8bda-4b9d-9324-4d2ae15d7f55
2024-02-15 06:59:53,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 06:59:53,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 06:59:53,440 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:53,441 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:53,442 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43523
2024-02-15 06:59:53,442 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43523
2024-02-15 06:59:53,442 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38135
2024-02-15 06:59:53,442 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36703
2024-02-15 06:59:53,442 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:53,442 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36703
2024-02-15 06:59:53,442 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:53,442 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:53,442 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33075
2024-02-15 06:59:53,442 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:53,442 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:53,442 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ebvzbwb0
2024-02-15 06:59:53,442 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:53,442 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:53,442 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:53,442 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j9uf0rpm
2024-02-15 06:59:53,442 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bf55d5e1-3505-409b-ae82-9f418291ca17
2024-02-15 06:59:53,443 - distributed.worker - INFO - Starting Worker plugin PreImport-ce4de238-1894-48f0-861a-72b56e60743b
2024-02-15 06:59:53,443 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1fd41ecb-61c2-45d3-b472-35ce78fb0198
2024-02-15 06:59:53,443 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f952dea-a0aa-4400-8ab8-d5964d3418d7
2024-02-15 06:59:53,443 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 06:59:53,444 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38943
2024-02-15 06:59:53,444 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38943
2024-02-15 06:59:53,445 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41117
2024-02-15 06:59:53,445 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 06:59:53,445 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:53,445 - distributed.worker - INFO -               Threads:                          1
2024-02-15 06:59:53,445 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 06:59:53,445 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-faqqfx94
2024-02-15 06:59:53,445 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94762657-b459-47d5-8104-c450b034ad59
2024-02-15 06:59:53,446 - distributed.worker - INFO - Starting Worker plugin PreImport-cc53c89d-4a27-4106-ad3c-d030182aa9b0
2024-02-15 06:59:53,446 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e826fa7f-5293-48c2-8058-3cc03c92afe9
2024-02-15 06:59:53,735 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c3ea408d-1f7f-4fd8-bfec-1cc554b3069c
2024-02-15 06:59:53,736 - distributed.worker - INFO - Starting Worker plugin PreImport-e03eade5-ebbf-42a7-90d1-2bded702c165
2024-02-15 06:59:53,737 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:53,764 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40349', status: init, memory: 0, processing: 0>
2024-02-15 06:59:53,765 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40349
2024-02-15 06:59:53,765 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33136
2024-02-15 06:59:53,767 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:53,768 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:53,768 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:53,770 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:55,476 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,482 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9efc21c-7081-4418-af51-2ffd5fc8f311
2024-02-15 06:59:55,483 - distributed.worker - INFO - Starting Worker plugin PreImport-908e010f-bb4a-4da6-9479-602aba47e1d1
2024-02-15 06:59:55,484 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,508 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45559', status: init, memory: 0, processing: 0>
2024-02-15 06:59:55,509 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45559
2024-02-15 06:59:55,509 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33154
2024-02-15 06:59:55,510 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:55,511 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:55,511 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,513 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:55,521 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40153', status: init, memory: 0, processing: 0>
2024-02-15 06:59:55,522 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40153
2024-02-15 06:59:55,522 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33162
2024-02-15 06:59:55,523 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:55,524 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:55,524 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,526 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:55,532 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,545 - distributed.worker - INFO - Starting Worker plugin PreImport-834077e5-d1b9-4328-8606-7c4b5a9a56b2
2024-02-15 06:59:55,545 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,546 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,548 - distributed.worker - INFO - Starting Worker plugin PreImport-51748e21-4947-417e-b0bd-5b3e1eafc06b
2024-02-15 06:59:55,548 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d7e2e16b-5026-41f1-a8d3-6bf6cb167266
2024-02-15 06:59:55,549 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,549 - distributed.worker - INFO - Starting Worker plugin PreImport-4075ee17-f1a4-4364-b120-eeb7b7c8c16c
2024-02-15 06:59:55,550 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f802f22f-c713-455a-bfab-fd94d4ce1829
2024-02-15 06:59:55,550 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,554 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43523', status: init, memory: 0, processing: 0>
2024-02-15 06:59:55,555 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43523
2024-02-15 06:59:55,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33168
2024-02-15 06:59:55,556 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:55,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:55,556 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:55,568 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37491', status: init, memory: 0, processing: 0>
2024-02-15 06:59:55,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37491
2024-02-15 06:59:55,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33176
2024-02-15 06:59:55,570 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:55,570 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:55,570 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,572 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:55,572 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38943', status: init, memory: 0, processing: 0>
2024-02-15 06:59:55,573 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38943
2024-02-15 06:59:55,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33180
2024-02-15 06:59:55,574 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:55,575 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:55,575 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,575 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36703', status: init, memory: 0, processing: 0>
2024-02-15 06:59:55,576 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36703
2024-02-15 06:59:55,576 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33206
2024-02-15 06:59:55,577 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:55,577 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:55,578 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:55,578 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,579 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:55,598 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39019', status: init, memory: 0, processing: 0>
2024-02-15 06:59:55,599 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39019
2024-02-15 06:59:55,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33190
2024-02-15 06:59:55,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 06:59:55,602 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 06:59:55,602 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 06:59:55,604 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 06:59:55,709 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:55,709 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:55,709 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:55,709 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:55,709 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:55,710 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:55,710 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:55,710 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:55,721 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:55,721 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:55,721 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:55,721 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:55,721 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:55,721 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:55,721 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:55,722 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 06:59:55,730 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:55,732 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 06:59:55,734 - distributed.scheduler - INFO - Remove client Client-cf21d274-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:59:55,734 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33122; closing.
2024-02-15 06:59:55,734 - distributed.scheduler - INFO - Remove client Client-cf21d274-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:59:55,735 - distributed.scheduler - INFO - Close client connection: Client-cf21d274-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 06:59:55,736 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43907'. Reason: nanny-close
2024-02-15 06:59:55,736 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:55,737 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40929'. Reason: nanny-close
2024-02-15 06:59:55,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:55,738 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37491. Reason: nanny-close
2024-02-15 06:59:55,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44673'. Reason: nanny-close
2024-02-15 06:59:55,738 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:55,738 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44113'. Reason: nanny-close
2024-02-15 06:59:55,739 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40153. Reason: nanny-close
2024-02-15 06:59:55,739 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:55,739 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40349. Reason: nanny-close
2024-02-15 06:59:55,739 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37775'. Reason: nanny-close
2024-02-15 06:59:55,739 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:55,739 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33176; closing.
2024-02-15 06:59:55,739 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:55,740 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42633'. Reason: nanny-close
2024-02-15 06:59:55,740 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37491', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980395.7401562')
2024-02-15 06:59:55,740 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45559. Reason: nanny-close
2024-02-15 06:59:55,740 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:55,740 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33127'. Reason: nanny-close
2024-02-15 06:59:55,740 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43523. Reason: nanny-close
2024-02-15 06:59:55,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:55,741 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:55,741 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:55,741 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40267'. Reason: nanny-close
2024-02-15 06:59:55,741 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36703. Reason: nanny-close
2024-02-15 06:59:55,741 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:55,741 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 06:59:55,742 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33162; closing.
2024-02-15 06:59:55,742 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:55,742 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38943. Reason: nanny-close
2024-02-15 06:59:55,742 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:55,742 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40153', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980395.7429233')
2024-02-15 06:59:55,743 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:55,743 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:55,743 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33136; closing.
2024-02-15 06:59:55,743 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39019. Reason: nanny-close
2024-02-15 06:59:55,743 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:55,743 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980395.743903')
2024-02-15 06:59:55,744 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:55,744 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33154; closing.
2024-02-15 06:59:55,744 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:55,744 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:55,744 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:55,744 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45559', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980395.7449062')
2024-02-15 06:59:55,745 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33168; closing.
2024-02-15 06:59:55,745 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 06:59:55,745 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33206; closing.
2024-02-15 06:59:55,746 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:55,746 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43523', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980395.7459953')
2024-02-15 06:59:55,746 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36703', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980395.746371')
2024-02-15 06:59:55,746 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33180; closing.
2024-02-15 06:59:55,747 - distributed.nanny - INFO - Worker closed
2024-02-15 06:59:55,747 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980395.7472494')
2024-02-15 06:59:55,747 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33190; closing.
2024-02-15 06:59:55,748 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39019', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980395.7479777')
2024-02-15 06:59:55,748 - distributed.scheduler - INFO - Lost all workers
2024-02-15 06:59:56,752 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 06:59:56,752 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 06:59:56,752 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 06:59:56,753 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-15 06:59:56,754 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-02-15 06:59:59,053 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:59:59,058 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-15 06:59:59,062 - distributed.scheduler - INFO - State start
2024-02-15 06:59:59,571 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 06:59:59,573 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-15 06:59:59,574 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-15 06:59:59,574 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 07:00:01,147 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45695'
2024-02-15 07:00:01,160 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38305'
2024-02-15 07:00:01,172 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34253'
2024-02-15 07:00:01,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44991'
2024-02-15 07:00:01,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44885'
2024-02-15 07:00:01,197 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38397'
2024-02-15 07:00:01,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45627'
2024-02-15 07:00:01,213 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34149'
2024-02-15 07:00:01,261 - distributed.scheduler - INFO - Receive client connection: Client-d40c72a3-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:01,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34114
2024-02-15 07:00:03,037 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:03,037 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:03,042 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:03,042 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41413
2024-02-15 07:00:03,043 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41413
2024-02-15 07:00:03,043 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36045
2024-02-15 07:00:03,043 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:03,043 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:03,043 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:03,043 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:03,043 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qh2r3mli
2024-02-15 07:00:03,043 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d6bd2cb-219b-427d-973b-eb876bdedfd8
2024-02-15 07:00:03,044 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74cd4fd9-2835-4f6c-a719-55b5484119d6
2024-02-15 07:00:03,106 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:03,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:03,110 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:03,111 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37057
2024-02-15 07:00:03,111 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37057
2024-02-15 07:00:03,111 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41619
2024-02-15 07:00:03,111 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:03,112 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:03,112 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:03,112 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:03,112 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-95vzi7qx
2024-02-15 07:00:03,112 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa54fd78-8c58-456a-a2f7-4433d137379e
2024-02-15 07:00:03,112 - distributed.worker - INFO - Starting Worker plugin PreImport-5bad1e32-e2b2-4ab1-9c8f-9f00717345ab
2024-02-15 07:00:03,112 - distributed.worker - INFO - Starting Worker plugin RMMSetup-53d6ff79-f85a-4122-84ef-2ec47b69920b
2024-02-15 07:00:03,113 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:03,113 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:03,117 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:03,118 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46693
2024-02-15 07:00:03,118 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46693
2024-02-15 07:00:03,118 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40987
2024-02-15 07:00:03,118 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:03,118 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:03,118 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:03,118 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:03,118 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fq1vrcxp
2024-02-15 07:00:03,118 - distributed.worker - INFO - Starting Worker plugin PreImport-a95f2dbb-8049-4344-8ad5-db35eb3e7a6f
2024-02-15 07:00:03,119 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cd2aa8e5-bf19-46eb-9b28-fea9425f4653
2024-02-15 07:00:03,119 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f163c15-1f1a-415f-814a-8da507e44e2f
2024-02-15 07:00:03,309 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:03,309 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:03,314 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:03,315 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:03,315 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:03,316 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39991
2024-02-15 07:00:03,316 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39991
2024-02-15 07:00:03,317 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37593
2024-02-15 07:00:03,317 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:03,317 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:03,317 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:03,317 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:03,317 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kjmp1my_
2024-02-15 07:00:03,317 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70c9fda9-9084-4de3-80e2-58df3ce7d6f6
2024-02-15 07:00:03,318 - distributed.worker - INFO - Starting Worker plugin PreImport-aad17c0e-e5dc-48ba-a150-28ed834351b7
2024-02-15 07:00:03,318 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3df41d9-fc83-4467-9438-36156cb40977
2024-02-15 07:00:03,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:03,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:03,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:03,320 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43555
2024-02-15 07:00:03,320 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43555
2024-02-15 07:00:03,320 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41547
2024-02-15 07:00:03,320 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:03,320 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:03,321 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:03,321 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:03,321 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wmo0ohuy
2024-02-15 07:00:03,321 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54ccfa3a-b2ac-4293-9b9d-6930b24f6aab
2024-02-15 07:00:03,321 - distributed.worker - INFO - Starting Worker plugin PreImport-1387db7e-3831-4575-96cc-ab094da37212
2024-02-15 07:00:03,321 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba7f77e0-c645-4418-b305-0fad3d4992af
2024-02-15 07:00:03,326 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:03,327 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:03,329 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:03,331 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42739
2024-02-15 07:00:03,331 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42739
2024-02-15 07:00:03,331 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43691
2024-02-15 07:00:03,331 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:03,332 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:03,332 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:03,332 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:03,332 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-55egr11e
2024-02-15 07:00:03,332 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0e99521c-9c16-40f5-9f99-b2bf582b7eb3
2024-02-15 07:00:03,333 - distributed.worker - INFO - Starting Worker plugin PreImport-ff5c03b2-2c01-439e-812e-016248fe4f14
2024-02-15 07:00:03,333 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0a00909-dde9-4a85-aee4-3be8b2697af6
2024-02-15 07:00:03,333 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:03,334 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43183
2024-02-15 07:00:03,334 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43183
2024-02-15 07:00:03,334 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41201
2024-02-15 07:00:03,334 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:03,334 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:03,334 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:03,334 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:03,334 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-__4pcmdz
2024-02-15 07:00:03,335 - distributed.worker - INFO - Starting Worker plugin RMMSetup-780bf6a5-7fbf-46f5-b261-7d8d6675e8ad
2024-02-15 07:00:03,351 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:03,351 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:03,358 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:03,359 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36155
2024-02-15 07:00:03,359 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36155
2024-02-15 07:00:03,359 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40557
2024-02-15 07:00:03,359 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:03,359 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:03,359 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:03,359 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:03,359 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tddg4bjq
2024-02-15 07:00:03,360 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-84e3709a-7c1a-465b-ab86-d15b6829d90e
2024-02-15 07:00:03,360 - distributed.worker - INFO - Starting Worker plugin PreImport-f09cbfd9-2089-4b17-ad60-739f191718ff
2024-02-15 07:00:03,360 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73776f31-0b44-456b-a0b8-d1b42b1bd7e8
2024-02-15 07:00:03,583 - distributed.worker - INFO - Starting Worker plugin PreImport-8e3fa92c-cf6f-4e4f-ab90-cfcf8090d5f5
2024-02-15 07:00:03,584 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:03,603 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41413', status: init, memory: 0, processing: 0>
2024-02-15 07:00:03,604 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41413
2024-02-15 07:00:03,604 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34140
2024-02-15 07:00:03,605 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:03,606 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:03,606 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:03,607 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:05,250 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,278 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46693', status: init, memory: 0, processing: 0>
2024-02-15 07:00:05,278 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46693
2024-02-15 07:00:05,278 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34158
2024-02-15 07:00:05,280 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:05,281 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:05,281 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:05,315 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,343 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37057', status: init, memory: 0, processing: 0>
2024-02-15 07:00:05,343 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37057
2024-02-15 07:00:05,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34172
2024-02-15 07:00:05,345 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:05,346 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:05,346 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:05,449 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39991', status: init, memory: 0, processing: 0>
2024-02-15 07:00:05,478 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39991
2024-02-15 07:00:05,478 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34188
2024-02-15 07:00:05,479 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:05,480 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:05,480 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,482 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:05,494 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,515 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36155', status: init, memory: 0, processing: 0>
2024-02-15 07:00:05,516 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36155
2024-02-15 07:00:05,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34200
2024-02-15 07:00:05,517 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:05,518 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:05,518 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,518 - distributed.worker - INFO - Starting Worker plugin PreImport-9cf621f3-a8a1-4cad-88de-1a6997d3f765
2024-02-15 07:00:05,519 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa515e63-248b-4b5b-9002-378e39991c63
2024-02-15 07:00:05,519 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:05,519 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,529 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,540 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43183', status: init, memory: 0, processing: 0>
2024-02-15 07:00:05,541 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43183
2024-02-15 07:00:05,541 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34204
2024-02-15 07:00:05,542 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:05,543 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:05,543 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,544 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:05,545 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,549 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43555', status: init, memory: 0, processing: 0>
2024-02-15 07:00:05,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43555
2024-02-15 07:00:05,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34218
2024-02-15 07:00:05,551 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:05,552 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:05,552 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:05,565 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42739', status: init, memory: 0, processing: 0>
2024-02-15 07:00:05,566 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42739
2024-02-15 07:00:05,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34224
2024-02-15 07:00:05,567 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:05,568 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:05,568 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:05,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:05,628 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:05,628 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:05,628 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:05,628 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:05,628 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:05,628 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:05,628 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:05,629 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:05,634 - distributed.scheduler - INFO - Remove client Client-d40c72a3-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:05,635 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34114; closing.
2024-02-15 07:00:05,635 - distributed.scheduler - INFO - Remove client Client-d40c72a3-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:05,635 - distributed.scheduler - INFO - Close client connection: Client-d40c72a3-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:05,637 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45695'. Reason: nanny-close
2024-02-15 07:00:05,638 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:05,638 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38305'. Reason: nanny-close
2024-02-15 07:00:05,639 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:05,639 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34253'. Reason: nanny-close
2024-02-15 07:00:05,639 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41413. Reason: nanny-close
2024-02-15 07:00:05,639 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:05,640 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44991'. Reason: nanny-close
2024-02-15 07:00:05,640 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:05,640 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42739. Reason: nanny-close
2024-02-15 07:00:05,640 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44885'. Reason: nanny-close
2024-02-15 07:00:05,640 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:05,640 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38397'. Reason: nanny-close
2024-02-15 07:00:05,640 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46693. Reason: nanny-close
2024-02-15 07:00:05,641 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:05,641 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45627'. Reason: nanny-close
2024-02-15 07:00:05,641 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43555. Reason: nanny-close
2024-02-15 07:00:05,641 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:05,641 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37057. Reason: nanny-close
2024-02-15 07:00:05,641 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34149'. Reason: nanny-close
2024-02-15 07:00:05,642 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:05,642 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43183. Reason: nanny-close
2024-02-15 07:00:05,642 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34224; closing.
2024-02-15 07:00:05,642 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:05,642 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:05,642 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42739', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980405.6426191')
2024-02-15 07:00:05,643 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36155. Reason: nanny-close
2024-02-15 07:00:05,643 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34140; closing.
2024-02-15 07:00:05,643 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39991. Reason: nanny-close
2024-02-15 07:00:05,643 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:05,643 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:05,643 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41413', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980405.643832')
2024-02-15 07:00:05,643 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:05,644 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:05,644 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:05,644 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:05,645 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:05,645 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34158; closing.
2024-02-15 07:00:05,645 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34218; closing.
2024-02-15 07:00:05,645 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:05,645 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:05,645 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:05,645 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:05,646 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:05,647 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:05,646 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34140>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-02-15 07:00:05,647 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46693', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980405.6476922')
2024-02-15 07:00:05,648 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43555', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980405.648012')
2024-02-15 07:00:05,648 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:05,648 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34172; closing.
2024-02-15 07:00:05,648 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34204; closing.
2024-02-15 07:00:05,649 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37057', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980405.6491938')
2024-02-15 07:00:05,649 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43183', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980405.64954')
2024-02-15 07:00:05,649 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34200; closing.
2024-02-15 07:00:05,650 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34188; closing.
2024-02-15 07:00:05,650 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36155', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980405.650562')
2024-02-15 07:00:05,650 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39991', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980405.650844')
2024-02-15 07:00:05,651 - distributed.scheduler - INFO - Lost all workers
2024-02-15 07:00:06,752 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 07:00:06,753 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 07:00:06,753 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 07:00:06,754 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-15 07:00:06,754 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-02-15 07:00:09,123 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:09,127 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-15 07:00:09,131 - distributed.scheduler - INFO - State start
2024-02-15 07:00:09,154 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:09,155 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-15 07:00:09,155 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-15 07:00:09,156 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 07:00:09,330 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42143'
2024-02-15 07:00:09,900 - distributed.scheduler - INFO - Receive client connection: Client-da0ac50a-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:09,912 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34354
2024-02-15 07:00:11,168 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:11,168 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:11,778 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:11,779 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39651
2024-02-15 07:00:11,779 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39651
2024-02-15 07:00:11,779 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-02-15 07:00:11,779 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:11,779 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:11,779 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:11,779 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-15 07:00:11,779 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i2phcjty
2024-02-15 07:00:11,779 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2fdbb48b-ca77-47b3-95ef-f3e7f3dbb4f8
2024-02-15 07:00:11,779 - distributed.worker - INFO - Starting Worker plugin PreImport-c540230e-b6a4-49cb-9d70-2c49f446dc03
2024-02-15 07:00:11,779 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-917a44ef-f360-4ede-b27c-18ebd1a6f72d
2024-02-15 07:00:11,780 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:11,836 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39651', status: init, memory: 0, processing: 0>
2024-02-15 07:00:11,837 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39651
2024-02-15 07:00:11,837 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37346
2024-02-15 07:00:11,838 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:11,840 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:11,840 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:11,841 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:11,929 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:11,932 - distributed.scheduler - INFO - Remove client Client-da0ac50a-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:11,933 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34354; closing.
2024-02-15 07:00:11,933 - distributed.scheduler - INFO - Remove client Client-da0ac50a-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:11,933 - distributed.scheduler - INFO - Close client connection: Client-da0ac50a-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:11,934 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42143'. Reason: nanny-close
2024-02-15 07:00:11,935 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:11,936 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39651. Reason: nanny-close
2024-02-15 07:00:11,938 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37346; closing.
2024-02-15 07:00:11,938 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:11,938 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39651', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980411.9382443')
2024-02-15 07:00:11,938 - distributed.scheduler - INFO - Lost all workers
2024-02-15 07:00:11,939 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:12,800 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 07:00:12,800 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 07:00:12,801 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 07:00:12,801 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-15 07:00:12,802 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-02-15 07:00:17,452 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:17,457 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-15 07:00:17,462 - distributed.scheduler - INFO - State start
2024-02-15 07:00:17,683 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:17,684 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-15 07:00:17,686 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-15 07:00:17,686 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 07:00:17,788 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35399'
2024-02-15 07:00:18,001 - distributed.scheduler - INFO - Receive client connection: Client-defe22e0-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:18,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37406
2024-02-15 07:00:19,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:19,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:20,455 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:20,456 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37167
2024-02-15 07:00:20,456 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37167
2024-02-15 07:00:20,456 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45375
2024-02-15 07:00:20,456 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:20,456 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:20,456 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:20,457 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-15 07:00:20,457 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oxvpeiec
2024-02-15 07:00:20,457 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cf9543f5-e5a4-4090-a87f-04b4e9177106
2024-02-15 07:00:20,457 - distributed.worker - INFO - Starting Worker plugin PreImport-800f6210-8a70-4039-b866-45b31e932ef1
2024-02-15 07:00:20,458 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-10a234b7-3b09-421e-a599-b96079213dc1
2024-02-15 07:00:20,458 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:20,536 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37167', status: init, memory: 0, processing: 0>
2024-02-15 07:00:20,537 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37167
2024-02-15 07:00:20,537 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40068
2024-02-15 07:00:20,538 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:20,539 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:20,539 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:20,540 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:20,568 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:20,570 - distributed.scheduler - INFO - Remove client Client-defe22e0-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:20,571 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37406; closing.
2024-02-15 07:00:20,571 - distributed.scheduler - INFO - Remove client Client-defe22e0-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:20,571 - distributed.scheduler - INFO - Close client connection: Client-defe22e0-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:20,573 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35399'. Reason: nanny-close
2024-02-15 07:00:20,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:20,578 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37167. Reason: nanny-close
2024-02-15 07:00:20,580 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:20,580 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40068; closing.
2024-02-15 07:00:20,580 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37167', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980420.5806355')
2024-02-15 07:00:20,581 - distributed.scheduler - INFO - Lost all workers
2024-02-15 07:00:20,581 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:21,338 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 07:00:21,338 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 07:00:21,339 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 07:00:21,339 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-15 07:00:21,340 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-02-15 07:00:23,873 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:23,880 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-15 07:00:23,885 - distributed.scheduler - INFO - State start
2024-02-15 07:00:23,914 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:23,915 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-15 07:00:23,917 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-15 07:00:23,918 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 07:00:26,801 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:40072'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40072>: Stream is closed
2024-02-15 07:00:27,208 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 07:00:27,209 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 07:00:27,209 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 07:00:27,210 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-15 07:00:27,210 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-02-15 07:00:29,610 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:29,615 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-15 07:00:29,619 - distributed.scheduler - INFO - State start
2024-02-15 07:00:29,790 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:29,792 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-02-15 07:00:29,793 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-15 07:00:29,794 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 07:00:30,320 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46813'
2024-02-15 07:00:31,087 - distributed.scheduler - INFO - Receive client connection: Client-e63c7e22-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:31,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37642
2024-02-15 07:00:32,200 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:32,200 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:32,204 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:32,204 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41747
2024-02-15 07:00:32,205 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41747
2024-02-15 07:00:32,205 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38919
2024-02-15 07:00:32,205 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-02-15 07:00:32,205 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:32,205 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:32,205 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-15 07:00:32,205 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-kwjnpslc
2024-02-15 07:00:32,205 - distributed.worker - INFO - Starting Worker plugin PreImport-8946f2ca-58cb-4bd1-8ad3-f4543a00aa9f
2024-02-15 07:00:32,205 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f4897dbb-fde1-4ba9-80fd-7a786c9c8948
2024-02-15 07:00:32,205 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-250ad991-9222-4680-96e7-6ff476fb1c57
2024-02-15 07:00:32,205 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:33,126 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41747', status: init, memory: 0, processing: 0>
2024-02-15 07:00:33,128 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41747
2024-02-15 07:00:33,128 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37656
2024-02-15 07:00:33,128 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:33,129 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-02-15 07:00:33,129 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:33,130 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-02-15 07:00:33,145 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:33,148 - distributed.scheduler - INFO - Remove client Client-e63c7e22-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:33,148 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37642; closing.
2024-02-15 07:00:33,148 - distributed.scheduler - INFO - Remove client Client-e63c7e22-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:33,148 - distributed.scheduler - INFO - Close client connection: Client-e63c7e22-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:33,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46813'. Reason: nanny-close
2024-02-15 07:00:33,153 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:33,154 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41747. Reason: nanny-close
2024-02-15 07:00:33,156 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37656; closing.
2024-02-15 07:00:33,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-02-15 07:00:33,156 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41747', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980433.1562521')
2024-02-15 07:00:33,156 - distributed.scheduler - INFO - Lost all workers
2024-02-15 07:00:33,157 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:33,764 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 07:00:33,765 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 07:00:33,765 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 07:00:33,767 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-02-15 07:00:33,767 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-02-15 07:00:36,248 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:36,253 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-15 07:00:36,257 - distributed.scheduler - INFO - State start
2024-02-15 07:00:36,280 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:36,281 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-15 07:00:36,282 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-15 07:00:36,282 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 07:00:36,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45467'
2024-02-15 07:00:36,477 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39589'
2024-02-15 07:00:36,494 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40355'
2024-02-15 07:00:36,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40491'
2024-02-15 07:00:36,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38275'
2024-02-15 07:00:36,513 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36195'
2024-02-15 07:00:36,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34983'
2024-02-15 07:00:36,533 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41673'
2024-02-15 07:00:38,409 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:38,409 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:38,409 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:38,409 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:38,413 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:38,413 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:38,414 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34073
2024-02-15 07:00:38,414 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42869
2024-02-15 07:00:38,414 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42869
2024-02-15 07:00:38,414 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34073
2024-02-15 07:00:38,414 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39177
2024-02-15 07:00:38,414 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44215
2024-02-15 07:00:38,414 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:38,414 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:38,414 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:38,414 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:38,414 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:38,414 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:38,414 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:38,414 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8uglb4_6
2024-02-15 07:00:38,414 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:38,415 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6dw9al6v
2024-02-15 07:00:38,415 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-236d3c69-bc26-4178-b8ae-b7c455bdd90e
2024-02-15 07:00:38,415 - distributed.worker - INFO - Starting Worker plugin PreImport-25fd1b11-af33-455f-ac9d-35ffee0cc3c2
2024-02-15 07:00:38,415 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-733fbde6-22c1-465f-9bff-08eb46b0d350
2024-02-15 07:00:38,415 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f69a70fe-7c03-43cc-990b-e10a103da0c8
2024-02-15 07:00:38,415 - distributed.worker - INFO - Starting Worker plugin PreImport-d1dec539-751f-4ab6-9458-c8b2d5c3bd9e
2024-02-15 07:00:38,416 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6cff0474-dda1-458d-91b0-049d42c0e100
2024-02-15 07:00:38,451 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:38,451 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:38,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:38,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:38,453 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:38,453 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:38,455 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:38,456 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44257
2024-02-15 07:00:38,456 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44257
2024-02-15 07:00:38,456 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42353
2024-02-15 07:00:38,456 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:38,456 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:38,456 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:38,456 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:38,456 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3_gteswa
2024-02-15 07:00:38,457 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:38,457 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a885ebf-e247-4af5-ac4f-09d5011eba90
2024-02-15 07:00:38,457 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:38,457 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42097
2024-02-15 07:00:38,458 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42097
2024-02-15 07:00:38,458 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46431
2024-02-15 07:00:38,458 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:38,458 - distributed.worker - INFO - Starting Worker plugin PreImport-f860fb3f-acfb-4795-ae48-74a29c6fe96a
2024-02-15 07:00:38,458 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:38,458 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:38,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-edb35111-e2e2-4eea-829a-5ce917d200de
2024-02-15 07:00:38,458 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:38,458 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-15nwb_e9
2024-02-15 07:00:38,458 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f2d83aa-7b7c-453d-9e53-ac264172f9fe
2024-02-15 07:00:38,458 - distributed.worker - INFO - Starting Worker plugin RMMSetup-20da7b6f-bc59-4eec-b0ff-5d8ce862f15e
2024-02-15 07:00:38,458 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43897
2024-02-15 07:00:38,458 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43897
2024-02-15 07:00:38,458 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38829
2024-02-15 07:00:38,458 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:38,459 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:38,459 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:38,459 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:38,459 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-atj1ynlw
2024-02-15 07:00:38,459 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-131b602f-63ba-4cfd-9740-732e28e22724
2024-02-15 07:00:38,459 - distributed.worker - INFO - Starting Worker plugin PreImport-fbae73e2-a442-496e-aff8-37e2985b4146
2024-02-15 07:00:38,459 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b20a5170-174f-4006-b8eb-f975bccf98a4
2024-02-15 07:00:38,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:38,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:38,514 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:38,514 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:38,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:38,517 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36749
2024-02-15 07:00:38,518 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36749
2024-02-15 07:00:38,518 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41229
2024-02-15 07:00:38,518 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:38,518 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:38,518 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:38,518 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:38,518 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pox6rpx5
2024-02-15 07:00:38,518 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70d12e69-29ea-409d-aa22-3b574ad062a0
2024-02-15 07:00:38,519 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:38,519 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45115
2024-02-15 07:00:38,519 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45115
2024-02-15 07:00:38,519 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44899
2024-02-15 07:00:38,520 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:38,520 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:38,520 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:38,520 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:38,520 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qg69clr6
2024-02-15 07:00:38,520 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e87a50b9-6d26-4a24-9776-f0af9f5d5975
2024-02-15 07:00:38,898 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:38,899 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:38,906 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:38,908 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46489
2024-02-15 07:00:38,908 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46489
2024-02-15 07:00:38,908 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37335
2024-02-15 07:00:38,908 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:38,908 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:38,908 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:38,908 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:00:38,908 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e7d1_f2d
2024-02-15 07:00:38,909 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9a34af32-7c51-46f0-84c9-c1e1deb59adf
2024-02-15 07:00:38,910 - distributed.worker - INFO - Starting Worker plugin PreImport-da92b1d9-cc3d-4728-881d-d0eaeb0172f7
2024-02-15 07:00:38,910 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77bbaa28-c825-436e-905c-5d4ccf8e9e89
2024-02-15 07:00:40,720 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,735 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,753 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34073', status: init, memory: 0, processing: 0>
2024-02-15 07:00:40,767 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34073
2024-02-15 07:00:40,767 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35318
2024-02-15 07:00:40,768 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:40,769 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42869', status: init, memory: 0, processing: 0>
2024-02-15 07:00:40,769 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42869
2024-02-15 07:00:40,769 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35324
2024-02-15 07:00:40,770 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:40,770 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,771 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:40,771 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:40,772 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,772 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:40,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:40,809 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,830 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,841 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44257', status: init, memory: 0, processing: 0>
2024-02-15 07:00:40,842 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44257
2024-02-15 07:00:40,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35334
2024-02-15 07:00:40,843 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:40,844 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:40,844 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:40,854 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43897', status: init, memory: 0, processing: 0>
2024-02-15 07:00:40,854 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43897
2024-02-15 07:00:40,855 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35340
2024-02-15 07:00:40,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:40,856 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:40,856 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:40,858 - distributed.worker - INFO - Starting Worker plugin PreImport-e3a3e93c-4a8f-4bf3-ad34-6362cc56f828
2024-02-15 07:00:40,858 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bbbc3b04-2200-4582-a48d-89a6fbd529a7
2024-02-15 07:00:40,860 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,881 - distributed.worker - INFO - Starting Worker plugin PreImport-250f42a9-f70e-4c6f-9474-b76529bedb22
2024-02-15 07:00:40,882 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,891 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45115', status: init, memory: 0, processing: 0>
2024-02-15 07:00:40,892 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45115
2024-02-15 07:00:40,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35344
2024-02-15 07:00:40,894 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:40,895 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:40,895 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:40,904 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42097', status: init, memory: 0, processing: 0>
2024-02-15 07:00:40,905 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42097
2024-02-15 07:00:40,905 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35356
2024-02-15 07:00:40,905 - distributed.worker - INFO - Starting Worker plugin PreImport-d153853d-66d5-4693-af53-dd453acd5133
2024-02-15 07:00:40,906 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-01d1f973-6273-4763-b938-d6ac3f531208
2024-02-15 07:00:40,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:40,906 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,907 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:40,907 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:40,926 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,928 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36749', status: init, memory: 0, processing: 0>
2024-02-15 07:00:40,929 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36749
2024-02-15 07:00:40,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35362
2024-02-15 07:00:40,930 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:40,931 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:40,931 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,932 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:40,956 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46489', status: init, memory: 0, processing: 0>
2024-02-15 07:00:40,957 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46489
2024-02-15 07:00:40,957 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35370
2024-02-15 07:00:40,958 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:40,959 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:40,959 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:40,961 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:41,458 - distributed.scheduler - INFO - Receive client connection: Client-ea24d976-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:41,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35378
2024-02-15 07:00:41,471 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:41,471 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:41,471 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:41,471 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:41,471 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:41,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:41,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:41,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-02-15 07:00:41,486 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:41,486 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:41,486 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:41,486 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:41,486 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:41,486 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:41,487 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:41,487 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:41,491 - distributed.scheduler - INFO - Remove client Client-ea24d976-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:41,492 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35378; closing.
2024-02-15 07:00:41,492 - distributed.scheduler - INFO - Remove client Client-ea24d976-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:41,492 - distributed.scheduler - INFO - Close client connection: Client-ea24d976-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:41,493 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45467'. Reason: nanny-close
2024-02-15 07:00:41,493 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:41,494 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39589'. Reason: nanny-close
2024-02-15 07:00:41,494 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:41,495 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40355'. Reason: nanny-close
2024-02-15 07:00:41,495 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42097. Reason: nanny-close
2024-02-15 07:00:41,495 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:41,495 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40491'. Reason: nanny-close
2024-02-15 07:00:41,495 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42869. Reason: nanny-close
2024-02-15 07:00:41,495 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:41,495 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38275'. Reason: nanny-close
2024-02-15 07:00:41,496 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:41,496 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44257. Reason: nanny-close
2024-02-15 07:00:41,496 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36195'. Reason: nanny-close
2024-02-15 07:00:41,496 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:41,496 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34073. Reason: nanny-close
2024-02-15 07:00:41,496 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34983'. Reason: nanny-close
2024-02-15 07:00:41,497 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:41,497 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35356; closing.
2024-02-15 07:00:41,497 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:41,497 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43897. Reason: nanny-close
2024-02-15 07:00:41,497 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41673'. Reason: nanny-close
2024-02-15 07:00:41,497 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:41,497 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42097', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980441.4974585')
2024-02-15 07:00:41,497 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36749. Reason: nanny-close
2024-02-15 07:00:41,497 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:41,498 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46489. Reason: nanny-close
2024-02-15 07:00:41,498 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35324; closing.
2024-02-15 07:00:41,498 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45115. Reason: nanny-close
2024-02-15 07:00:41,498 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:41,498 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:41,498 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:41,499 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:41,499 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42869', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980441.4993627')
2024-02-15 07:00:41,499 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:41,499 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:41,500 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35334; closing.
2024-02-15 07:00:41,500 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35318; closing.
2024-02-15 07:00:41,500 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:41,500 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:41,501 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:41,501 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:41,501 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44257', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980441.5013282')
2024-02-15 07:00:41,501 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:41,501 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34073', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980441.5016239')
2024-02-15 07:00:41,501 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:41,501 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35340; closing.
2024-02-15 07:00:41,502 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35362; closing.
2024-02-15 07:00:41,502 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43897', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980441.502717')
2024-02-15 07:00:41,502 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:41,503 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36749', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980441.5030944')
2024-02-15 07:00:41,503 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35370; closing.
2024-02-15 07:00:41,503 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35344; closing.
2024-02-15 07:00:41,504 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:41,504 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46489', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980441.504129')
2024-02-15 07:00:41,504 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980441.5044205')
2024-02-15 07:00:41,504 - distributed.scheduler - INFO - Lost all workers
2024-02-15 07:00:42,559 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 07:00:42,559 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 07:00:42,559 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 07:00:42,560 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-15 07:00:42,561 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-02-15 07:00:44,965 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:44,971 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-15 07:00:44,975 - distributed.scheduler - INFO - State start
2024-02-15 07:00:45,118 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:45,119 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-15 07:00:45,121 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-15 07:00:45,121 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 07:00:45,209 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40985'
2024-02-15 07:00:46,639 - distributed.scheduler - INFO - Receive client connection: Client-ef5846aa-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:46,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35492
2024-02-15 07:00:47,448 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:47,448 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:47,454 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:47,456 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36875
2024-02-15 07:00:47,456 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36875
2024-02-15 07:00:47,456 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44189
2024-02-15 07:00:47,456 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:47,456 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:47,456 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:47,456 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-15 07:00:47,456 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sc35fo5f
2024-02-15 07:00:47,457 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21b711c9-d2dc-4861-98d4-01d1738a3b76
2024-02-15 07:00:49,149 - distributed.worker - INFO - Starting Worker plugin PreImport-af939e88-269a-4908-9af2-fc94f89421fd
2024-02-15 07:00:49,149 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-32447d60-b250-4171-8ca9-ffaf5b3d316d
2024-02-15 07:00:49,149 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:49,220 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36875', status: init, memory: 0, processing: 0>
2024-02-15 07:00:49,221 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36875
2024-02-15 07:00:49,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35504
2024-02-15 07:00:49,224 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:49,225 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:49,225 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:49,227 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:49,311 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 07:00:49,316 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:49,318 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:49,321 - distributed.scheduler - INFO - Remove client Client-ef5846aa-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:49,321 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35492; closing.
2024-02-15 07:00:49,322 - distributed.scheduler - INFO - Remove client Client-ef5846aa-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:49,322 - distributed.scheduler - INFO - Close client connection: Client-ef5846aa-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:49,323 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40985'. Reason: nanny-close
2024-02-15 07:00:49,323 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:49,325 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36875. Reason: nanny-close
2024-02-15 07:00:49,327 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35504; closing.
2024-02-15 07:00:49,327 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:49,328 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980449.327919')
2024-02-15 07:00:49,328 - distributed.scheduler - INFO - Lost all workers
2024-02-15 07:00:49,329 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:50,038 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 07:00:50,039 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 07:00:50,039 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 07:00:50,040 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-15 07:00:50,040 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-02-15 07:00:52,446 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:52,450 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-02-15 07:00:52,454 - distributed.scheduler - INFO - State start
2024-02-15 07:00:52,477 - distributed.scheduler - INFO - -----------------------------------------------
2024-02-15 07:00:52,478 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-02-15 07:00:52,478 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-02-15 07:00:52,479 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-02-15 07:00:52,696 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43565'
2024-02-15 07:00:54,560 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:00:54,560 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:00:54,564 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:00:54,565 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46121
2024-02-15 07:00:54,565 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46121
2024-02-15 07:00:54,565 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37343
2024-02-15 07:00:54,565 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-02-15 07:00:54,565 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:54,565 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:00:54,565 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-15 07:00:54,565 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qrxxx7_v
2024-02-15 07:00:54,566 - distributed.worker - INFO - Starting Worker plugin PreImport-de386c75-303c-4b76-8ccc-b51061d3b5c6
2024-02-15 07:00:54,566 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eeada456-05ea-4cea-91fb-5ebe9bdeef1b
2024-02-15 07:00:54,566 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2caa7e8-1491-4e72-a1f0-eddf26908bb4
2024-02-15 07:00:54,863 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:54,926 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46121', status: init, memory: 0, processing: 0>
2024-02-15 07:00:54,943 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46121
2024-02-15 07:00:54,943 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37818
2024-02-15 07:00:54,944 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:00:54,945 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-02-15 07:00:54,945 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:00:54,947 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-02-15 07:00:57,244 - distributed.scheduler - INFO - Receive client connection: Client-f3d6bafc-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:57,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37844
2024-02-15 07:00:57,250 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-02-15 07:00:57,372 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-02-15 07:00:57,377 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:57,379 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:00:57,381 - distributed.scheduler - INFO - Remove client Client-f3d6bafc-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:57,381 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37844; closing.
2024-02-15 07:00:57,382 - distributed.scheduler - INFO - Remove client Client-f3d6bafc-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:57,382 - distributed.scheduler - INFO - Close client connection: Client-f3d6bafc-cbcf-11ee-91e7-d8c49764f6bb
2024-02-15 07:00:57,383 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43565'. Reason: nanny-close
2024-02-15 07:00:57,383 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-02-15 07:00:57,384 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46121. Reason: nanny-close
2024-02-15 07:00:57,387 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37818; closing.
2024-02-15 07:00:57,387 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-02-15 07:00:57,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46121', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1707980457.3874676')
2024-02-15 07:00:57,388 - distributed.scheduler - INFO - Lost all workers
2024-02-15 07:00:57,389 - distributed.nanny - INFO - Worker closed
2024-02-15 07:00:58,199 - distributed._signals - INFO - Received signal SIGINT (2)
2024-02-15 07:00:58,199 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-02-15 07:00:58,200 - distributed.scheduler - INFO - Scheduler closing all comms
2024-02-15 07:00:58,201 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-02-15 07:00:58,201 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33811 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44213 instead
  warnings.warn(
2024-02-15 07:01:21,061 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-15 07:01:21,062 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1027' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 55, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1016' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 55, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42817 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39677 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] 2024-02-15 07:02:09,601 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:46595 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7f353892f1c0 error: Connection reset by remote peer')
2024-02-15 07:02:09,617 - distributed.scheduler - ERROR - broadcast to ucxx://10.33.225.163:37237 failed: CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7f353892f740 error: Connection reset by remote peer')
Process SpawnProcess-6:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 200, in _test_ucx_infiniband_nvlink
    assert all(client.run(check_ucx_options).values())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2998, in run
    return self.sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 2903, in _run
    raise exc
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXConnectionResetError('Endpoint 0x7f353892f740 error: Connection reset by remote peer')
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39493 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42831 instead
  warnings.warn(
2024-02-15 07:02:47,406 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-15 07:02:47,407 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-15 07:02:47,409 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-15 07:02:47,410 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-15 07:02:47,411 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-15 07:02:47,412 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1360' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1365' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1376' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37485 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33813 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45765 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35541 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] [1707980625.940942] [dgx13:60281:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:44102) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] [1707980656.808615] [dgx13:60859:0]            sock.c:481  UCX  ERROR bind(fd=153 addr=0.0.0.0:39508) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36283 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41719 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36451 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37097 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33889 instead
  warnings.warn(
[1707980981.474440] [dgx13:64610:0]            sock.c:481  UCX  ERROR bind(fd=125 addr=0.0.0.0:51254) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34007 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42315 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45651 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33601 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36537 instead
  warnings.warn(
[1707981113.785929] [dgx13:66701:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:33487) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34967 instead
  warnings.warn(
[1707981140.423252] [dgx13:66992:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:45285) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41097 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43325 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39171 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46869 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43203 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38975 instead
  warnings.warn(
[1707981353.214895] [dgx13:70468:0]            sock.c:481  UCX  ERROR bind(fd=135 addr=0.0.0.0:57184) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41341 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38665 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34865 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46587 instead
  warnings.warn(
[1707981458.231001] [dgx13:72138:0]            sock.c:481  UCX  ERROR bind(fd=124 addr=0.0.0.0:39554) failed: Address already in use
[1707981462.407561] [dgx13:72227:0]            sock.c:481  UCX  ERROR bind(fd=153 addr=0.0.0.0:58658) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] [1707981484.131131] [dgx13:72440:0]            sock.c:481  UCX  ERROR bind(fd=155 addr=0.0.0.0:48660) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41729 instead
  warnings.warn(
[1707981503.908852] [dgx13:72676:0]            sock.c:481  UCX  ERROR bind(fd=158 addr=0.0.0.0:59994) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38295 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] [1707981559.713996] [dgx13:73417:0]            sock.c:481  UCX  ERROR bind(fd=155 addr=0.0.0.0:38247) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44035 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39883 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34307 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39531 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36771 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39279 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42603 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34007 instead
  warnings.warn(
2024-02-15 07:22:15,808 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-02-15 07:22:15,809 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44359 instead
  warnings.warn(
[1707981746.934755] [dgx13:75990:0]            sock.c:481  UCX  ERROR bind(fd=130 addr=0.0.0.0:35298) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34395 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40993 instead
  warnings.warn(
[1707981774.849751] [dgx13:76278:0]            sock.c:481  UCX  ERROR bind(fd=133 addr=0.0.0.0:38545) failed: Address already in use
[1707981776.304579] [dgx13:76446:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:32793) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46819 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45447 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34897 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40517 instead
  warnings.warn(
[1707981833.615264] [dgx13:77111:0]            sock.c:481  UCX  ERROR bind(fd=121 addr=0.0.0.0:56798) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46403 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38041 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36065 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42707 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40363 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39989 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1707981975.512455] [dgx13:78874:0]            sock.c:481  UCX  ERROR bind(fd=163 addr=0.0.0.0:56108) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] 2024-02-15 07:26:26,203 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 439, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 413, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 841, in wait
  File "libucxx.pyx", line 825, in wait_yield
  File "libucxx.pyx", line 820, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 445, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] 2024-02-15 07:26:32,415 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:51818 remote=tcp://127.0.0.1:45963>: Stream is closed
PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37405 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46747 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40673 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43459 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32863 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44971 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36771 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34025 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] [1707982090.079449] [dgx13:53735:0]            sock.c:481  UCX  ERROR bind(fd=256 addr=0.0.0.0:49671) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-02-15 07:28:31,774 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:28:31,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:28:31,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:28:31,784 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:28:31,886 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:28:31,886 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:28:31,947 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:28:31,947 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:28:31,960 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:28:31,960 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:28:32,274 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:28:32,274 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:28:32,274 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:28:32,274 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:28:32,274 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:28:32,274 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:28:32,418 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:28:32,419 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33559
2024-02-15 07:28:32,419 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33559
2024-02-15 07:28:32,420 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36251
2024-02-15 07:28:32,420 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,420 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,420 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:28:32,420 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ku2xteru
2024-02-15 07:28:32,420 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49def5eb-3b27-4cfb-a94d-54603aaba7d8
2024-02-15 07:28:32,420 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e29b592-c416-42d6-8829-886c82d82111
2024-02-15 07:28:32,420 - distributed.worker - INFO - Starting Worker plugin PreImport-4e0fdee1-2368-4050-8792-a9129013d5be
2024-02-15 07:28:32,420 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:28:32,432 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45689
2024-02-15 07:28:32,432 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45689
2024-02-15 07:28:32,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35043
2024-02-15 07:28:32,433 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,433 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,433 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:28:32,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yr8rm99t
2024-02-15 07:28:32,433 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8416c76b-8690-45f4-a526-0c7bac1ffa08
2024-02-15 07:28:32,433 - distributed.worker - INFO - Starting Worker plugin PreImport-227fca1b-b156-4f38-bb7b-c2ba63ee1cc7
2024-02-15 07:28:32,433 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d7c20369-b49a-4626-9165-e65d6fd1ed2c
2024-02-15 07:28:32,434 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,502 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:28:32,503 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,503 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,504 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40677
2024-02-15 07:28:32,532 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:28:32,533 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,533 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,534 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40677
2024-02-15 07:28:32,564 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:28:32,565 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39285
2024-02-15 07:28:32,565 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39285
2024-02-15 07:28:32,565 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34601
2024-02-15 07:28:32,565 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,565 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,565 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:28:32,565 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fk3avy96
2024-02-15 07:28:32,566 - distributed.worker - INFO - Starting Worker plugin PreImport-e4c76d23-f1f4-46bc-9afe-5617a1610e25
2024-02-15 07:28:32,566 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c72e9dea-2572-4884-a2b8-e6c7d881240d
2024-02-15 07:28:32,566 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd85ed0e-809a-44dc-aa18-3de6647ba279
2024-02-15 07:28:32,566 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,600 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:28:32,601 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45735
2024-02-15 07:28:32,602 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45735
2024-02-15 07:28:32,602 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33865
2024-02-15 07:28:32,602 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,602 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,602 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:28:32,602 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ibfacikx
2024-02-15 07:28:32,602 - distributed.worker - INFO - Starting Worker plugin PreImport-e341691d-ddc9-474b-ae69-cde369439890
2024-02-15 07:28:32,602 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0af568b-ec6d-4ea1-aeb3-31aa3eac71da
2024-02-15 07:28:32,602 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75c46a16-df6f-4568-b8c7-70d0abfa4ea4
2024-02-15 07:28:32,603 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,636 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:28:32,637 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,637 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,638 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40677
2024-02-15 07:28:32,642 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:28:32,643 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41013
2024-02-15 07:28:32,643 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41013
2024-02-15 07:28:32,643 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39173
2024-02-15 07:28:32,643 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,643 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,643 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:28:32,643 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gzsq7vhj
2024-02-15 07:28:32,644 - distributed.worker - INFO - Starting Worker plugin PreImport-f4054413-1ed4-471e-9935-cdf71e773633
2024-02-15 07:28:32,644 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9f5e93a-5e8b-4745-badf-cf72b9e19113
2024-02-15 07:28:32,644 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a21a74d0-2d24-4dcf-8b57-d74d241912e7
2024-02-15 07:28:32,644 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:28:32,698 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,698 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,699 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40677
2024-02-15 07:28:32,724 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:28:32,724 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,724 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,726 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40677
2024-02-15 07:28:32,942 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:28:32,943 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36265
2024-02-15 07:28:32,943 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36265
2024-02-15 07:28:32,943 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39419
2024-02-15 07:28:32,943 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,943 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,943 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:28:32,943 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-epc6g8_q
2024-02-15 07:28:32,943 - distributed.worker - INFO - Starting Worker plugin PreImport-bb93fdfa-05b9-4589-8cb0-b3514cc05eb4
2024-02-15 07:28:32,943 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54324951-5b1e-47ec-b96a-100866802a90
2024-02-15 07:28:32,944 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ce013fc3-2bff-4722-9c70-1d137b7a312c
2024-02-15 07:28:32,944 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,946 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:28:32,946 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36603
2024-02-15 07:28:32,947 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36603
2024-02-15 07:28:32,947 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46463
2024-02-15 07:28:32,947 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,947 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,947 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:28:32,947 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_gyka9qx
2024-02-15 07:28:32,947 - distributed.worker - INFO - Starting Worker plugin PreImport-bd2d1451-1dba-4810-9eb0-bcb61d55383c
2024-02-15 07:28:32,947 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb8f3fde-7df4-441d-a7fc-f7beb33c89df
2024-02-15 07:28:32,947 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27f136bb-3484-42dd-a566-127186c24c45
2024-02-15 07:28:32,948 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,956 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:28:32,957 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41517
2024-02-15 07:28:32,957 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41517
2024-02-15 07:28:32,957 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38379
2024-02-15 07:28:32,957 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40677
2024-02-15 07:28:32,958 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:32,958 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:28:32,958 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s8xiav9c
2024-02-15 07:28:32,958 - distributed.worker - INFO - Starting Worker plugin PreImport-1964c3aa-6a05-4c3a-98c2-3787976471a8
2024-02-15 07:28:32,958 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3de21723-13f3-42ab-ab5f-0469f4d6d7c1
2024-02-15 07:28:32,958 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c04e201d-26ad-46d3-8071-0ddf456c8e6f
2024-02-15 07:28:32,958 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:33,083 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:28:33,084 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40677
2024-02-15 07:28:33,084 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:33,085 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:28:33,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40677
2024-02-15 07:28:33,086 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40677
2024-02-15 07:28:33,086 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:33,087 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40677
2024-02-15 07:28:33,091 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:28:33,092 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:40677
2024-02-15 07:28:33,092 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:28:33,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40677
2024-02-15 07:28:33,129 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:28:33,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:28:33,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:28:33,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:28:33,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:28:33,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:28:33,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:28:33,130 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-02-15 07:28:33,137 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45689. Reason: nanny-close
2024-02-15 07:28:33,137 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33559. Reason: nanny-close
2024-02-15 07:28:33,139 - distributed.core - INFO - Connection to tcp://127.0.0.1:40677 has been closed.
2024-02-15 07:28:33,139 - distributed.core - INFO - Connection to tcp://127.0.0.1:40677 has been closed.
2024-02-15 07:28:33,139 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41013. Reason: nanny-close
2024-02-15 07:28:33,140 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45735. Reason: nanny-close
2024-02-15 07:28:33,140 - distributed.nanny - INFO - Worker closed
2024-02-15 07:28:33,140 - distributed.nanny - INFO - Worker closed
2024-02-15 07:28:33,141 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39285. Reason: nanny-close
2024-02-15 07:28:33,141 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41517. Reason: nanny-close
2024-02-15 07:28:33,141 - distributed.core - INFO - Connection to tcp://127.0.0.1:40677 has been closed.
2024-02-15 07:28:33,142 - distributed.core - INFO - Connection to tcp://127.0.0.1:40677 has been closed.
2024-02-15 07:28:33,143 - distributed.nanny - INFO - Worker closed
2024-02-15 07:28:33,143 - distributed.core - INFO - Connection to tcp://127.0.0.1:40677 has been closed.
2024-02-15 07:28:33,143 - distributed.core - INFO - Connection to tcp://127.0.0.1:40677 has been closed.
2024-02-15 07:28:33,143 - distributed.nanny - INFO - Worker closed
2024-02-15 07:28:33,143 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36603. Reason: nanny-close
2024-02-15 07:28:33,143 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36265. Reason: nanny-close
2024-02-15 07:28:33,144 - distributed.nanny - INFO - Worker closed
2024-02-15 07:28:33,144 - distributed.nanny - INFO - Worker closed
2024-02-15 07:28:33,145 - distributed.core - INFO - Connection to tcp://127.0.0.1:40677 has been closed.
2024-02-15 07:28:33,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:40677 has been closed.
2024-02-15 07:28:33,147 - distributed.nanny - INFO - Worker closed
2024-02-15 07:28:33,147 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size 2024-02-15 07:29:00,305 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:44330 remote=tcp://127.0.0.1:36101>: Stream is closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-02-15 07:29:12,016 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:29:12,016 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:29:12,023 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:29:12,024 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37093
2024-02-15 07:29:12,024 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37093
2024-02-15 07:29:12,024 - distributed.worker - INFO -           Worker name:                          0
2024-02-15 07:29:12,024 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42169
2024-02-15 07:29:12,024 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36981
2024-02-15 07:29:12,024 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:12,024 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:29:12,024 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-02-15 07:29:12,025 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gkpg2ssc
2024-02-15 07:29:12,025 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ed4743f4-7f9c-43c3-932c-1bca954666d0
2024-02-15 07:29:12,025 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81a97038-809f-4a58-b21f-4390b79b81a2
2024-02-15 07:29:12,025 - distributed.worker - INFO - Starting Worker plugin PreImport-e40f0e9a-2470-4151-863a-15f2da3fc97d
2024-02-15 07:29:12,032 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-02-15 07:29:12,033 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37093. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-02-15 07:29:12,033 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-02-15 07:29:12,036 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-02-15 07:29:17,109 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:29:17,109 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:29:17,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:29:17,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:29:17,224 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:29:17,224 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:29:17,243 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:29:17,244 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:29:17,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:29:17,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:29:17,352 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:29:17,352 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:29:17,403 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:29:17,403 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:29:17,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-02-15 07:29:17,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-02-15 07:29:17,847 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:29:17,848 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40905
2024-02-15 07:29:17,848 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40905
2024-02-15 07:29:17,848 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45655
2024-02-15 07:29:17,848 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38555
2024-02-15 07:29:17,848 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:17,848 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:29:17,849 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:29:17,849 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h9exd82l
2024-02-15 07:29:17,849 - distributed.worker - INFO - Starting Worker plugin PreImport-465d4110-7163-424d-a0da-e2607079996f
2024-02-15 07:29:17,849 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-329b4622-2d94-45c0-ba34-feb563a5a50a
2024-02-15 07:29:17,849 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1d3fc144-eb39-4513-b4ec-5f083f2442c4
2024-02-15 07:29:17,849 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:17,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:29:17,898 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36767
2024-02-15 07:29:17,898 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36767
2024-02-15 07:29:17,898 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39351
2024-02-15 07:29:17,898 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38555
2024-02-15 07:29:17,898 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:17,898 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:29:17,898 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:29:17,898 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z0jnqnaz
2024-02-15 07:29:17,899 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1a83523-d2db-42de-a10a-deaf2fc7ac97
2024-02-15 07:29:17,899 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd53822e-6f27-4fea-8f61-b33c5a965c65
2024-02-15 07:29:17,899 - distributed.worker - INFO - Starting Worker plugin PreImport-057feb6c-9049-4df0-b793-82ae7c5df417
2024-02-15 07:29:17,899 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:17,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:29:17,938 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38555
2024-02-15 07:29:17,938 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:17,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38555
2024-02-15 07:29:17,982 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:29:17,984 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38555
2024-02-15 07:29:17,984 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:17,985 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38555
2024-02-15 07:29:18,038 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:29:18,040 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42957
2024-02-15 07:29:18,040 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42957
2024-02-15 07:29:18,040 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40691
2024-02-15 07:29:18,040 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,040 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,040 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:29:18,040 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:29:18,040 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-19i522we
2024-02-15 07:29:18,041 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b9eb334d-8b39-4e02-8a34-4a7508448bb3
2024-02-15 07:29:18,041 - distributed.worker - INFO - Starting Worker plugin PreImport-0b3014fc-6ef8-4c20-8688-aea7272fed19
2024-02-15 07:29:18,041 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4124fae-9378-4e5f-9a52-09ebd2db9a9d
2024-02-15 07:29:18,042 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,044 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:29:18,046 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40025
2024-02-15 07:29:18,046 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40025
2024-02-15 07:29:18,046 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41999
2024-02-15 07:29:18,046 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,046 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,046 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:29:18,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:29:18,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4pgmhjam
2024-02-15 07:29:18,047 - distributed.worker - INFO - Starting Worker plugin PreImport-8e1da837-37b8-44e2-adb9-26884b54a8be
2024-02-15 07:29:18,047 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c23728c8-acc0-4d36-9af7-5db651e7c276
2024-02-15 07:29:18,056 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed66a403-f2a1-40bd-a1b2-e434714d635f
2024-02-15 07:29:18,056 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,139 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:29:18,140 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,140 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,141 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:29:18,142 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38555
2024-02-15 07:29:18,142 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38345
2024-02-15 07:29:18,142 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38345
2024-02-15 07:29:18,142 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42335
2024-02-15 07:29:18,142 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,142 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,143 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:29:18,143 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:29:18,143 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-063qvoze
2024-02-15 07:29:18,143 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1edc424e-6c47-425d-855f-f83291ce72d7
2024-02-15 07:29:18,143 - distributed.worker - INFO - Starting Worker plugin PreImport-927beb1f-230c-4964-9760-6b383c81fa36
2024-02-15 07:29:18,143 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f964683-4bab-47ed-a816-6470f45e0e1a
2024-02-15 07:29:18,143 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,155 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:29:18,156 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,156 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,158 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38555
2024-02-15 07:29:18,183 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:29:18,184 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42701
2024-02-15 07:29:18,184 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42701
2024-02-15 07:29:18,184 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44201
2024-02-15 07:29:18,184 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,184 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,184 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:29:18,185 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:29:18,185 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5cew0khh
2024-02-15 07:29:18,185 - distributed.worker - INFO - Starting Worker plugin RMMSetup-94919b78-7763-4317-a02a-7a635bf5b129
2024-02-15 07:29:18,185 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6be30b10-59ea-4bfe-987f-3085d882b78e
2024-02-15 07:29:18,185 - distributed.worker - INFO - Starting Worker plugin PreImport-bb8ff9fd-bbac-4dfd-806c-3f095a43070b
2024-02-15 07:29:18,186 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,220 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:29:18,221 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,221 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,222 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38555
2024-02-15 07:29:18,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:29:18,257 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35241
2024-02-15 07:29:18,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35241
2024-02-15 07:29:18,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42835
2024-02-15 07:29:18,257 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,257 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,257 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:29:18,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:29:18,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wt5piuf8
2024-02-15 07:29:18,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc53331a-ce37-4bc2-a025-2c1ee476ed7d
2024-02-15 07:29:18,257 - distributed.worker - INFO - Starting Worker plugin PreImport-eee28bd0-5e39-4d20-80a6-2179e0a7bcda
2024-02-15 07:29:18,258 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae7b661c-c724-4234-b21d-cba32903a555
2024-02-15 07:29:18,258 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,263 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:29:18,264 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,264 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38555
2024-02-15 07:29:18,298 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-02-15 07:29:18,299 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43235
2024-02-15 07:29:18,299 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43235
2024-02-15 07:29:18,299 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42039
2024-02-15 07:29:18,299 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,299 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,299 - distributed.worker - INFO -               Threads:                          1
2024-02-15 07:29:18,300 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-02-15 07:29:18,300 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d9e4aybn
2024-02-15 07:29:18,300 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9906c913-9ab3-49da-b428-68161a7c0b7b
2024-02-15 07:29:18,300 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b994369f-4483-4bac-9cc3-c34bd82b6bc9
2024-02-15 07:29:18,300 - distributed.worker - INFO - Starting Worker plugin PreImport-d676dbeb-9d01-46c1-9aa1-e5d8727ac173
2024-02-15 07:29:18,301 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:29:18,327 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,327 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,329 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38555
2024-02-15 07:29:18,393 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-02-15 07:29:18,395 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38555
2024-02-15 07:29:18,395 - distributed.worker - INFO - -------------------------------------------------
2024-02-15 07:29:18,397 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38555
2024-02-15 07:29:18,412 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40905. Reason: nanny-close
2024-02-15 07:29:18,413 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36767. Reason: nanny-close
2024-02-15 07:29:18,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40025. Reason: nanny-close
2024-02-15 07:29:18,414 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42957. Reason: nanny-close
2024-02-15 07:29:18,415 - distributed.core - INFO - Connection to tcp://127.0.0.1:38555 has been closed.
2024-02-15 07:29:18,415 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35241. Reason: nanny-close
2024-02-15 07:29:18,415 - distributed.core - INFO - Connection to tcp://127.0.0.1:38555 has been closed.
2024-02-15 07:29:18,415 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38345. Reason: nanny-close
2024-02-15 07:29:18,416 - distributed.nanny - INFO - Worker closed
2024-02-15 07:29:18,416 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42701. Reason: nanny-close
2024-02-15 07:29:18,416 - distributed.nanny - INFO - Worker closed
2024-02-15 07:29:18,417 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43235. Reason: nanny-close
2024-02-15 07:29:18,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:38555 has been closed.
2024-02-15 07:29:18,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:38555 has been closed.
2024-02-15 07:29:18,417 - distributed.core - INFO - Connection to tcp://127.0.0.1:38555 has been closed.
2024-02-15 07:29:18,418 - distributed.nanny - INFO - Worker closed
2024-02-15 07:29:18,418 - distributed.nanny - INFO - Worker closed
2024-02-15 07:29:18,419 - distributed.core - INFO - Connection to tcp://127.0.0.1:38555 has been closed.
2024-02-15 07:29:18,419 - distributed.nanny - INFO - Worker closed
2024-02-15 07:29:18,420 - distributed.nanny - INFO - Worker closed
2024-02-15 07:29:18,420 - distributed.core - INFO - Connection to tcp://127.0.0.1:38555 has been closed.
2024-02-15 07:29:18,422 - distributed.nanny - INFO - Worker closed
2024-02-15 07:29:18,430 - distributed.core - INFO - Connection to tcp://127.0.0.1:38555 has been closed.
2024-02-15 07:29:18,435 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] [1707982178.823138] [dgx13:81471:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:54766) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-02-15 07:30:30,660 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
2024-02-15 07:30:30,667 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f95731e8d00>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:69: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-02-15 07:31:06,210 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-15 07:31:06,213 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
