============================= test session starts ==============================
platform linux -- Python 3.9.17, pytest-7.4.0, pluggy-1.2.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-08-12 05:38:59,496 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:38:59,502 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39019 instead
  warnings.warn(
2023-08-12 05:38:59,507 - distributed.scheduler - INFO - State start
2023-08-12 05:38:59,845 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:38:59,846 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-08-12 05:38:59,847 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39019/status
2023-08-12 05:39:00,021 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40425'
2023-08-12 05:39:00,032 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38905'
2023-08-12 05:39:00,044 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37155'
2023-08-12 05:39:00,052 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46577'
2023-08-12 05:39:01,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:01,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:01,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:01,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:01,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:01,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:01,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:01,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:01,973 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:01,973 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:01,974 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:01,974 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-08-12 05:39:01,993 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41669
2023-08-12 05:39:01,993 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41669
2023-08-12 05:39:01,994 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34955
2023-08-12 05:39:01,994 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-12 05:39:01,994 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:01,994 - distributed.worker - INFO -               Threads:                          4
2023-08-12 05:39:01,994 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-12 05:39:01,994 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iw336e7e
2023-08-12 05:39:01,994 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dbb4d62d-7bb7-4c39-b19a-6727127ca560
2023-08-12 05:39:01,994 - distributed.worker - INFO - Starting Worker plugin PreImport-1a88df1e-dc44-43c8-ad7a-cd1c009153c0
2023-08-12 05:39:01,994 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe22d83e-62c1-4c29-b836-65d28063553a
2023-08-12 05:39:01,994 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:02,010 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41669', status: init, memory: 0, processing: 0>
2023-08-12 05:39:02,021 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41669
2023-08-12 05:39:02,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40918
2023-08-12 05:39:02,022 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-12 05:39:02,022 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:02,024 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-12 05:39:03,783 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39747
2023-08-12 05:39:03,783 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39747
2023-08-12 05:39:03,784 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37261
2023-08-12 05:39:03,784 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-12 05:39:03,784 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:03,784 - distributed.worker - INFO -               Threads:                          4
2023-08-12 05:39:03,784 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-12 05:39:03,784 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c5wiotbq
2023-08-12 05:39:03,784 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7d4e3de1-43ce-4298-911e-54b5e2bd3140
2023-08-12 05:39:03,784 - distributed.worker - INFO - Starting Worker plugin PreImport-a4ae61d2-1ecb-4740-864f-15905ada7146
2023-08-12 05:39:03,785 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee6e7494-0052-412f-8fb0-147da3dca357
2023-08-12 05:39:03,785 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:03,787 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45787
2023-08-12 05:39:03,788 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45787
2023-08-12 05:39:03,788 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33445
2023-08-12 05:39:03,788 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-12 05:39:03,788 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:03,788 - distributed.worker - INFO -               Threads:                          4
2023-08-12 05:39:03,788 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-12 05:39:03,788 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-34id0mp3
2023-08-12 05:39:03,788 - distributed.worker - INFO - Starting Worker plugin PreImport-cefca973-d794-4103-980e-f0312f37c375
2023-08-12 05:39:03,789 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d27bbc0-679e-40ab-9207-3095af55e6a1
2023-08-12 05:39:03,789 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc4d3d6b-c328-4123-91cb-e14d7fb68015
2023-08-12 05:39:03,789 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:03,790 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43825
2023-08-12 05:39:03,791 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43825
2023-08-12 05:39:03,791 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38673
2023-08-12 05:39:03,791 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-12 05:39:03,791 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:03,791 - distributed.worker - INFO -               Threads:                          4
2023-08-12 05:39:03,791 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-08-12 05:39:03,791 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3ar79pts
2023-08-12 05:39:03,792 - distributed.worker - INFO - Starting Worker plugin PreImport-2e7bd64a-e94e-44d9-b6a5-e5326d250d1a
2023-08-12 05:39:03,792 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-41e4bbd8-be16-4ebc-91a8-3c39e0516340
2023-08-12 05:39:03,794 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7a6b816-373c-4c89-bc29-8cbdccc5f2be
2023-08-12 05:39:03,795 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:03,806 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39747', status: init, memory: 0, processing: 0>
2023-08-12 05:39:03,807 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39747
2023-08-12 05:39:03,807 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40936
2023-08-12 05:39:03,808 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-12 05:39:03,808 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:03,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-12 05:39:03,826 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43825', status: init, memory: 0, processing: 0>
2023-08-12 05:39:03,827 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43825
2023-08-12 05:39:03,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40952
2023-08-12 05:39:03,827 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-12 05:39:03,828 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:03,828 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45787', status: init, memory: 0, processing: 0>
2023-08-12 05:39:03,830 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45787
2023-08-12 05:39:03,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40938
2023-08-12 05:39:03,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-12 05:39:03,831 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-12 05:39:03,831 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:03,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-12 05:39:04,748 - distributed.scheduler - INFO - Receive client connection: Client-887561b3-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:04,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40954
2023-08-12 05:39:04,760 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-12 05:39:04,761 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-12 05:39:04,761 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-12 05:39:04,762 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-08-12 05:39:04,767 - distributed.scheduler - INFO - Remove client Client-887561b3-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:04,767 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40954; closing.
2023-08-12 05:39:04,768 - distributed.scheduler - INFO - Remove client Client-887561b3-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:04,768 - distributed.scheduler - INFO - Close client connection: Client-887561b3-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:04,769 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40425'. Reason: nanny-close
2023-08-12 05:39:04,770 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:04,770 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38905'. Reason: nanny-close
2023-08-12 05:39:04,771 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:04,771 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46577'. Reason: nanny-close
2023-08-12 05:39:04,771 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43825. Reason: nanny-close
2023-08-12 05:39:04,771 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:04,772 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39747. Reason: nanny-close
2023-08-12 05:39:04,772 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37155'. Reason: nanny-close
2023-08-12 05:39:04,772 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:04,773 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45787. Reason: nanny-close
2023-08-12 05:39:04,773 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41669. Reason: nanny-close
2023-08-12 05:39:04,773 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-12 05:39:04,773 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-12 05:39:04,773 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40952; closing.
2023-08-12 05:39:04,774 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43825', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:04,774 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43825
2023-08-12 05:39:04,775 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:04,775 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:04,775 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40936; closing.
2023-08-12 05:39:04,775 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-12 05:39:04,775 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39747', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:04,776 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39747
2023-08-12 05:39:04,776 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-12 05:39:04,776 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:04,777 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40918; closing.
2023-08-12 05:39:04,778 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:04,777 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:40936>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-12 05:39:04,780 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41669', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:04,780 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41669
2023-08-12 05:39:04,781 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40938; closing.
2023-08-12 05:39:04,781 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45787', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:04,781 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45787
2023-08-12 05:39:04,782 - distributed.scheduler - INFO - Lost all workers
2023-08-12 05:39:06,737 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:39:06,738 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:39:06,738 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:39:06,739 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-08-12 05:39:06,739 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-08-12 05:39:08,675 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:08,680 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36073 instead
  warnings.warn(
2023-08-12 05:39:08,684 - distributed.scheduler - INFO - State start
2023-08-12 05:39:08,704 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:08,705 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-12 05:39:08,706 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36073/status
2023-08-12 05:39:08,784 - distributed.scheduler - INFO - Receive client connection: Client-8df9930e-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:08,797 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36928
2023-08-12 05:39:09,064 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38403'
2023-08-12 05:39:09,082 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41265'
2023-08-12 05:39:09,091 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44731'
2023-08-12 05:39:09,093 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44141'
2023-08-12 05:39:09,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35609'
2023-08-12 05:39:09,112 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39609'
2023-08-12 05:39:09,120 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33167'
2023-08-12 05:39:09,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38785'
2023-08-12 05:39:10,727 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:10,727 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:10,727 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:10,727 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:10,734 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:10,734 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:10,740 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:10,740 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:10,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:10,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:10,754 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:10,754 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:10,760 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:10,768 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:10,769 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:10,780 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:10,780 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:10,790 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:10,790 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:10,791 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:10,791 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:10,808 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:10,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:10,829 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:15,106 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37353
2023-08-12 05:39:15,107 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37353
2023-08-12 05:39:15,107 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46525
2023-08-12 05:39:15,107 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,107 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,107 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:15,107 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:15,107 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-byw3p7gy
2023-08-12 05:39:15,107 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ed86a7a4-b2a7-48ae-94fd-315a2cba079f
2023-08-12 05:39:15,110 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33851
2023-08-12 05:39:15,111 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33851
2023-08-12 05:39:15,111 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46787
2023-08-12 05:39:15,111 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,111 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,111 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:15,112 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:15,112 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e84ph6kw
2023-08-12 05:39:15,112 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2263b268-eb25-47a5-9b1c-c59e9b6791e7
2023-08-12 05:39:15,360 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4f871e2-5f70-4622-b39e-5a5c4de84505
2023-08-12 05:39:15,360 - distributed.worker - INFO - Starting Worker plugin PreImport-caa0957c-3dc6-459d-9407-b3d6faa8a70d
2023-08-12 05:39:15,361 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,362 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60403ba8-c4b3-4f0e-be08-403507107c57
2023-08-12 05:39:15,362 - distributed.worker - INFO - Starting Worker plugin PreImport-418d0a29-b378-4877-8645-6ef2962eec92
2023-08-12 05:39:15,363 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,384 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39325
2023-08-12 05:39:15,385 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39325
2023-08-12 05:39:15,385 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44051
2023-08-12 05:39:15,385 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,385 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,385 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:15,385 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:15,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wvq1z3jy
2023-08-12 05:39:15,385 - distributed.worker - INFO - Starting Worker plugin PreImport-afd0b801-9fc9-4808-b3f7-71b5201fe5c7
2023-08-12 05:39:15,386 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b1523d7-15a4-4d50-90f9-18d9b0519134
2023-08-12 05:39:15,386 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c010bf8f-18e0-4b53-9780-253c3910928d
2023-08-12 05:39:15,387 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46429
2023-08-12 05:39:15,388 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46429
2023-08-12 05:39:15,388 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45839
2023-08-12 05:39:15,388 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,388 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,389 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:15,389 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:15,389 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fdxhv0kz
2023-08-12 05:39:15,389 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8632595f-291e-49ad-914f-e7f2c51e0393
2023-08-12 05:39:15,393 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34579
2023-08-12 05:39:15,393 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34579
2023-08-12 05:39:15,394 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37449
2023-08-12 05:39:15,394 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,394 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,394 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:15,394 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:15,394 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-s5oekc8p
2023-08-12 05:39:15,394 - distributed.worker - INFO - Starting Worker plugin PreImport-99db1891-1f40-47aa-aaec-b6fe00c2490c
2023-08-12 05:39:15,394 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f01a455-e4a7-4c9e-8931-093166c97727
2023-08-12 05:39:15,395 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2c228b40-f20f-46ee-b8a3-f92c0b8fb8e1
2023-08-12 05:39:15,396 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44185
2023-08-12 05:39:15,396 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46371
2023-08-12 05:39:15,397 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46371
2023-08-12 05:39:15,397 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44185
2023-08-12 05:39:15,397 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39111
2023-08-12 05:39:15,397 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37151
2023-08-12 05:39:15,397 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,397 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,397 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44937
2023-08-12 05:39:15,397 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,397 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,397 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44937
2023-08-12 05:39:15,397 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:15,397 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:15,397 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45045
2023-08-12 05:39:15,397 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:15,397 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:15,397 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fljm4ec_
2023-08-12 05:39:15,397 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,398 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qeivxt9m
2023-08-12 05:39:15,398 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,398 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:15,398 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c7b570e-de48-4e7c-854a-983b1e763b82
2023-08-12 05:39:15,398 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:15,398 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uf_us2l4
2023-08-12 05:39:15,398 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9278e2db-236f-4daa-8062-150f8104a9c3
2023-08-12 05:39:15,398 - distributed.worker - INFO - Starting Worker plugin PreImport-b14b4ce4-c787-4452-ba8b-fdb9db90d44e
2023-08-12 05:39:15,398 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b692bec-6328-4a4a-b143-2d965229345f
2023-08-12 05:39:15,399 - distributed.worker - INFO - Starting Worker plugin RMMSetup-387623d7-3e00-4d62-bd3a-a3ea1461e7cd
2023-08-12 05:39:15,400 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33851', status: init, memory: 0, processing: 0>
2023-08-12 05:39:15,402 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33851
2023-08-12 05:39:15,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42020
2023-08-12 05:39:15,403 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,403 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,403 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37353', status: init, memory: 0, processing: 0>
2023-08-12 05:39:15,404 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37353
2023-08-12 05:39:15,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42012
2023-08-12 05:39:15,404 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,404 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,406 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:15,407 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:15,575 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-978a6aae-f445-4f42-9ac0-665d653c53b2
2023-08-12 05:39:15,576 - distributed.worker - INFO - Starting Worker plugin PreImport-6d991159-4123-4aac-9e04-ca1b7adc2a04
2023-08-12 05:39:15,576 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,578 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,578 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,578 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5deb099a-7a5a-49ef-9c05-3a962fa02789
2023-08-12 05:39:15,578 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,579 - distributed.worker - INFO - Starting Worker plugin PreImport-336283d4-ae4f-496e-8b12-eaa7c60eb510
2023-08-12 05:39:15,579 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f2f4c0b-288a-4d86-9e1c-c87d19ecb8e2
2023-08-12 05:39:15,579 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,580 - distributed.worker - INFO - Starting Worker plugin PreImport-2a34ddd6-350c-43a3-a44d-5ee0ed0ee255
2023-08-12 05:39:15,580 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,732 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34579', status: init, memory: 0, processing: 0>
2023-08-12 05:39:15,733 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34579
2023-08-12 05:39:15,733 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42056
2023-08-12 05:39:15,734 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,734 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,734 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44937', status: init, memory: 0, processing: 0>
2023-08-12 05:39:15,735 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44937
2023-08-12 05:39:15,735 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42064
2023-08-12 05:39:15,735 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,736 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46371', status: init, memory: 0, processing: 0>
2023-08-12 05:39:15,736 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:15,736 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46371
2023-08-12 05:39:15,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42076
2023-08-12 05:39:15,737 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,737 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,737 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39325', status: init, memory: 0, processing: 0>
2023-08-12 05:39:15,738 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39325
2023-08-12 05:39:15,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42044
2023-08-12 05:39:15,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:15,738 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,738 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,739 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46429', status: init, memory: 0, processing: 0>
2023-08-12 05:39:15,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:15,739 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46429
2023-08-12 05:39:15,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42036
2023-08-12 05:39:15,740 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,740 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44185', status: init, memory: 0, processing: 0>
2023-08-12 05:39:15,740 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:15,740 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44185
2023-08-12 05:39:15,741 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42086
2023-08-12 05:39:15,741 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:15,741 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:15,742 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:15,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:15,751 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:15,751 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:15,751 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:15,751 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:15,751 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:15,751 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:15,751 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:15,752 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:15,755 - distributed.scheduler - INFO - Remove client Client-8df9930e-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:15,756 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36928; closing.
2023-08-12 05:39:15,756 - distributed.scheduler - INFO - Remove client Client-8df9930e-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:15,756 - distributed.scheduler - INFO - Close client connection: Client-8df9930e-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:15,757 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41265'. Reason: nanny-close
2023-08-12 05:39:15,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35609'. Reason: nanny-close
2023-08-12 05:39:15,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38403'. Reason: nanny-close
2023-08-12 05:39:15,758 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44731'. Reason: nanny-close
2023-08-12 05:39:15,760 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44141'. Reason: nanny-close
2023-08-12 05:39:15,760 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:15,761 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39609'. Reason: nanny-close
2023-08-12 05:39:15,761 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:15,762 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33851. Reason: nanny-close
2023-08-12 05:39:15,762 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33167'. Reason: nanny-close
2023-08-12 05:39:15,762 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38785'. Reason: nanny-close
2023-08-12 05:39:15,762 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37353. Reason: nanny-close
2023-08-12 05:39:15,764 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42020; closing.
2023-08-12 05:39:15,764 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:15,764 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33851', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:15,764 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33851
2023-08-12 05:39:15,765 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:15,765 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33851
2023-08-12 05:39:15,765 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:15,766 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33851
2023-08-12 05:39:15,766 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33851
2023-08-12 05:39:15,766 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33851
2023-08-12 05:39:15,766 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:15,766 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42012; closing.
2023-08-12 05:39:15,766 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33851
2023-08-12 05:39:15,766 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33851
2023-08-12 05:39:15,767 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37353', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:15,767 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37353
2023-08-12 05:39:15,771 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37353
2023-08-12 05:39:15,771 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37353
2023-08-12 05:39:15,771 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37353
2023-08-12 05:39:15,771 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37353
2023-08-12 05:39:15,771 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37353
2023-08-12 05:39:15,771 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37353
2023-08-12 05:39:15,785 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:15,785 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:15,785 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:15,786 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34579. Reason: nanny-close
2023-08-12 05:39:15,786 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:15,786 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44185. Reason: nanny-close
2023-08-12 05:39:15,786 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:15,786 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44937. Reason: nanny-close
2023-08-12 05:39:15,787 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46429. Reason: nanny-close
2023-08-12 05:39:15,787 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:15,787 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42056; closing.
2023-08-12 05:39:15,788 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34579', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:15,788 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34579
2023-08-12 05:39:15,788 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46371. Reason: nanny-close
2023-08-12 05:39:15,788 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:15,788 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:15,789 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:15,789 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34579
2023-08-12 05:39:15,789 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42064; closing.
2023-08-12 05:39:15,789 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34579
2023-08-12 05:39:15,789 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34579
2023-08-12 05:39:15,789 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:15,789 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:15,789 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44937', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:15,790 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44937
2023-08-12 05:39:15,790 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:15,790 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42086; closing.
2023-08-12 05:39:15,790 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:15,790 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:15,790 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44185', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:15,790 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44185
2023-08-12 05:39:15,791 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42036; closing.
2023-08-12 05:39:15,791 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:15,791 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:15,791 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39325. Reason: nanny-close
2023-08-12 05:39:15,791 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42076; closing.
2023-08-12 05:39:15,791 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46429', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:15,791 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46429
2023-08-12 05:39:15,792 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46371', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:15,792 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46371
2023-08-12 05:39:15,793 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:15,793 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42044; closing.
2023-08-12 05:39:15,793 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39325', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:15,793 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39325
2023-08-12 05:39:15,793 - distributed.scheduler - INFO - Lost all workers
2023-08-12 05:39:15,794 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:17,576 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:39:17,576 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:39:17,576 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:39:17,577 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-12 05:39:17,578 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-08-12 05:39:19,300 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:19,306 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38651 instead
  warnings.warn(
2023-08-12 05:39:19,310 - distributed.scheduler - INFO - State start
2023-08-12 05:39:19,331 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:19,332 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-12 05:39:19,332 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38651/status
2023-08-12 05:39:19,443 - distributed.scheduler - INFO - Receive client connection: Client-945e72ac-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:19,455 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42166
2023-08-12 05:39:19,506 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38185'
2023-08-12 05:39:19,522 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37117'
2023-08-12 05:39:19,524 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41369'
2023-08-12 05:39:19,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35323'
2023-08-12 05:39:19,539 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46163'
2023-08-12 05:39:19,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38045'
2023-08-12 05:39:19,557 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41941'
2023-08-12 05:39:19,567 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33169'
2023-08-12 05:39:21,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:21,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:21,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:21,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:21,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:21,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:21,178 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:21,179 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:21,179 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:21,209 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:21,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:21,211 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:21,211 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:21,217 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:21,217 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:21,218 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:21,218 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:21,241 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:21,242 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:21,250 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:21,250 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:21,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:21,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:21,539 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:23,553 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45155
2023-08-12 05:39:23,554 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45155
2023-08-12 05:39:23,554 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38371
2023-08-12 05:39:23,554 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:23,554 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:23,554 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:23,554 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:23,554 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-a7rkku5d
2023-08-12 05:39:23,555 - distributed.worker - INFO - Starting Worker plugin PreImport-57875259-73d6-4603-96af-f6cf5f68c59e
2023-08-12 05:39:23,555 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27f33271-be8e-42e1-b6b2-10dfc39d7f34
2023-08-12 05:39:23,555 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3772d794-80b6-4364-b990-cbd4b743a5e6
2023-08-12 05:39:23,589 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:23,620 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45155', status: init, memory: 0, processing: 0>
2023-08-12 05:39:23,621 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45155
2023-08-12 05:39:23,621 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42276
2023-08-12 05:39:23,622 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:23,622 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:23,625 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:23,787 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41577
2023-08-12 05:39:23,788 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41577
2023-08-12 05:39:23,788 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34801
2023-08-12 05:39:23,788 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:23,788 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:23,788 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:23,788 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:23,789 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7q3_68hj
2023-08-12 05:39:23,789 - distributed.worker - INFO - Starting Worker plugin RMMSetup-47c12f84-b795-4072-bc39-fd30ba5cb2b5
2023-08-12 05:39:24,302 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3c8b3319-0a08-4975-933e-b66cb1010ee5
2023-08-12 05:39:24,302 - distributed.worker - INFO - Starting Worker plugin PreImport-77eab134-a243-436f-b6e6-bf7f396877d9
2023-08-12 05:39:24,302 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:24,742 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41577', status: init, memory: 0, processing: 0>
2023-08-12 05:39:24,742 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41577
2023-08-12 05:39:24,742 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42280
2023-08-12 05:39:24,743 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:24,743 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:24,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:25,103 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38527
2023-08-12 05:39:25,103 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38527
2023-08-12 05:39:25,103 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34943
2023-08-12 05:39:25,103 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,104 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,104 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:25,104 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:25,104 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xidw5kuy
2023-08-12 05:39:25,104 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d04c1d27-85cf-4db8-831a-247f47c278c9
2023-08-12 05:39:25,133 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46311
2023-08-12 05:39:25,134 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46311
2023-08-12 05:39:25,134 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43711
2023-08-12 05:39:25,134 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,134 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,134 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:25,134 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:25,135 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0estdnsh
2023-08-12 05:39:25,135 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ffbb6bf-3380-448e-96b7-e29cb19d6075
2023-08-12 05:39:25,157 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38303
2023-08-12 05:39:25,158 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38303
2023-08-12 05:39:25,158 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42527
2023-08-12 05:39:25,158 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,158 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,158 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:25,158 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:25,158 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wz9ibnwj
2023-08-12 05:39:25,159 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a23fc611-6b70-4715-9aa4-6844843046a8
2023-08-12 05:39:25,160 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44579
2023-08-12 05:39:25,161 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44579
2023-08-12 05:39:25,161 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39455
2023-08-12 05:39:25,161 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,161 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,161 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:25,162 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:25,162 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-objg7apq
2023-08-12 05:39:25,162 - distributed.worker - INFO - Starting Worker plugin PreImport-890a17a4-ba3e-4c71-b499-4fc371ca71e7
2023-08-12 05:39:25,162 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4070d23-6881-455d-bdd6-cfa1a9ff3d2c
2023-08-12 05:39:25,162 - distributed.worker - INFO - Starting Worker plugin RMMSetup-53c8c10d-7c30-437e-afd8-d692b06c9433
2023-08-12 05:39:25,165 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45761
2023-08-12 05:39:25,166 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45761
2023-08-12 05:39:25,166 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46279
2023-08-12 05:39:25,166 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,166 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,166 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:25,167 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:25,167 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4hiogje2
2023-08-12 05:39:25,167 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3677dd6-542d-420d-bf3b-974fd8ea5fec
2023-08-12 05:39:25,174 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44465
2023-08-12 05:39:25,174 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44465
2023-08-12 05:39:25,174 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34037
2023-08-12 05:39:25,174 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,175 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,175 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:25,175 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:25,175 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bpulvept
2023-08-12 05:39:25,175 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7b97282f-02aa-49e9-8dd2-206f58d7bde4
2023-08-12 05:39:25,236 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98b79523-f7d7-4b2d-952c-cf6a9041137f
2023-08-12 05:39:25,236 - distributed.worker - INFO - Starting Worker plugin PreImport-30504513-0a6d-410f-884c-26993166e3eb
2023-08-12 05:39:25,237 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,237 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a45e65c3-f6b5-4bf5-8ca0-9693cb8a55fa
2023-08-12 05:39:25,237 - distributed.worker - INFO - Starting Worker plugin PreImport-332866db-f4bc-4091-aa96-c0c700a1d7f0
2023-08-12 05:39:25,238 - distributed.worker - INFO - Starting Worker plugin PreImport-875ab7d5-3e3c-4f61-8d58-7b14dfdc0448
2023-08-12 05:39:25,238 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e10ddcff-83a5-4457-9796-4979cf6e9939
2023-08-12 05:39:25,238 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,238 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,263 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46311', status: init, memory: 0, processing: 0>
2023-08-12 05:39:25,263 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46311
2023-08-12 05:39:25,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48416
2023-08-12 05:39:25,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,264 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,267 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38527', status: init, memory: 0, processing: 0>
2023-08-12 05:39:25,268 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:25,268 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38527
2023-08-12 05:39:25,268 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48426
2023-08-12 05:39:25,268 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,268 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,270 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:25,271 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45761', status: init, memory: 0, processing: 0>
2023-08-12 05:39:25,271 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45761
2023-08-12 05:39:25,272 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48432
2023-08-12 05:39:25,272 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,272 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:25,279 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-50a14c0f-14f0-4ef4-bd74-41bbd9ef40eb
2023-08-12 05:39:25,281 - distributed.worker - INFO - Starting Worker plugin PreImport-e97425f0-d218-45e6-b937-fa75c16d439d
2023-08-12 05:39:25,281 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,281 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,287 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ca8aac5-6544-45b5-8604-7520f305c5f2
2023-08-12 05:39:25,294 - distributed.worker - INFO - Starting Worker plugin PreImport-d134d4f4-7bc0-46a8-96a0-889f4478785b
2023-08-12 05:39:25,294 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,313 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44579', status: init, memory: 0, processing: 0>
2023-08-12 05:39:25,313 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44579
2023-08-12 05:39:25,314 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48446
2023-08-12 05:39:25,314 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,314 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,316 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38303', status: init, memory: 0, processing: 0>
2023-08-12 05:39:25,317 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38303
2023-08-12 05:39:25,317 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:25,317 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48460
2023-08-12 05:39:25,317 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,318 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,320 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:25,328 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44465', status: init, memory: 0, processing: 0>
2023-08-12 05:39:25,329 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44465
2023-08-12 05:39:25,329 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48468
2023-08-12 05:39:25,330 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:25,330 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:25,333 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:25,372 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:25,372 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:25,373 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:25,373 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:25,373 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:25,373 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:25,373 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:25,373 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:25,378 - distributed.scheduler - INFO - Remove client Client-945e72ac-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:25,378 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42166; closing.
2023-08-12 05:39:25,378 - distributed.scheduler - INFO - Remove client Client-945e72ac-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:25,379 - distributed.scheduler - INFO - Close client connection: Client-945e72ac-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:25,380 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35323'. Reason: nanny-close
2023-08-12 05:39:25,380 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:25,381 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46163'. Reason: nanny-close
2023-08-12 05:39:25,381 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:25,382 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38303. Reason: nanny-close
2023-08-12 05:39:25,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38185'. Reason: nanny-close
2023-08-12 05:39:25,382 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:25,382 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37117'. Reason: nanny-close
2023-08-12 05:39:25,382 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44465. Reason: nanny-close
2023-08-12 05:39:25,383 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:25,383 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38527. Reason: nanny-close
2023-08-12 05:39:25,383 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41369'. Reason: nanny-close
2023-08-12 05:39:25,383 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:25,383 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41577. Reason: nanny-close
2023-08-12 05:39:25,384 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38045'. Reason: nanny-close
2023-08-12 05:39:25,384 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:25,384 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48460; closing.
2023-08-12 05:39:25,384 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:25,384 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41941'. Reason: nanny-close
2023-08-12 05:39:25,384 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45155. Reason: nanny-close
2023-08-12 05:39:25,384 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38303', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:25,384 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38303
2023-08-12 05:39:25,384 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:25,384 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:25,384 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:25,385 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44579. Reason: nanny-close
2023-08-12 05:39:25,385 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33169'. Reason: nanny-close
2023-08-12 05:39:25,385 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:25,385 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:25,385 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:25,385 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45761. Reason: nanny-close
2023-08-12 05:39:25,385 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:25,386 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:25,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48468; closing.
2023-08-12 05:39:25,386 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38303
2023-08-12 05:39:25,386 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38303
2023-08-12 05:39:25,386 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48426; closing.
2023-08-12 05:39:25,386 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:25,386 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46311. Reason: nanny-close
2023-08-12 05:39:25,386 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:25,386 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38303
2023-08-12 05:39:25,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44465', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:25,387 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44465
2023-08-12 05:39:25,387 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:25,387 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38527', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:25,387 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38303
2023-08-12 05:39:25,387 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38527
2023-08-12 05:39:25,387 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42280; closing.
2023-08-12 05:39:25,388 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:25,388 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:25,388 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:25,388 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41577', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:25,388 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41577
2023-08-12 05:39:25,388 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:25,388 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42276; closing.
2023-08-12 05:39:25,389 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:25,389 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:25,389 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45155', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:25,389 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45155
2023-08-12 05:39:25,390 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48446; closing.
2023-08-12 05:39:25,390 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48432; closing.
2023-08-12 05:39:25,390 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48416; closing.
2023-08-12 05:39:25,390 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44579', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:25,390 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44579
2023-08-12 05:39:25,391 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45761', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:25,391 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45761
2023-08-12 05:39:25,391 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46311', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:25,391 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46311
2023-08-12 05:39:25,392 - distributed.scheduler - INFO - Lost all workers
2023-08-12 05:39:26,947 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:39:26,947 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:39:26,948 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:39:26,949 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-12 05:39:26,949 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-08-12 05:39:28,749 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:28,754 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36803 instead
  warnings.warn(
2023-08-12 05:39:28,757 - distributed.scheduler - INFO - State start
2023-08-12 05:39:28,780 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:28,781 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-12 05:39:28,781 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36803/status
2023-08-12 05:39:29,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42895'
2023-08-12 05:39:29,081 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44803'
2023-08-12 05:39:29,096 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44039'
2023-08-12 05:39:29,097 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39037'
2023-08-12 05:39:29,105 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42805'
2023-08-12 05:39:29,116 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44191'
2023-08-12 05:39:29,125 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35957'
2023-08-12 05:39:29,134 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38591'
2023-08-12 05:39:30,727 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:30,727 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:30,727 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:30,727 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:30,742 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:30,742 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:30,743 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:30,743 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:30,754 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:30,755 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:30,759 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:30,759 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:30,762 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:30,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:30,764 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:30,765 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:30,770 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:30,770 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:30,790 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:30,790 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:30,790 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:30,794 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:30,799 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:30,824 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:31,269 - distributed.scheduler - INFO - Receive client connection: Client-99f8a0c0-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:31,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48616
2023-08-12 05:39:33,401 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43899
2023-08-12 05:39:33,401 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43899
2023-08-12 05:39:33,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35433
2023-08-12 05:39:33,402 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,402 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,402 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:33,402 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:33,402 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bt9gpaah
2023-08-12 05:39:33,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-900389d7-49ce-4911-b5cd-ce4d6b20f4ab
2023-08-12 05:39:33,430 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35005
2023-08-12 05:39:33,431 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35005
2023-08-12 05:39:33,431 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44253
2023-08-12 05:39:33,431 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,431 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,431 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:33,432 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:33,432 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y92nffqn
2023-08-12 05:39:33,432 - distributed.worker - INFO - Starting Worker plugin RMMSetup-749dd180-1717-43b6-8606-e7736f6bc28c
2023-08-12 05:39:33,432 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36773
2023-08-12 05:39:33,432 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36773
2023-08-12 05:39:33,432 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38579
2023-08-12 05:39:33,432 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41639
2023-08-12 05:39:33,433 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38579
2023-08-12 05:39:33,433 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,432 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38559
2023-08-12 05:39:33,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33309
2023-08-12 05:39:33,433 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,433 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38559
2023-08-12 05:39:33,433 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,433 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,433 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39521
2023-08-12 05:39:33,433 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,433 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:33,433 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,433 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:33,433 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:33,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hphk_aer
2023-08-12 05:39:33,433 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:33,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9ymp6htr
2023-08-12 05:39:33,433 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:33,433 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:33,433 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7v6ker9y
2023-08-12 05:39:33,433 - distributed.worker - INFO - Starting Worker plugin PreImport-f1feeeb2-43a6-4155-9b47-0a464c51a086
2023-08-12 05:39:33,433 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d0e8475-daf6-4a11-8c7f-fe2e92b0f44a
2023-08-12 05:39:33,433 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9452920b-ad4a-4fba-9770-e7d224376d48
2023-08-12 05:39:33,434 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19423ee4-3592-4fdd-ac6d-c9681fa6e6a0
2023-08-12 05:39:33,434 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c505c54e-993e-460d-8aa1-5152f1572acc
2023-08-12 05:39:33,435 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42125
2023-08-12 05:39:33,435 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42125
2023-08-12 05:39:33,436 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43329
2023-08-12 05:39:33,436 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,435 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38621
2023-08-12 05:39:33,436 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,436 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38621
2023-08-12 05:39:33,436 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42439
2023-08-12 05:39:33,436 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:33,436 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,436 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:33,436 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,436 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3uyoo_pu
2023-08-12 05:39:33,436 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:33,436 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:33,436 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-torwybjb
2023-08-12 05:39:33,436 - distributed.worker - INFO - Starting Worker plugin PreImport-9122f066-0f02-4f6a-be48-f9064c4431df
2023-08-12 05:39:33,436 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c20ba2f3-7835-490d-b92d-324e9d53c38b
2023-08-12 05:39:33,437 - distributed.worker - INFO - Starting Worker plugin RMMSetup-465ae7d3-10d9-46bb-9a57-c5b38c604c14
2023-08-12 05:39:33,437 - distributed.worker - INFO - Starting Worker plugin PreImport-7963e346-f515-4aae-8dc1-954344747fe5
2023-08-12 05:39:33,437 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0342dd8-3b9a-4263-b9ac-423b9c6ca841
2023-08-12 05:39:33,437 - distributed.worker - INFO - Starting Worker plugin RMMSetup-154c66f2-2ee1-4c72-9353-940eaca6e656
2023-08-12 05:39:33,438 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36281
2023-08-12 05:39:33,439 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36281
2023-08-12 05:39:33,439 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40025
2023-08-12 05:39:33,439 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,439 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,439 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:33,440 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:33,440 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xq8pc62z
2023-08-12 05:39:33,440 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f126d391-e001-4e9e-9431-0ff5b3df27f4
2023-08-12 05:39:33,575 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-26608f63-d4b3-486a-9f4b-0c7c58b0e07e
2023-08-12 05:39:33,575 - distributed.worker - INFO - Starting Worker plugin PreImport-e3123607-4ecc-497a-ab83-7c4714c3fe1e
2023-08-12 05:39:33,576 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,611 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43899', status: init, memory: 0, processing: 0>
2023-08-12 05:39:33,612 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43899
2023-08-12 05:39:33,612 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48642
2023-08-12 05:39:33,613 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,613 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:33,646 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40c75dcd-304a-4b2a-bbf0-bc2709fe431c
2023-08-12 05:39:33,647 - distributed.worker - INFO - Starting Worker plugin PreImport-90d52743-26d1-485f-a5a0-1d3f77defd95
2023-08-12 05:39:33,647 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,656 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,671 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-38088ee2-7e2f-4969-98b9-9540bb943057
2023-08-12 05:39:33,671 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d2c64f1e-784b-4a34-90b1-22a579cb3ee8
2023-08-12 05:39:33,671 - distributed.worker - INFO - Starting Worker plugin PreImport-d57b3814-dd2a-4800-9ffe-96bae50316dd
2023-08-12 05:39:33,671 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,671 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,671 - distributed.worker - INFO - Starting Worker plugin PreImport-5e590d9b-f17c-402e-b152-aafd815d5141
2023-08-12 05:39:33,672 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,672 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,672 - distributed.worker - INFO - Starting Worker plugin PreImport-386c0f31-0b23-49c8-8e34-6f4283dbe912
2023-08-12 05:39:33,672 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d4ff7c4-9f94-400d-a06d-23d86f2aa398
2023-08-12 05:39:33,673 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,680 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42125', status: init, memory: 0, processing: 0>
2023-08-12 05:39:33,680 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42125
2023-08-12 05:39:33,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48658
2023-08-12 05:39:33,681 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,681 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,683 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:33,683 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35005', status: init, memory: 0, processing: 0>
2023-08-12 05:39:33,684 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35005
2023-08-12 05:39:33,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48644
2023-08-12 05:39:33,685 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,685 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,688 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:33,696 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36773', status: init, memory: 0, processing: 0>
2023-08-12 05:39:33,697 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36773
2023-08-12 05:39:33,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48670
2023-08-12 05:39:33,698 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,698 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,700 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:33,698 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38621', status: init, memory: 0, processing: 0>
2023-08-12 05:39:33,702 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38621
2023-08-12 05:39:33,702 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48682
2023-08-12 05:39:33,702 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,702 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:33,705 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38579', status: init, memory: 0, processing: 0>
2023-08-12 05:39:33,705 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38579
2023-08-12 05:39:33,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48664
2023-08-12 05:39:33,706 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,706 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,708 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:33,714 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38559', status: init, memory: 0, processing: 0>
2023-08-12 05:39:33,714 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38559
2023-08-12 05:39:33,714 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48686
2023-08-12 05:39:33,715 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,715 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,718 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:33,723 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36281', status: init, memory: 0, processing: 0>
2023-08-12 05:39:33,724 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36281
2023-08-12 05:39:33,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48692
2023-08-12 05:39:33,724 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:33,725 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:33,728 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:33,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:33,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:33,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:33,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:33,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:33,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:33,834 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:33,834 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:33,844 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-12 05:39:33,844 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-12 05:39:33,844 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-12 05:39:33,845 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-12 05:39:33,845 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-12 05:39:33,845 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-12 05:39:33,845 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-12 05:39:33,845 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-12 05:39:33,851 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:39:33,853 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:39:33,855 - distributed.scheduler - INFO - Remove client Client-99f8a0c0-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:33,855 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48616; closing.
2023-08-12 05:39:33,855 - distributed.scheduler - INFO - Remove client Client-99f8a0c0-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:33,856 - distributed.scheduler - INFO - Close client connection: Client-99f8a0c0-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:33,857 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39037'. Reason: nanny-close
2023-08-12 05:39:33,857 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:33,858 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42895'. Reason: nanny-close
2023-08-12 05:39:33,858 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:33,859 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44803'. Reason: nanny-close
2023-08-12 05:39:33,859 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36281. Reason: nanny-close
2023-08-12 05:39:33,859 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:33,859 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44039'. Reason: nanny-close
2023-08-12 05:39:33,860 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:33,859 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35005. Reason: nanny-close
2023-08-12 05:39:33,860 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38621. Reason: nanny-close
2023-08-12 05:39:33,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42805'. Reason: nanny-close
2023-08-12 05:39:33,860 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:33,860 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36773. Reason: nanny-close
2023-08-12 05:39:33,861 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44191'. Reason: nanny-close
2023-08-12 05:39:33,861 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:33,861 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35957'. Reason: nanny-close
2023-08-12 05:39:33,861 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48692; closing.
2023-08-12 05:39:33,861 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:33,861 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43899. Reason: nanny-close
2023-08-12 05:39:33,861 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:33,862 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:33,862 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36281', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:33,862 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36281
2023-08-12 05:39:33,862 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38591'. Reason: nanny-close
2023-08-12 05:39:33,862 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38559. Reason: nanny-close
2023-08-12 05:39:33,862 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:33,862 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:33,862 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38579. Reason: nanny-close
2023-08-12 05:39:33,863 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:33,863 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48682; closing.
2023-08-12 05:39:33,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:33,863 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:33,863 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:33,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:33,863 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42125. Reason: nanny-close
2023-08-12 05:39:33,864 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36281
2023-08-12 05:39:33,864 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38621', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:33,864 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38621
2023-08-12 05:39:33,864 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:33,864 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36281
2023-08-12 05:39:33,864 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48670; closing.
2023-08-12 05:39:33,864 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:33,865 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48644; closing.
2023-08-12 05:39:33,865 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:33,865 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:33,865 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:33,865 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36773', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:33,865 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36773
2023-08-12 05:39:33,865 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36281
2023-08-12 05:39:33,866 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35005', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:33,866 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35005
2023-08-12 05:39:33,866 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:33,866 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:33,867 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48642; closing.
2023-08-12 05:39:33,867 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48664; closing.
2023-08-12 05:39:33,867 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48686; closing.
2023-08-12 05:39:33,868 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:33,868 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43899', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:33,868 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43899
2023-08-12 05:39:33,868 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38579', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:33,868 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38579
2023-08-12 05:39:33,869 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38559', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:33,869 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38559
2023-08-12 05:39:33,869 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48658; closing.
2023-08-12 05:39:33,870 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42125', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:33,870 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42125
2023-08-12 05:39:33,870 - distributed.scheduler - INFO - Lost all workers
2023-08-12 05:39:36,826 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:39:36,827 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:39:36,827 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:39:36,828 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-12 05:39:36,829 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-08-12 05:39:38,869 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:38,873 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46619 instead
  warnings.warn(
2023-08-12 05:39:38,877 - distributed.scheduler - INFO - State start
2023-08-12 05:39:39,007 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:39,009 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-12 05:39:39,010 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46619/status
2023-08-12 05:39:39,224 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45351'
2023-08-12 05:39:39,245 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36913'
2023-08-12 05:39:39,248 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37015'
2023-08-12 05:39:39,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33043'
2023-08-12 05:39:39,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46843'
2023-08-12 05:39:39,274 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42715'
2023-08-12 05:39:39,283 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45575'
2023-08-12 05:39:39,291 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39229'
2023-08-12 05:39:39,934 - distributed.scheduler - INFO - Receive client connection: Client-9fe1fddf-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:39,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36354
2023-08-12 05:39:40,976 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:40,976 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:40,976 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:40,976 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:40,981 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:40,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:40,993 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:40,993 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:41,004 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:41,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:41,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:41,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:41,006 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:41,007 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:41,009 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:41,021 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:41,021 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:41,046 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:41,062 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:41,062 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:41,065 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:41,123 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:41,123 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:39:41,229 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:43,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45465
2023-08-12 05:39:43,638 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45465
2023-08-12 05:39:43,638 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40037
2023-08-12 05:39:43,638 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,638 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,638 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:43,638 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:43,638 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n2luagr8
2023-08-12 05:39:43,638 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45923
2023-08-12 05:39:43,639 - distributed.worker - INFO - Starting Worker plugin PreImport-8a4f75c7-3cac-4778-8637-af308b8d86af
2023-08-12 05:39:43,639 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45923
2023-08-12 05:39:43,639 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-940f6da5-db85-4a5b-8e2a-efc5d127be3e
2023-08-12 05:39:43,639 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46503
2023-08-12 05:39:43,639 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,639 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0527acd6-d6e1-4f27-b83b-47407df721bd
2023-08-12 05:39:43,639 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,639 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:43,639 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:43,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7mow7zgy
2023-08-12 05:39:43,640 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25be0ecb-7ace-4e13-860f-12a112214e22
2023-08-12 05:39:43,733 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44625
2023-08-12 05:39:43,735 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44625
2023-08-12 05:39:43,735 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34701
2023-08-12 05:39:43,735 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,735 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,735 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:43,735 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:43,735 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h76j_fjy
2023-08-12 05:39:43,736 - distributed.worker - INFO - Starting Worker plugin RMMSetup-16eaf963-ca1a-4a1a-a02e-cad71d05497d
2023-08-12 05:39:43,736 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44903
2023-08-12 05:39:43,737 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44903
2023-08-12 05:39:43,737 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44699
2023-08-12 05:39:43,737 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,737 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,737 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:43,737 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:43,737 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-c4q0dj68
2023-08-12 05:39:43,738 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe236472-0857-468c-b25d-5cc93b410184
2023-08-12 05:39:43,737 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44349
2023-08-12 05:39:43,738 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44349
2023-08-12 05:39:43,738 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41279
2023-08-12 05:39:43,738 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,738 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,738 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:43,738 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:43,738 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5c23nwes
2023-08-12 05:39:43,739 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5cc706c1-7022-41bb-bdf6-ab953e3b0ffc
2023-08-12 05:39:43,740 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44161
2023-08-12 05:39:43,741 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44161
2023-08-12 05:39:43,741 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42847
2023-08-12 05:39:43,741 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,741 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,741 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:43,741 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:43,741 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-22xpd3mt
2023-08-12 05:39:43,742 - distributed.worker - INFO - Starting Worker plugin PreImport-9fc61e44-7f43-4be7-8cdb-7fece4bf0be6
2023-08-12 05:39:43,742 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5045cf5d-196e-47d4-9fff-770959f035b3
2023-08-12 05:39:43,742 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ec128504-3070-4094-b7ae-cf50365d3697
2023-08-12 05:39:43,781 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,782 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0c23742-9593-41ea-8be0-13122afc61d1
2023-08-12 05:39:43,782 - distributed.worker - INFO - Starting Worker plugin PreImport-431c6424-5bea-406e-81f5-8870099bbb5b
2023-08-12 05:39:43,782 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,791 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33833
2023-08-12 05:39:43,792 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33833
2023-08-12 05:39:43,792 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43839
2023-08-12 05:39:43,792 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,792 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,792 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:43,792 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:43,792 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0h_p1wbt
2023-08-12 05:39:43,792 - distributed.worker - INFO - Starting Worker plugin PreImport-c4851506-fd95-48a0-b9f3-0984a2789ac3
2023-08-12 05:39:43,793 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86f052e0-1c25-4be3-91a8-219c423153e8
2023-08-12 05:39:43,793 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0edffa1-66e6-4045-8191-e6f32d31cf36
2023-08-12 05:39:43,793 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34161
2023-08-12 05:39:43,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34161
2023-08-12 05:39:43,793 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44093
2023-08-12 05:39:43,794 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,794 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,794 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:43,794 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:39:43,794 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8x_wngmp
2023-08-12 05:39:43,794 - distributed.worker - INFO - Starting Worker plugin RMMSetup-78fb9201-674c-4f84-a59a-d764f91024ba
2023-08-12 05:39:43,812 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45465', status: init, memory: 0, processing: 0>
2023-08-12 05:39:43,814 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45465
2023-08-12 05:39:43,814 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36370
2023-08-12 05:39:43,815 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,815 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:43,829 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45923', status: init, memory: 0, processing: 0>
2023-08-12 05:39:43,829 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45923
2023-08-12 05:39:43,829 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36378
2023-08-12 05:39:43,830 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,830 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:43,881 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34113c2f-b893-4193-adce-b16ae467a2fa
2023-08-12 05:39:43,881 - distributed.worker - INFO - Starting Worker plugin PreImport-a6d13e05-9b97-420a-9490-0b4bbb94082f
2023-08-12 05:39:43,881 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,887 - distributed.worker - INFO - Starting Worker plugin PreImport-8719071d-82a4-4e15-a7c0-83a8604cd1e5
2023-08-12 05:39:43,887 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b849b5d-7904-4fa3-b48c-f69f58f3c260
2023-08-12 05:39:43,888 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,896 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d29daf4e-a435-4025-83ea-a888962d06b8
2023-08-12 05:39:43,898 - distributed.worker - INFO - Starting Worker plugin PreImport-aa513a9c-7380-4e11-aa0a-743d469ab491
2023-08-12 05:39:43,899 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,903 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,915 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44625', status: init, memory: 0, processing: 0>
2023-08-12 05:39:43,915 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44625
2023-08-12 05:39:43,916 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36388
2023-08-12 05:39:43,916 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,916 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,919 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:43,919 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b0707bf-f7e0-4568-b0ac-8ed6b0b1b9c6
2023-08-12 05:39:43,919 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,920 - distributed.worker - INFO - Starting Worker plugin PreImport-2a9dbc62-d4e2-4408-9b2f-1a2982bde6c2
2023-08-12 05:39:43,920 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,924 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44903', status: init, memory: 0, processing: 0>
2023-08-12 05:39:43,925 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44903
2023-08-12 05:39:43,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36392
2023-08-12 05:39:43,926 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,926 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,929 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:43,933 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44161', status: init, memory: 0, processing: 0>
2023-08-12 05:39:43,933 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44161
2023-08-12 05:39:43,933 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36412
2023-08-12 05:39:43,934 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,934 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,934 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44349', status: init, memory: 0, processing: 0>
2023-08-12 05:39:43,934 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44349
2023-08-12 05:39:43,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36400
2023-08-12 05:39:43,935 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,935 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,936 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:43,937 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:43,945 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33833', status: init, memory: 0, processing: 0>
2023-08-12 05:39:43,945 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33833
2023-08-12 05:39:43,945 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36424
2023-08-12 05:39:43,946 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,946 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,946 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34161', status: init, memory: 0, processing: 0>
2023-08-12 05:39:43,947 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34161
2023-08-12 05:39:43,947 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36436
2023-08-12 05:39:43,947 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:43,947 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:43,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:43,949 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:44,051 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:44,051 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:44,052 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:44,052 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:44,052 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:44,052 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:44,053 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:44,053 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:39:44,056 - distributed.scheduler - INFO - Remove client Client-9fe1fddf-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:44,057 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36354; closing.
2023-08-12 05:39:44,057 - distributed.scheduler - INFO - Remove client Client-9fe1fddf-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:44,057 - distributed.scheduler - INFO - Close client connection: Client-9fe1fddf-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:44,058 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33043'. Reason: nanny-close
2023-08-12 05:39:44,059 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:44,060 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46843'. Reason: nanny-close
2023-08-12 05:39:44,061 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:44,061 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44903. Reason: nanny-close
2023-08-12 05:39:44,061 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45351'. Reason: nanny-close
2023-08-12 05:39:44,062 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:44,062 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44349. Reason: nanny-close
2023-08-12 05:39:44,062 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36913'. Reason: nanny-close
2023-08-12 05:39:44,063 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:44,063 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33833. Reason: nanny-close
2023-08-12 05:39:44,063 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37015'. Reason: nanny-close
2023-08-12 05:39:44,064 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:44,064 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36392; closing.
2023-08-12 05:39:44,064 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45465. Reason: nanny-close
2023-08-12 05:39:44,064 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:44,064 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44903', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:44,064 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42715'. Reason: nanny-close
2023-08-12 05:39:44,064 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44903
2023-08-12 05:39:44,064 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:44,065 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:44,065 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:44,065 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45923. Reason: nanny-close
2023-08-12 05:39:44,065 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45575'. Reason: nanny-close
2023-08-12 05:39:44,065 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:44,065 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:44,065 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44903
2023-08-12 05:39:44,065 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44625. Reason: nanny-close
2023-08-12 05:39:44,065 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44903
2023-08-12 05:39:44,066 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:44,066 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39229'. Reason: nanny-close
2023-08-12 05:39:44,066 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36424; closing.
2023-08-12 05:39:44,066 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:44,066 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36400; closing.
2023-08-12 05:39:44,066 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:44,066 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34161. Reason: nanny-close
2023-08-12 05:39:44,066 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:44,067 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33833', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:44,067 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:44,067 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33833
2023-08-12 05:39:44,067 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44903
2023-08-12 05:39:44,067 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44349', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:44,067 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44349
2023-08-12 05:39:44,067 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44161. Reason: nanny-close
2023-08-12 05:39:44,068 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:44,068 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44903
2023-08-12 05:39:44,068 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36370; closing.
2023-08-12 05:39:44,068 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45465', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:44,068 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:44,068 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:44,068 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45465
2023-08-12 05:39:44,069 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36378; closing.
2023-08-12 05:39:44,069 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36388; closing.
2023-08-12 05:39:44,069 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:44,069 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45923', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:44,069 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45923
2023-08-12 05:39:44,069 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:44,070 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:44,070 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44625', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:44,070 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44625
2023-08-12 05:39:44,070 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:44,070 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36436; closing.
2023-08-12 05:39:44,070 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34161', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:44,071 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34161
2023-08-12 05:39:44,071 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36412; closing.
2023-08-12 05:39:44,072 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44161', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:44,072 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44161
2023-08-12 05:39:44,072 - distributed.scheduler - INFO - Lost all workers
2023-08-12 05:39:44,072 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:44,072 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36412>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-08-12 05:39:45,525 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:39:45,526 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:39:45,526 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:39:45,527 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-12 05:39:45,528 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-08-12 05:39:47,368 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:47,373 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40957 instead
  warnings.warn(
2023-08-12 05:39:47,377 - distributed.scheduler - INFO - State start
2023-08-12 05:39:47,398 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:47,399 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-12 05:39:47,399 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40957/status
2023-08-12 05:39:47,471 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46019'
2023-08-12 05:39:47,857 - distributed.scheduler - INFO - Receive client connection: Client-a50f4870-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:47,869 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51310
2023-08-12 05:39:48,852 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:48,853 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-08-12 05:39:49,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:50,568 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40133
2023-08-12 05:39:50,568 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40133
2023-08-12 05:39:50,568 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-08-12 05:39:50,568 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:50,568 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:50,568 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:50,569 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-12 05:39:50,569 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7tmpd2xx
2023-08-12 05:39:50,569 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8890087e-7837-4bfe-b28d-dd26ddcfe95a
2023-08-12 05:39:50,569 - distributed.worker - INFO - Starting Worker plugin PreImport-4218cbdc-696e-4a9d-9ac8-3305abf8a424
2023-08-12 05:39:50,569 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8bec26d5-95d4-488f-9a76-0941d2e0fd6a
2023-08-12 05:39:50,570 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:50,606 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40133', status: init, memory: 0, processing: 0>
2023-08-12 05:39:50,608 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40133
2023-08-12 05:39:50,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51322
2023-08-12 05:39:50,608 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:50,609 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:50,612 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:50,677 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:39:50,679 - distributed.scheduler - INFO - Remove client Client-a50f4870-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:50,680 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51310; closing.
2023-08-12 05:39:50,680 - distributed.scheduler - INFO - Remove client Client-a50f4870-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:50,680 - distributed.scheduler - INFO - Close client connection: Client-a50f4870-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:50,681 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46019'. Reason: nanny-close
2023-08-12 05:39:50,682 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:50,683 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40133. Reason: nanny-close
2023-08-12 05:39:50,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51322; closing.
2023-08-12 05:39:50,685 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:50,685 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40133', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:50,686 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40133
2023-08-12 05:39:50,686 - distributed.scheduler - INFO - Lost all workers
2023-08-12 05:39:50,686 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:51,948 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:39:51,948 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:39:51,949 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:39:51,950 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-12 05:39:51,950 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-08-12 05:39:55,450 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:55,454 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38483 instead
  warnings.warn(
2023-08-12 05:39:55,457 - distributed.scheduler - INFO - State start
2023-08-12 05:39:55,476 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:39:55,477 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-12 05:39:55,478 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38483/status
2023-08-12 05:39:55,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39037'
2023-08-12 05:39:55,694 - distributed.scheduler - INFO - Receive client connection: Client-a9d6cda3-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:55,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39064
2023-08-12 05:39:56,880 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:39:56,880 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-08-12 05:39:57,347 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:39:58,097 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44753
2023-08-12 05:39:58,098 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44753
2023-08-12 05:39:58,098 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39059
2023-08-12 05:39:58,098 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:39:58,098 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:58,098 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:39:58,098 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-12 05:39:58,098 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vz4bmbrj
2023-08-12 05:39:58,098 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0560b74f-f64e-4e24-b798-8c985ff4b6ff
2023-08-12 05:39:58,099 - distributed.worker - INFO - Starting Worker plugin PreImport-51d43bfe-b624-4813-8e58-5b501d5fbbf4
2023-08-12 05:39:58,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5d12a18c-56b9-438b-be36-5062b5d1d1e5
2023-08-12 05:39:58,100 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:58,127 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44753', status: init, memory: 0, processing: 0>
2023-08-12 05:39:58,128 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44753
2023-08-12 05:39:58,128 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39086
2023-08-12 05:39:58,128 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:39:58,129 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:39:58,131 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:39:58,193 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:39:58,195 - distributed.scheduler - INFO - Remove client Client-a9d6cda3-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:58,195 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39064; closing.
2023-08-12 05:39:58,196 - distributed.scheduler - INFO - Remove client Client-a9d6cda3-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:58,196 - distributed.scheduler - INFO - Close client connection: Client-a9d6cda3-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:39:58,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39037'. Reason: nanny-close
2023-08-12 05:39:58,198 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:39:58,199 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44753. Reason: nanny-close
2023-08-12 05:39:58,201 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39086; closing.
2023-08-12 05:39:58,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:39:58,201 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44753', status: closing, memory: 0, processing: 0>
2023-08-12 05:39:58,201 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44753
2023-08-12 05:39:58,202 - distributed.scheduler - INFO - Lost all workers
2023-08-12 05:39:58,202 - distributed.nanny - INFO - Worker closed
2023-08-12 05:39:59,163 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:39:59,163 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:39:59,164 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:39:59,165 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-12 05:39:59,165 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-08-12 05:40:00,981 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:40:00,986 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35681 instead
  warnings.warn(
2023-08-12 05:40:00,989 - distributed.scheduler - INFO - State start
2023-08-12 05:40:01,008 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:40:01,009 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-12 05:40:01,009 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35681/status
2023-08-12 05:40:04,408 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:40:04,408 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:40:04,408 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:40:04,409 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-12 05:40:04,409 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-08-12 05:40:06,168 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:40:06,172 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38709 instead
  warnings.warn(
2023-08-12 05:40:06,175 - distributed.scheduler - INFO - State start
2023-08-12 05:40:06,194 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:40:06,195 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-08-12 05:40:06,195 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38709/status
2023-08-12 05:40:06,203 - distributed.scheduler - INFO - Receive client connection: Client-b0498f00-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:06,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48894
2023-08-12 05:40:06,285 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46105'
2023-08-12 05:40:07,612 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:07,612 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:07,618 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:08,399 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45785
2023-08-12 05:40:08,399 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45785
2023-08-12 05:40:08,399 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39363
2023-08-12 05:40:08,399 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-08-12 05:40:08,399 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:08,399 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:08,400 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-12 05:40:08,400 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pebiu72o
2023-08-12 05:40:08,400 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0cbfc767-5570-493e-b45f-e20bc6889e3c
2023-08-12 05:40:08,400 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ad13abe9-a5b2-4143-8fee-8b569f828ab0
2023-08-12 05:40:08,400 - distributed.worker - INFO - Starting Worker plugin PreImport-a8b2ed75-a980-4b57-9ce4-5d127384a245
2023-08-12 05:40:08,401 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:08,425 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45785', status: init, memory: 0, processing: 0>
2023-08-12 05:40:08,426 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45785
2023-08-12 05:40:08,426 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48916
2023-08-12 05:40:08,427 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-08-12 05:40:08,427 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:08,429 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-08-12 05:40:08,513 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:08,515 - distributed.scheduler - INFO - Remove client Client-b0498f00-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:08,515 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48894; closing.
2023-08-12 05:40:08,515 - distributed.scheduler - INFO - Remove client Client-b0498f00-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:08,516 - distributed.scheduler - INFO - Close client connection: Client-b0498f00-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:08,517 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46105'. Reason: nanny-close
2023-08-12 05:40:08,517 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:40:08,519 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45785. Reason: nanny-close
2023-08-12 05:40:08,520 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-08-12 05:40:08,520 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48916; closing.
2023-08-12 05:40:08,521 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45785', status: closing, memory: 0, processing: 0>
2023-08-12 05:40:08,521 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45785
2023-08-12 05:40:08,521 - distributed.scheduler - INFO - Lost all workers
2023-08-12 05:40:08,521 - distributed.nanny - INFO - Worker closed
2023-08-12 05:40:09,383 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:40:09,383 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:40:09,383 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:40:09,384 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-08-12 05:40:09,385 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-08-12 05:40:11,119 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:40:11,123 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42257 instead
  warnings.warn(
2023-08-12 05:40:11,127 - distributed.scheduler - INFO - State start
2023-08-12 05:40:11,147 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:40:11,148 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-12 05:40:11,148 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42257/status
2023-08-12 05:40:11,199 - distributed.scheduler - INFO - Receive client connection: Client-b3436021-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:11,210 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41358
2023-08-12 05:40:11,405 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46497'
2023-08-12 05:40:11,419 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38795'
2023-08-12 05:40:11,431 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35633'
2023-08-12 05:40:11,433 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45501'
2023-08-12 05:40:11,441 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45543'
2023-08-12 05:40:11,450 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37377'
2023-08-12 05:40:11,458 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44321'
2023-08-12 05:40:11,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34903'
2023-08-12 05:40:13,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:13,080 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:13,080 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:13,080 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:13,081 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:13,081 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:13,082 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:13,082 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:13,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:13,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:13,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:13,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:13,108 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:13,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:13,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:13,110 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:13,123 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:13,124 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:13,132 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:13,132 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:13,136 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:13,138 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:13,156 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:13,166 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:15,421 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46623
2023-08-12 05:40:15,422 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46623
2023-08-12 05:40:15,422 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39739
2023-08-12 05:40:15,422 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,422 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,422 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:15,422 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:40:15,422 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ssgrr1bg
2023-08-12 05:40:15,422 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33601
2023-08-12 05:40:15,423 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33601
2023-08-12 05:40:15,423 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36345
2023-08-12 05:40:15,423 - distributed.worker - INFO - Starting Worker plugin PreImport-3ccf099e-e07f-4fcb-8b80-22b1075c234e
2023-08-12 05:40:15,423 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,423 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,423 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f33964f1-0a3e-4167-94cc-89ef3f9b9c23
2023-08-12 05:40:15,423 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59d5b44b-f202-493b-a8af-3573b404ab1a
2023-08-12 05:40:15,423 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:15,423 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:40:15,423 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rcskliyo
2023-08-12 05:40:15,424 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0bdc597-0740-4f19-b16e-bc4b8815e0ce
2023-08-12 05:40:15,550 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45011
2023-08-12 05:40:15,551 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45011
2023-08-12 05:40:15,551 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36959
2023-08-12 05:40:15,551 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,551 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,551 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:15,552 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:40:15,552 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tuf2n6to
2023-08-12 05:40:15,551 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37711
2023-08-12 05:40:15,552 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a19848c2-b60f-47e7-8008-9b6f0d148e3f
2023-08-12 05:40:15,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37711
2023-08-12 05:40:15,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42471
2023-08-12 05:40:15,552 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,552 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,552 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:15,553 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:40:15,553 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z1_ncrtb
2023-08-12 05:40:15,553 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5266a1c-f137-46a9-8c55-9cb3b515034a
2023-08-12 05:40:15,557 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40165
2023-08-12 05:40:15,558 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40165
2023-08-12 05:40:15,558 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34651
2023-08-12 05:40:15,558 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39771
2023-08-12 05:40:15,558 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34651
2023-08-12 05:40:15,558 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,558 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,558 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42407
2023-08-12 05:40:15,558 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,558 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,558 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:15,559 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:40:15,558 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:15,559 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-e306dvrg
2023-08-12 05:40:15,559 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:40:15,559 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ya7568x0
2023-08-12 05:40:15,559 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3a026e22-92ba-47ff-8ed9-c9b46dbc5f30
2023-08-12 05:40:15,559 - distributed.worker - INFO - Starting Worker plugin PreImport-52d8a8d1-1198-465d-834e-113661ce0c67
2023-08-12 05:40:15,559 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-015c97ea-9dfa-4c9b-b7be-e76bcea9450d
2023-08-12 05:40:15,559 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e971ad17-e1e5-4a47-aa45-9b7d8bb75311
2023-08-12 05:40:15,561 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33385
2023-08-12 05:40:15,562 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33385
2023-08-12 05:40:15,562 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34453
2023-08-12 05:40:15,562 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,562 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,562 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:15,562 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:40:15,562 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qbtg9h0p
2023-08-12 05:40:15,562 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f92c2022-0081-4c1c-99c1-6fd53431842b
2023-08-12 05:40:15,563 - distributed.worker - INFO - Starting Worker plugin PreImport-0bb799bc-8a70-49f0-bd41-eb96917d7983
2023-08-12 05:40:15,563 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ab5ad53-c4cb-4f19-bc8d-4ddf7c241d05
2023-08-12 05:40:15,563 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,567 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,570 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40909
2023-08-12 05:40:15,571 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40909
2023-08-12 05:40:15,571 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46511
2023-08-12 05:40:15,571 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,571 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,571 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:15,571 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-08-12 05:40:15,571 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-qzpbby23
2023-08-12 05:40:15,572 - distributed.worker - INFO - Starting Worker plugin PreImport-5f21e129-fe6f-41c0-9401-3bca89b2d977
2023-08-12 05:40:15,572 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91312ab1-6c94-4f42-9282-20f5438399d6
2023-08-12 05:40:15,572 - distributed.worker - INFO - Starting Worker plugin RMMSetup-284ecd2a-c1f3-44b3-8c4d-025909f07355
2023-08-12 05:40:15,590 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46623', status: init, memory: 0, processing: 0>
2023-08-12 05:40:15,591 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46623
2023-08-12 05:40:15,591 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48462
2023-08-12 05:40:15,592 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,592 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,592 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33601', status: init, memory: 0, processing: 0>
2023-08-12 05:40:15,593 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33601
2023-08-12 05:40:15,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48456
2023-08-12 05:40:15,593 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,593 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,594 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:40:15,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:40:15,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae195c37-a106-4479-927a-2a9aa418f817
2023-08-12 05:40:15,708 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f328bc4-0c97-42d0-9339-b01f456d9340
2023-08-12 05:40:15,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a0eaf71d-b2f0-42e3-99e3-b1ae134880f0
2023-08-12 05:40:15,708 - distributed.worker - INFO - Starting Worker plugin PreImport-9ced68e9-536e-4b4b-b2e2-e498dd1b1a61
2023-08-12 05:40:15,708 - distributed.worker - INFO - Starting Worker plugin PreImport-117c4a5b-f96d-4d51-bd64-23d570aac9d3
2023-08-12 05:40:15,708 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,708 - distributed.worker - INFO - Starting Worker plugin PreImport-47df2908-b239-4276-be22-f9310b1b2e8f
2023-08-12 05:40:15,708 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,708 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3ca4223-189e-4330-86a2-a960edc7d4c8
2023-08-12 05:40:15,708 - distributed.worker - INFO - Starting Worker plugin PreImport-9a907282-f0b6-4170-bd5c-219475009f28
2023-08-12 05:40:15,709 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,709 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,731 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34651', status: init, memory: 0, processing: 0>
2023-08-12 05:40:15,732 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34651
2023-08-12 05:40:15,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48472
2023-08-12 05:40:15,733 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,733 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,733 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40909', status: init, memory: 0, processing: 0>
2023-08-12 05:40:15,734 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40909
2023-08-12 05:40:15,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48488
2023-08-12 05:40:15,734 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,735 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,735 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37711', status: init, memory: 0, processing: 0>
2023-08-12 05:40:15,735 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:40:15,735 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37711
2023-08-12 05:40:15,735 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48496
2023-08-12 05:40:15,736 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,736 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,737 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:40:15,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:40:15,743 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45011', status: init, memory: 0, processing: 0>
2023-08-12 05:40:15,744 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45011
2023-08-12 05:40:15,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48500
2023-08-12 05:40:15,745 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,745 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,745 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40165', status: init, memory: 0, processing: 0>
2023-08-12 05:40:15,745 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40165
2023-08-12 05:40:15,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48510
2023-08-12 05:40:15,746 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,746 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,746 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33385', status: init, memory: 0, processing: 0>
2023-08-12 05:40:15,747 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33385
2023-08-12 05:40:15,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48518
2023-08-12 05:40:15,747 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:40:15,748 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:15,748 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:40:15,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:40:15,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:40:15,816 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:40:15,816 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:40:15,816 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:40:15,816 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:40:15,816 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:40:15,817 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:40:15,817 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:40:15,817 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-08-12 05:40:15,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:15,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:15,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:15,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:15,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:15,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:15,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:15,829 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:15,833 - distributed.scheduler - INFO - Remove client Client-b3436021-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:15,833 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41358; closing.
2023-08-12 05:40:15,833 - distributed.scheduler - INFO - Remove client Client-b3436021-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:15,834 - distributed.scheduler - INFO - Close client connection: Client-b3436021-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:15,835 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38795'. Reason: nanny-close
2023-08-12 05:40:15,835 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:40:15,836 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45543'. Reason: nanny-close
2023-08-12 05:40:15,836 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:40:15,837 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33385. Reason: nanny-close
2023-08-12 05:40:15,837 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46497'. Reason: nanny-close
2023-08-12 05:40:15,837 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:40:15,837 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35633'. Reason: nanny-close
2023-08-12 05:40:15,837 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40165. Reason: nanny-close
2023-08-12 05:40:15,838 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:40:15,838 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34651. Reason: nanny-close
2023-08-12 05:40:15,838 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45501'. Reason: nanny-close
2023-08-12 05:40:15,838 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:40:15,838 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46623. Reason: nanny-close
2023-08-12 05:40:15,838 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37377'. Reason: nanny-close
2023-08-12 05:40:15,839 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:40:15,839 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48518; closing.
2023-08-12 05:40:15,839 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:40:15,839 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33385', status: closing, memory: 0, processing: 0>
2023-08-12 05:40:15,839 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33601. Reason: nanny-close
2023-08-12 05:40:15,839 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44321'. Reason: nanny-close
2023-08-12 05:40:15,839 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33385
2023-08-12 05:40:15,839 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:40:15,839 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:40:15,840 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:40:15,840 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45011. Reason: nanny-close
2023-08-12 05:40:15,840 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34903'. Reason: nanny-close
2023-08-12 05:40:15,840 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:40:15,840 - distributed.nanny - INFO - Worker closed
2023-08-12 05:40:15,840 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37711. Reason: nanny-close
2023-08-12 05:40:15,840 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:40:15,841 - distributed.nanny - INFO - Worker closed
2023-08-12 05:40:15,841 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33385
2023-08-12 05:40:15,841 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48510; closing.
2023-08-12 05:40:15,841 - distributed.nanny - INFO - Worker closed
2023-08-12 05:40:15,841 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48472; closing.
2023-08-12 05:40:15,841 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33385
2023-08-12 05:40:15,841 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40909. Reason: nanny-close
2023-08-12 05:40:15,842 - distributed.nanny - INFO - Worker closed
2023-08-12 05:40:15,842 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40165', status: closing, memory: 0, processing: 0>
2023-08-12 05:40:15,842 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33385
2023-08-12 05:40:15,842 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40165
2023-08-12 05:40:15,842 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33385
2023-08-12 05:40:15,842 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:40:15,842 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34651', status: closing, memory: 0, processing: 0>
2023-08-12 05:40:15,842 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34651
2023-08-12 05:40:15,842 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:40:15,842 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:40:15,843 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48462; closing.
2023-08-12 05:40:15,843 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:40:15,843 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46623', status: closing, memory: 0, processing: 0>
2023-08-12 05:40:15,843 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46623
2023-08-12 05:40:15,843 - distributed.nanny - INFO - Worker closed
2023-08-12 05:40:15,843 - distributed.nanny - INFO - Worker closed
2023-08-12 05:40:15,843 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48456; closing.
2023-08-12 05:40:15,844 - distributed.nanny - INFO - Worker closed
2023-08-12 05:40:15,844 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33601', status: closing, memory: 0, processing: 0>
2023-08-12 05:40:15,844 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33601
2023-08-12 05:40:15,844 - distributed.nanny - INFO - Worker closed
2023-08-12 05:40:15,845 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48496; closing.
2023-08-12 05:40:15,845 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48500; closing.
2023-08-12 05:40:15,845 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48488; closing.
2023-08-12 05:40:15,846 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37711', status: closing, memory: 0, processing: 0>
2023-08-12 05:40:15,846 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37711
2023-08-12 05:40:15,846 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45011', status: closing, memory: 0, processing: 0>
2023-08-12 05:40:15,846 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45011
2023-08-12 05:40:15,847 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40909', status: closing, memory: 0, processing: 0>
2023-08-12 05:40:15,847 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40909
2023-08-12 05:40:15,847 - distributed.scheduler - INFO - Lost all workers
2023-08-12 05:40:17,252 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:40:17,252 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:40:17,253 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:40:17,254 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-12 05:40:17,254 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-08-12 05:40:18,991 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:40:18,995 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46849 instead
  warnings.warn(
2023-08-12 05:40:18,999 - distributed.scheduler - INFO - State start
2023-08-12 05:40:19,017 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:40:19,018 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-12 05:40:19,019 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46849/status
2023-08-12 05:40:19,130 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33809'
2023-08-12 05:40:19,153 - distributed.scheduler - INFO - Receive client connection: Client-b7f0b19f-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:19,164 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48584
2023-08-12 05:40:20,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:20,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:20,528 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:21,270 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44221
2023-08-12 05:40:21,271 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44221
2023-08-12 05:40:21,271 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38767
2023-08-12 05:40:21,271 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:40:21,271 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:21,271 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:21,271 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-12 05:40:21,271 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-gmgak9qb
2023-08-12 05:40:21,271 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be348dfa-9b15-4eee-b141-d46e959abd35
2023-08-12 05:40:21,360 - distributed.worker - INFO - Starting Worker plugin PreImport-c8423ece-1a57-4c94-9611-2bd73309de01
2023-08-12 05:40:21,361 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-50c251dc-ecc3-42bc-8194-c37f4ba4327a
2023-08-12 05:40:21,361 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:21,386 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44221', status: init, memory: 0, processing: 0>
2023-08-12 05:40:21,387 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44221
2023-08-12 05:40:21,387 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48602
2023-08-12 05:40:21,387 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-08-12 05:40:21,388 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:21,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-08-12 05:40:21,404 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-08-12 05:40:21,407 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:21,409 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-08-12 05:40:21,411 - distributed.scheduler - INFO - Remove client Client-b7f0b19f-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:21,412 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48584; closing.
2023-08-12 05:40:21,412 - distributed.scheduler - INFO - Remove client Client-b7f0b19f-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:21,412 - distributed.scheduler - INFO - Close client connection: Client-b7f0b19f-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:21,413 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33809'. Reason: nanny-close
2023-08-12 05:40:21,414 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-08-12 05:40:21,415 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44221. Reason: nanny-close
2023-08-12 05:40:21,416 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-08-12 05:40:21,416 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48602; closing.
2023-08-12 05:40:21,417 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44221', status: closing, memory: 0, processing: 0>
2023-08-12 05:40:21,417 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44221
2023-08-12 05:40:21,417 - distributed.scheduler - INFO - Lost all workers
2023-08-12 05:40:21,417 - distributed.nanny - INFO - Worker closed
2023-08-12 05:40:22,279 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:40:22,279 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:40:22,279 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:40:22,280 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-12 05:40:22,281 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-08-12 05:40:24,128 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:40:24,132 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39911 instead
  warnings.warn(
2023-08-12 05:40:24,135 - distributed.scheduler - INFO - State start
2023-08-12 05:40:24,153 - distributed.scheduler - INFO - -----------------------------------------------
2023-08-12 05:40:24,154 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-08-12 05:40:24,155 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39911/status
2023-08-12 05:40:24,238 - distributed.scheduler - INFO - Receive client connection: Client-baf0acbe-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:24,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48666
2023-08-12 05:40:24,251 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41573'
2023-08-12 05:40:25,608 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:25,608 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:25,630 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-08-12 05:40:26,401 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44097
2023-08-12 05:40:26,401 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44097
2023-08-12 05:40:26,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38001
2023-08-12 05:40:26,401 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-08-12 05:40:26,401 - distributed.worker - INFO - -------------------------------------------------
2023-08-12 05:40:26,401 - distributed.worker - INFO -               Threads:                          1
2023-08-12 05:40:26,401 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-08-12 05:40:26,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-25nr0rd8
2023-08-12 05:40:26,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0b1538f3-5b7d-46a8-8d4f-5f919d25ff47
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 880, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 940, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 375, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-08-12 05:40:26,646 - distributed.worker - INFO - Starting Worker plugin PreImport-64fda8a6-139f-4e2f-b527-22fb6d9f8d40
2023-08-12 05:40:26,646 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bfd609bd-899a-4468-9d62-e5650a8342cf
2023-08-12 05:40:26,647 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44097. Reason: worker-close
2023-08-12 05:40:26,647 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-08-12 05:40:26,650 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 880, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 940, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 375, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-08-12 05:40:26,678 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 880, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 940, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 375, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-08-12 05:40:26,681 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41573'. Reason: nanny-instantiate-failed
2023-08-12 05:40:26,681 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-08-12 05:40:27,077 - distributed.nanny - INFO - Worker process 55445 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 880, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 940, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 375, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-08-12 05:40:34,289 - distributed.scheduler - INFO - Remove client Client-baf0acbe-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:34,289 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48666; closing.
2023-08-12 05:40:34,289 - distributed.scheduler - INFO - Remove client Client-baf0acbe-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:34,290 - distributed.scheduler - INFO - Close client connection: Client-baf0acbe-38d2-11ee-8ca6-d8c49764f6bb
2023-08-12 05:40:34,291 - distributed._signals - INFO - Received signal SIGINT (2)
2023-08-12 05:40:34,291 - distributed.scheduler - INFO - Scheduler closing...
2023-08-12 05:40:34,291 - distributed.scheduler - INFO - Scheduler closing all comms
2023-08-12 05:40:34,292 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-08-12 05:40:34,292 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34617 instead
  warnings.warn(
2023-08-12 05:40:43,140 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:43,140 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:43,208 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:43,209 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:43,225 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:43,225 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:43,226 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:43,226 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:43,227 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:43,227 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:43,243 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:43,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:43,258 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:43,259 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:43,340 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:43,340 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39121 instead
  warnings.warn(
2023-08-12 05:40:51,943 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:51,944 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:51,947 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:51,947 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:51,948 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:51,948 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:52,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:52,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:52,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:52,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:52,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:52,032 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:52,037 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:52,037 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:52,038 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:52,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38745 instead
  warnings.warn(
2023-08-12 05:40:59,455 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:59,455 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:59,455 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:59,456 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:59,456 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:59,456 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:59,457 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:59,458 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:59,474 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:59,474 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:59,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:59,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:59,513 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:59,513 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:40:59,541 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:40:59,541 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44847 instead
  warnings.warn(
2023-08-12 05:41:07,965 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:07,965 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:07,968 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:07,968 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:07,991 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:07,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:08,000 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:08,000 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:08,008 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:08,008 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:08,045 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:08,045 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:08,046 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:08,046 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:08,047 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:08,047 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36777 instead
  warnings.warn(
2023-08-12 05:41:17,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:17,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:17,735 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:17,735 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:17,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:17,736 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:17,793 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:17,793 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:17,793 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:17,794 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:17,800 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:17,800 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:17,800 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:17,800 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:17,800 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:17,801 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
2023-08-12 05:41:27,570 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:27,570 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:27,570 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:27,570 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:27,616 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:27,616 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:27,619 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:27,619 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:27,630 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:27,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:27,633 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:27,633 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:27,653 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:27,653 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:27,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:27,654 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:31,417 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1262, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task was destroyed but it is pending!
task: <Task pending name='Task-1398' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> wait_for=<Future pending cb=[<TaskWakeupMethWrapper object at 0x7fa0c2c62d60>()]>>
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44767 instead
  warnings.warn(
2023-08-12 05:41:37,784 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:37,784 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:37,785 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:37,785 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:37,803 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:37,803 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:37,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:37,807 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:37,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:37,819 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:37,830 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:37,831 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:37,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:37,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:37,837 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:37,837 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.2.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45427 instead
  warnings.warn(
2023-08-12 05:41:47,599 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:47,599 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:47,605 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:47,606 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:47,650 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:47,650 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:47,662 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:47,662 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:47,663 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:47,663 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:47,669 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:47,669 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:47,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:47,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-08-12 05:41:47,725 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-08-12 05:41:47,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39201 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37639 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43087 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46357 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46029 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39045 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36075 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33075 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40057 instead
  warnings.warn(
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-3499' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[ucx-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 05:47:09,705 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 05:47:09,713 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe4d1d2bf70>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 05:47:09,714 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 05:47:09,722 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fb3cf010f70>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 05:47:09,724 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 05:47:09,725 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 05:47:09,732 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f604c02bf70>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 05:47:09,733 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f0d80c4ef70>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-08-12 05:47:11,717 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-08-12 05:47:11,725 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-08-12 05:47:11,736 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-08-12 05:47:11,736 - distributed.nanny - ERROR - Worker process died unexpectedly
