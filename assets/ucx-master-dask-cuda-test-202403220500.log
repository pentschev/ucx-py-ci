============================= test session starts ==============================
platform linux -- Python 3.9.19, pytest-8.1.1, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.6
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-03-22 05:58:50,017 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 05:58:50,022 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 05:58:50,025 - distributed.scheduler - INFO - State start
2024-03-22 05:58:50,046 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 05:58:50,047 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-22 05:58:50,047 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 05:58:50,048 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 05:58:50,266 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40923'
2024-03-22 05:58:50,283 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43589'
2024-03-22 05:58:50,285 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41195'
2024-03-22 05:58:50,293 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36705'
2024-03-22 05:58:51,798 - distributed.scheduler - INFO - Receive client connection: Client-40284382-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:58:51,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55546
2024-03-22 05:58:52,079 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:52,079 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:52,081 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:52,081 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:52,081 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:52,081 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:52,083 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:52,084 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37061
2024-03-22 05:58:52,084 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37061
2024-03-22 05:58:52,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45787
2024-03-22 05:58:52,084 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-22 05:58:52,084 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,084 - distributed.worker - INFO -               Threads:                          4
2024-03-22 05:58:52,084 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-22 05:58:52,084 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-0u87orvs
2024-03-22 05:58:52,085 - distributed.worker - INFO - Starting Worker plugin RMMSetup-355a5265-eb20-4b53-9f2d-c3d10d73c139
2024-03-22 05:58:52,085 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eba5304c-580b-4fcb-8ae3-adab41ce4124
2024-03-22 05:58:52,085 - distributed.worker - INFO - Starting Worker plugin PreImport-7495e768-e2e3-4413-901c-6b81cdb40b16
2024-03-22 05:58:52,085 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,085 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:52,086 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:52,086 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36739
2024-03-22 05:58:52,086 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36739
2024-03-22 05:58:52,086 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44113
2024-03-22 05:58:52,086 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-22 05:58:52,086 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,086 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37875
2024-03-22 05:58:52,086 - distributed.worker - INFO -               Threads:                          4
2024-03-22 05:58:52,086 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37875
2024-03-22 05:58:52,086 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-22 05:58:52,086 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33149
2024-03-22 05:58:52,086 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-nj33vn3j
2024-03-22 05:58:52,086 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-22 05:58:52,087 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,087 - distributed.worker - INFO -               Threads:                          4
2024-03-22 05:58:52,087 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-22 05:58:52,087 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7564a289-40eb-49ba-ae86-354c8238ec83
2024-03-22 05:58:52,087 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-tgwj80el
2024-03-22 05:58:52,087 - distributed.worker - INFO - Starting Worker plugin PreImport-109d17fd-3371-4e2c-a5ad-0b7713e5d703
2024-03-22 05:58:52,087 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce188532-51c6-43a7-a0ce-2f468d6a2746
2024-03-22 05:58:52,087 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1a658a5-d7bd-4462-9009-486f3d782384
2024-03-22 05:58:52,087 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,088 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:52,088 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:52,091 - distributed.worker - INFO - Starting Worker plugin PreImport-c63ad50d-87a3-4b8a-a1af-9ef7bef05a13
2024-03-22 05:58:52,092 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ba771a4-7a3c-4e80-8b14-23146d78f1c9
2024-03-22 05:58:52,092 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,092 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:52,093 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32933
2024-03-22 05:58:52,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32933
2024-03-22 05:58:52,093 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44977
2024-03-22 05:58:52,093 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-22 05:58:52,093 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,093 - distributed.worker - INFO -               Threads:                          4
2024-03-22 05:58:52,093 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-22 05:58:52,093 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-6z5hl5xv
2024-03-22 05:58:52,093 - distributed.worker - INFO - Starting Worker plugin RMMSetup-da9208ce-6aab-4105-9ec2-a3471743dcc2
2024-03-22 05:58:52,094 - distributed.worker - INFO - Starting Worker plugin PreImport-8bbc1359-3bf4-4a1c-8324-1c7af4d02955
2024-03-22 05:58:52,094 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a849506-d511-488b-b317-c91874d851e0
2024-03-22 05:58:52,094 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,210 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36739', status: init, memory: 0, processing: 0>
2024-03-22 05:58:52,211 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36739
2024-03-22 05:58:52,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55572
2024-03-22 05:58:52,212 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:52,212 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-22 05:58:52,212 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,213 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37061', status: init, memory: 0, processing: 0>
2024-03-22 05:58:52,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-22 05:58:52,214 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37061
2024-03-22 05:58:52,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55568
2024-03-22 05:58:52,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:52,215 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32933', status: init, memory: 0, processing: 0>
2024-03-22 05:58:52,216 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-22 05:58:52,216 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,216 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32933
2024-03-22 05:58:52,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55588
2024-03-22 05:58:52,216 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:52,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-22 05:58:52,217 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37875', status: init, memory: 0, processing: 0>
2024-03-22 05:58:52,217 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-22 05:58:52,217 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37875
2024-03-22 05:58:52,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55578
2024-03-22 05:58:52,218 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:52,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-22 05:58:52,219 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-22 05:58:52,219 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:52,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-22 05:58:52,225 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-22 05:58:52,225 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-22 05:58:52,226 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-22 05:58:52,226 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-22 05:58:52,230 - distributed.scheduler - INFO - Remove client Client-40284382-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:58:52,230 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55546; closing.
2024-03-22 05:58:52,231 - distributed.scheduler - INFO - Remove client Client-40284382-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:58:52,231 - distributed.scheduler - INFO - Close client connection: Client-40284382-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:58:52,232 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40923'. Reason: nanny-close
2024-03-22 05:58:52,232 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:58:52,233 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43589'. Reason: nanny-close
2024-03-22 05:58:52,233 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:58:52,233 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41195'. Reason: nanny-close
2024-03-22 05:58:52,233 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36739. Reason: nanny-close
2024-03-22 05:58:52,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:58:52,234 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36705'. Reason: nanny-close
2024-03-22 05:58:52,234 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37061. Reason: nanny-close
2024-03-22 05:58:52,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:58:52,234 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32933. Reason: nanny-close
2024-03-22 05:58:52,235 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37875. Reason: nanny-close
2024-03-22 05:58:52,235 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-22 05:58:52,235 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55572; closing.
2024-03-22 05:58:52,236 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-22 05:58:52,236 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36739', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087132.2361119')
2024-03-22 05:58:52,236 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-22 05:58:52,236 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55568; closing.
2024-03-22 05:58:52,237 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-22 05:58:52,237 - distributed.nanny - INFO - Worker closed
2024-03-22 05:58:52,237 - distributed.nanny - INFO - Worker closed
2024-03-22 05:58:52,237 - distributed.nanny - INFO - Worker closed
2024-03-22 05:58:52,237 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37061', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087132.23779')
2024-03-22 05:58:52,238 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55578; closing.
2024-03-22 05:58:52,238 - distributed.nanny - INFO - Worker closed
2024-03-22 05:58:52,238 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55588; closing.
2024-03-22 05:58:52,238 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37875', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087132.2389226')
2024-03-22 05:58:52,239 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32933', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087132.2393126')
2024-03-22 05:58:52,239 - distributed.scheduler - INFO - Lost all workers
2024-03-22 05:58:52,897 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 05:58:52,897 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 05:58:52,898 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 05:58:52,899 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-22 05:58:52,899 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-03-22 05:58:54,922 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 05:58:54,927 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 05:58:54,930 - distributed.scheduler - INFO - State start
2024-03-22 05:58:54,951 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 05:58:54,952 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 05:58:54,952 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 05:58:54,953 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 05:58:55,080 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34731'
2024-03-22 05:58:55,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36947'
2024-03-22 05:58:55,100 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36251'
2024-03-22 05:58:55,114 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35745'
2024-03-22 05:58:55,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39439'
2024-03-22 05:58:55,124 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38367'
2024-03-22 05:58:55,132 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42757'
2024-03-22 05:58:55,140 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42309'
2024-03-22 05:58:56,203 - distributed.scheduler - INFO - Receive client connection: Client-430bd65f-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:58:56,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35066
2024-03-22 05:58:56,907 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:56,907 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:56,912 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:56,913 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38289
2024-03-22 05:58:56,913 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38289
2024-03-22 05:58:56,913 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37947
2024-03-22 05:58:56,913 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:58:56,913 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:56,913 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:58:56,913 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:58:56,913 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o2cat545
2024-03-22 05:58:56,914 - distributed.worker - INFO - Starting Worker plugin RMMSetup-048aad47-7cec-45f4-b064-cc076b683208
2024-03-22 05:58:57,153 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:57,153 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:57,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:57,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:57,154 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:57,154 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:57,158 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:57,159 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:57,159 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:57,159 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35037
2024-03-22 05:58:57,159 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35037
2024-03-22 05:58:57,159 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40875
2024-03-22 05:58:57,159 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:58:57,159 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:57,159 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:58:57,159 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:58:57,159 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-60th8_7w
2024-03-22 05:58:57,160 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36977
2024-03-22 05:58:57,160 - distributed.worker - INFO - Starting Worker plugin PreImport-15ac9503-e30c-46bb-8c32-989f4b6071af
2024-03-22 05:58:57,160 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36977
2024-03-22 05:58:57,160 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34667
2024-03-22 05:58:57,160 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33371
2024-03-22 05:58:57,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f785f174-d467-40a3-854a-811087cb1780
2024-03-22 05:58:57,160 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:58:57,160 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33371
2024-03-22 05:58:57,160 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:57,160 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44301
2024-03-22 05:58:57,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76779460-7378-4f14-8987-6d5097f23d5d
2024-03-22 05:58:57,160 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:58:57,160 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:58:57,160 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:58:57,160 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:57,160 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:58:57,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o0hd6nn1
2024-03-22 05:58:57,160 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:58:57,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vpi6thbq
2024-03-22 05:58:57,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-413de531-4e1b-4d54-b6fa-164d21c3c5a8
2024-03-22 05:58:57,160 - distributed.worker - INFO - Starting Worker plugin PreImport-ef196813-63fc-4bae-8076-936f2f1b3eac
2024-03-22 05:58:57,160 - distributed.worker - INFO - Starting Worker plugin PreImport-940d6d02-9dd6-419b-9d4c-64d7d7cfb447
2024-03-22 05:58:57,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11abfd93-59b2-45f9-a47e-c14d323daa50
2024-03-22 05:58:57,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c4783cdc-2157-4e33-88db-d66cebcb95d8
2024-03-22 05:58:57,161 - distributed.worker - INFO - Starting Worker plugin RMMSetup-61b8ef16-4916-49b5-818e-98045c554d60
2024-03-22 05:58:57,190 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:57,190 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:57,190 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:57,190 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:57,194 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:57,194 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:57,195 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:57,195 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:57,195 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:58:57,196 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:58:57,196 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35743
2024-03-22 05:58:57,196 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35743
2024-03-22 05:58:57,196 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36719
2024-03-22 05:58:57,196 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:58:57,196 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:57,196 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:58:57,197 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:58:57,197 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pyvc8mrs
2024-03-22 05:58:57,197 - distributed.worker - INFO - Starting Worker plugin PreImport-133c344e-a1a2-4d72-a226-8c5e02b9e9bb
2024-03-22 05:58:57,197 - distributed.worker - INFO - Starting Worker plugin RMMSetup-629759ea-6fb9-4a49-9c65-4be1759109f4
2024-03-22 05:58:57,200 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37939
2024-03-22 05:58:57,200 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37939
2024-03-22 05:58:57,200 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43487
2024-03-22 05:58:57,200 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:58:57,201 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:57,201 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:58:57,201 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:58:57,201 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-59swfryf
2024-03-22 05:58:57,201 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:57,201 - distributed.worker - INFO - Starting Worker plugin PreImport-eb510c4b-4f02-4184-a206-85d44227c24a
2024-03-22 05:58:57,202 - distributed.worker - INFO - Starting Worker plugin RMMSetup-52a76481-d15a-42e6-8811-61e4d0bf60fd
2024-03-22 05:58:57,202 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41529
2024-03-22 05:58:57,202 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41529
2024-03-22 05:58:57,202 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44645
2024-03-22 05:58:57,203 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:58:57,203 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:57,203 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:58:57,203 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:58:57,203 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:58:57,203 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-we_olm9l
2024-03-22 05:58:57,203 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea031274-12c9-4fb7-8ceb-f55b6bdf8938
2024-03-22 05:58:57,204 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38619
2024-03-22 05:58:57,204 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38619
2024-03-22 05:58:57,204 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39857
2024-03-22 05:58:57,204 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:58:57,204 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:57,204 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:58:57,204 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:58:57,204 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kj5hgc25
2024-03-22 05:58:57,205 - distributed.worker - INFO - Starting Worker plugin PreImport-a933ae08-9f78-4429-843c-f8c2a10d95e7
2024-03-22 05:58:57,205 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-533b9e61-3fe2-485c-a718-4a4d0a38e4f6
2024-03-22 05:58:57,206 - distributed.worker - INFO - Starting Worker plugin PreImport-689a8f69-39da-4200-ad91-05344c081eeb
2024-03-22 05:58:57,206 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1a7834d9-d520-4f6f-af73-a5dfdd3f0f50
2024-03-22 05:58:57,206 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a6c7ae54-4a45-465c-b90a-df15e0b5ac9c
2024-03-22 05:58:57,355 - distributed.worker - INFO - Starting Worker plugin PreImport-7442c428-d955-4556-8587-2a60be26bd8e
2024-03-22 05:58:57,356 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c05e6ce6-0449-4665-b845-f1fba4a24adc
2024-03-22 05:58:57,360 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:57,384 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38289', status: init, memory: 0, processing: 0>
2024-03-22 05:58:57,386 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38289
2024-03-22 05:58:57,386 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35080
2024-03-22 05:58:57,387 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:57,387 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:58:57,387 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:57,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:58:59,398 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,415 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,419 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,427 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36977', status: init, memory: 0, processing: 0>
2024-03-22 05:58:59,427 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36977
2024-03-22 05:58:59,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35092
2024-03-22 05:58:59,429 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:59,430 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:58:59,430 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,432 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:58:59,444 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33371', status: init, memory: 0, processing: 0>
2024-03-22 05:58:59,445 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33371
2024-03-22 05:58:59,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35098
2024-03-22 05:58:59,446 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:59,447 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:58:59,447 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:58:59,449 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35037', status: init, memory: 0, processing: 0>
2024-03-22 05:58:59,449 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35037
2024-03-22 05:58:59,449 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35112
2024-03-22 05:58:59,451 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:59,452 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:58:59,452 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,454 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:58:59,456 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,469 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbad50cc-a6ca-44e3-9c3c-3974a099bf98
2024-03-22 05:58:59,470 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,476 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-43ec0753-f3ac-4db7-9cc4-2a769babe68e
2024-03-22 05:58:59,477 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,479 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38619', status: init, memory: 0, processing: 0>
2024-03-22 05:58:59,480 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38619
2024-03-22 05:58:59,480 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35120
2024-03-22 05:58:59,481 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:59,482 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:58:59,482 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,484 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:58:59,491 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37939', status: init, memory: 0, processing: 0>
2024-03-22 05:58:59,492 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37939
2024-03-22 05:58:59,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35132
2024-03-22 05:58:59,493 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:59,493 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:58:59,493 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:58:59,497 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35743', status: init, memory: 0, processing: 0>
2024-03-22 05:58:59,498 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35743
2024-03-22 05:58:59,498 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35140
2024-03-22 05:58:59,499 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:58:59,500 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:58:59,500 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:58:59,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:58:59,514 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 05:58:59,515 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41529. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 05:58:59,515 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 05:58:59,520 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 05:58:59,564 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 05:58:59,567 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34731'. Reason: nanny-instantiate-failed
2024-03-22 05:58:59,568 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 05:58:59,614 - distributed.nanny - INFO - Worker process 40644 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 05:58:59,615 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:34982'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34982>: Stream is closed
2024-03-22 05:58:59,618 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40675 parent=40476 started daemon>
2024-03-22 05:58:59,618 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40671 parent=40476 started daemon>
2024-03-22 05:58:59,618 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40667 parent=40476 started daemon>
2024-03-22 05:58:59,619 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40663 parent=40476 started daemon>
2024-03-22 05:58:59,619 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40659 parent=40476 started daemon>
2024-03-22 05:58:59,619 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40654 parent=40476 started daemon>
2024-03-22 05:58:59,619 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=40649 parent=40476 started daemon>
2024-03-22 05:58:59,641 - distributed.core - INFO - Connection to tcp://127.0.0.1:35140 has been closed.
2024-03-22 05:58:59,642 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35743', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087139.6420984')
2024-03-22 05:58:59,643 - distributed.core - INFO - Connection to tcp://127.0.0.1:35132 has been closed.
2024-03-22 05:58:59,643 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37939', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087139.643224')
2024-03-22 05:58:59,644 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:35132>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-22 05:58:59,645 - distributed.core - INFO - Connection to tcp://127.0.0.1:35098 has been closed.
2024-03-22 05:58:59,646 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33371', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087139.646036')
2024-03-22 05:58:59,647 - distributed.core - INFO - Connection to tcp://127.0.0.1:35092 has been closed.
2024-03-22 05:58:59,647 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36977', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087139.647294')
2024-03-22 05:58:59,647 - distributed.core - INFO - Connection to tcp://127.0.0.1:35080 has been closed.
2024-03-22 05:58:59,648 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38289', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087139.6481378')
2024-03-22 05:58:59,648 - distributed.core - INFO - Connection to tcp://127.0.0.1:35112 has been closed.
2024-03-22 05:58:59,648 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35037', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087139.6488416')
2024-03-22 05:58:59,649 - distributed.core - INFO - Connection to tcp://127.0.0.1:35120 has been closed.
2024-03-22 05:58:59,649 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38619', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087139.649244')
2024-03-22 05:58:59,649 - distributed.scheduler - INFO - Lost all workers
2024-03-22 05:58:59,888 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 40667 exit status was already read will report exitcode 255
2024-03-22 05:59:12,310 - distributed.scheduler - INFO - Remove client Client-430bd65f-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:12,311 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35066; closing.
2024-03-22 05:59:12,311 - distributed.scheduler - INFO - Remove client Client-430bd65f-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:12,312 - distributed.scheduler - INFO - Close client connection: Client-430bd65f-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:12,312 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 05:59:12,313 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 05:59:12,313 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 05:59:12,315 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 05:59:12,315 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-03-22 05:59:14,649 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 05:59:14,654 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 05:59:14,657 - distributed.scheduler - INFO - State start
2024-03-22 05:59:14,658 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-60th8_7w', purging
2024-03-22 05:59:14,659 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-pyvc8mrs', purging
2024-03-22 05:59:14,659 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-o2cat545', purging
2024-03-22 05:59:14,660 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-kj5hgc25', purging
2024-03-22 05:59:14,660 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vpi6thbq', purging
2024-03-22 05:59:14,660 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-o0hd6nn1', purging
2024-03-22 05:59:14,660 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-59swfryf', purging
2024-03-22 05:59:14,680 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 05:59:14,681 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 05:59:14,682 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 05:59:14,682 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 05:59:14,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44451'
2024-03-22 05:59:14,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33635'
2024-03-22 05:59:14,807 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39617'
2024-03-22 05:59:14,822 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37195'
2024-03-22 05:59:14,824 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38111'
2024-03-22 05:59:14,832 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37275'
2024-03-22 05:59:14,841 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38385'
2024-03-22 05:59:14,849 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46591'
2024-03-22 05:59:15,343 - distributed.scheduler - INFO - Receive client connection: Client-4ec283c5-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:15,354 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59284
2024-03-22 05:59:16,574 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:16,574 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:16,579 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:16,580 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39083
2024-03-22 05:59:16,580 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39083
2024-03-22 05:59:16,580 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43735
2024-03-22 05:59:16,580 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:16,580 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:16,580 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:16,580 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:16,580 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nab5kqcg
2024-03-22 05:59:16,580 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee8a2adf-d258-41cc-bc01-9439c6cea65b
2024-03-22 05:59:16,581 - distributed.worker - INFO - Starting Worker plugin PreImport-88625eb0-9490-473c-9714-b086028b6b30
2024-03-22 05:59:16,581 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8566ace1-4016-471c-829a-089395018d10
2024-03-22 05:59:16,814 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:16,815 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:16,815 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:16,815 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:16,819 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:16,820 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:16,820 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40401
2024-03-22 05:59:16,820 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40401
2024-03-22 05:59:16,820 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35679
2024-03-22 05:59:16,820 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:16,821 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:16,821 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:16,821 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:16,821 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2lta34jh
2024-03-22 05:59:16,821 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34969
2024-03-22 05:59:16,821 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34969
2024-03-22 05:59:16,821 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43001
2024-03-22 05:59:16,821 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:16,821 - distributed.worker - INFO - Starting Worker plugin PreImport-daea22bc-882f-45e9-8b23-13c8e24d58da
2024-03-22 05:59:16,821 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:16,821 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:16,821 - distributed.worker - INFO - Starting Worker plugin RMMSetup-991c2bd2-dce9-455f-9873-e0741c6ac632
2024-03-22 05:59:16,821 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:16,821 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-055uy69j
2024-03-22 05:59:16,821 - distributed.worker - INFO - Starting Worker plugin PreImport-08fbf79f-9543-4315-9423-e5fde9bf5297
2024-03-22 05:59:16,821 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9a1a39f-7247-4863-8263-353c8fffacf4
2024-03-22 05:59:16,822 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a52e8f49-deb2-4192-89c0-539d199e34e6
2024-03-22 05:59:16,839 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:16,839 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:16,843 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:16,844 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33463
2024-03-22 05:59:16,844 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33463
2024-03-22 05:59:16,845 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36259
2024-03-22 05:59:16,845 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:16,845 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:16,845 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:16,845 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:16,845 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8n5yd15b
2024-03-22 05:59:16,845 - distributed.worker - INFO - Starting Worker plugin PreImport-40fe59cb-5cc2-47c5-8d40-f07e4b66b100
2024-03-22 05:59:16,845 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f977bd71-d442-4ee8-825a-93ecb481773c
2024-03-22 05:59:16,858 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:16,858 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:16,863 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:16,864 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38215
2024-03-22 05:59:16,864 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38215
2024-03-22 05:59:16,864 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40609
2024-03-22 05:59:16,864 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:16,864 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:16,864 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:16,864 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:16,864 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2x44vyco
2024-03-22 05:59:16,864 - distributed.worker - INFO - Starting Worker plugin PreImport-00bc84fc-0bca-41b9-af6a-38fadb6025fd
2024-03-22 05:59:16,865 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fbb69373-5286-42f1-bf34-b9dc2ed665ae
2024-03-22 05:59:16,868 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6926540c-355b-4376-8329-6f2413796ec2
2024-03-22 05:59:16,878 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:16,878 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:16,882 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:16,883 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44073
2024-03-22 05:59:16,883 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44073
2024-03-22 05:59:16,883 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43987
2024-03-22 05:59:16,883 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:16,883 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:16,883 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:16,883 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:16,883 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d0a_jjf6
2024-03-22 05:59:16,883 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a004cbc7-ebd3-4e35-a642-dafe1a6bce44
2024-03-22 05:59:16,899 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:16,899 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:16,899 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:16,901 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:16,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:16,908 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39717
2024-03-22 05:59:16,909 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39717
2024-03-22 05:59:16,909 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41501
2024-03-22 05:59:16,909 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:16,909 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:16,909 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:16,909 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:16,909 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fxj_8lpp
2024-03-22 05:59:16,909 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:16,909 - distributed.worker - INFO - Starting Worker plugin PreImport-049cbbe2-0980-4b8e-8e06-24ae1b5ea310
2024-03-22 05:59:16,910 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f29a0b51-1fd5-4935-8604-3e9fb33aa133
2024-03-22 05:59:16,910 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f913f98e-76dd-4478-a151-5f458fe55d32
2024-03-22 05:59:16,910 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40975
2024-03-22 05:59:16,911 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40975
2024-03-22 05:59:16,911 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46173
2024-03-22 05:59:16,911 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:16,911 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:16,911 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:16,911 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:16,911 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v1l66qxd
2024-03-22 05:59:16,911 - distributed.worker - INFO - Starting Worker plugin PreImport-cd46f9f9-628b-4d63-a331-7de12383bd24
2024-03-22 05:59:16,911 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b44393f-d723-4e7c-baad-b299f600e1fc
2024-03-22 05:59:17,038 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:17,066 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39083', status: init, memory: 0, processing: 0>
2024-03-22 05:59:17,067 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39083
2024-03-22 05:59:17,067 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59308
2024-03-22 05:59:17,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:17,069 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:17,069 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:17,070 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:18,819 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a78fcfcb-8274-40ed-acf7-34a0037a8932
2024-03-22 05:59:18,821 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:18,846 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40401', status: init, memory: 0, processing: 0>
2024-03-22 05:59:18,846 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40401
2024-03-22 05:59:18,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59330
2024-03-22 05:59:18,847 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:18,848 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:18,848 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:18,850 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:18,868 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-788fa065-2225-461e-b4b2-546d48e0404c
2024-03-22 05:59:18,869 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:18,908 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33463', status: init, memory: 0, processing: 0>
2024-03-22 05:59:18,909 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33463
2024-03-22 05:59:18,909 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59334
2024-03-22 05:59:18,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:18,913 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:18,913 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:18,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:18,996 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:19,029 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34969', status: init, memory: 0, processing: 0>
2024-03-22 05:59:19,030 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34969
2024-03-22 05:59:19,030 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59338
2024-03-22 05:59:19,031 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:19,032 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:19,032 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:19,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:19,049 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:19,079 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38215', status: init, memory: 0, processing: 0>
2024-03-22 05:59:19,080 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38215
2024-03-22 05:59:19,080 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59342
2024-03-22 05:59:19,082 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:19,082 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:19,083 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:19,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:19,087 - distributed.worker - INFO - Starting Worker plugin PreImport-fd3c308e-4c5a-4c61-8069-ccfe88a91a01
2024-03-22 05:59:19,087 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-540e5b20-d641-4a49-9069-26f5aa558a66
2024-03-22 05:59:19,088 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:19,091 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-292cd741-75f5-4088-b8d5-a1e85c987000
2024-03-22 05:59:19,095 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:19,096 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:19,110 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44073', status: init, memory: 0, processing: 0>
2024-03-22 05:59:19,111 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44073
2024-03-22 05:59:19,111 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59358
2024-03-22 05:59:19,112 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:19,113 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:19,113 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:19,114 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:19,115 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39717', status: init, memory: 0, processing: 0>
2024-03-22 05:59:19,116 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39717
2024-03-22 05:59:19,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59370
2024-03-22 05:59:19,117 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:19,118 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:19,118 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:19,119 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:19,127 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40975', status: init, memory: 0, processing: 0>
2024-03-22 05:59:19,127 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40975
2024-03-22 05:59:19,127 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59378
2024-03-22 05:59:19,129 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:19,130 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:19,130 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:19,132 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:19,157 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 05:59:19,158 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 05:59:19,158 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 05:59:19,158 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 05:59:19,158 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 05:59:19,158 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 05:59:19,158 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 05:59:19,158 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-22 05:59:19,163 - distributed.scheduler - INFO - Remove client Client-4ec283c5-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:19,163 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59284; closing.
2024-03-22 05:59:19,163 - distributed.scheduler - INFO - Remove client Client-4ec283c5-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:19,163 - distributed.scheduler - INFO - Close client connection: Client-4ec283c5-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:19,165 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44451'. Reason: nanny-close
2024-03-22 05:59:19,165 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:59:19,166 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33635'. Reason: nanny-close
2024-03-22 05:59:19,167 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:59:19,167 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39617'. Reason: nanny-close
2024-03-22 05:59:19,167 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39083. Reason: nanny-close
2024-03-22 05:59:19,168 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:59:19,168 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37195'. Reason: nanny-close
2024-03-22 05:59:19,168 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34969. Reason: nanny-close
2024-03-22 05:59:19,168 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:59:19,169 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33463. Reason: nanny-close
2024-03-22 05:59:19,169 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38111'. Reason: nanny-close
2024-03-22 05:59:19,169 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:59:19,169 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40401. Reason: nanny-close
2024-03-22 05:59:19,169 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37275'. Reason: nanny-close
2024-03-22 05:59:19,169 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 05:59:19,170 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59308; closing.
2024-03-22 05:59:19,170 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:59:19,170 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39083', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087159.170328')
2024-03-22 05:59:19,170 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38385'. Reason: nanny-close
2024-03-22 05:59:19,170 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38215. Reason: nanny-close
2024-03-22 05:59:19,170 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 05:59:19,170 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:59:19,171 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 05:59:19,171 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40975. Reason: nanny-close
2024-03-22 05:59:19,171 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46591'. Reason: nanny-close
2024-03-22 05:59:19,171 - distributed.nanny - INFO - Worker closed
2024-03-22 05:59:19,171 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 05:59:19,171 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39717. Reason: nanny-close
2024-03-22 05:59:19,172 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 05:59:19,172 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59334; closing.
2024-03-22 05:59:19,172 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59338; closing.
2024-03-22 05:59:19,172 - distributed.nanny - INFO - Worker closed
2024-03-22 05:59:19,173 - distributed.nanny - INFO - Worker closed
2024-03-22 05:59:19,173 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33463', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087159.1731253')
2024-03-22 05:59:19,173 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44073. Reason: nanny-close
2024-03-22 05:59:19,173 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59330; closing.
2024-03-22 05:59:19,173 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 05:59:19,173 - distributed.nanny - INFO - Worker closed
2024-03-22 05:59:19,173 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087159.1737483')
2024-03-22 05:59:19,174 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40401', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087159.1742601')
2024-03-22 05:59:19,174 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 05:59:19,174 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 05:59:19,175 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59342; closing.
2024-03-22 05:59:19,175 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59378; closing.
2024-03-22 05:59:19,175 - distributed.nanny - INFO - Worker closed
2024-03-22 05:59:19,175 - distributed.nanny - INFO - Worker closed
2024-03-22 05:59:19,175 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38215', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087159.1758752')
2024-03-22 05:59:19,176 - distributed.nanny - INFO - Worker closed
2024-03-22 05:59:19,176 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40975', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087159.176173')
2024-03-22 05:59:19,176 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 05:59:19,176 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59370; closing.
2024-03-22 05:59:19,177 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39717', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087159.1770654')
2024-03-22 05:59:19,177 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59358; closing.
2024-03-22 05:59:19,177 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44073', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087159.1777284')
2024-03-22 05:59:19,177 - distributed.scheduler - INFO - Lost all workers
2024-03-22 05:59:19,178 - distributed.nanny - INFO - Worker closed
2024-03-22 05:59:20,180 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 05:59:20,181 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 05:59:20,181 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 05:59:20,182 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 05:59:20,183 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-03-22 05:59:22,416 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 05:59:22,421 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 05:59:22,425 - distributed.scheduler - INFO - State start
2024-03-22 05:59:22,446 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 05:59:22,447 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 05:59:22,448 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 05:59:22,448 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 05:59:22,453 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46397'
2024-03-22 05:59:22,468 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41891'
2024-03-22 05:59:22,478 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34583'
2024-03-22 05:59:22,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34201'
2024-03-22 05:59:22,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36369'
2024-03-22 05:59:22,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33935'
2024-03-22 05:59:22,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36643'
2024-03-22 05:59:22,515 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44587'
2024-03-22 05:59:24,160 - distributed.scheduler - INFO - Receive client connection: Client-534fcdd9-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:24,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44228
2024-03-22 05:59:24,509 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:24,510 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:24,514 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:24,515 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34711
2024-03-22 05:59:24,515 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34711
2024-03-22 05:59:24,515 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39541
2024-03-22 05:59:24,515 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:24,515 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:24,515 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:24,515 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:24,515 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jy2rw3np
2024-03-22 05:59:24,516 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ec533e85-4593-4752-a91a-1e40532e452a
2024-03-22 05:59:24,516 - distributed.worker - INFO - Starting Worker plugin PreImport-6330480e-f124-4106-b795-ee94e8776791
2024-03-22 05:59:24,516 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b9d9ab8-8612-4f32-9af3-8dfee050c288
2024-03-22 05:59:24,523 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:24,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:24,530 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:24,531 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34725
2024-03-22 05:59:24,531 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34725
2024-03-22 05:59:24,531 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45393
2024-03-22 05:59:24,531 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:24,531 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:24,532 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:24,532 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:24,532 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rho3afjm
2024-03-22 05:59:24,532 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c45dc7dd-7180-42a4-8a03-545303a01839
2024-03-22 05:59:24,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:24,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:24,552 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:24,553 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39267
2024-03-22 05:59:24,553 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39267
2024-03-22 05:59:24,553 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35377
2024-03-22 05:59:24,553 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:24,553 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:24,553 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:24,553 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:24,553 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bry1vaw4
2024-03-22 05:59:24,553 - distributed.worker - INFO - Starting Worker plugin PreImport-9378793b-75fa-41b5-988e-aaee9c880641
2024-03-22 05:59:24,554 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-239a13fb-9c7e-4d98-ab35-c05efe59c3b4
2024-03-22 05:59:24,554 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7e21b397-bfae-4917-bd1f-df92fdd68e83
2024-03-22 05:59:24,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:24,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:24,563 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:24,563 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45807
2024-03-22 05:59:24,563 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45807
2024-03-22 05:59:24,564 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44347
2024-03-22 05:59:24,564 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:24,564 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:24,564 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:24,564 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:24,564 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2e57jzq7
2024-03-22 05:59:24,564 - distributed.worker - INFO - Starting Worker plugin PreImport-bb5d9f2d-25f9-4f1e-8a6f-77b3dd39b5d2
2024-03-22 05:59:24,564 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0853c6c-7de7-45d6-be88-9d53f230713c
2024-03-22 05:59:24,567 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:24,567 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:24,572 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:24,573 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40925
2024-03-22 05:59:24,573 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40925
2024-03-22 05:59:24,573 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45559
2024-03-22 05:59:24,573 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:24,573 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:24,573 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:24,573 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:24,573 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dkfmsd6d
2024-03-22 05:59:24,574 - distributed.worker - INFO - Starting Worker plugin PreImport-03d85db6-c982-474d-8a79-7f42bd3408c9
2024-03-22 05:59:24,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08dd8657-da83-4030-a6b4-34aca7856935
2024-03-22 05:59:24,575 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c39d44eb-20d2-4fc8-9d9c-b6e5f0076eea
2024-03-22 05:59:24,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:24,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:24,578 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:24,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:24,583 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:24,584 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:24,585 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39961
2024-03-22 05:59:24,585 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39961
2024-03-22 05:59:24,585 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33821
2024-03-22 05:59:24,585 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:24,585 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:24,585 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:24,585 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:24,585 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c10xw4kc
2024-03-22 05:59:24,585 - distributed.worker - INFO - Starting Worker plugin PreImport-b66e082b-a2a9-43fb-a264-b95598b6736e
2024-03-22 05:59:24,585 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c922d223-85dd-4914-b293-90b8612f6db3
2024-03-22 05:59:24,585 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37185
2024-03-22 05:59:24,585 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37185
2024-03-22 05:59:24,586 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34929
2024-03-22 05:59:24,586 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:24,586 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:24,586 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:24,586 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:24,586 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qzrkaf37
2024-03-22 05:59:24,586 - distributed.worker - INFO - Starting Worker plugin PreImport-6fef4d28-72c9-4fcb-aa59-3895aab80887
2024-03-22 05:59:24,586 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4872d140-ea43-43ab-9719-3b054257e6ce
2024-03-22 05:59:24,604 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:24,604 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:24,610 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:24,611 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43597
2024-03-22 05:59:24,612 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43597
2024-03-22 05:59:24,612 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45247
2024-03-22 05:59:24,612 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:24,612 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:24,612 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:24,612 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:24,612 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6qvex0ja
2024-03-22 05:59:24,612 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df7ed67e-103b-4e05-bdbb-0bcaa4b5bc83
2024-03-22 05:59:26,868 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-22 05:59:26,869 - distributed.worker - INFO - Starting Worker plugin PreImport-37cf9530-5ce1-496b-8b5b-5db5a1d6b564
2024-03-22 05:59:26,869 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7cbe05b5-1692-4a22-91c2-3c42a174e4f5
2024-03-22 05:59:26,870 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34725. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 05:59:26,870 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 05:59:26,872 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 05:59:26,892 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 05:59:26,895 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46397'. Reason: nanny-instantiate-failed
2024-03-22 05:59:26,896 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 05:59:26,899 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:26,924 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34711', status: init, memory: 0, processing: 0>
2024-03-22 05:59:26,927 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34711
2024-03-22 05:59:26,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44252
2024-03-22 05:59:26,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:26,928 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:26,928 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:26,930 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:27,065 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:27,090 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39267', status: init, memory: 0, processing: 0>
2024-03-22 05:59:27,091 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39267
2024-03-22 05:59:27,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44266
2024-03-22 05:59:27,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:27,092 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:27,092 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:27,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:27,117 - distributed.nanny - INFO - Worker process 41228 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 05:59:27,121 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41259 parent=41060 started daemon>
2024-03-22 05:59:27,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41256 parent=41060 started daemon>
2024-03-22 05:59:27,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41252 parent=41060 started daemon>
2024-03-22 05:59:27,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41247 parent=41060 started daemon>
2024-03-22 05:59:27,118 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44158'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44158>: Stream is closed
2024-03-22 05:59:27,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41244 parent=41060 started daemon>
2024-03-22 05:59:27,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41239 parent=41060 started daemon>
2024-03-22 05:59:27,122 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41233 parent=41060 started daemon>
2024-03-22 05:59:27,151 - distributed.core - INFO - Connection to tcp://127.0.0.1:44266 has been closed.
2024-03-22 05:59:27,152 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39267', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087167.152095')
2024-03-22 05:59:27,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:44252 has been closed.
2024-03-22 05:59:27,153 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34711', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087167.1536586')
2024-03-22 05:59:27,153 - distributed.scheduler - INFO - Lost all workers
2024-03-22 05:59:27,366 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 41233 exit status was already read will report exitcode 255
2024-03-22 05:59:27,398 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 41244 exit status was already read will report exitcode 255
2024-03-22 05:59:27,424 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 41256 exit status was already read will report exitcode 255
2024-03-22 05:59:27,625 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44212'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44212>: Stream is closed
2024-03-22 05:59:27,626 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44196'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44196>: Stream is closed
2024-03-22 05:59:27,626 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44188'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44188>: Stream is closed
2024-03-22 05:59:27,627 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44176'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44176>: Stream is closed
2024-03-22 05:59:27,627 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:44164'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44164>: Stream is closed
2024-03-22 05:59:40,250 - distributed.scheduler - INFO - Remove client Client-534fcdd9-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:40,250 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44228; closing.
2024-03-22 05:59:40,250 - distributed.scheduler - INFO - Remove client Client-534fcdd9-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:40,251 - distributed.scheduler - INFO - Close client connection: Client-534fcdd9-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:40,252 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 05:59:40,252 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 05:59:40,253 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 05:59:40,254 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 05:59:40,255 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-03-22 05:59:42,485 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 05:59:42,490 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 05:59:42,494 - distributed.scheduler - INFO - State start
2024-03-22 05:59:42,495 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6qvex0ja', purging
2024-03-22 05:59:42,496 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jy2rw3np', purging
2024-03-22 05:59:42,496 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-bry1vaw4', purging
2024-03-22 05:59:42,497 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-qzrkaf37', purging
2024-03-22 05:59:42,497 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-c10xw4kc', purging
2024-03-22 05:59:42,497 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2e57jzq7', purging
2024-03-22 05:59:42,498 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-dkfmsd6d', purging
2024-03-22 05:59:42,519 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 05:59:42,520 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 05:59:42,521 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 05:59:42,521 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 05:59:42,627 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42875'
2024-03-22 05:59:42,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33219'
2024-03-22 05:59:42,648 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38283'
2024-03-22 05:59:42,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43629'
2024-03-22 05:59:42,665 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36745'
2024-03-22 05:59:42,674 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41621'
2024-03-22 05:59:42,682 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33973'
2024-03-22 05:59:42,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39099'
2024-03-22 05:59:44,492 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:44,492 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:44,496 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:44,497 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44893
2024-03-22 05:59:44,497 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44893
2024-03-22 05:59:44,497 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45043
2024-03-22 05:59:44,497 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:44,497 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:44,497 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:44,497 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:44,497 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nfe89nyt
2024-03-22 05:59:44,497 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-841820c1-aba7-41ac-9286-ba2b41bb51cd
2024-03-22 05:59:44,498 - distributed.worker - INFO - Starting Worker plugin PreImport-a2af82b8-31e7-4e28-b8b2-6360537c7f70
2024-03-22 05:59:44,498 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72972840-863a-4c61-b263-af2ef318b824
2024-03-22 05:59:44,679 - distributed.scheduler - INFO - Receive client connection: Client-5f5736e5-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 05:59:44,691 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48756
2024-03-22 05:59:44,727 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:44,727 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:44,732 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:44,732 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46815
2024-03-22 05:59:44,732 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46815
2024-03-22 05:59:44,733 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37857
2024-03-22 05:59:44,733 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:44,733 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:44,733 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:44,733 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:44,733 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vhlvmk34
2024-03-22 05:59:44,733 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c67edfda-0616-4ecf-bba6-cbbde167d6a7
2024-03-22 05:59:44,733 - distributed.worker - INFO - Starting Worker plugin PreImport-99f35515-a377-4b87-afc5-67403c622feb
2024-03-22 05:59:44,734 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d098aefe-fcad-4171-a912-bd1e8144a015
2024-03-22 05:59:44,739 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:44,739 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:44,745 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:44,747 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36307
2024-03-22 05:59:44,747 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36307
2024-03-22 05:59:44,747 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35657
2024-03-22 05:59:44,747 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:44,747 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:44,747 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:44,747 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:44,747 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-03u3hunf
2024-03-22 05:59:44,747 - distributed.worker - INFO - Starting Worker plugin PreImport-476e02c3-0531-4bf3-b296-9ee98cb33cda
2024-03-22 05:59:44,748 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5b295d75-59bc-49d5-bd8d-29641d0e293b
2024-03-22 05:59:44,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:44,751 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:44,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:44,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:44,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:44,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:44,762 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:44,762 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:44,762 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:44,762 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:44,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:44,762 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36513
2024-03-22 05:59:44,763 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36513
2024-03-22 05:59:44,763 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39899
2024-03-22 05:59:44,763 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39899
2024-03-22 05:59:44,763 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42479
2024-03-22 05:59:44,763 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:44,763 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33789
2024-03-22 05:59:44,763 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:44,763 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:44,763 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:44,763 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:44,763 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:44,763 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:44,763 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:44,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rzi60a4e
2024-03-22 05:59:44,763 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4o73ozoz
2024-03-22 05:59:44,763 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 05:59:44,763 - distributed.worker - INFO - Starting Worker plugin PreImport-40623044-4c6a-4e08-86c5-0705a90a5b0f
2024-03-22 05:59:44,763 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 05:59:44,763 - distributed.worker - INFO - Starting Worker plugin PreImport-2bbde423-7ce9-4123-bb80-9994d5cfb599
2024-03-22 05:59:44,763 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c1bd419-ce88-41fd-aeaa-222204f27504
2024-03-22 05:59:44,765 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43163
2024-03-22 05:59:44,765 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43163
2024-03-22 05:59:44,765 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46479
2024-03-22 05:59:44,765 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:44,765 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:44,765 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:44,765 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:44,765 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bt6tmxlm
2024-03-22 05:59:44,766 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e31f9dfd-f22e-4929-87a4-f45a8be8f0c7
2024-03-22 05:59:44,766 - distributed.worker - INFO - Starting Worker plugin PreImport-95f324cb-6333-424c-b4d0-fd466febe6f8
2024-03-22 05:59:44,766 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3749ca5e-67be-4a2b-a77c-ede2ea17faf4
2024-03-22 05:59:44,766 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cc5d31d0-9a9f-41fc-bacf-9ec649b7df8b
2024-03-22 05:59:44,769 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:44,770 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 05:59:44,771 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45581
2024-03-22 05:59:44,771 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45581
2024-03-22 05:59:44,771 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35195
2024-03-22 05:59:44,771 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:44,771 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:44,771 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:44,771 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:44,771 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kfni4dpx
2024-03-22 05:59:44,772 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33611
2024-03-22 05:59:44,772 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33611
2024-03-22 05:59:44,772 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39029
2024-03-22 05:59:44,772 - distributed.worker - INFO - Starting Worker plugin PreImport-9698d4b9-4d4a-4e30-bad3-b4ed233d41ae
2024-03-22 05:59:44,772 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 05:59:44,772 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:44,772 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d2ccb114-f5c0-420e-aace-61d02d125bd0
2024-03-22 05:59:44,772 - distributed.worker - INFO -               Threads:                          1
2024-03-22 05:59:44,772 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 05:59:44,772 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-04xf6g43
2024-03-22 05:59:44,772 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fd2d48a0-e6eb-4e31-8b3a-877a69d35ffd
2024-03-22 05:59:44,772 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb977a81-ccc5-455f-abcd-82263aeace2c
2024-03-22 05:59:44,773 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d62a1d4f-7789-4fc1-a0bf-02e890351b00
2024-03-22 05:59:45,046 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:45,072 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44893', status: init, memory: 0, processing: 0>
2024-03-22 05:59:45,074 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44893
2024-03-22 05:59:45,074 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48768
2024-03-22 05:59:45,075 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:45,075 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:45,076 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:45,077 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:47,241 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,273 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46815', status: init, memory: 0, processing: 0>
2024-03-22 05:59:47,274 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46815
2024-03-22 05:59:47,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48784
2024-03-22 05:59:47,276 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:47,277 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:47,277 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,279 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:47,288 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6ccc44c-492f-4763-9598-4656b7f9aa45
2024-03-22 05:59:47,288 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,302 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-22 05:59:47,304 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43163. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 05:59:47,304 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 05:59:47,308 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 05:59:47,311 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36513', status: init, memory: 0, processing: 0>
2024-03-22 05:59:47,312 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36513
2024-03-22 05:59:47,312 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48794
2024-03-22 05:59:47,313 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:47,313 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:47,313 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,315 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:47,317 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9d5d6a1-bdbf-4c2e-85e6-90044cbbe2a9
2024-03-22 05:59:47,317 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,318 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65c2978d-73a0-4bf9-8fa3-4841880db8c3
2024-03-22 05:59:47,319 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,323 - distributed.worker - INFO - Starting Worker plugin PreImport-8741056a-38b3-4869-b065-bb01beeda25a
2024-03-22 05:59:47,324 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,325 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,341 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39899', status: init, memory: 0, processing: 0>
2024-03-22 05:59:47,341 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39899
2024-03-22 05:59:47,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48800
2024-03-22 05:59:47,342 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:47,343 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:47,343 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,345 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:47,345 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33611', status: init, memory: 0, processing: 0>
2024-03-22 05:59:47,345 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33611
2024-03-22 05:59:47,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48808
2024-03-22 05:59:47,346 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:47,347 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:47,347 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:47,355 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36307', status: init, memory: 0, processing: 0>
2024-03-22 05:59:47,356 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36307
2024-03-22 05:59:47,356 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48820
2024-03-22 05:59:47,357 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45581', status: init, memory: 0, processing: 0>
2024-03-22 05:59:47,358 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45581
2024-03-22 05:59:47,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48822
2024-03-22 05:59:47,358 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:47,359 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:47,359 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,359 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 05:59:47,360 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 05:59:47,360 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 05:59:47,361 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:47,362 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 05:59:47,361 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 05:59:47,364 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42875'. Reason: nanny-instantiate-failed
2024-03-22 05:59:47,364 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 05:59:47,420 - distributed.nanny - INFO - Worker process 41498 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 05:59:47,423 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41528 parent=41329 started daemon>
2024-03-22 05:59:47,424 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41524 parent=41329 started daemon>
2024-03-22 05:59:47,424 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41521 parent=41329 started daemon>
2024-03-22 05:59:47,424 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41517 parent=41329 started daemon>
2024-03-22 05:59:47,424 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41512 parent=41329 started daemon>
2024-03-22 05:59:47,424 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41507 parent=41329 started daemon>
2024-03-22 05:59:47,421 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:48666'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48666>: Stream is closed
2024-03-22 05:59:47,424 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41502 parent=41329 started daemon>
2024-03-22 05:59:47,446 - distributed.core - INFO - Connection to tcp://127.0.0.1:48800 has been closed.
2024-03-22 05:59:47,446 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39899', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087187.4464688')
2024-03-22 05:59:47,448 - distributed.core - INFO - Connection to tcp://127.0.0.1:48808 has been closed.
2024-03-22 05:59:47,448 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33611', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087187.4481945')
2024-03-22 05:59:47,448 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48808>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:48808>: Stream is closed
2024-03-22 05:59:47,450 - distributed.core - INFO - Connection to tcp://127.0.0.1:48794 has been closed.
2024-03-22 05:59:47,450 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36513', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087187.450417')
2024-03-22 05:59:47,451 - distributed.core - INFO - Connection to tcp://127.0.0.1:48768 has been closed.
2024-03-22 05:59:47,451 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44893', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087187.4511316')
2024-03-22 05:59:47,453 - distributed.core - INFO - Connection to tcp://127.0.0.1:48820 has been closed.
2024-03-22 05:59:47,453 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36307', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087187.4538558')
2024-03-22 05:59:47,455 - distributed.core - INFO - Connection to tcp://127.0.0.1:48784 has been closed.
2024-03-22 05:59:47,455 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46815', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087187.4555886')
2024-03-22 05:59:47,455 - distributed.core - INFO - Connection to tcp://127.0.0.1:48822 has been closed.
2024-03-22 05:59:47,456 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45581', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087187.4560246')
2024-03-22 05:59:47,456 - distributed.scheduler - INFO - Lost all workers
2024-03-22 05:59:47,721 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 41517 exit status was already read will report exitcode 255
2024-03-22 05:59:47,758 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 41521 exit status was already read will report exitcode 255
2024-03-22 06:00:00,702 - distributed.scheduler - INFO - Remove client Client-5f5736e5-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:00,702 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48756; closing.
2024-03-22 06:00:00,703 - distributed.scheduler - INFO - Remove client Client-5f5736e5-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:00,703 - distributed.scheduler - INFO - Close client connection: Client-5f5736e5-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:00,704 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:00:00,704 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:00:00,705 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:00:00,706 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:00:00,706 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-03-22 06:00:02,897 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:02,902 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 06:00:02,905 - distributed.scheduler - INFO - State start
2024-03-22 06:00:02,906 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-03u3hunf', purging
2024-03-22 06:00:02,907 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-kfni4dpx', purging
2024-03-22 06:00:02,908 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-04xf6g43', purging
2024-03-22 06:00:02,908 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vhlvmk34', purging
2024-03-22 06:00:02,908 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4o73ozoz', purging
2024-03-22 06:00:02,909 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rzi60a4e', purging
2024-03-22 06:00:02,909 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nfe89nyt', purging
2024-03-22 06:00:02,929 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:02,929 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:00:02,930 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 06:00:02,930 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:00:02,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41653'
2024-03-22 06:00:02,973 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38883'
2024-03-22 06:00:02,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40563'
2024-03-22 06:00:02,991 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44141'
2024-03-22 06:00:03,005 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35205'
2024-03-22 06:00:03,008 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34049'
2024-03-22 06:00:03,017 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37433'
2024-03-22 06:00:03,026 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43197'
2024-03-22 06:00:04,038 - distributed.scheduler - INFO - Receive client connection: Client-6b792e88-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:04,051 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50574
2024-03-22 06:00:04,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:04,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:04,834 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:04,835 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46211
2024-03-22 06:00:04,835 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46211
2024-03-22 06:00:04,835 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46561
2024-03-22 06:00:04,835 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:04,835 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:04,835 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:04,835 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:04,835 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qnoibpmi
2024-03-22 06:00:04,835 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f093e3f-854c-42f2-a884-e867d0bdc679
2024-03-22 06:00:04,835 - distributed.worker - INFO - Starting Worker plugin PreImport-af79d8fe-93b9-490d-9e65-2111c14e3daf
2024-03-22 06:00:04,835 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a880b095-1422-4454-9bc6-5070abc86aaa
2024-03-22 06:00:05,054 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:05,054 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:05,059 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:05,060 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33459
2024-03-22 06:00:05,060 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33459
2024-03-22 06:00:05,060 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41699
2024-03-22 06:00:05,060 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:05,060 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:05,060 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:05,060 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:05,060 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-md3utyl9
2024-03-22 06:00:05,061 - distributed.worker - INFO - Starting Worker plugin PreImport-237d1610-1332-4f36-9c1a-5affa64b0117
2024-03-22 06:00:05,061 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5fc39b2b-8d9d-449d-9829-3f3ec3965639
2024-03-22 06:00:05,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:05,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:05,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:05,104 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:05,104 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:05,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:05,106 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:05,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:05,109 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:05,109 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:05,110 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:05,111 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:05,111 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:05,111 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:05,112 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33357
2024-03-22 06:00:05,112 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33357
2024-03-22 06:00:05,112 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41765
2024-03-22 06:00:05,112 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:05,112 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:05,112 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:05,112 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:05,112 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:05,112 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vij0re9t
2024-03-22 06:00:05,112 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39087
2024-03-22 06:00:05,112 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39087
2024-03-22 06:00:05,112 - distributed.worker - INFO - Starting Worker plugin PreImport-69efe55f-3e5c-4e81-a10e-c25b1b3d3b33
2024-03-22 06:00:05,112 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33939
2024-03-22 06:00:05,112 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:05,112 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:05,112 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ed643aa-e318-4ae5-9018-4bf220663552
2024-03-22 06:00:05,112 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:05,113 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:05,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-avnhnw8c
2024-03-22 06:00:05,113 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:05,113 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27fc39b1-f637-4259-aa28-7400e23d6107
2024-03-22 06:00:05,113 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43759
2024-03-22 06:00:05,113 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43759
2024-03-22 06:00:05,113 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a813177-9f0b-4557-8b1f-87ca91b670bb
2024-03-22 06:00:05,113 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34073
2024-03-22 06:00:05,113 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:05,113 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:05,113 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:05,113 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:05,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nd_7tvq7
2024-03-22 06:00:05,114 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f7efdf5-b9e6-4b7b-af3c-1156c934f4b4
2024-03-22 06:00:05,114 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39199
2024-03-22 06:00:05,114 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39199
2024-03-22 06:00:05,114 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43817
2024-03-22 06:00:05,114 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:05,114 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:05,114 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:05,114 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:05,114 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y49__73c
2024-03-22 06:00:05,115 - distributed.worker - INFO - Starting Worker plugin PreImport-b7a46adf-c71e-4f9d-bb38-d4cb8034a9ae
2024-03-22 06:00:05,115 - distributed.worker - INFO - Starting Worker plugin RMMSetup-050b4aef-d5b7-4f05-8850-bad339be9d7f
2024-03-22 06:00:05,116 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:05,117 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42631
2024-03-22 06:00:05,117 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42631
2024-03-22 06:00:05,117 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42377
2024-03-22 06:00:05,117 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:05,117 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:05,117 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:05,118 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:05,118 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b4u11aea
2024-03-22 06:00:05,118 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:05,118 - distributed.worker - INFO - Starting Worker plugin PreImport-de16b7cb-2873-44e7-9764-129667053fc7
2024-03-22 06:00:05,118 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60619675-86a0-4d5b-9232-45db1ac243a1
2024-03-22 06:00:05,119 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36843
2024-03-22 06:00:05,119 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36843
2024-03-22 06:00:05,119 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33489
2024-03-22 06:00:05,119 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:05,119 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:05,119 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:05,120 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:05,120 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p6q71che
2024-03-22 06:00:05,120 - distributed.worker - INFO - Starting Worker plugin PreImport-00d341b6-498d-4b88-9179-5e8e4caa7e23
2024-03-22 06:00:05,120 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8140a206-031a-4226-ad87-bd62316a47c4
2024-03-22 06:00:05,316 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:00:05,318 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46211. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 06:00:05,318 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:00:05,321 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:00:05,351 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:00:05,355 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41653'. Reason: nanny-instantiate-failed
2024-03-22 06:00:05,356 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 06:00:05,623 - distributed.nanny - INFO - Worker process 41781 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:00:05,625 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:50494'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50494>: Stream is closed
2024-03-22 06:00:05,629 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41812 parent=41613 started daemon>
2024-03-22 06:00:05,629 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41808 parent=41613 started daemon>
2024-03-22 06:00:05,629 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41805 parent=41613 started daemon>
2024-03-22 06:00:05,629 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41801 parent=41613 started daemon>
2024-03-22 06:00:05,630 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41796 parent=41613 started daemon>
2024-03-22 06:00:05,630 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41791 parent=41613 started daemon>
2024-03-22 06:00:05,630 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=41786 parent=41613 started daemon>
2024-03-22 06:00:05,653 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 41808 exit status was already read will report exitcode 255
2024-03-22 06:00:05,667 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 41791 exit status was already read will report exitcode 255
2024-03-22 06:00:05,689 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 41812 exit status was already read will report exitcode 255
2024-03-22 06:00:05,982 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:50558'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50558>: Stream is closed
2024-03-22 06:00:05,982 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:50556'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50556>: Stream is closed
2024-03-22 06:00:05,983 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:50546'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50546>: Stream is closed
2024-03-22 06:00:05,984 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:50536'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50536>: Stream is closed
2024-03-22 06:00:05,984 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:50532'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50532>: Stream is closed
2024-03-22 06:00:05,984 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:50516'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50516>: Stream is closed
2024-03-22 06:00:05,985 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:50502'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50502>: Stream is closed
2024-03-22 06:00:20,088 - distributed.scheduler - INFO - Remove client Client-6b792e88-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:20,089 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50574; closing.
2024-03-22 06:00:20,089 - distributed.scheduler - INFO - Remove client Client-6b792e88-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:20,090 - distributed.scheduler - INFO - Close client connection: Client-6b792e88-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:20,091 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:00:20,091 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:00:20,092 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:00:20,093 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:00:20,094 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-03-22 06:00:22,283 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:22,288 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 06:00:22,291 - distributed.scheduler - INFO - State start
2024-03-22 06:00:22,293 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nd_7tvq7', purging
2024-03-22 06:00:22,293 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-md3utyl9', purging
2024-03-22 06:00:22,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-y49__73c', purging
2024-03-22 06:00:22,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vij0re9t', purging
2024-03-22 06:00:22,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-p6q71che', purging
2024-03-22 06:00:22,294 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-avnhnw8c', purging
2024-03-22 06:00:22,295 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-b4u11aea', purging
2024-03-22 06:00:22,316 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:22,317 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:00:22,318 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 06:00:22,318 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:00:22,395 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43049'
2024-03-22 06:00:23,110 - distributed.scheduler - INFO - Receive client connection: Client-771aaf31-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:23,123 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35366
2024-03-22 06:00:24,187 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:24,187 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:24,728 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:24,729 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46715
2024-03-22 06:00:24,730 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46715
2024-03-22 06:00:24,730 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-03-22 06:00:24,730 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:24,730 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:24,730 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:24,730 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:00:24,730 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-apz3207x
2024-03-22 06:00:24,731 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee480d6a-abcf-41b3-ba0f-ee92fd205f1c
2024-03-22 06:00:24,731 - distributed.worker - INFO - Starting Worker plugin PreImport-b4e9806a-c58e-4ef6-b93a-41b2339adb41
2024-03-22 06:00:24,731 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fbbb8066-75cc-4275-b655-d8aaf5837af3
2024-03-22 06:00:24,731 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:24,783 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46715', status: init, memory: 0, processing: 0>
2024-03-22 06:00:24,784 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46715
2024-03-22 06:00:24,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35384
2024-03-22 06:00:24,785 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:00:24,786 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:00:24,786 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:24,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:00:24,856 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:00:24,858 - distributed.scheduler - INFO - Remove client Client-771aaf31-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:24,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35366; closing.
2024-03-22 06:00:24,859 - distributed.scheduler - INFO - Remove client Client-771aaf31-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:24,859 - distributed.scheduler - INFO - Close client connection: Client-771aaf31-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:24,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43049'. Reason: nanny-close
2024-03-22 06:00:24,860 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:00:24,861 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46715. Reason: nanny-close
2024-03-22 06:00:24,863 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35384; closing.
2024-03-22 06:00:24,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:00:24,863 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46715', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087224.8639007')
2024-03-22 06:00:24,864 - distributed.scheduler - INFO - Lost all workers
2024-03-22 06:00:24,865 - distributed.nanny - INFO - Worker closed
2024-03-22 06:00:25,475 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:00:25,475 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:00:25,476 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:00:25,477 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:00:25,477 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-03-22 06:00:29,566 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:29,571 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 06:00:29,574 - distributed.scheduler - INFO - State start
2024-03-22 06:00:29,596 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:29,597 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:00:29,597 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 06:00:29,598 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:00:29,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46843'
2024-03-22 06:00:30,676 - distributed.scheduler - INFO - Receive client connection: Client-7b75e1ba-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:30,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47778
2024-03-22 06:00:31,460 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:31,460 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:31,978 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:31,980 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44711
2024-03-22 06:00:31,980 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44711
2024-03-22 06:00:31,980 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37137
2024-03-22 06:00:31,980 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:31,980 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:31,980 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:31,981 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:00:31,981 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2ln_1l7u
2024-03-22 06:00:31,981 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-06c81088-f495-41f4-aacf-d6e0b2d901a6
2024-03-22 06:00:31,981 - distributed.worker - INFO - Starting Worker plugin PreImport-d8704d3e-b4cb-4a51-8cfb-264ff6bcc690
2024-03-22 06:00:31,982 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cb96e053-6a21-4ede-8724-7a67ada19bac
2024-03-22 06:00:31,983 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:32,033 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44711', status: init, memory: 0, processing: 0>
2024-03-22 06:00:32,034 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44711
2024-03-22 06:00:32,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47796
2024-03-22 06:00:32,035 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:00:32,036 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:00:32,036 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:32,038 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:00:32,114 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:00:32,116 - distributed.scheduler - INFO - Remove client Client-7b75e1ba-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:32,117 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47778; closing.
2024-03-22 06:00:32,117 - distributed.scheduler - INFO - Remove client Client-7b75e1ba-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:32,117 - distributed.scheduler - INFO - Close client connection: Client-7b75e1ba-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:32,118 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46843'. Reason: nanny-close
2024-03-22 06:00:32,118 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:00:32,120 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44711. Reason: nanny-close
2024-03-22 06:00:32,121 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-22 06:00:32,122 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47796; closing.
2024-03-22 06:00:32,122 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44711', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087232.122238')
2024-03-22 06:00:32,122 - distributed.scheduler - INFO - Lost all workers
2024-03-22 06:00:32,123 - distributed.nanny - INFO - Worker closed
2024-03-22 06:00:32,683 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:00:32,683 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:00:32,684 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:00:32,685 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:00:32,685 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-03-22 06:00:34,791 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:34,795 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 06:00:34,798 - distributed.scheduler - INFO - State start
2024-03-22 06:00:34,819 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:34,820 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:00:34,820 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 06:00:34,820 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:00:37,297 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:47808'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47808>: Stream is closed
2024-03-22 06:00:37,558 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:00:37,558 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:00:37,558 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:00:37,559 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:00:37,559 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-03-22 06:00:39,576 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:39,581 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 06:00:39,584 - distributed.scheduler - INFO - State start
2024-03-22 06:00:39,604 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:39,605 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-22 06:00:39,605 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 06:00:39,605 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:00:39,697 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37699'
2024-03-22 06:00:41,424 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:41,424 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:41,428 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:41,429 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38601
2024-03-22 06:00:41,429 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38601
2024-03-22 06:00:41,429 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45349
2024-03-22 06:00:41,429 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-22 06:00:41,429 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:41,429 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:41,429 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:00:41,429 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-6og4_5um
2024-03-22 06:00:41,430 - distributed.worker - INFO - Starting Worker plugin PreImport-489761e4-46a2-4b2b-9a0b-7bec3592c397
2024-03-22 06:00:41,430 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58ddece2-8281-460f-acba-b240e4c6cd99
2024-03-22 06:00:41,430 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1cb31c14-3aab-4da0-b25c-f141a178146d
2024-03-22 06:00:41,430 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:41,478 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38601', status: init, memory: 0, processing: 0>
2024-03-22 06:00:41,489 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38601
2024-03-22 06:00:41,489 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35262
2024-03-22 06:00:41,490 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:00:41,491 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-22 06:00:41,491 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:41,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-22 06:00:42,307 - distributed.scheduler - INFO - Receive client connection: Client-817069ea-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:42,308 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35274
2024-03-22 06:00:42,314 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:00:42,320 - distributed.scheduler - INFO - Remove client Client-817069ea-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:42,321 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35274; closing.
2024-03-22 06:00:42,321 - distributed.scheduler - INFO - Remove client Client-817069ea-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:42,321 - distributed.scheduler - INFO - Close client connection: Client-817069ea-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:42,322 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37699'. Reason: nanny-close
2024-03-22 06:00:42,322 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-22 06:00:42,323 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38601. Reason: nanny-close
2024-03-22 06:00:42,325 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35262; closing.
2024-03-22 06:00:42,325 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-22 06:00:42,325 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38601', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087242.3256624')
2024-03-22 06:00:42,325 - distributed.scheduler - INFO - Lost all workers
2024-03-22 06:00:42,326 - distributed.nanny - INFO - Worker closed
2024-03-22 06:00:42,787 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:00:42,787 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:00:42,788 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:00:42,789 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-22 06:00:42,789 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-03-22 06:00:44,904 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:44,908 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 06:00:44,912 - distributed.scheduler - INFO - State start
2024-03-22 06:00:44,933 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:00:44,934 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:00:44,934 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 06:00:44,935 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:00:45,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35265'
2024-03-22 06:00:45,040 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37961'
2024-03-22 06:00:45,048 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41451'
2024-03-22 06:00:45,064 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35755'
2024-03-22 06:00:45,066 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33015'
2024-03-22 06:00:45,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44397'
2024-03-22 06:00:45,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33549'
2024-03-22 06:00:45,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39719'
2024-03-22 06:00:47,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:47,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:47,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:47,129 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:47,134 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:47,135 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:47,136 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34099
2024-03-22 06:00:47,136 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34099
2024-03-22 06:00:47,136 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32977
2024-03-22 06:00:47,136 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:47,136 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:47,136 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:47,136 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:47,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4e96pbj8
2024-03-22 06:00:47,136 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44963
2024-03-22 06:00:47,136 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44963
2024-03-22 06:00:47,136 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33905
2024-03-22 06:00:47,136 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:47,136 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:47,136 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:47,136 - distributed.worker - INFO - Starting Worker plugin PreImport-a91f1df7-7e1d-47d2-81ef-776d21240deb
2024-03-22 06:00:47,136 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:47,137 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e4ab526-aa3f-4ff2-ad8e-fd6558239861
2024-03-22 06:00:47,137 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9i1qqs32
2024-03-22 06:00:47,137 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-21b81b5a-586f-4646-aef5-8182d4572e0e
2024-03-22 06:00:47,137 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ba3f6dc8-75e2-4746-970b-08b42f700eff
2024-03-22 06:00:47,152 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:47,152 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:47,157 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:47,158 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38351
2024-03-22 06:00:47,158 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38351
2024-03-22 06:00:47,158 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43181
2024-03-22 06:00:47,158 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:47,158 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:47,158 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:47,158 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:47,158 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7e_n84s4
2024-03-22 06:00:47,159 - distributed.worker - INFO - Starting Worker plugin PreImport-08bb6c58-a774-4996-b655-29c791b542b6
2024-03-22 06:00:47,159 - distributed.worker - INFO - Starting Worker plugin RMMSetup-79ce904b-1637-44ab-a086-bcd3ef5a132b
2024-03-22 06:00:47,171 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:47,171 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:47,175 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:47,176 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34991
2024-03-22 06:00:47,176 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34991
2024-03-22 06:00:47,176 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42199
2024-03-22 06:00:47,177 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:47,177 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:47,177 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:47,177 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:47,177 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2dbz9e6_
2024-03-22 06:00:47,177 - distributed.worker - INFO - Starting Worker plugin PreImport-c9cdcdbc-813b-4d83-b175-14c22f22e757
2024-03-22 06:00:47,177 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e690e382-52ac-4160-80b4-1407cda148f8
2024-03-22 06:00:47,177 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7b562f57-bbe0-4c06-aefe-d5a12433a7c8
2024-03-22 06:00:47,183 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:47,183 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:47,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:47,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:47,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:47,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:47,190 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:47,190 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:00:47,190 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:00:47,191 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46883
2024-03-22 06:00:47,191 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46883
2024-03-22 06:00:47,191 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41397
2024-03-22 06:00:47,191 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:47,191 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:47,191 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:47,192 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:47,192 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kexxhmbi
2024-03-22 06:00:47,192 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72221340-6ac7-4489-b721-4f9847a237d9
2024-03-22 06:00:47,192 - distributed.worker - INFO - Starting Worker plugin PreImport-76d91249-3fcd-4c78-9ef8-775bee1e90c3
2024-03-22 06:00:47,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-96dda00e-cab0-4f8a-956f-9fd40dcb5e11
2024-03-22 06:00:47,193 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:47,194 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41711
2024-03-22 06:00:47,194 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41711
2024-03-22 06:00:47,194 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34317
2024-03-22 06:00:47,194 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:47,195 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:47,195 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:47,195 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:47,195 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n_12gnut
2024-03-22 06:00:47,195 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e91b30b3-39c6-458e-8a09-c74a5d78a7de
2024-03-22 06:00:47,195 - distributed.worker - INFO - Starting Worker plugin PreImport-cfe46419-fe4b-4164-9159-57ba5268d2ed
2024-03-22 06:00:47,195 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70542797-c705-437d-a712-fd2f8cf77c83
2024-03-22 06:00:47,196 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:47,197 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38427
2024-03-22 06:00:47,197 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38427
2024-03-22 06:00:47,197 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34211
2024-03-22 06:00:47,197 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:47,197 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:00:47,197 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:47,197 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:47,197 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:47,197 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ishs1vp
2024-03-22 06:00:47,198 - distributed.worker - INFO - Starting Worker plugin PreImport-8ec15ed7-f880-4d70-9c4f-1d6026a38440
2024-03-22 06:00:47,198 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe25b48a-9a99-44d5-afa6-edf2a9de9da6
2024-03-22 06:00:47,198 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42323
2024-03-22 06:00:47,199 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42323
2024-03-22 06:00:47,199 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45263
2024-03-22 06:00:47,199 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:00:47,199 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:47,199 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:00:47,199 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:00:47,199 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lij6vcsa
2024-03-22 06:00:47,199 - distributed.worker - INFO - Starting Worker plugin PreImport-ec192055-1957-4180-84c6-289985b522d5
2024-03-22 06:00:47,199 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b3780cb-c06f-4c40-9a9e-1219ceddaa94
2024-03-22 06:00:47,199 - distributed.worker - INFO - Starting Worker plugin RMMSetup-41d461e0-e290-4b2d-aa1f-2dda53da2010
2024-03-22 06:00:47,200 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9df6c72c-0745-48a7-b81c-00166a1fc34e
2024-03-22 06:00:48,282 - distributed.scheduler - INFO - Receive client connection: Client-848baedb-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:00:48,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54578
2024-03-22 06:00:49,883 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ab85837-4ca9-4ecd-955f-b01615613bbb
2024-03-22 06:00:49,884 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,888 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de3cbb7b-110e-42c5-87cc-239d76f84b5c
2024-03-22 06:00:49,892 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,913 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,922 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34099', status: init, memory: 0, processing: 0>
2024-03-22 06:00:49,924 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34099
2024-03-22 06:00:49,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54602
2024-03-22 06:00:49,922 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:00:49,925 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38351', status: init, memory: 0, processing: 0>
2024-03-22 06:00:49,925 - distributed.worker - INFO - Starting Worker plugin PreImport-595e9205-f1a6-45ca-9b12-374e22757120
2024-03-22 06:00:49,925 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38351
2024-03-22 06:00:49,925 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54618
2024-03-22 06:00:49,926 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44963. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 06:00:49,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:00:49,926 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:00:49,927 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:00:49,928 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:00:49,929 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,929 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:00:49,929 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,930 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,931 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:00:49,932 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:00:49,934 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:00:49,936 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34991', status: init, memory: 0, processing: 0>
2024-03-22 06:00:49,936 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34991
2024-03-22 06:00:49,936 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54336
2024-03-22 06:00:49,937 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:00:49,938 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:00:49,938 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:00:49,951 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42323', status: init, memory: 0, processing: 0>
2024-03-22 06:00:49,952 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42323
2024-03-22 06:00:49,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54346
2024-03-22 06:00:49,952 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,953 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:00:49,954 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:00:49,954 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,951 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:00:49,955 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35265'. Reason: nanny-instantiate-failed
2024-03-22 06:00:49,955 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:00:49,956 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 06:00:49,957 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,965 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,975 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38427', status: init, memory: 0, processing: 0>
2024-03-22 06:00:49,975 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38427
2024-03-22 06:00:49,975 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54348
2024-03-22 06:00:49,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:00:49,977 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41711', status: init, memory: 0, processing: 0>
2024-03-22 06:00:49,977 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41711
2024-03-22 06:00:49,978 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54352
2024-03-22 06:00:49,978 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:00:49,978 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,979 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:00:49,979 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:00:49,979 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,980 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:00:49,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:00:49,985 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46883', status: init, memory: 0, processing: 0>
2024-03-22 06:00:49,986 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46883
2024-03-22 06:00:49,986 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54358
2024-03-22 06:00:49,987 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:00:49,988 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-22 06:00:49,988 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:00:49,989 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-22 06:00:50,008 - distributed.nanny - INFO - Worker process 42859 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:00:50,010 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:54502'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54502>: Stream is closed
2024-03-22 06:00:50,014 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42890 parent=42691 started daemon>
2024-03-22 06:00:50,015 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42886 parent=42691 started daemon>
2024-03-22 06:00:50,015 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42882 parent=42691 started daemon>
2024-03-22 06:00:50,015 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42879 parent=42691 started daemon>
2024-03-22 06:00:50,015 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42874 parent=42691 started daemon>
2024-03-22 06:00:50,016 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42869 parent=42691 started daemon>
2024-03-22 06:00:50,016 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42864 parent=42691 started daemon>
2024-03-22 06:00:50,040 - distributed.core - INFO - Connection to tcp://127.0.0.1:54358 has been closed.
2024-03-22 06:00:50,040 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46883', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087250.0406303')
2024-03-22 06:00:50,041 - distributed.core - INFO - Connection to tcp://127.0.0.1:54352 has been closed.
2024-03-22 06:00:50,041 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41711', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087250.0418665')
2024-03-22 06:00:50,042 - distributed.core - INFO - Connection to tcp://127.0.0.1:54346 has been closed.
2024-03-22 06:00:50,042 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42323', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087250.0428784')
2024-03-22 06:00:50,043 - distributed.core - INFO - Connection to tcp://127.0.0.1:54336 has been closed.
2024-03-22 06:00:50,043 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34991', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087250.043377')
2024-03-22 06:00:50,043 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54336>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54336>: Stream is closed
2024-03-22 06:00:50,044 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54352>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-22 06:00:50,045 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54346>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54346>: Stream is closed
2024-03-22 06:00:50,046 - distributed.core - INFO - Connection to tcp://127.0.0.1:54348 has been closed.
2024-03-22 06:00:50,046 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38427', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087250.0461593')
2024-03-22 06:00:50,046 - distributed.core - INFO - Connection to tcp://127.0.0.1:54602 has been closed.
2024-03-22 06:00:50,046 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34099', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087250.0465462')
2024-03-22 06:00:50,046 - distributed.core - INFO - Connection to tcp://127.0.0.1:54618 has been closed.
2024-03-22 06:00:50,046 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38351', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711087250.046893')
2024-03-22 06:00:50,047 - distributed.scheduler - INFO - Lost all workers
2024-03-22 06:00:50,109 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 42879 exit status was already read will report exitcode 255
2024-03-22 06:00:50,164 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 42874 exit status was already read will report exitcode 255
2024-03-22 06:00:50,549 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:54528'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54528>: Stream is closed
2024-03-22 06:01:04,300 - distributed.scheduler - INFO - Remove client Client-848baedb-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:04,301 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54578; closing.
2024-03-22 06:01:04,301 - distributed.scheduler - INFO - Remove client Client-848baedb-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:04,302 - distributed.scheduler - INFO - Close client connection: Client-848baedb-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:04,302 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:01:04,303 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:01:04,303 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:01:04,304 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:01:04,305 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-03-22 06:01:06,473 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:01:06,478 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 06:01:06,481 - distributed.scheduler - INFO - State start
2024-03-22 06:01:06,483 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4e96pbj8', purging
2024-03-22 06:01:06,483 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5ishs1vp', purging
2024-03-22 06:01:06,484 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7e_n84s4', purging
2024-03-22 06:01:06,484 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2dbz9e6_', purging
2024-03-22 06:01:06,485 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-kexxhmbi', purging
2024-03-22 06:01:06,485 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-n_12gnut', purging
2024-03-22 06:01:06,486 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-lij6vcsa', purging
2024-03-22 06:01:06,506 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:01:06,507 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:01:06,508 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 06:01:06,508 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:01:06,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44531'
2024-03-22 06:01:07,622 - distributed.scheduler - INFO - Receive client connection: Client-916a0e5b-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:07,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40374
2024-03-22 06:01:08,205 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:01:08,205 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:01:08,209 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:01:08,210 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33811
2024-03-22 06:01:08,210 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33811
2024-03-22 06:01:08,210 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46595
2024-03-22 06:01:08,210 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:01:08,210 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:01:08,210 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:01:08,211 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:01:08,211 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-weedw0ck
2024-03-22 06:01:08,211 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-44aabe5a-ae44-4ad9-befb-7de65e09976e
2024-03-22 06:01:08,211 - distributed.worker - INFO - Starting Worker plugin PreImport-81c59be0-826f-45c0-962f-a50ccc3b40a5
2024-03-22 06:01:08,211 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a0c4de3-9f22-4262-88c2-73909cf9ac4b
2024-03-22 06:01:08,523 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:01:08,524 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33811. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 06:01:08,524 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:01:08,525 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:01:08,551 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:01:08,554 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44531'. Reason: nanny-instantiate-failed
2024-03-22 06:01:08,554 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 06:01:08,599 - distributed.nanny - INFO - Worker process 43143 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:01:08,601 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:40358'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40358>: Stream is closed
2024-03-22 06:01:17,675 - distributed.scheduler - INFO - Remove client Client-916a0e5b-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:17,675 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40374; closing.
2024-03-22 06:01:17,676 - distributed.scheduler - INFO - Remove client Client-916a0e5b-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:17,676 - distributed.scheduler - INFO - Close client connection: Client-916a0e5b-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:17,677 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:01:17,677 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:01:17,678 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:01:17,679 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:01:17,679 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-03-22 06:01:19,870 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:01:19,874 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-22 06:01:19,877 - distributed.scheduler - INFO - State start
2024-03-22 06:01:19,897 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-22 06:01:19,897 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-22 06:01:19,898 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-22 06:01:19,898 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-22 06:01:19,985 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36201'
2024-03-22 06:01:21,254 - distributed.scheduler - INFO - Receive client connection: Client-9962ce58-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:21,265 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45994
2024-03-22 06:01:21,712 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:01:21,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:01:21,716 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:01:21,717 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37601
2024-03-22 06:01:21,717 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37601
2024-03-22 06:01:21,717 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38377
2024-03-22 06:01:21,717 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-22 06:01:21,717 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:01:21,717 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:01:21,718 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:01:21,718 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2o3jyj6c
2024-03-22 06:01:21,718 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9dc6d2f7-fa1d-4171-81e8-99eae7736c76
2024-03-22 06:01:21,718 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd140d6d-1243-43ad-a708-23f2d592d229
2024-03-22 06:01:22,051 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:01:22,052 - distributed.worker - INFO - Starting Worker plugin PreImport-4b98199a-6baf-4e02-ac85-09c045d9c669
2024-03-22 06:01:22,052 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37601. Reason: failure-to-start-<class 'MemoryError'>
2024-03-22 06:01:22,052 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:01:22,054 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:01:22,061 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:01:22,064 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36201'. Reason: nanny-instantiate-failed
2024-03-22 06:01:22,064 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-22 06:01:22,110 - distributed.nanny - INFO - Worker process 43327 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 209, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-22 06:01:22,111 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:45988'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4442, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45988>: Stream is closed
2024-03-22 06:01:31,283 - distributed.scheduler - INFO - Remove client Client-9962ce58-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:31,283 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45994; closing.
2024-03-22 06:01:31,283 - distributed.scheduler - INFO - Remove client Client-9962ce58-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:31,284 - distributed.scheduler - INFO - Close client connection: Client-9962ce58-e811-11ee-9c8b-d8c49764f6bb
2024-03-22 06:01:31,284 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-22 06:01:31,285 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-22 06:01:31,285 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-22 06:01:31,286 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-22 06:01:31,286 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] SKIPPED (could ...)
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] 2024-03-22 06:02:31,155 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:02:31,159 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:02:31,181 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:02:31,285 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fac652a70a0>>, <Task finished name='Task-41' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-839' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-850' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42161 instead
  warnings.warn(
2024-03-22 06:02:44,472 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:02:44,475 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:02:44,502 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:02:44,610 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f24fb81b070>>, <Task finished name='Task-41' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-715' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-728' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:02:45,539 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1564, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1674, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1676, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33863 instead
  warnings.warn(
2024-03-22 06:02:55,075 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:02:55,078 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:02:55,115 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-22 06:02:55,765 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f7dcc0a4e20>>, <Task finished name='Task-39' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-619' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-630' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers SKIPPED (h...)
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] SKIPPED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43679 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39671 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42889 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34291 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-03-22 06:04:26,136 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:26,136 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:26,140 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:26,141 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:26,155 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:26,155 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:26,221 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:26,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:26,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:26,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:26,301 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:26,301 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:26,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:26,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:26,353 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:04:26,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:04:26,781 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:26,782 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33271
2024-03-22 06:04:26,782 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33271
2024-03-22 06:04:26,782 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37075
2024-03-22 06:04:26,782 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,782 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,782 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:26,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j996fulb
2024-03-22 06:04:26,782 - distributed.worker - INFO - Starting Worker plugin PreImport-6283b5cd-2abf-4736-83c8-5631b1894504
2024-03-22 06:04:26,782 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bde94bbe-6db8-46ce-83d1-398ecd3cc5ba
2024-03-22 06:04:26,783 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45e70cd1-5008-4c8f-90ce-ca4227350658
2024-03-22 06:04:26,783 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,801 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:26,802 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40261
2024-03-22 06:04:26,802 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40261
2024-03-22 06:04:26,802 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40245
2024-03-22 06:04:26,802 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,802 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,802 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:26,802 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o7w2njq2
2024-03-22 06:04:26,802 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e6e848c8-3b6c-4495-9394-621b6db843b8
2024-03-22 06:04:26,803 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8814a354-85cb-48dc-9ff0-586982487aa5
2024-03-22 06:04:26,803 - distributed.worker - INFO - Starting Worker plugin PreImport-353b3be0-5b30-4616-9cf3-a5feefcafd5b
2024-03-22 06:04:26,803 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,807 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:26,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44351
2024-03-22 06:04:26,808 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44351
2024-03-22 06:04:26,808 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44749
2024-03-22 06:04:26,808 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,808 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,808 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:26,808 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-20fa5t2s
2024-03-22 06:04:26,809 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9e5c828c-c452-44d3-a0b3-6bb9a9ee547d
2024-03-22 06:04:26,809 - distributed.worker - INFO - Starting Worker plugin PreImport-0a704667-e9b5-4eb4-89ae-830c9050683f
2024-03-22 06:04:26,809 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12cd8ff5-8d23-49ac-b6fb-07a48d595541
2024-03-22 06:04:26,809 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:26,854 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,854 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,855 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45165
2024-03-22 06:04:26,865 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:26,866 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40131
2024-03-22 06:04:26,866 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40131
2024-03-22 06:04:26,866 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38621
2024-03-22 06:04:26,866 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,866 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,866 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:26,866 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8l0y5lh2
2024-03-22 06:04:26,867 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c95dd31-965b-4987-9ea1-d0d433f5ed6d
2024-03-22 06:04:26,867 - distributed.worker - INFO - Starting Worker plugin PreImport-43ec2918-e7ce-4b2b-88e0-3469ce81d0c6
2024-03-22 06:04:26,867 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f224be77-5c7f-4d53-a3a6-c8720d83a6df
2024-03-22 06:04:26,867 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,891 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:26,892 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38733
2024-03-22 06:04:26,892 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38733
2024-03-22 06:04:26,892 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43065
2024-03-22 06:04:26,892 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,892 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,893 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:26,893 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ue4itv45
2024-03-22 06:04:26,893 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8406d09e-2af9-42b3-91f3-be14ade5019e
2024-03-22 06:04:26,893 - distributed.worker - INFO - Starting Worker plugin PreImport-8991dafe-fe21-4ba3-a56f-56f39c42f83c
2024-03-22 06:04:26,893 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d60c93f-4e0f-4ac5-afd2-8774e618b64a
2024-03-22 06:04:26,893 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,906 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:26,907 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,907 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,908 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45165
2024-03-22 06:04:26,909 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:26,910 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,910 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,912 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45165
2024-03-22 06:04:26,935 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:26,935 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35149
2024-03-22 06:04:26,936 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35149
2024-03-22 06:04:26,936 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38085
2024-03-22 06:04:26,936 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,936 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,936 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:26,936 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1gscjgdv
2024-03-22 06:04:26,936 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0b8e000c-eef5-465a-ad36-1fb176ff25c0
2024-03-22 06:04:26,936 - distributed.worker - INFO - Starting Worker plugin PreImport-93cc563f-0d5f-466d-b84e-273ee88e2640
2024-03-22 06:04:26,937 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1f44c063-2fdf-4cd8-9e8b-26cbf2369fa6
2024-03-22 06:04:26,937 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,940 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:26,941 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44193
2024-03-22 06:04:26,941 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44193
2024-03-22 06:04:26,941 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40917
2024-03-22 06:04:26,941 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,941 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,941 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:26,941 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bwum96i8
2024-03-22 06:04:26,941 - distributed.worker - INFO - Starting Worker plugin RMMSetup-db9241e5-01de-451a-97b2-c1463a281e9c
2024-03-22 06:04:26,941 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c352f091-5b0f-453a-a4f8-8513765b68e4
2024-03-22 06:04:26,942 - distributed.worker - INFO - Starting Worker plugin PreImport-33ef639c-8629-4e38-9df2-26f21de772c5
2024-03-22 06:04:26,942 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,959 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:26,960 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,960 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45165
2024-03-22 06:04:26,973 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:04:26,974 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33275
2024-03-22 06:04:26,974 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33275
2024-03-22 06:04:26,974 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37871
2024-03-22 06:04:26,974 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,974 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,974 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:04:26,974 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n3janvfg
2024-03-22 06:04:26,975 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3f3d3dd2-49d7-4644-ac1d-cc466f64756b
2024-03-22 06:04:26,975 - distributed.worker - INFO - Starting Worker plugin PreImport-5e0689d2-4779-41d3-b6ae-ef8c5b146453
2024-03-22 06:04:26,975 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d74f4289-8683-447e-8833-9290462bd915
2024-03-22 06:04:26,975 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,984 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:26,985 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45165
2024-03-22 06:04:26,985 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:26,986 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45165
2024-03-22 06:04:27,051 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:27,052 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45165
2024-03-22 06:04:27,052 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:27,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45165
2024-03-22 06:04:27,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:27,059 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45165
2024-03-22 06:04:27,059 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:27,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45165
2024-03-22 06:04:27,087 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:04:27,088 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45165
2024-03-22 06:04:27,088 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:04:27,089 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45165
2024-03-22 06:04:27,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:04:27,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:04:27,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:04:27,135 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:04:27,136 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:04:27,136 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:04:27,136 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:04:27,136 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-22 06:04:27,140 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44351. Reason: nanny-close
2024-03-22 06:04:27,141 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33271. Reason: nanny-close
2024-03-22 06:04:27,142 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40261. Reason: nanny-close
2024-03-22 06:04:27,142 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40131. Reason: nanny-close
2024-03-22 06:04:27,143 - distributed.core - INFO - Connection to tcp://127.0.0.1:45165 has been closed.
2024-03-22 06:04:27,143 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38733. Reason: nanny-close
2024-03-22 06:04:27,143 - distributed.core - INFO - Connection to tcp://127.0.0.1:45165 has been closed.
2024-03-22 06:04:27,143 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44193. Reason: nanny-close
2024-03-22 06:04:27,144 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:27,144 - distributed.core - INFO - Connection to tcp://127.0.0.1:45165 has been closed.
2024-03-22 06:04:27,144 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35149. Reason: nanny-close
2024-03-22 06:04:27,144 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:27,144 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33275. Reason: nanny-close
2024-03-22 06:04:27,145 - distributed.core - INFO - Connection to tcp://127.0.0.1:45165 has been closed.
2024-03-22 06:04:27,145 - distributed.core - INFO - Connection to tcp://127.0.0.1:45165 has been closed.
2024-03-22 06:04:27,145 - distributed.core - INFO - Connection to tcp://127.0.0.1:45165 has been closed.
2024-03-22 06:04:27,145 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:27,146 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:27,146 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:27,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:45165 has been closed.
2024-03-22 06:04:27,146 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:27,146 - distributed.core - INFO - Connection to tcp://127.0.0.1:45165 has been closed.
2024-03-22 06:04:27,147 - distributed.nanny - INFO - Worker closed
2024-03-22 06:04:27,148 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool 2024-03-22 06:04:36,919 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:04:36,924 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async 2024-03-22 06:04:47,275 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-22 06:04:47,279 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size 2024-03-22 06:04:50,728 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-22 06:04:50,735 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging 2024-03-22 06:04:57,732 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:04:57,737 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-03-22 06:05:02,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:02,753 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:02,757 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:02,758 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41849
2024-03-22 06:05:02,758 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41849
2024-03-22 06:05:02,759 - distributed.worker - INFO -           Worker name:                          0
2024-03-22 06:05:02,759 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39691
2024-03-22 06:05:02,759 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38483
2024-03-22 06:05:02,759 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:02,759 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:02,759 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-22 06:05:02,759 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1t_8w9vx
2024-03-22 06:05:02,759 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26f486e1-0045-4a8b-b0f7-19b64fe685ff
2024-03-22 06:05:02,759 - distributed.worker - INFO - Starting Worker plugin PreImport-0f876f6e-85b7-4db5-9387-b50ac8fc5604
2024-03-22 06:05:02,763 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-03-22 06:05:02,763 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae49fd29-c214-4684-a0a8-7388c550167a
2024-03-22 06:05:02,764 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41849. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-03-22 06:05:02,764 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-22 06:05:02,765 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-03-22 06:05:07,328 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:07,328 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:07,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:07,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:07,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:07,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:07,377 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:07,377 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:07,388 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:07,388 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:07,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:07,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:07,489 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:07,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:07,519 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-22 06:05:07,519 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-22 06:05:07,937 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:07,938 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41829
2024-03-22 06:05:07,938 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41829
2024-03-22 06:05:07,938 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37637
2024-03-22 06:05:07,938 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36819
2024-03-22 06:05:07,938 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:07,938 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:07,938 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:07,938 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rfkxsty3
2024-03-22 06:05:07,938 - distributed.worker - INFO - Starting Worker plugin RMMSetup-841a2502-d4ee-4dbe-b5d4-97914faf8726
2024-03-22 06:05:07,939 - distributed.worker - INFO - Starting Worker plugin PreImport-42c461ff-f083-4102-ab66-b75bfbff6b7c
2024-03-22 06:05:07,939 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c5cfb153-288a-47bd-bb4a-d3d3d856515b
2024-03-22 06:05:07,939 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:07,964 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:07,965 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39703
2024-03-22 06:05:07,965 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39703
2024-03-22 06:05:07,965 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42447
2024-03-22 06:05:07,965 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36819
2024-03-22 06:05:07,965 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:07,966 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:07,966 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:07,966 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1auvx0yx
2024-03-22 06:05:07,966 - distributed.worker - INFO - Starting Worker plugin PreImport-728ef92d-ad94-4d41-b805-2ed56fbd4df7
2024-03-22 06:05:07,966 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-404bd107-704f-45d2-b55f-dd7dfedb50a8
2024-03-22 06:05:07,966 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87b929e1-7859-48e8-b306-46f6e0dd6bda
2024-03-22 06:05:07,966 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:07,982 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:07,983 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42183
2024-03-22 06:05:07,983 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42183
2024-03-22 06:05:07,983 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37489
2024-03-22 06:05:07,983 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36819
2024-03-22 06:05:07,983 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:07,983 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:07,983 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:07,983 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wmjm3jyb
2024-03-22 06:05:07,984 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-96a6537e-f346-47aa-bed3-c37fad96e749
2024-03-22 06:05:07,985 - distributed.worker - INFO - Starting Worker plugin PreImport-5cae130f-48a3-4f9b-aae6-c9ae9658a3c1
2024-03-22 06:05:07,985 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bbbcc0ce-2cd8-476c-90d4-d2b9132b15ab
2024-03-22 06:05:07,985 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,000 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:05:08,001 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,001 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,002 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36819
2024-03-22 06:05:08,007 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:08,008 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45841
2024-03-22 06:05:08,008 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45841
2024-03-22 06:05:08,009 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35737
2024-03-22 06:05:08,009 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,009 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,009 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:08,009 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:08,009 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q0ubur_f
2024-03-22 06:05:08,009 - distributed.worker - INFO - Starting Worker plugin PreImport-8b4f7a16-1638-4594-9f55-06f754fd2571
2024-03-22 06:05:08,009 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f28f21f-f58c-4eba-ac2f-412c190c286d
2024-03-22 06:05:08,010 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9f009f1-0104-4520-9be5-ac60d1c47bd8
2024-03-22 06:05:08,010 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,044 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:05:08,044 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,044 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,046 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36819
2024-03-22 06:05:08,079 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:05:08,080 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,080 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,081 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36819
2024-03-22 06:05:08,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:05:08,093 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,093 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,093 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:08,094 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45997
2024-03-22 06:05:08,094 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45997
2024-03-22 06:05:08,094 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36855
2024-03-22 06:05:08,094 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,094 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36819
2024-03-22 06:05:08,094 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,094 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:08,094 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:08,094 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xvptntct
2024-03-22 06:05:08,095 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5be61ffb-ea70-41ec-93c4-8234e7a1f0e1
2024-03-22 06:05:08,095 - distributed.worker - INFO - Starting Worker plugin PreImport-a78994b5-a40c-405a-a003-a21b8f15fc83
2024-03-22 06:05:08,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4563d90-0deb-43ad-bc88-12ea24299117
2024-03-22 06:05:08,096 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,098 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:08,100 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46871
2024-03-22 06:05:08,100 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46871
2024-03-22 06:05:08,100 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40493
2024-03-22 06:05:08,100 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,100 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,100 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:08,100 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:08,100 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-33etmigt
2024-03-22 06:05:08,100 - distributed.worker - INFO - Starting Worker plugin PreImport-6491286e-2be7-4e1b-a5d3-6af229fbf817
2024-03-22 06:05:08,101 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fcabd8ef-7468-4b93-998d-eb888c99b381
2024-03-22 06:05:08,103 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9159dc57-0beb-4ced-8e2d-05e6be0ba5b0
2024-03-22 06:05:08,104 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,128 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:08,129 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41153
2024-03-22 06:05:08,129 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41153
2024-03-22 06:05:08,129 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38809
2024-03-22 06:05:08,129 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,130 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,130 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:08,130 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:08,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v09z9w1p
2024-03-22 06:05:08,130 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86d86f15-2a23-43ec-9731-3103cca3e492
2024-03-22 06:05:08,130 - distributed.worker - INFO - Starting Worker plugin PreImport-6b9b943e-186c-4ae4-aed6-647c1f8dbf94
2024-03-22 06:05:08,130 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ee73a29-e718-42db-be98-2122390cdf8e
2024-03-22 06:05:08,130 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,170 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-22 06:05:08,170 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40369
2024-03-22 06:05:08,171 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40369
2024-03-22 06:05:08,171 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41271
2024-03-22 06:05:08,171 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,171 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,171 - distributed.worker - INFO -               Threads:                          1
2024-03-22 06:05:08,171 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-22 06:05:08,171 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4_tn7ysg
2024-03-22 06:05:08,171 - distributed.worker - INFO - Starting Worker plugin PreImport-26d87283-76b5-49c8-8be0-8cbe9d6710a6
2024-03-22 06:05:08,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80ff8520-4352-48e8-8686-2a77c22a6b25
2024-03-22 06:05:08,172 - distributed.worker - INFO - Starting Worker plugin RMMSetup-daa1c800-ec84-469b-bfdb-18c0500be67e
2024-03-22 06:05:08,172 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,186 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:05:08,187 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,187 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36819
2024-03-22 06:05:08,199 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:05:08,199 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,199 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36819
2024-03-22 06:05:08,230 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:05:08,231 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,231 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,232 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36819
2024-03-22 06:05:08,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-22 06:05:08,247 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36819
2024-03-22 06:05:08,247 - distributed.worker - INFO - -------------------------------------------------
2024-03-22 06:05:08,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36819
2024-03-22 06:05:08,254 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41829. Reason: nanny-close
2024-03-22 06:05:08,256 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39703. Reason: nanny-close
2024-03-22 06:05:08,256 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42183. Reason: nanny-close
2024-03-22 06:05:08,256 - distributed.core - INFO - Connection to tcp://127.0.0.1:36819 has been closed.
2024-03-22 06:05:08,257 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46871. Reason: nanny-close
2024-03-22 06:05:08,258 - distributed.core - INFO - Connection to tcp://127.0.0.1:36819 has been closed.
2024-03-22 06:05:08,258 - distributed.core - INFO - Connection to tcp://127.0.0.1:36819 has been closed.
2024-03-22 06:05:08,258 - distributed.nanny - INFO - Worker closed
2024-03-22 06:05:08,259 - distributed.core - INFO - Connection to tcp://127.0.0.1:36819 has been closed.
2024-03-22 06:05:08,260 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41153. Reason: nanny-close
2024-03-22 06:05:08,260 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40369. Reason: nanny-close
2024-03-22 06:05:08,260 - distributed.nanny - INFO - Worker closed
2024-03-22 06:05:08,260 - distributed.nanny - INFO - Worker closed
2024-03-22 06:05:08,260 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45997. Reason: nanny-close
2024-03-22 06:05:08,260 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45841. Reason: nanny-close
2024-03-22 06:05:08,261 - distributed.nanny - INFO - Worker closed
2024-03-22 06:05:08,262 - distributed.core - INFO - Connection to tcp://127.0.0.1:36819 has been closed.
2024-03-22 06:05:08,262 - distributed.core - INFO - Connection to tcp://127.0.0.1:36819 has been closed.
2024-03-22 06:05:08,262 - distributed.core - INFO - Connection to tcp://127.0.0.1:36819 has been closed.
2024-03-22 06:05:08,262 - distributed.core - INFO - Connection to tcp://127.0.0.1:36819 has been closed.
2024-03-22 06:05:08,263 - distributed.nanny - INFO - Worker closed
2024-03-22 06:05:08,263 - distributed.nanny - INFO - Worker closed
2024-03-22 06:05:08,264 - distributed.nanny - INFO - Worker closed
2024-03-22 06:05:08,264 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations 2024-03-22 06:05:14,692 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:05:14,696 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration 2024-03-22 06:05:18,340 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:05:18,342 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-03-22 06:05:20,594 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:05:20,596 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] 2024-03-22 06:07:09,086 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:07:09,105 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] 2024-03-22 06:07:15,641 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:07:15,649 - distributed.worker - WARNING - Scheduler was unaware of this worker; shutting down.
2024-03-22 06:07:15,651 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x153c411c14c0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:07:17,654 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] 2024-03-22 06:07:38,462 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:07:38,470 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x1548dd7344c0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:07:40,474 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] 2024-03-22 06:08:06,936 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:08:06,943 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x1473075171f0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:08:08,948 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-03-22 06:08:37,673 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:08:37,681 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x14cd44e31490>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:08:39,684 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-03-22 06:09:08,491 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:09:08,498 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x1552e70114c0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:09:10,502 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-03-22 06:09:38,784 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:09:38,791 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x145b17d754c0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:09:40,795 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-03-22 06:10:09,575 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:10:09,582 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x14ef7ef7c4c0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1302, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-22 06:10:11,586 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-03-22 06:10:44,547 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-22 06:10:44,550 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1476, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1876, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 9 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
