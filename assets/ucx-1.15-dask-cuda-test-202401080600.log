============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-08 06:25:18,432 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:18,438 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:25:18,441 - distributed.scheduler - INFO - State start
2024-01-08 06:25:18,463 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:18,464 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-08 06:25:18,465 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:25:18,465 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:25:18,472 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42293'
2024-01-08 06:25:18,493 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40709'
2024-01-08 06:25:18,498 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39021'
2024-01-08 06:25:18,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40919'
2024-01-08 06:25:20,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:20,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:20,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:20,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:20,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:20,284 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:20,286 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:20,286 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:20,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:20,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:20,288 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:20,288 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34765
2024-01-08 06:25:20,288 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33927
2024-01-08 06:25:20,288 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34765
2024-01-08 06:25:20,289 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33927
2024-01-08 06:25:20,289 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43447
2024-01-08 06:25:20,289 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36971
2024-01-08 06:25:20,289 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46619
2024-01-08 06:25:20,289 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43447
2024-01-08 06:25:20,289 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:25:20,289 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:25:20,289 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,289 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,289 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45747
2024-01-08 06:25:20,289 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:25:20,289 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:25:20,289 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:25:20,289 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:25:20,289 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:25:20,289 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-8fan5ctw
2024-01-08 06:25:20,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-9lp5w45o
2024-01-08 06:25:20,289 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:25:20,289 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:25:20,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-dzq5jz05
2024-01-08 06:25:20,289 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27e9c2bb-6e3e-4891-8955-2abc3b292a85
2024-01-08 06:25:20,289 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a73811af-7a3c-437c-adf4-dc1d8ca72f15
2024-01-08 06:25:20,289 - distributed.worker - INFO - Starting Worker plugin PreImport-4b7768b5-f359-4bac-8d1f-8bed3e568dfb
2024-01-08 06:25:20,289 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dc53dcc8-af2f-482e-bbf0-0c16ec322998
2024-01-08 06:25:20,289 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aad52d77-72b8-46b2-aac4-6180f199c3af
2024-01-08 06:25:20,289 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,289 - distributed.worker - INFO - Starting Worker plugin PreImport-f915f459-c2d4-4f83-bd73-9af3668c994e
2024-01-08 06:25:20,290 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ba8b8b8-c2ef-4dcc-aabb-714c7c8ef286
2024-01-08 06:25:20,290 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:20,290 - distributed.worker - INFO - Starting Worker plugin PreImport-2ce9cae7-aaaa-4b12-9e71-2f94ca14438f
2024-01-08 06:25:20,290 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,290 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f818106d-57b4-4815-9c62-94800dd55cf9
2024-01-08 06:25:20,290 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,291 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41647
2024-01-08 06:25:20,291 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41647
2024-01-08 06:25:20,291 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38827
2024-01-08 06:25:20,291 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:25:20,291 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,291 - distributed.worker - INFO -               Threads:                          4
2024-01-08 06:25:20,291 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-08 06:25:20,291 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-eenhizgo
2024-01-08 06:25:20,291 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f3ca90e-0f30-4565-a1b5-d354df973e6d
2024-01-08 06:25:20,292 - distributed.worker - INFO - Starting Worker plugin PreImport-c54b566f-7e42-4f12-a9bf-46a1aaf44e6f
2024-01-08 06:25:20,292 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e3bb3f0-7bba-410d-bb9f-6e50d138f890
2024-01-08 06:25:20,292 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,410 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41647', status: init, memory: 0, processing: 0>
2024-01-08 06:25:20,435 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41647
2024-01-08 06:25:20,435 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38170
2024-01-08 06:25:20,436 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:20,437 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:25:20,437 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,438 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:25:20,438 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33927', status: init, memory: 0, processing: 0>
2024-01-08 06:25:20,439 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33927
2024-01-08 06:25:20,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38134
2024-01-08 06:25:20,440 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:20,440 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43447', status: init, memory: 0, processing: 0>
2024-01-08 06:25:20,441 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:25:20,441 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,442 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43447
2024-01-08 06:25:20,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38142
2024-01-08 06:25:20,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:25:20,443 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34765', status: init, memory: 0, processing: 0>
2024-01-08 06:25:20,443 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:20,444 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:25:20,444 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,444 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34765
2024-01-08 06:25:20,444 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38154
2024-01-08 06:25:20,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:25:20,445 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:20,447 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:25:20,447 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:20,449 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:25:21,249 - distributed.scheduler - INFO - Receive client connection: Client-b03ee7d1-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:21,250 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38184
2024-01-08 06:25:21,259 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:25:21,259 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:25:21,259 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:25:21,259 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-08 06:25:21,264 - distributed.scheduler - INFO - Remove client Client-b03ee7d1-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:21,264 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38184; closing.
2024-01-08 06:25:21,264 - distributed.scheduler - INFO - Remove client Client-b03ee7d1-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:21,265 - distributed.scheduler - INFO - Close client connection: Client-b03ee7d1-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:21,266 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40919'. Reason: nanny-close
2024-01-08 06:25:21,266 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:21,266 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39021'. Reason: nanny-close
2024-01-08 06:25:21,267 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:21,267 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40709'. Reason: nanny-close
2024-01-08 06:25:21,267 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34765. Reason: nanny-close
2024-01-08 06:25:21,267 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:21,268 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42293'. Reason: nanny-close
2024-01-08 06:25:21,268 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33927. Reason: nanny-close
2024-01-08 06:25:21,268 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:21,268 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43447. Reason: nanny-close
2024-01-08 06:25:21,269 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41647. Reason: nanny-close
2024-01-08 06:25:21,269 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:25:21,270 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38154; closing.
2024-01-08 06:25:21,270 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:25:21,270 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34765', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695121.2704082')
2024-01-08 06:25:21,270 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:25:21,271 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:25:21,271 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38134; closing.
2024-01-08 06:25:21,271 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:21,271 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:21,271 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33927', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695121.2717445')
2024-01-08 06:25:21,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38170; closing.
2024-01-08 06:25:21,272 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:21,272 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:21,272 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41647', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695121.2727714')
2024-01-08 06:25:21,273 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38142; closing.
2024-01-08 06:25:21,273 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:38134>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:25:21,274 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:38170>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:38170>: Stream is closed
2024-01-08 06:25:21,275 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43447', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695121.2754486')
2024-01-08 06:25:21,275 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:25:21,981 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:25:21,982 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:25:21,982 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:25:21,983 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-08 06:25:21,984 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-08 06:25:24,148 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:24,153 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:25:24,156 - distributed.scheduler - INFO - State start
2024-01-08 06:25:24,179 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:24,180 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:25:24,181 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:25:24,181 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:25:24,292 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39767'
2024-01-08 06:25:24,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41719'
2024-01-08 06:25:24,314 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44941'
2024-01-08 06:25:24,328 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42767'
2024-01-08 06:25:24,332 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34631'
2024-01-08 06:25:24,340 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34591'
2024-01-08 06:25:24,350 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37973'
2024-01-08 06:25:24,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34377'
2024-01-08 06:25:24,579 - distributed.scheduler - INFO - Receive client connection: Client-b3b84bc1-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:24,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52638
2024-01-08 06:25:26,267 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:26,267 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:26,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:26,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:26,273 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:26,274 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41693
2024-01-08 06:25:26,274 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41693
2024-01-08 06:25:26,275 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44633
2024-01-08 06:25:26,275 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:26,275 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:26,275 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:26,275 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:26,275 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o2mo_bwy
2024-01-08 06:25:26,275 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e3c4fbd9-b85b-47e1-a6aa-113fcb2e54cc
2024-01-08 06:25:26,276 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:26,277 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44555
2024-01-08 06:25:26,277 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44555
2024-01-08 06:25:26,277 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35737
2024-01-08 06:25:26,277 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:26,277 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:26,277 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:26,277 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:26,277 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n03y995f
2024-01-08 06:25:26,278 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-89556494-4e2d-457f-845b-4db82908bd19
2024-01-08 06:25:26,278 - distributed.worker - INFO - Starting Worker plugin PreImport-1d35e31f-8da9-4ceb-b523-9d7c5449ca75
2024-01-08 06:25:26,278 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3e7d646-7019-4c37-8c3e-588fa259b1a6
2024-01-08 06:25:26,332 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:26,332 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:26,338 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:26,339 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38979
2024-01-08 06:25:26,340 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38979
2024-01-08 06:25:26,340 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37349
2024-01-08 06:25:26,340 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:26,340 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:26,340 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:26,340 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:26,340 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yufrktq5
2024-01-08 06:25:26,341 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe1c4342-e0c7-417c-a7cd-2471f3633e13
2024-01-08 06:25:26,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:26,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:26,387 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:26,392 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34579
2024-01-08 06:25:26,392 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34579
2024-01-08 06:25:26,392 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36499
2024-01-08 06:25:26,392 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:26,392 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:26,392 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:26,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:26,393 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n0q1xwjh
2024-01-08 06:25:26,393 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b1ab2eb-ddda-45cc-98f3-8395c9f20a9d
2024-01-08 06:25:26,600 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:26,600 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:26,604 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:26,606 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35343
2024-01-08 06:25:26,606 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35343
2024-01-08 06:25:26,606 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35669
2024-01-08 06:25:26,606 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:26,606 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:26,606 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:26,606 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:26,606 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ybbgh022
2024-01-08 06:25:26,606 - distributed.worker - INFO - Starting Worker plugin RMMSetup-700c201b-14e3-4a54-aabf-4ec2b6e1487f
2024-01-08 06:25:26,619 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:26,619 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:26,624 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:26,624 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:26,624 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:26,626 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39813
2024-01-08 06:25:26,626 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39813
2024-01-08 06:25:26,626 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45595
2024-01-08 06:25:26,626 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:26,626 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:26,626 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:26,626 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:26,626 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uuiln6nd
2024-01-08 06:25:26,626 - distributed.worker - INFO - Starting Worker plugin PreImport-21aafc5c-f3b9-438d-994f-af2d73baca1c
2024-01-08 06:25:26,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a6f2de9-9928-4d26-a9da-9e2bbf355993
2024-01-08 06:25:26,627 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4cb586dd-ef98-4f19-8d83-27ae4937ec3b
2024-01-08 06:25:26,628 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:26,629 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:26,630 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:26,631 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38109
2024-01-08 06:25:26,631 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38109
2024-01-08 06:25:26,631 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39981
2024-01-08 06:25:26,631 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:26,631 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:26,631 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:26,631 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:26,631 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-32hl6sz4
2024-01-08 06:25:26,632 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d95e6745-db9d-4fcf-9075-846f9f2bcd26
2024-01-08 06:25:26,634 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:26,635 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37635
2024-01-08 06:25:26,635 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37635
2024-01-08 06:25:26,636 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37191
2024-01-08 06:25:26,636 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:26,636 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:26,636 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:26,636 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:26,636 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lch4mx98
2024-01-08 06:25:26,636 - distributed.worker - INFO - Starting Worker plugin PreImport-fc99fdac-b65c-441b-81b1-66bf4d39d996
2024-01-08 06:25:26,636 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-791e7074-4af0-47b1-941f-1f3333796a6f
2024-01-08 06:25:26,637 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a0b12d6-38bd-4eef-97c4-ff9de2321261
2024-01-08 06:25:27,208 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d898a97f-df3a-470f-971d-5ba7dd9d1078
2024-01-08 06:25:27,211 - distributed.worker - INFO - Starting Worker plugin PreImport-8f79b8b6-7abf-4bef-9b36-12e81dbeb960
2024-01-08 06:25:27,212 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:27,242 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41693', status: init, memory: 0, processing: 0>
2024-01-08 06:25:27,243 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41693
2024-01-08 06:25:27,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52660
2024-01-08 06:25:27,245 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:27,246 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:27,246 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:27,248 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:28,367 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:28,401 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44555', status: init, memory: 0, processing: 0>
2024-01-08 06:25:28,402 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44555
2024-01-08 06:25:28,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52682
2024-01-08 06:25:28,404 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:28,405 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:28,405 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:28,406 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:28,720 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d61dc5e0-60b7-4434-809f-084653b4c9a8
2024-01-08 06:25:28,721 - distributed.worker - INFO - Starting Worker plugin PreImport-0f1732ab-a87f-4891-88a0-c6e20671596e
2024-01-08 06:25:28,722 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:28,758 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38979', status: init, memory: 0, processing: 0>
2024-01-08 06:25:28,759 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38979
2024-01-08 06:25:28,759 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52698
2024-01-08 06:25:28,760 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:28,761 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:28,761 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:28,763 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:28,801 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-690dfef3-abec-4026-8713-cf00ab012aa9
2024-01-08 06:25:28,802 - distributed.worker - INFO - Starting Worker plugin PreImport-a9e49b40-5196-4698-97fd-471f441a9492
2024-01-08 06:25:28,802 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:28,824 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34579', status: init, memory: 0, processing: 0>
2024-01-08 06:25:28,825 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34579
2024-01-08 06:25:28,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52708
2024-01-08 06:25:28,826 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:28,827 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:28,827 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:28,828 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:29,561 - distributed.worker - INFO - Starting Worker plugin PreImport-a2a57c3d-e37e-4099-9fe4-cfecec0ec302
2024-01-08 06:25:29,562 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-948080cc-d812-49bd-a7aa-79285d707944
2024-01-08 06:25:29,563 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:29,595 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35343', status: init, memory: 0, processing: 0>
2024-01-08 06:25:29,596 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35343
2024-01-08 06:25:29,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52740
2024-01-08 06:25:29,597 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:29,598 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:29,598 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:29,600 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:29,935 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d0dbaa3c-affd-48e4-a7c4-90ae3225ae01
2024-01-08 06:25:29,936 - distributed.worker - INFO - Starting Worker plugin PreImport-1a63de1f-a9e3-4ec1-b05d-98fea2970f26
2024-01-08 06:25:29,936 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:29,960 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38109', status: init, memory: 0, processing: 0>
2024-01-08 06:25:29,960 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38109
2024-01-08 06:25:29,960 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47306
2024-01-08 06:25:29,961 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:29,962 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:29,962 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:29,964 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:30,220 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:30,228 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:30,244 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37635', status: init, memory: 0, processing: 0>
2024-01-08 06:25:30,245 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37635
2024-01-08 06:25:30,245 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47320
2024-01-08 06:25:30,246 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:30,247 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:30,247 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:30,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:30,253 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39813', status: init, memory: 0, processing: 0>
2024-01-08 06:25:30,254 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39813
2024-01-08 06:25:30,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47332
2024-01-08 06:25:30,255 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:30,256 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:30,256 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:30,257 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:30,359 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:30,359 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:30,360 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:30,360 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:30,360 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:30,360 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:30,361 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:30,361 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:30,365 - distributed.scheduler - INFO - Remove client Client-b3b84bc1-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:30,365 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52638; closing.
2024-01-08 06:25:30,365 - distributed.scheduler - INFO - Remove client Client-b3b84bc1-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:30,366 - distributed.scheduler - INFO - Close client connection: Client-b3b84bc1-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:30,366 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39767'. Reason: nanny-close
2024-01-08 06:25:30,367 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:30,368 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41719'. Reason: nanny-close
2024-01-08 06:25:30,368 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:30,368 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44941'. Reason: nanny-close
2024-01-08 06:25:30,369 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44555. Reason: nanny-close
2024-01-08 06:25:30,369 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:30,369 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42767'. Reason: nanny-close
2024-01-08 06:25:30,369 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:30,369 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34631'. Reason: nanny-close
2024-01-08 06:25:30,370 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39813. Reason: nanny-close
2024-01-08 06:25:30,370 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41693. Reason: nanny-close
2024-01-08 06:25:30,370 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:30,370 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34591'. Reason: nanny-close
2024-01-08 06:25:30,370 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38109. Reason: nanny-close
2024-01-08 06:25:30,370 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:30,370 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37973'. Reason: nanny-close
2024-01-08 06:25:30,371 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35343. Reason: nanny-close
2024-01-08 06:25:30,371 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:30,371 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:30,371 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34377'. Reason: nanny-close
2024-01-08 06:25:30,371 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38979. Reason: nanny-close
2024-01-08 06:25:30,371 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52682; closing.
2024-01-08 06:25:30,371 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:30,372 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44555', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695130.3719847')
2024-01-08 06:25:30,372 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:30,372 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37635. Reason: nanny-close
2024-01-08 06:25:30,372 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:30,372 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:30,373 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47306; closing.
2024-01-08 06:25:30,373 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47332; closing.
2024-01-08 06:25:30,373 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34579. Reason: nanny-close
2024-01-08 06:25:30,373 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:30,373 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:30,373 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:30,373 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:30,373 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:30,373 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:30,374 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38109', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695130.3741736')
2024-01-08 06:25:30,374 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39813', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695130.3745248')
2024-01-08 06:25:30,374 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:30,375 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:30,375 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52660; closing.
2024-01-08 06:25:30,375 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:30,375 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:30,375 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:30,376 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:30,375 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47306>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47306>: Stream is closed
2024-01-08 06:25:30,377 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52698; closing.
2024-01-08 06:25:30,377 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52740; closing.
2024-01-08 06:25:30,377 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41693', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695130.3778458')
2024-01-08 06:25:30,378 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38979', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695130.378667')
2024-01-08 06:25:30,379 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35343', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695130.3789814')
2024-01-08 06:25:30,379 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47320; closing.
2024-01-08 06:25:30,379 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695130.3798878')
2024-01-08 06:25:30,380 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52708; closing.
2024-01-08 06:25:30,380 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34579', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695130.380642')
2024-01-08 06:25:30,380 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:25:30,381 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52708>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:25:31,383 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:25:31,383 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:25:31,384 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:25:31,385 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:25:31,385 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-08 06:25:33,685 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:33,690 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:25:33,694 - distributed.scheduler - INFO - State start
2024-01-08 06:25:33,717 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:33,718 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:25:33,719 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:25:33,719 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:25:33,778 - distributed.scheduler - INFO - Receive client connection: Client-b95b597c-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:33,795 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47428
2024-01-08 06:25:33,897 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32863'
2024-01-08 06:25:33,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36527'
2024-01-08 06:25:33,930 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42457'
2024-01-08 06:25:33,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35085'
2024-01-08 06:25:33,942 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43195'
2024-01-08 06:25:33,952 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43811'
2024-01-08 06:25:33,961 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37349'
2024-01-08 06:25:33,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41099'
2024-01-08 06:25:35,848 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:35,848 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:35,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:35,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:35,853 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:35,853 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:35,854 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37413
2024-01-08 06:25:35,854 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44209
2024-01-08 06:25:35,854 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44209
2024-01-08 06:25:35,854 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37413
2024-01-08 06:25:35,854 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38533
2024-01-08 06:25:35,854 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35721
2024-01-08 06:25:35,854 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:35,854 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:35,854 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:35,854 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:35,854 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:35,854 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:35,854 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:35,854 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:35,854 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4by9tr6k
2024-01-08 06:25:35,854 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-614qlno1
2024-01-08 06:25:35,854 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4f9f1cf-b061-414c-a529-3bc314135f57
2024-01-08 06:25:35,854 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59b9df08-2520-4b5d-b407-2e41b276c663
2024-01-08 06:25:35,923 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:35,923 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:35,923 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:35,924 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:35,924 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:35,925 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:35,928 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:35,928 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:35,928 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:35,928 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:35,928 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40587
2024-01-08 06:25:35,928 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40587
2024-01-08 06:25:35,929 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46195
2024-01-08 06:25:35,929 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:35,929 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:35,929 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41391
2024-01-08 06:25:35,929 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:35,929 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41391
2024-01-08 06:25:35,929 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:35,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5kdp4eah
2024-01-08 06:25:35,929 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34287
2024-01-08 06:25:35,929 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:35,929 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:35,929 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:35,929 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:35,929 - distributed.worker - INFO - Starting Worker plugin RMMSetup-44650986-4b58-448f-bd77-8abf32ecbcce
2024-01-08 06:25:35,929 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:35,929 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0zv1tp9c
2024-01-08 06:25:35,929 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-10b5365e-ca00-4e68-b408-bc1ed8cb1da4
2024-01-08 06:25:35,930 - distributed.worker - INFO - Starting Worker plugin PreImport-b601057f-1d40-4d6c-a0fd-fe381df8edad
2024-01-08 06:25:35,930 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37961
2024-01-08 06:25:35,930 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72d6debe-396d-493d-8b6b-192f59b12469
2024-01-08 06:25:35,930 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37961
2024-01-08 06:25:35,930 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39817
2024-01-08 06:25:35,930 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:35,930 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:35,930 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:35,930 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:35,930 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k63_gqjq
2024-01-08 06:25:35,930 - distributed.worker - INFO - Starting Worker plugin RMMSetup-35f77cdb-b261-4b22-9b9f-fced5580025b
2024-01-08 06:25:35,932 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:35,933 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37807
2024-01-08 06:25:35,933 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37807
2024-01-08 06:25:35,933 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33771
2024-01-08 06:25:35,933 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:35,933 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:35,933 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:35,933 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:35,933 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-36w_dznz
2024-01-08 06:25:35,934 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26945bcd-4992-4ba2-9098-aad7a2d3fa20
2024-01-08 06:25:36,327 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:36,327 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:36,329 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:36,329 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:36,332 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:36,333 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34749
2024-01-08 06:25:36,333 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34749
2024-01-08 06:25:36,333 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37111
2024-01-08 06:25:36,333 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:36,333 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:36,333 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:36,333 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:36,333 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oi80m13i
2024-01-08 06:25:36,334 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aa2b3468-d2ce-4b6f-bcda-5bdfe0f5f0d4
2024-01-08 06:25:36,335 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:36,336 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34971
2024-01-08 06:25:36,336 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34971
2024-01-08 06:25:36,336 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38179
2024-01-08 06:25:36,337 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:36,337 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:36,337 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:36,337 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:36,337 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bduyb_bx
2024-01-08 06:25:36,337 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e0941db4-a5bb-4b4c-9d75-40627a7aaf30
2024-01-08 06:25:36,505 - distributed.worker - INFO - Starting Worker plugin PreImport-029d001e-c5f0-4dc6-919e-117587739e44
2024-01-08 06:25:36,506 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-740d9cf1-8195-4232-ae4c-e99c0a740de3
2024-01-08 06:25:36,506 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:36,527 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44209', status: init, memory: 0, processing: 0>
2024-01-08 06:25:36,529 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44209
2024-01-08 06:25:36,529 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47520
2024-01-08 06:25:36,530 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:36,531 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:36,531 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:36,532 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:37,704 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9821c07d-0e13-4a56-8383-ad2c3f8b7b60
2024-01-08 06:25:37,706 - distributed.worker - INFO - Starting Worker plugin PreImport-eb2d79fd-f207-48a7-a7e9-0932a5fbfefc
2024-01-08 06:25:37,707 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:37,742 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37413', status: init, memory: 0, processing: 0>
2024-01-08 06:25:37,743 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37413
2024-01-08 06:25:37,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47530
2024-01-08 06:25:37,744 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:37,745 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:37,745 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:37,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:37,968 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-25132e05-3366-4c84-804a-4291e0bb5dc8
2024-01-08 06:25:37,968 - distributed.worker - INFO - Starting Worker plugin PreImport-70ce3ba0-9111-4def-b797-a71445205dd0
2024-01-08 06:25:37,969 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:37,969 - distributed.worker - INFO - Starting Worker plugin PreImport-7e9bd45d-3415-4389-bd78-f54317dd36f8
2024-01-08 06:25:37,970 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3df96f36-b2af-450b-a002-26bbcf02c41c
2024-01-08 06:25:37,970 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:37,990 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:37,995 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37807', status: init, memory: 0, processing: 0>
2024-01-08 06:25:37,996 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37807
2024-01-08 06:25:37,996 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47546
2024-01-08 06:25:37,997 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:37,997 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37961', status: init, memory: 0, processing: 0>
2024-01-08 06:25:37,998 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37961
2024-01-08 06:25:37,998 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47552
2024-01-08 06:25:37,998 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:37,998 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:37,999 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:38,000 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:38,000 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:38,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:38,001 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:38,021 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de7f5f48-2cd4-4458-b442-3d760aa2ec11
2024-01-08 06:25:38,022 - distributed.worker - INFO - Starting Worker plugin PreImport-067da402-2b9e-4c0a-9422-767f34297741
2024-01-08 06:25:38,024 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:38,025 - distributed.worker - INFO - Starting Worker plugin PreImport-6ff490a2-db25-4632-97f9-4ea02a0bd32f
2024-01-08 06:25:38,027 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1a0f3884-ae19-4e99-8d3a-3e977ba71744
2024-01-08 06:25:38,027 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-92284709-633b-4a5c-9de9-49b21be6f1a9
2024-01-08 06:25:38,027 - distributed.worker - INFO - Starting Worker plugin PreImport-f67e231a-4925-4432-8276-9c21bf3250e5
2024-01-08 06:25:38,028 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:38,028 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:38,029 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41391', status: init, memory: 0, processing: 0>
2024-01-08 06:25:38,030 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41391
2024-01-08 06:25:38,030 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47564
2024-01-08 06:25:38,031 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:38,032 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:38,032 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:38,035 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:38,051 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34749', status: init, memory: 0, processing: 0>
2024-01-08 06:25:38,052 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34749
2024-01-08 06:25:38,052 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47586
2024-01-08 06:25:38,053 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:38,054 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:38,054 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:38,056 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:38,057 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34971', status: init, memory: 0, processing: 0>
2024-01-08 06:25:38,058 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34971
2024-01-08 06:25:38,058 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47574
2024-01-08 06:25:38,059 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:38,060 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:38,060 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:38,062 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:38,063 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40587', status: init, memory: 0, processing: 0>
2024-01-08 06:25:38,064 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40587
2024-01-08 06:25:38,064 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47594
2024-01-08 06:25:38,065 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:38,066 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:38,066 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:38,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:38,139 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:38,140 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:38,140 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:38,140 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:38,140 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:38,140 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:38,140 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:38,141 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:38,145 - distributed.scheduler - INFO - Remove client Client-b95b597c-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:38,146 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47428; closing.
2024-01-08 06:25:38,146 - distributed.scheduler - INFO - Remove client Client-b95b597c-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:38,146 - distributed.scheduler - INFO - Close client connection: Client-b95b597c-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:38,147 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32863'. Reason: nanny-close
2024-01-08 06:25:38,148 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:38,148 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36527'. Reason: nanny-close
2024-01-08 06:25:38,149 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:38,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42457'. Reason: nanny-close
2024-01-08 06:25:38,149 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:38,149 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40587. Reason: nanny-close
2024-01-08 06:25:38,149 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35085'. Reason: nanny-close
2024-01-08 06:25:38,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:38,150 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37413. Reason: nanny-close
2024-01-08 06:25:38,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43195'. Reason: nanny-close
2024-01-08 06:25:38,150 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44209. Reason: nanny-close
2024-01-08 06:25:38,150 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:38,150 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43811'. Reason: nanny-close
2024-01-08 06:25:38,150 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37807. Reason: nanny-close
2024-01-08 06:25:38,151 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:38,151 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37349'. Reason: nanny-close
2024-01-08 06:25:38,151 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:38,151 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41391. Reason: nanny-close
2024-01-08 06:25:38,151 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41099'. Reason: nanny-close
2024-01-08 06:25:38,152 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:38,152 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:38,152 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34971. Reason: nanny-close
2024-01-08 06:25:38,152 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37961. Reason: nanny-close
2024-01-08 06:25:38,152 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:38,152 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47520; closing.
2024-01-08 06:25:38,152 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:38,152 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47530; closing.
2024-01-08 06:25:38,153 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34749. Reason: nanny-close
2024-01-08 06:25:38,153 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44209', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695138.1534343')
2024-01-08 06:25:38,153 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:38,153 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:38,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:38,154 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:38,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:38,154 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:38,154 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37413', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695138.1544156')
2024-01-08 06:25:38,154 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:38,155 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47594; closing.
2024-01-08 06:25:38,155 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:38,155 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:38,155 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:38,156 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:38,156 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:38,156 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695138.1565924')
2024-01-08 06:25:38,156 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:38,157 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47546; closing.
2024-01-08 06:25:38,159 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37807', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695138.1589308')
2024-01-08 06:25:38,159 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47564; closing.
2024-01-08 06:25:38,159 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47552; closing.
2024-01-08 06:25:38,160 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47574; closing.
2024-01-08 06:25:38,160 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47546>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47546>: Stream is closed
2024-01-08 06:25:38,164 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47594>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:25:38,165 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41391', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695138.165524')
2024-01-08 06:25:38,166 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37961', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695138.166101')
2024-01-08 06:25:38,166 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34971', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695138.1666412')
2024-01-08 06:25:38,167 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47586; closing.
2024-01-08 06:25:38,167 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34749', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695138.1677508')
2024-01-08 06:25:38,168 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:25:39,264 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:25:39,264 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:25:39,265 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:25:39,266 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:25:39,267 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-08 06:25:41,548 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:41,553 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:25:41,557 - distributed.scheduler - INFO - State start
2024-01-08 06:25:41,579 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:41,580 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:25:41,580 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:25:41,581 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:25:41,748 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42053'
2024-01-08 06:25:41,771 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40965'
2024-01-08 06:25:41,786 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46791'
2024-01-08 06:25:41,796 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33725'
2024-01-08 06:25:41,799 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43911'
2024-01-08 06:25:41,808 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36669'
2024-01-08 06:25:41,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32909'
2024-01-08 06:25:41,825 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33731'
2024-01-08 06:25:42,517 - distributed.scheduler - INFO - Receive client connection: Client-be123df9-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:42,534 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54776
2024-01-08 06:25:43,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:43,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:43,646 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:43,647 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33947
2024-01-08 06:25:43,647 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33947
2024-01-08 06:25:43,647 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37877
2024-01-08 06:25:43,647 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:43,647 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:43,647 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:43,648 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:43,648 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dgtn6p7u
2024-01-08 06:25:43,648 - distributed.worker - INFO - Starting Worker plugin RMMSetup-68ff1e9d-575d-4481-b604-48b57f0dccbd
2024-01-08 06:25:43,668 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:43,668 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:43,672 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:43,673 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38571
2024-01-08 06:25:43,673 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38571
2024-01-08 06:25:43,673 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39801
2024-01-08 06:25:43,673 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:43,674 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:43,674 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:43,674 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:43,674 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pe3rdjr7
2024-01-08 06:25:43,674 - distributed.worker - INFO - Starting Worker plugin PreImport-6da84f30-ef1d-4f26-b5fa-4ce71053d356
2024-01-08 06:25:43,674 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6458b2d8-b0c4-42eb-8980-7d5ff71786ae
2024-01-08 06:25:43,674 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4a235b2-41ba-438c-ad43-116196767ef6
2024-01-08 06:25:43,694 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:43,694 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:43,699 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:43,699 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:43,699 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:43,700 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38503
2024-01-08 06:25:43,700 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38503
2024-01-08 06:25:43,701 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33949
2024-01-08 06:25:43,701 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:43,701 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:43,701 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:43,701 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:43,701 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wiwtdu3s
2024-01-08 06:25:43,701 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7a5ef9ea-c20d-4c50-8129-de280d21390c
2024-01-08 06:25:43,703 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:43,703 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:43,704 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:43,704 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34657
2024-01-08 06:25:43,704 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34657
2024-01-08 06:25:43,704 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45037
2024-01-08 06:25:43,704 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:43,704 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:43,705 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:43,705 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:43,705 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2cuuxft2
2024-01-08 06:25:43,705 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d0247cf-be10-4520-b247-7ce6f4a0343f
2024-01-08 06:25:43,708 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:43,708 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35113
2024-01-08 06:25:43,708 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35113
2024-01-08 06:25:43,708 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36585
2024-01-08 06:25:43,709 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:43,709 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:43,709 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:43,709 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:43,709 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s5xa5idc
2024-01-08 06:25:43,709 - distributed.worker - INFO - Starting Worker plugin PreImport-f9148340-76fc-4de1-a525-0631e7511003
2024-01-08 06:25:43,709 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a63622ec-be7e-4afa-82f4-98b731f987e8
2024-01-08 06:25:43,710 - distributed.worker - INFO - Starting Worker plugin RMMSetup-23e442ff-9a0a-4105-a83b-dc1c42f7a54b
2024-01-08 06:25:43,711 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:43,712 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:43,716 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:43,717 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38903
2024-01-08 06:25:43,717 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38903
2024-01-08 06:25:43,717 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45575
2024-01-08 06:25:43,717 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:43,717 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:43,717 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:43,717 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:43,717 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hrj1_nr0
2024-01-08 06:25:43,717 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ae62ef2-22e0-4f76-b753-072a821af9f0
2024-01-08 06:25:43,721 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:43,721 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:43,725 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:43,726 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44307
2024-01-08 06:25:43,726 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44307
2024-01-08 06:25:43,726 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42735
2024-01-08 06:25:43,726 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:43,726 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:43,726 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:43,726 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:43,726 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3y0aqwqa
2024-01-08 06:25:43,726 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f866b94b-aace-4458-af11-210c25d07d8d
2024-01-08 06:25:43,902 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:43,903 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:43,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:43,908 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39169
2024-01-08 06:25:43,908 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39169
2024-01-08 06:25:43,908 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45261
2024-01-08 06:25:43,908 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:43,908 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:43,908 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:43,908 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:43,908 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ebk5_kgu
2024-01-08 06:25:43,909 - distributed.worker - INFO - Starting Worker plugin PreImport-da525e2c-1421-4eef-b682-7b5491afef08
2024-01-08 06:25:43,909 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4551999-72d7-4765-9350-049801fa8887
2024-01-08 06:25:43,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-059bffd3-995f-4f82-9b58-5e0f6dd45b54
2024-01-08 06:25:46,000 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8a016837-acd5-4c16-b35d-8ecdc707c4f7
2024-01-08 06:25:46,001 - distributed.worker - INFO - Starting Worker plugin PreImport-e141b7e5-720e-439e-b236-be67ca4df707
2024-01-08 06:25:46,002 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,013 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,034 - distributed.worker - INFO - Starting Worker plugin PreImport-26541d71-7962-46ef-a03e-26c9dc046f40
2024-01-08 06:25:46,035 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eae20d1b-d493-4322-93b1-72a15a4b656f
2024-01-08 06:25:46,036 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,037 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33947', status: init, memory: 0, processing: 0>
2024-01-08 06:25:46,039 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33947
2024-01-08 06:25:46,039 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54798
2024-01-08 06:25:46,040 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38571', status: init, memory: 0, processing: 0>
2024-01-08 06:25:46,040 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38571
2024-01-08 06:25:46,040 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:46,041 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54812
2024-01-08 06:25:46,041 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:46,041 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,041 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:46,042 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:46,042 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,043 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:46,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:46,067 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34657', status: init, memory: 0, processing: 0>
2024-01-08 06:25:46,067 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34657
2024-01-08 06:25:46,067 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54824
2024-01-08 06:25:46,068 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:46,069 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:46,069 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,071 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:46,138 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3527d940-3c26-4d0a-a7a0-a9c9d4eebcc7
2024-01-08 06:25:46,140 - distributed.worker - INFO - Starting Worker plugin PreImport-a0bfbae7-d446-430b-97c9-ecd1e581f2af
2024-01-08 06:25:46,141 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,169 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38503', status: init, memory: 0, processing: 0>
2024-01-08 06:25:46,170 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38503
2024-01-08 06:25:46,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54836
2024-01-08 06:25:46,171 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:46,172 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:46,172 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,173 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:46,388 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7cd651e-d8fa-4e8e-ada9-48eddc0e3fac
2024-01-08 06:25:46,389 - distributed.worker - INFO - Starting Worker plugin PreImport-e0cc2ad7-0134-411a-83eb-6c7dc76344b9
2024-01-08 06:25:46,390 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,393 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ecb96eb-c542-4ae6-83c5-9b08db55123b
2024-01-08 06:25:46,394 - distributed.worker - INFO - Starting Worker plugin PreImport-fd8e42d3-deb7-4893-b277-e4b658c82c4d
2024-01-08 06:25:46,396 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,420 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38903', status: init, memory: 0, processing: 0>
2024-01-08 06:25:46,421 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38903
2024-01-08 06:25:46,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54838
2024-01-08 06:25:46,422 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:46,423 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:46,423 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,425 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:46,428 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44307', status: init, memory: 0, processing: 0>
2024-01-08 06:25:46,428 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44307
2024-01-08 06:25:46,429 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54842
2024-01-08 06:25:46,430 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:46,431 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:46,431 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:46,563 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,598 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35113', status: init, memory: 0, processing: 0>
2024-01-08 06:25:46,599 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35113
2024-01-08 06:25:46,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54854
2024-01-08 06:25:46,600 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:46,601 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:46,601 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,604 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:46,920 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,947 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39169', status: init, memory: 0, processing: 0>
2024-01-08 06:25:46,948 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39169
2024-01-08 06:25:46,948 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54870
2024-01-08 06:25:46,949 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:46,950 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:46,950 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:46,951 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:46,979 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:46,979 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:46,979 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:46,979 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:46,979 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:46,979 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:47,090 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:47,090 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:25:47,100 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:47,101 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:47,101 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:47,101 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:47,101 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:47,101 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:47,101 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:47,101 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:47,110 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:47,112 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:47,114 - distributed.scheduler - INFO - Remove client Client-be123df9-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:47,115 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54776; closing.
2024-01-08 06:25:47,115 - distributed.scheduler - INFO - Remove client Client-be123df9-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:47,115 - distributed.scheduler - INFO - Close client connection: Client-be123df9-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:47,116 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42053'. Reason: nanny-close
2024-01-08 06:25:47,117 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:47,117 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40965'. Reason: nanny-close
2024-01-08 06:25:47,118 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:47,118 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46791'. Reason: nanny-close
2024-01-08 06:25:47,118 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:47,118 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33725'. Reason: nanny-close
2024-01-08 06:25:47,118 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33947. Reason: nanny-close
2024-01-08 06:25:47,118 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:47,119 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43911'. Reason: nanny-close
2024-01-08 06:25:47,119 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35113. Reason: nanny-close
2024-01-08 06:25:47,119 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38503. Reason: nanny-close
2024-01-08 06:25:47,119 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:47,119 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36669'. Reason: nanny-close
2024-01-08 06:25:47,119 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38571. Reason: nanny-close
2024-01-08 06:25:47,119 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:47,119 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32909'. Reason: nanny-close
2024-01-08 06:25:47,120 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:47,120 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33731'. Reason: nanny-close
2024-01-08 06:25:47,120 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44307. Reason: nanny-close
2024-01-08 06:25:47,120 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:47,120 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34657. Reason: nanny-close
2024-01-08 06:25:47,120 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38903. Reason: nanny-close
2024-01-08 06:25:47,121 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:47,121 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:47,121 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54798; closing.
2024-01-08 06:25:47,121 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:47,121 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39169. Reason: nanny-close
2024-01-08 06:25:47,121 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:47,121 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33947', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695147.1217072')
2024-01-08 06:25:47,122 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54812; closing.
2024-01-08 06:25:47,122 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54836; closing.
2024-01-08 06:25:47,122 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:47,122 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:47,122 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:47,122 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:47,123 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:47,123 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:47,123 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38571', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695147.1231554')
2024-01-08 06:25:47,123 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:47,123 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:47,123 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38503', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695147.123522')
2024-01-08 06:25:47,123 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54854; closing.
2024-01-08 06:25:47,124 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:47,124 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:47,124 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:47,124 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:47,125 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35113', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695147.1250281')
2024-01-08 06:25:47,125 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54842; closing.
2024-01-08 06:25:47,125 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54824; closing.
2024-01-08 06:25:47,126 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54836>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:25:47,127 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:54812>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:25:47,128 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44307', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695147.127983')
2024-01-08 06:25:47,128 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34657', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695147.1283762')
2024-01-08 06:25:47,128 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54838; closing.
2024-01-08 06:25:47,128 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54870; closing.
2024-01-08 06:25:47,129 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38903', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695147.1292262')
2024-01-08 06:25:47,129 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39169', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695147.129597')
2024-01-08 06:25:47,129 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:25:48,584 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:25:48,584 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:25:48,585 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:25:48,586 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:25:48,587 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-08 06:25:50,937 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:50,943 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:25:50,947 - distributed.scheduler - INFO - State start
2024-01-08 06:25:50,975 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:50,976 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:25:50,977 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:25:50,978 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:25:51,126 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40493'
2024-01-08 06:25:51,141 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37091'
2024-01-08 06:25:51,155 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45271'
2024-01-08 06:25:51,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43407'
2024-01-08 06:25:51,167 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44929'
2024-01-08 06:25:51,177 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44499'
2024-01-08 06:25:51,186 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46563'
2024-01-08 06:25:51,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37325'
2024-01-08 06:25:51,512 - distributed.scheduler - INFO - Receive client connection: Client-c389e82a-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:51,531 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48166
2024-01-08 06:25:53,002 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:53,002 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:53,006 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:53,007 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33511
2024-01-08 06:25:53,007 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33511
2024-01-08 06:25:53,007 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39837
2024-01-08 06:25:53,007 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:53,007 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:53,007 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:53,007 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:53,007 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mpbsijam
2024-01-08 06:25:53,007 - distributed.worker - INFO - Starting Worker plugin PreImport-cbd9e777-01d2-4f8b-a40c-77c57059cf02
2024-01-08 06:25:53,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bf40b1fd-88db-4af6-bdb1-849f35890ad8
2024-01-08 06:25:53,008 - distributed.worker - INFO - Starting Worker plugin RMMSetup-990f2083-f892-42a6-a721-0756e20cef05
2024-01-08 06:25:53,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:53,030 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:53,034 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:53,034 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:53,034 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:53,035 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44741
2024-01-08 06:25:53,035 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44741
2024-01-08 06:25:53,036 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35995
2024-01-08 06:25:53,036 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:53,036 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:53,036 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:53,036 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:53,036 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iffeskxr
2024-01-08 06:25:53,036 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b41dd337-95e0-4479-9476-4eb133e56f99
2024-01-08 06:25:53,038 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:53,039 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44131
2024-01-08 06:25:53,040 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44131
2024-01-08 06:25:53,040 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36473
2024-01-08 06:25:53,040 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:53,040 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:53,040 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:53,040 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:53,040 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tstsowzx
2024-01-08 06:25:53,040 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fd5ae9df-21a1-4560-9e42-0eb0ae13b552
2024-01-08 06:25:53,040 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:53,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:53,045 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:53,045 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40113
2024-01-08 06:25:53,045 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40113
2024-01-08 06:25:53,046 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40985
2024-01-08 06:25:53,046 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:53,046 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:53,046 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:53,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:53,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-sx_hvkg3
2024-01-08 06:25:53,046 - distributed.worker - INFO - Starting Worker plugin RMMSetup-660e2568-f3d8-45d5-94f3-c43b313e0ef9
2024-01-08 06:25:53,077 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:53,077 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:53,082 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:53,083 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41137
2024-01-08 06:25:53,083 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41137
2024-01-08 06:25:53,083 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45221
2024-01-08 06:25:53,083 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:53,083 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:53,083 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:53,083 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:53,083 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-und4wuub
2024-01-08 06:25:53,083 - distributed.worker - INFO - Starting Worker plugin PreImport-4aecea46-86b8-4f6a-92a2-63daa1b50414
2024-01-08 06:25:53,083 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be83fb37-42f0-4f1d-b124-e007aa99bc40
2024-01-08 06:25:53,086 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7331ade6-ddb0-4002-8077-9b4bcc41581b
2024-01-08 06:25:53,096 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:53,096 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:53,100 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:53,101 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41785
2024-01-08 06:25:53,101 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41785
2024-01-08 06:25:53,101 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39905
2024-01-08 06:25:53,101 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:53,101 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:53,101 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:53,101 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:53,102 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rcl_4sx1
2024-01-08 06:25:53,102 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9bd923be-6e0a-408d-b600-f8186a9d880b
2024-01-08 06:25:53,264 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:53,264 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:53,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:25:53,266 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:25:53,271 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:53,271 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:25:53,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43229
2024-01-08 06:25:53,272 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42783
2024-01-08 06:25:53,272 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43229
2024-01-08 06:25:53,272 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42783
2024-01-08 06:25:53,272 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43799
2024-01-08 06:25:53,272 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37201
2024-01-08 06:25:53,272 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:53,272 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:25:53,272 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:53,272 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:53,272 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:53,272 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:25:53,272 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:53,272 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:25:53,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v071ps90
2024-01-08 06:25:53,273 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i48qkc13
2024-01-08 06:25:53,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7de819dd-dc1a-4f40-9752-a6b58d06c12b
2024-01-08 06:25:53,273 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cb3580c0-22fa-4c50-b12a-75fd2848a356
2024-01-08 06:25:55,368 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,397 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33511', status: init, memory: 0, processing: 0>
2024-01-08 06:25:55,398 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33511
2024-01-08 06:25:55,398 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48184
2024-01-08 06:25:55,399 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:55,400 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:55,400 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:55,508 - distributed.worker - INFO - Starting Worker plugin PreImport-c3750172-6888-4d51-8f34-46026b78b0a9
2024-01-08 06:25:55,508 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8069d3a4-3b6c-4210-aac5-f910747a6825
2024-01-08 06:25:55,509 - distributed.worker - INFO - Starting Worker plugin PreImport-32d02c1a-2090-4aa1-a80e-6fe54781781c
2024-01-08 06:25:55,509 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,510 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-82b38a74-8782-4c9c-8ce5-5c0ab7ad00d0
2024-01-08 06:25:55,512 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,529 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8af9854a-bce1-4272-aa92-c7758ae3a4bf
2024-01-08 06:25:55,531 - distributed.worker - INFO - Starting Worker plugin PreImport-8f49c6e6-0996-41a5-a9a7-4d2f7f923624
2024-01-08 06:25:55,533 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,541 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,549 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40113', status: init, memory: 0, processing: 0>
2024-01-08 06:25:55,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40113
2024-01-08 06:25:55,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48198
2024-01-08 06:25:55,551 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:55,552 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:55,553 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,553 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44131', status: init, memory: 0, processing: 0>
2024-01-08 06:25:55,553 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44131
2024-01-08 06:25:55,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48192
2024-01-08 06:25:55,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:55,555 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:55,556 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:55,556 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:55,567 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4884c430-c537-44b1-8f46-ec74451fc7a2
2024-01-08 06:25:55,567 - distributed.worker - INFO - Starting Worker plugin PreImport-88cce045-8666-4cdd-bf54-eb67d1014589
2024-01-08 06:25:55,568 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,569 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41137', status: init, memory: 0, processing: 0>
2024-01-08 06:25:55,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41137
2024-01-08 06:25:55,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48220
2024-01-08 06:25:55,570 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:55,571 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:55,571 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:55,576 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44741', status: init, memory: 0, processing: 0>
2024-01-08 06:25:55,576 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44741
2024-01-08 06:25:55,576 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48210
2024-01-08 06:25:55,579 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:55,581 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-513b6426-4ef0-4e72-b603-e28e3d61d121
2024-01-08 06:25:55,581 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:55,581 - distributed.worker - INFO - Starting Worker plugin PreImport-f24238b3-c1d1-46e0-a4c0-ddf67da23612
2024-01-08 06:25:55,581 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,581 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,585 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:55,592 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41785', status: init, memory: 0, processing: 0>
2024-01-08 06:25:55,593 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41785
2024-01-08 06:25:55,593 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48224
2024-01-08 06:25:55,594 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:55,594 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:55,595 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:55,598 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fd1fc6c2-e16f-40aa-8e7d-447d68887df5
2024-01-08 06:25:55,599 - distributed.worker - INFO - Starting Worker plugin PreImport-8e815f20-b7da-4e32-b9e6-754dff835043
2024-01-08 06:25:55,601 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,603 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43229', status: init, memory: 0, processing: 0>
2024-01-08 06:25:55,603 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43229
2024-01-08 06:25:55,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48226
2024-01-08 06:25:55,604 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:55,605 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:55,605 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,606 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:55,651 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42783', status: init, memory: 0, processing: 0>
2024-01-08 06:25:55,652 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42783
2024-01-08 06:25:55,652 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48234
2024-01-08 06:25:55,656 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:25:55,658 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:25:55,658 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:25:55,662 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:25:55,687 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:55,687 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:55,687 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:55,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:55,688 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:55,691 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:55,691 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:55,690 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:55,706 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:55,706 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:55,706 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:55,706 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:55,706 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:55,706 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:55,706 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:55,706 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:25:55,714 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:55,716 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:25:55,718 - distributed.scheduler - INFO - Remove client Client-c389e82a-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:55,718 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48166; closing.
2024-01-08 06:25:55,718 - distributed.scheduler - INFO - Remove client Client-c389e82a-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:55,719 - distributed.scheduler - INFO - Close client connection: Client-c389e82a-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:55,720 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40493'. Reason: nanny-close
2024-01-08 06:25:55,720 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:55,721 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37091'. Reason: nanny-close
2024-01-08 06:25:55,721 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:55,721 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45271'. Reason: nanny-close
2024-01-08 06:25:55,722 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:55,722 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44131. Reason: nanny-close
2024-01-08 06:25:55,722 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43407'. Reason: nanny-close
2024-01-08 06:25:55,722 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:55,722 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44741. Reason: nanny-close
2024-01-08 06:25:55,722 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44929'. Reason: nanny-close
2024-01-08 06:25:55,723 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33511. Reason: nanny-close
2024-01-08 06:25:55,723 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:55,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44499'. Reason: nanny-close
2024-01-08 06:25:55,723 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41785. Reason: nanny-close
2024-01-08 06:25:55,723 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:55,723 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46563'. Reason: nanny-close
2024-01-08 06:25:55,724 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:55,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40113. Reason: nanny-close
2024-01-08 06:25:55,724 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37325'. Reason: nanny-close
2024-01-08 06:25:55,724 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:25:55,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42783. Reason: nanny-close
2024-01-08 06:25:55,724 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:55,724 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48192; closing.
2024-01-08 06:25:55,724 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41137. Reason: nanny-close
2024-01-08 06:25:55,724 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:55,725 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44131', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695155.724995')
2024-01-08 06:25:55,725 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:55,725 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43229. Reason: nanny-close
2024-01-08 06:25:55,725 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:55,725 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48184; closing.
2024-01-08 06:25:55,726 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:55,726 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:55,726 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:55,726 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:55,726 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:55,726 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:55,726 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33511', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695155.7268865')
2024-01-08 06:25:55,727 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:25:55,727 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:55,727 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48224; closing.
2024-01-08 06:25:55,727 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48210; closing.
2024-01-08 06:25:55,728 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:55,728 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:55,728 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:55,728 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41785', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695155.7283516')
2024-01-08 06:25:55,728 - distributed.nanny - INFO - Worker closed
2024-01-08 06:25:55,728 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44741', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695155.7287197')
2024-01-08 06:25:55,729 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48198; closing.
2024-01-08 06:25:55,729 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48234; closing.
2024-01-08 06:25:55,729 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40113', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695155.7297685')
2024-01-08 06:25:55,730 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42783', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695155.7301104')
2024-01-08 06:25:55,730 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48220; closing.
2024-01-08 06:25:55,730 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48226; closing.
2024-01-08 06:25:55,731 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41137', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695155.7309842')
2024-01-08 06:25:55,731 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43229', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695155.7313135')
2024-01-08 06:25:55,731 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:25:56,636 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:25:56,636 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:25:56,637 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:25:56,638 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:25:56,638 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-08 06:25:59,018 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:59,022 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:25:59,026 - distributed.scheduler - INFO - State start
2024-01-08 06:25:59,093 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:25:59,094 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:25:59,095 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:25:59,095 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:25:59,385 - distributed.scheduler - INFO - Receive client connection: Client-c85dbaac-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:25:59,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48346
2024-01-08 06:25:59,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33417'
2024-01-08 06:25:59,792 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34469'
2024-01-08 06:25:59,800 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34457'
2024-01-08 06:25:59,813 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40759'
2024-01-08 06:25:59,816 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35197'
2024-01-08 06:25:59,824 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37659'
2024-01-08 06:25:59,833 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44557'
2024-01-08 06:25:59,841 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38353'
2024-01-08 06:26:01,649 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:01,649 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:01,653 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:01,654 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33195
2024-01-08 06:26:01,654 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33195
2024-01-08 06:26:01,654 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43509
2024-01-08 06:26:01,654 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:01,654 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:01,654 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:01,654 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:01,654 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-82nwje55
2024-01-08 06:26:01,654 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df6afe73-9eb4-4c3d-a120-1d1d93fb81c8
2024-01-08 06:26:01,891 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:01,892 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:01,896 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:01,897 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39113
2024-01-08 06:26:01,897 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39113
2024-01-08 06:26:01,897 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46249
2024-01-08 06:26:01,897 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:01,897 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:01,897 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:01,897 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:01,897 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-prb72n8u
2024-01-08 06:26:01,898 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6c1855b2-9b4c-47c4-8877-4761d8182b16
2024-01-08 06:26:01,902 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:01,902 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:01,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:01,903 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:01,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:01,903 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:01,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:01,903 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:01,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:01,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:01,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:01,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:01,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:01,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:01,908 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:01,908 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38635
2024-01-08 06:26:01,908 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38635
2024-01-08 06:26:01,908 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34763
2024-01-08 06:26:01,908 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:01,908 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:01,908 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:01,908 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39785
2024-01-08 06:26:01,908 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:01,909 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39785
2024-01-08 06:26:01,909 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:01,909 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41131
2024-01-08 06:26:01,909 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uuauo3l4
2024-01-08 06:26:01,909 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:01,909 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:01,909 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:01,909 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:01,909 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:01,909 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37529
2024-01-08 06:26:01,909 - distributed.worker - INFO - Starting Worker plugin PreImport-0e3b7ae7-a4b3-47cf-bd7c-af2703bc742e
2024-01-08 06:26:01,909 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:01,909 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37529
2024-01-08 06:26:01,909 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-39bpbcaj
2024-01-08 06:26:01,909 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9fd58fef-9d66-478e-8e18-11964422d03c
2024-01-08 06:26:01,909 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38423
2024-01-08 06:26:01,909 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:01,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f27898b-633e-4998-8aa7-52170de6a676
2024-01-08 06:26:01,909 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:01,909 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:01,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-40931c5a-ca08-43bb-803b-801b50d2dd0b
2024-01-08 06:26:01,909 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:01,909 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-id_69mfy
2024-01-08 06:26:01,909 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37647
2024-01-08 06:26:01,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e12af731-5cd8-47de-9fc7-2db2544d5e10
2024-01-08 06:26:01,909 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37647
2024-01-08 06:26:01,909 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42081
2024-01-08 06:26:01,910 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:01,910 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:01,910 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:01,910 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:01,910 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33063
2024-01-08 06:26:01,910 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43719
2024-01-08 06:26:01,910 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zgg69m79
2024-01-08 06:26:01,910 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33063
2024-01-08 06:26:01,910 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43719
2024-01-08 06:26:01,910 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45343
2024-01-08 06:26:01,910 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38325
2024-01-08 06:26:01,910 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:01,910 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:01,910 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:01,910 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:01,910 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:01,910 - distributed.worker - INFO - Starting Worker plugin PreImport-13fd8221-82c2-401a-8250-02807c173ec9
2024-01-08 06:26:01,910 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:01,910 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:01,910 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u3xitzq3
2024-01-08 06:26:01,910 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5a0bb90-0501-4373-9926-43b219664649
2024-01-08 06:26:01,910 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:26:01,910 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-th96g6sq
2024-01-08 06:26:01,910 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c97fb36e-ea53-4720-86ca-a8b8ad405a92
2024-01-08 06:26:01,910 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6ac4afc-d72e-45f1-bdfa-47c73cf75bb2
2024-01-08 06:26:01,910 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85fa595e-7eaa-4483-8755-45a572c5dc49
2024-01-08 06:26:02,085 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2aa3351b-9afa-4851-95d9-e6ce03b4250a
2024-01-08 06:26:02,086 - distributed.worker - INFO - Starting Worker plugin PreImport-1104bcd2-7857-48f5-8ef4-6926552b86d7
2024-01-08 06:26:02,086 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:02,120 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33195', status: init, memory: 0, processing: 0>
2024-01-08 06:26:02,122 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33195
2024-01-08 06:26:02,122 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52616
2024-01-08 06:26:02,123 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:02,124 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:02,124 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:02,126 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:03,722 - distributed.worker - INFO - Starting Worker plugin PreImport-614fd9f3-ec00-4597-b3d0-c0f293687818
2024-01-08 06:26:03,723 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cfe61d4f-4be7-4b2f-bb3b-42e5daac2ed8
2024-01-08 06:26:03,724 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,756 - distributed.worker - INFO - Starting Worker plugin PreImport-83cab24f-719a-4f24-85a7-c84707001bef
2024-01-08 06:26:03,756 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d1b73425-4dfa-4f83-967c-e6abfb4eaa7f
2024-01-08 06:26:03,758 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,758 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39113', status: init, memory: 0, processing: 0>
2024-01-08 06:26:03,759 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39113
2024-01-08 06:26:03,759 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52622
2024-01-08 06:26:03,760 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:03,762 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:03,762 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,764 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:03,774 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb8d0227-dc4b-4bb8-93b2-d7b8514feca1
2024-01-08 06:26:03,775 - distributed.worker - INFO - Starting Worker plugin PreImport-757c1758-1b88-498a-948f-106bbba78e8a
2024-01-08 06:26:03,775 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,789 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33063', status: init, memory: 0, processing: 0>
2024-01-08 06:26:03,790 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33063
2024-01-08 06:26:03,790 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52626
2024-01-08 06:26:03,791 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:03,792 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:03,792 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,794 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:03,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a2f4243-03a5-462b-b0b1-94c503646e47
2024-01-08 06:26:03,799 - distributed.worker - INFO - Starting Worker plugin PreImport-31eae465-777a-4f65-a647-b8b7568ca0ce
2024-01-08 06:26:03,800 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,801 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39785', status: init, memory: 0, processing: 0>
2024-01-08 06:26:03,801 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39785
2024-01-08 06:26:03,801 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52634
2024-01-08 06:26:03,802 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,802 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:03,803 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:03,803 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,805 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:03,807 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be500cc3-9040-4ac7-8786-a360cc5bdeb2
2024-01-08 06:26:03,808 - distributed.worker - INFO - Starting Worker plugin PreImport-22129a79-41d7-49a0-8529-08035bca8119
2024-01-08 06:26:03,808 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,817 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,825 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37647', status: init, memory: 0, processing: 0>
2024-01-08 06:26:03,825 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37647
2024-01-08 06:26:03,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52656
2024-01-08 06:26:03,826 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:03,827 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:03,827 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,829 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:03,831 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43719', status: init, memory: 0, processing: 0>
2024-01-08 06:26:03,831 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43719
2024-01-08 06:26:03,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52642
2024-01-08 06:26:03,832 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37529', status: init, memory: 0, processing: 0>
2024-01-08 06:26:03,832 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37529
2024-01-08 06:26:03,832 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52672
2024-01-08 06:26:03,833 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:03,833 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:03,834 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:03,834 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,834 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:03,834 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:03,836 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:03,838 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38635', status: init, memory: 0, processing: 0>
2024-01-08 06:26:03,839 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38635
2024-01-08 06:26:03,839 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52686
2024-01-08 06:26:03,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:03,840 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:03,840 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:03,842 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:03,859 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:03,859 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:03,859 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:03,859 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:03,859 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:03,859 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:03,860 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:03,860 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-08 06:26:03,864 - distributed.scheduler - INFO - Remove client Client-c85dbaac-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:03,864 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48346; closing.
2024-01-08 06:26:03,864 - distributed.scheduler - INFO - Remove client Client-c85dbaac-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:03,865 - distributed.scheduler - INFO - Close client connection: Client-c85dbaac-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:03,865 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33417'. Reason: nanny-close
2024-01-08 06:26:03,866 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:03,866 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34469'. Reason: nanny-close
2024-01-08 06:26:03,867 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:03,867 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34457'. Reason: nanny-close
2024-01-08 06:26:03,867 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39113. Reason: nanny-close
2024-01-08 06:26:03,867 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:03,867 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40759'. Reason: nanny-close
2024-01-08 06:26:03,868 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33195. Reason: nanny-close
2024-01-08 06:26:03,868 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:03,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35197'. Reason: nanny-close
2024-01-08 06:26:03,868 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37647. Reason: nanny-close
2024-01-08 06:26:03,868 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:03,868 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37659'. Reason: nanny-close
2024-01-08 06:26:03,868 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39785. Reason: nanny-close
2024-01-08 06:26:03,869 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:03,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44557'. Reason: nanny-close
2024-01-08 06:26:03,869 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33063. Reason: nanny-close
2024-01-08 06:26:03,869 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:03,869 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:03,869 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38353'. Reason: nanny-close
2024-01-08 06:26:03,869 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:03,870 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52622; closing.
2024-01-08 06:26:03,870 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43719. Reason: nanny-close
2024-01-08 06:26:03,870 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:03,870 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38635. Reason: nanny-close
2024-01-08 06:26:03,870 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:03,870 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39113', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695163.8704307')
2024-01-08 06:26:03,870 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:03,870 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37529. Reason: nanny-close
2024-01-08 06:26:03,870 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52634; closing.
2024-01-08 06:26:03,871 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52616; closing.
2024-01-08 06:26:03,871 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:03,871 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:03,871 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39785', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695163.871694')
2024-01-08 06:26:03,871 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:03,871 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:03,872 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:03,872 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33195', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695163.8720636')
2024-01-08 06:26:03,872 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:03,872 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:03,872 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52656; closing.
2024-01-08 06:26:03,872 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:03,873 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37647', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695163.8733876')
2024-01-08 06:26:03,873 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:03,873 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:03,874 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:03,874 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:03,874 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52616>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:26:03,875 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:52634>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-08 06:26:03,876 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52626; closing.
2024-01-08 06:26:03,876 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52642; closing.
2024-01-08 06:26:03,876 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52686; closing.
2024-01-08 06:26:03,876 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33063', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695163.876939')
2024-01-08 06:26:03,877 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43719', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695163.877291')
2024-01-08 06:26:03,877 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38635', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695163.8776977')
2024-01-08 06:26:03,878 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52672; closing.
2024-01-08 06:26:03,878 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37529', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695163.878425')
2024-01-08 06:26:03,878 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:04,781 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:26:04,782 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:26:04,782 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:26:04,784 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:26:04,784 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-08 06:26:07,029 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:07,033 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:26:07,037 - distributed.scheduler - INFO - State start
2024-01-08 06:26:07,060 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:07,061 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:26:07,062 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:26:07,062 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:26:07,151 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46653'
2024-01-08 06:26:08,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:08,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:09,095 - distributed.scheduler - INFO - Receive client connection: Client-cd35ec64-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:09,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52824
2024-01-08 06:26:09,308 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:09,309 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36197
2024-01-08 06:26:09,309 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36197
2024-01-08 06:26:09,310 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-08 06:26:09,310 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:09,310 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:09,310 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:09,310 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:26:09,310 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xbss_mpr
2024-01-08 06:26:09,310 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f7a6343-fc11-4681-8675-76eb2c5befe7
2024-01-08 06:26:09,310 - distributed.worker - INFO - Starting Worker plugin PreImport-fea3506e-4db2-49ed-bf2e-0679faa8f548
2024-01-08 06:26:09,310 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80707113-8687-4e56-bad3-a072219aa7c7
2024-01-08 06:26:09,311 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:09,370 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36197', status: init, memory: 0, processing: 0>
2024-01-08 06:26:09,371 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36197
2024-01-08 06:26:09,371 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52850
2024-01-08 06:26:09,372 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:09,373 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:09,373 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:09,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:09,423 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:26:09,426 - distributed.scheduler - INFO - Remove client Client-cd35ec64-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:09,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52824; closing.
2024-01-08 06:26:09,426 - distributed.scheduler - INFO - Remove client Client-cd35ec64-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:09,426 - distributed.scheduler - INFO - Close client connection: Client-cd35ec64-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:09,427 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46653'. Reason: nanny-close
2024-01-08 06:26:09,428 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:09,429 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36197. Reason: nanny-close
2024-01-08 06:26:09,431 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:09,431 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52850; closing.
2024-01-08 06:26:09,431 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36197', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695169.4314632')
2024-01-08 06:26:09,431 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:09,432 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:10,043 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:26:10,043 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:26:10,044 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:26:10,045 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:26:10,045 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-08 06:26:14,177 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:14,181 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:26:14,185 - distributed.scheduler - INFO - State start
2024-01-08 06:26:14,208 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:14,209 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:26:14,209 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:26:14,210 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:26:14,219 - distributed.scheduler - INFO - Receive client connection: Client-d1886599-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:14,231 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41414
2024-01-08 06:26:14,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36343'
2024-01-08 06:26:15,854 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:15,854 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:16,369 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:16,370 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33397
2024-01-08 06:26:16,370 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33397
2024-01-08 06:26:16,370 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43337
2024-01-08 06:26:16,370 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:16,370 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:16,370 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:16,370 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:26:16,370 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6j4ob8z4
2024-01-08 06:26:16,371 - distributed.worker - INFO - Starting Worker plugin RMMSetup-59a7664d-a3c3-4aad-99dd-bb005b6c2288
2024-01-08 06:26:16,371 - distributed.worker - INFO - Starting Worker plugin PreImport-2eaaf23f-ecd9-4419-8e40-054b1a9bee5f
2024-01-08 06:26:16,372 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-af08fac6-a7fc-4ff9-8d1d-5618bc651d7f
2024-01-08 06:26:16,373 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:16,435 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33397', status: init, memory: 0, processing: 0>
2024-01-08 06:26:16,436 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33397
2024-01-08 06:26:16,436 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41452
2024-01-08 06:26:16,437 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:16,438 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:16,438 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:16,440 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:16,475 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:26:16,477 - distributed.scheduler - INFO - Remove client Client-d1886599-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:16,477 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41414; closing.
2024-01-08 06:26:16,478 - distributed.scheduler - INFO - Remove client Client-d1886599-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:16,478 - distributed.scheduler - INFO - Close client connection: Client-d1886599-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:16,479 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36343'. Reason: nanny-close
2024-01-08 06:26:16,488 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:16,489 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33397. Reason: nanny-close
2024-01-08 06:26:16,491 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:16,491 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:41452; closing.
2024-01-08 06:26:16,492 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33397', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695176.4921536')
2024-01-08 06:26:16,492 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:16,493 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:17,194 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:26:17,195 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:26:17,195 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:26:17,196 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:26:17,196 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-08 06:26:19,352 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:19,356 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:26:19,359 - distributed.scheduler - INFO - State start
2024-01-08 06:26:19,380 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:19,381 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:26:19,382 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:26:19,382 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:26:22,033 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:41468'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4428, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41468>: Stream is closed
2024-01-08 06:26:22,324 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:26:22,325 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:26:22,325 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:26:22,326 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:26:22,326 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-08 06:26:24,529 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:24,533 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:26:24,537 - distributed.scheduler - INFO - State start
2024-01-08 06:26:24,558 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:24,559 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-08 06:26:24,560 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:26:24,560 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:26:24,681 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46817'
2024-01-08 06:26:26,522 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:26,522 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:26,526 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:26,527 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40531
2024-01-08 06:26:26,527 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40531
2024-01-08 06:26:26,527 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39059
2024-01-08 06:26:26,527 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-08 06:26:26,527 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,527 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:26,527 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:26:26,527 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-fsvoyuqi
2024-01-08 06:26:26,527 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f82e677-40b8-46e1-9ec7-f725a620b1bd
2024-01-08 06:26:26,528 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2c06b55-ed19-4f25-8b3c-9248bc909068
2024-01-08 06:26:26,528 - distributed.worker - INFO - Starting Worker plugin PreImport-21dca166-075c-4989-be23-2f5b543c4f96
2024-01-08 06:26:26,528 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,585 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40531', status: init, memory: 0, processing: 0>
2024-01-08 06:26:26,600 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40531
2024-01-08 06:26:26,600 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38540
2024-01-08 06:26:26,601 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:26,602 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-08 06:26:26,602 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:26,603 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-08 06:26:26,605 - distributed.scheduler - INFO - Receive client connection: Client-d7af39b6-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:26,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38552
2024-01-08 06:26:26,611 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:26:26,618 - distributed.scheduler - INFO - Remove client Client-d7af39b6-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:26,619 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38552; closing.
2024-01-08 06:26:26,619 - distributed.scheduler - INFO - Remove client Client-d7af39b6-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:26,619 - distributed.scheduler - INFO - Close client connection: Client-d7af39b6-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:26,620 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46817'. Reason: nanny-close
2024-01-08 06:26:26,621 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:26,622 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40531. Reason: nanny-close
2024-01-08 06:26:26,623 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38540; closing.
2024-01-08 06:26:26,623 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-08 06:26:26,624 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40531', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695186.6242127')
2024-01-08 06:26:26,624 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:26,625 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:27,235 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:26:27,236 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:26:27,236 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:26:27,237 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-08 06:26:27,237 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 127, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 98, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 184, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 82, in <module>
    from dask.dataframe import backends, dispatch, rolling
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/backends.py", line 10, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/__init__.py", line 4, in <module>
    from dask.array import backends, fft, lib, linalg, ma, overlap, random
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/backends.py", line 8, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/core.py", line 37, in <module>
    from dask.array.chunk_types import is_valid_array_chunk, is_valid_chunk_type
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/chunk_types.py", line 110, in <module>
    import cupy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/__init__.py", line 47, in <module>
    from cupy import testing  # NOQA  # NOQA
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/testing/__init__.py", line 8, in <module>
    from cupy.testing._attr import multi_gpu  # NOQA
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/testing/_attr.py", line 4, in <module>
    from cupy.testing._pytest_impl import is_available, check_available
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/testing/_pytest_impl.py", line 6, in <module>
    import pytest
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pytest/__init__.py", line 5, in <module>
    from _pytest._code import ExceptionInfo
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/_code/__init__.py", line 2, in <module>
    from .code import Code
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/_code/code.py", line 47, in <module>
    from _pytest.deprecated import check_ispytest
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/deprecated.py", line 13, in <module>
    from _pytest.warning_types import PytestDeprecationWarning
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/warning_types.py", line 133, in <module>
    class UnformattedWarning(Generic[_W]):
  File "/opt/conda/envs/gdf/lib/python3.9/typing.py", line 1011, in __init_subclass__
    error = Generic in cls.__orig_bases__
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 127, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 98, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 184, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/__init__.py", line 9, in <module>
    import dask.dataframe.core
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/__init__.py", line 82, in <module>
    from dask.dataframe import backends, dispatch, rolling
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/backends.py", line 10, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/__init__.py", line 4, in <module>
    from dask.array import backends, fft, lib, linalg, ma, overlap, random
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/backends.py", line 8, in <module>
    from dask.array.core import Array
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/core.py", line 37, in <module>
    from dask.array.chunk_types import is_valid_array_chunk, is_valid_chunk_type
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/array/chunk_types.py", line 110, in <module>
    import cupy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/__init__.py", line 432, in <module>
    from cupy._io.npz import load  # NOQA
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_io/npz.py", line 8, in <module>
    _support_allow_pickle = (numpy.lib.NumpyVersion(numpy.__version__) >= '1.10.0')
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/lib/_version.py", line 76, in __init__
    self.is_devversion = bool(re.search(r'.dev', vstring))
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-08 06:26:31,490 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:31,494 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:26:31,498 - distributed.scheduler - INFO - State start
2024-01-08 06:26:31,519 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:31,520 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:26:31,521 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:26:31,521 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:26:31,720 - distributed.scheduler - INFO - Receive client connection: Client-dbe3a75b-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:31,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58198
2024-01-08 06:26:31,848 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42283'
2024-01-08 06:26:33,245 - distributed.scheduler - INFO - Receive client connection: Client-dc4dadae-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:26:33,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58294
2024-01-08 06:26:33,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:33,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:33,649 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:33,650 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43273
2024-01-08 06:26:33,650 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43273
2024-01-08 06:26:33,650 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39829
2024-01-08 06:26:33,650 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:33,650 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:33,650 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:33,650 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:26:33,650 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_qw1cnw5
2024-01-08 06:26:33,651 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c59bbcdd-3353-4ea4-9eb5-8d74fe8fe685
2024-01-08 06:26:33,973 - distributed.worker - INFO - Starting Worker plugin PreImport-d013a519-b456-45c1-9f17-6c979479e09b
2024-01-08 06:26:33,973 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ba7c3227-324d-4215-9473-d85f74c9b488
2024-01-08 06:26:33,974 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:34,036 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43273', status: init, memory: 0, processing: 0>
2024-01-08 06:26:34,037 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43273
2024-01-08 06:26:34,037 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58322
2024-01-08 06:26:34,038 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:34,039 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:34,039 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:34,041 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:34,102 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:26:34,107 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:26:34,109 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:26:34,111 - distributed.scheduler - INFO - Remove client Client-dbe3a75b-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:34,111 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58198; closing.
2024-01-08 06:26:34,111 - distributed.scheduler - INFO - Remove client Client-dbe3a75b-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:34,112 - distributed.scheduler - INFO - Close client connection: Client-dbe3a75b-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:34,113 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42283'. Reason: nanny-close
2024-01-08 06:26:34,113 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:34,114 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43273. Reason: nanny-close
2024-01-08 06:26:34,116 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58322; closing.
2024-01-08 06:26:34,116 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:34,117 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43273', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695194.1171346')
2024-01-08 06:26:34,117 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:34,118 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:35,079 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:26:35,079 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:26:35,079 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:26:35,081 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:26:35,082 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-08 06:26:37,348 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:37,353 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-08 06:26:37,357 - distributed.scheduler - INFO - State start
2024-01-08 06:26:37,381 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-08 06:26:37,382 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-08 06:26:37,382 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-08 06:26:37,383 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-08 06:26:37,415 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38473'
2024-01-08 06:26:37,534 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45333', status: init, memory: 0, processing: 0>
2024-01-08 06:26:37,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45333
2024-01-08 06:26:37,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59330
2024-01-08 06:26:37,590 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59330; closing.
2024-01-08 06:26:37,591 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45333', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695197.5911355')
2024-01-08 06:26:37,591 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:37,614 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35349', status: init, memory: 0, processing: 0>
2024-01-08 06:26:37,615 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35349
2024-01-08 06:26:37,615 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59346
2024-01-08 06:26:37,634 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59346; closing.
2024-01-08 06:26:37,634 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695197.6345255')
2024-01-08 06:26:37,634 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:37,756 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41981', status: init, memory: 0, processing: 0>
2024-01-08 06:26:37,757 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41981
2024-01-08 06:26:37,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59348
2024-01-08 06:26:37,811 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59348; closing.
2024-01-08 06:26:37,811 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41981', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695197.8115969')
2024-01-08 06:26:37,811 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:37,881 - distributed.scheduler - INFO - Receive client connection: Client-df49e716-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:37,881 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59354
2024-01-08 06:26:39,028 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:26:39,028 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:26:39,032 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:26:39,033 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33227
2024-01-08 06:26:39,033 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33227
2024-01-08 06:26:39,033 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42797
2024-01-08 06:26:39,033 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-08 06:26:39,033 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:39,033 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:26:39,033 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:26:39,033 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i_2d5imb
2024-01-08 06:26:39,033 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fe05614-9d74-46dc-96fa-508ab7b63619
2024-01-08 06:26:39,034 - distributed.worker - INFO - Starting Worker plugin PreImport-1fac391c-a45c-4396-831e-153916751551
2024-01-08 06:26:39,034 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee705a29-2982-4bd2-898a-63996081a4f3
2024-01-08 06:26:39,312 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:39,356 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33227', status: init, memory: 0, processing: 0>
2024-01-08 06:26:39,357 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33227
2024-01-08 06:26:39,357 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59378
2024-01-08 06:26:39,358 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:26:39,358 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-08 06:26:39,358 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:26:39,360 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-08 06:26:39,415 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-08 06:26:39,420 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-08 06:26:39,423 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:26:39,425 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:26:39,427 - distributed.scheduler - INFO - Remove client Client-df49e716-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:39,427 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59354; closing.
2024-01-08 06:26:39,428 - distributed.scheduler - INFO - Remove client Client-df49e716-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:39,428 - distributed.scheduler - INFO - Close client connection: Client-df49e716-adee-11ee-bc7c-d8c49764f6bb
2024-01-08 06:26:39,429 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38473'. Reason: nanny-close
2024-01-08 06:26:39,429 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-08 06:26:39,430 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33227. Reason: nanny-close
2024-01-08 06:26:39,432 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59378; closing.
2024-01-08 06:26:39,432 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-08 06:26:39,432 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33227', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704695199.432285')
2024-01-08 06:26:39,432 - distributed.scheduler - INFO - Lost all workers
2024-01-08 06:26:39,433 - distributed.nanny - INFO - Worker closed
2024-01-08 06:26:40,044 - distributed.scheduler - INFO - Receive client connection: Client-dc4dadae-adee-11ee-ba76-d8c49764f6bb
2024-01-08 06:26:40,044 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55606
2024-01-08 06:26:40,045 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-08 06:26:40,045 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-08 06:26:40,046 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-08 06:26:40,047 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-08 06:26:40,048 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37549 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36325 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33719 instead
  warnings.warn(
2024-01-08 06:28:14,267 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-08 06:28:14,268 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-08 06:28:14,273 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-08 06:28:14,276 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1280' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1281' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1279' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37405 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36133 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37315 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40789 instead
  warnings.warn(
[1704695357.255064] [dgx13:70964:0]            sock.c:470  UCX  ERROR bind(fd=183 addr=0.0.0.0:37344) failed: Address already in use
2024-01-08 06:29:24,392 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35349 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45835 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39437 instead
  warnings.warn(
[1704695390.698387] [dgx13:71618:0]            sock.c:470  UCX  ERROR bind(fd=123 addr=0.0.0.0:51768) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45995 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45113 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38331 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36895 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36267 instead
  warnings.warn(
[1704695570.062267] [dgx13:74661:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:48670) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41849 instead
  warnings.warn(
[1704695590.868428] [dgx13:75105:0]            sock.c:470  UCX  ERROR bind(fd=153 addr=0.0.0.0:58886) failed: Address already in use
[1704695593.871610] [dgx13:75274:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:37412) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37287 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39247 instead
  warnings.warn(
[1704695643.369130] [dgx13:75802:0]            sock.c:470  UCX  ERROR bind(fd=160 addr=0.0.0.0:47924) failed: Address already in use
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-6421' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46473 instead
  warnings.warn(
[1704695676.560994] [dgx13:76328:0]            sock.c:470  UCX  ERROR bind(fd=159 addr=0.0.0.0:50824) failed: Address already in use
[1704695677.937147] [dgx13:76417:0]            sock.c:470  UCX  ERROR bind(fd=123 addr=0.0.0.0:38844) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] [1704695811.307613] [dgx13:78017:0]            sock.c:470  UCX  ERROR bind(fd=159 addr=0.0.0.0:33846) failed: Address already in use
[1704695812.795169] [dgx13:78187:0]            sock.c:470  UCX  ERROR bind(fd=123 addr=0.0.0.0:54950) failed: Address already in use
[1704695812.795242] [dgx13:78187:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:43594) failed: Address already in use
[1704695812.795273] [dgx13:78187:0]            sock.c:470  UCX  ERROR bind(fd=123 addr=0.0.0.0:37726) failed: Address already in use
[1704695812.798715] [dgx13:78192:0]            sock.c:470  UCX  ERROR bind(fd=123 addr=0.0.0.0:37738) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37381 instead
  warnings.warn(
[1704696005.203776] [dgx13:81874:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:44562) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40947 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46005 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40391 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35437 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43427 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35095 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34317 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33227 instead
  warnings.warn(
[1704696230.577700] [dgx13:84801:0]            sock.c:470  UCX  ERROR bind(fd=155 addr=0.0.0.0:38315) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37485 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45423 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40511 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43399 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37633 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46419 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] [1704696340.347159] [dgx13:86464:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:55458) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44855 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42245 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32771 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43159 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33467 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41905 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33623 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41325 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36951 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] [1704696572.013283] [dgx13:89802:0]            sock.c:470  UCX  ERROR bind(fd=163 addr=0.0.0.0:56716) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] 2024-01-08 06:49:40,917 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:34398 remote=tcp://127.0.0.1:39375>: Stream is closed
PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44099 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44215 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40495 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35933 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46251 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37915 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-08 06:51:20,876 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:51:20,876 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:51:20,939 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:51:20,939 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:51:20,955 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:51:20,955 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:51:21,011 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:51:21,011 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:51:21,128 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:51:21,128 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:51:21,138 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:51:21,138 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:51:21,142 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:51:21,142 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:51:21,145 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:51:21,145 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:51:21,571 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:51:21,572 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34477
2024-01-08 06:51:21,572 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34477
2024-01-08 06:51:21,572 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46243
2024-01-08 06:51:21,572 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,572 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,572 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:51:21,572 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aogdtcof
2024-01-08 06:51:21,573 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d37b6aa7-fdbe-4cf1-92bc-1b7532be8707
2024-01-08 06:51:21,573 - distributed.worker - INFO - Starting Worker plugin PreImport-a08d2462-f2ed-4745-992a-ac9032095686
2024-01-08 06:51:21,573 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c1b7ad70-a1eb-4653-9802-8d5255c3285e
2024-01-08 06:51:21,573 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,605 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:51:21,605 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36133
2024-01-08 06:51:21,606 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36133
2024-01-08 06:51:21,606 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39803
2024-01-08 06:51:21,606 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,606 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,606 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:51:21,606 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ej3cuymn
2024-01-08 06:51:21,606 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ea75375-55fd-4196-a06d-096327e00908
2024-01-08 06:51:21,606 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6e47e5b-189c-4273-add3-f5410bd15465
2024-01-08 06:51:21,606 - distributed.worker - INFO - Starting Worker plugin PreImport-6ba82d07-e8ff-4442-a5d0-4ba9acdf547e
2024-01-08 06:51:21,606 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,634 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:51:21,636 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35721
2024-01-08 06:51:21,636 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35721
2024-01-08 06:51:21,636 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45207
2024-01-08 06:51:21,636 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,636 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,636 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:51:21,636 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_0ftdom8
2024-01-08 06:51:21,636 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5cfdd387-517e-4645-a703-e7b56bb2bec8
2024-01-08 06:51:21,636 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eca062b7-32d0-484a-acb5-53e28b1d7d28
2024-01-08 06:51:21,637 - distributed.worker - INFO - Starting Worker plugin PreImport-0f65a015-14b5-493c-be7b-a6379301979c
2024-01-08 06:51:21,637 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,645 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:51:21,646 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,646 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36131
2024-01-08 06:51:21,686 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:51:21,687 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,687 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,688 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36131
2024-01-08 06:51:21,691 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:51:21,692 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34073
2024-01-08 06:51:21,692 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34073
2024-01-08 06:51:21,692 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33537
2024-01-08 06:51:21,692 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,692 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,692 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:51:21,692 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ncn8jy3b
2024-01-08 06:51:21,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5596d1ce-789a-4d1c-886d-2aae2d5d0631
2024-01-08 06:51:21,693 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76998eb6-4317-4900-8ebe-e9835d066c51
2024-01-08 06:51:21,693 - distributed.worker - INFO - Starting Worker plugin PreImport-b6d8b4fc-e914-4b63-8f69-cdf57b9a066b
2024-01-08 06:51:21,693 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:51:21,719 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,719 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,721 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36131
2024-01-08 06:51:21,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:51:21,756 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,756 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36131
2024-01-08 06:51:21,792 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:51:21,792 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43239
2024-01-08 06:51:21,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43239
2024-01-08 06:51:21,793 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43837
2024-01-08 06:51:21,793 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,793 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,793 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:51:21,793 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hq3rferg
2024-01-08 06:51:21,793 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c87f4c52-a251-48d2-a58c-70ece7bf4bdf
2024-01-08 06:51:21,793 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0364c6d1-5994-4dc0-b077-198ae49524cf
2024-01-08 06:51:21,794 - distributed.worker - INFO - Starting Worker plugin PreImport-b53a49f8-a3aa-4534-be90-58c900bbae83
2024-01-08 06:51:21,794 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,799 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:51:21,800 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44591
2024-01-08 06:51:21,800 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44591
2024-01-08 06:51:21,800 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40493
2024-01-08 06:51:21,800 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,801 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,801 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:51:21,801 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-okdj3rid
2024-01-08 06:51:21,801 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4c72eb36-9836-4b22-ab72-75a8bedeb792
2024-01-08 06:51:21,801 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c96deee-8629-43de-a993-22080ecd7630
2024-01-08 06:51:21,801 - distributed.worker - INFO - Starting Worker plugin PreImport-1deab55a-5a45-4bb4-a6df-14b0ec50ec90
2024-01-08 06:51:21,801 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:51:21,829 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40315
2024-01-08 06:51:21,829 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40315
2024-01-08 06:51:21,829 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45547
2024-01-08 06:51:21,829 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,829 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,829 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:51:21,829 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ki4lxuh1
2024-01-08 06:51:21,829 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5679c201-798f-4ce7-aca6-db00e08f28fe
2024-01-08 06:51:21,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59bf1980-f252-47a0-b09d-6bcddf21cd8e
2024-01-08 06:51:21,830 - distributed.worker - INFO - Starting Worker plugin PreImport-c828d450-c9a8-4a03-bcca-317745bbccd0
2024-01-08 06:51:21,830 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,858 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:51:21,859 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41767
2024-01-08 06:51:21,859 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41767
2024-01-08 06:51:21,859 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32789
2024-01-08 06:51:21,859 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,859 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,859 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:51:21,859 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nhziuel3
2024-01-08 06:51:21,860 - distributed.worker - INFO - Starting Worker plugin RMMSetup-015d7feb-e48f-4b64-ba18-c4565e778304
2024-01-08 06:51:21,860 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-483621a2-a564-42cb-8387-a632357432d2
2024-01-08 06:51:21,860 - distributed.worker - INFO - Starting Worker plugin PreImport-80a48d17-d13d-48a1-af73-1f6f642a8741
2024-01-08 06:51:21,860 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,922 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:51:21,923 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,923 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,924 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36131
2024-01-08 06:51:21,924 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:51:21,925 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,925 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,926 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36131
2024-01-08 06:51:21,966 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:51:21,967 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,968 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36131
2024-01-08 06:51:21,974 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:51:21,975 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:36131
2024-01-08 06:51:21,975 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:51:21,976 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36131
2024-01-08 06:51:22,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:51:22,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:51:22,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:51:22,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:51:22,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:51:22,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:51:22,028 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:51:22,028 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-08 06:51:22,033 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34477. Reason: nanny-close
2024-01-08 06:51:22,033 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35721. Reason: nanny-close
2024-01-08 06:51:22,034 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36133. Reason: nanny-close
2024-01-08 06:51:22,034 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34073. Reason: nanny-close
2024-01-08 06:51:22,035 - distributed.core - INFO - Connection to tcp://127.0.0.1:36131 has been closed.
2024-01-08 06:51:22,036 - distributed.core - INFO - Connection to tcp://127.0.0.1:36131 has been closed.
2024-01-08 06:51:22,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43239. Reason: nanny-close
2024-01-08 06:51:22,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40315. Reason: nanny-close
2024-01-08 06:51:22,036 - distributed.core - INFO - Connection to tcp://127.0.0.1:36131 has been closed.
2024-01-08 06:51:22,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44591. Reason: nanny-close
2024-01-08 06:51:22,036 - distributed.nanny - INFO - Worker closed
2024-01-08 06:51:22,037 - distributed.core - INFO - Connection to tcp://127.0.0.1:36131 has been closed.
2024-01-08 06:51:22,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41767. Reason: nanny-close
2024-01-08 06:51:22,037 - distributed.nanny - INFO - Worker closed
2024-01-08 06:51:22,037 - distributed.core - INFO - Connection to tcp://127.0.0.1:36131 has been closed.
2024-01-08 06:51:22,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:36131 has been closed.
2024-01-08 06:51:22,038 - distributed.nanny - INFO - Worker closed
2024-01-08 06:51:22,038 - distributed.nanny - INFO - Worker closed
2024-01-08 06:51:22,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:36131 has been closed.
2024-01-08 06:51:22,039 - distributed.nanny - INFO - Worker closed
2024-01-08 06:51:22,039 - distributed.nanny - INFO - Worker closed
2024-01-08 06:51:22,039 - distributed.core - INFO - Connection to tcp://127.0.0.1:36131 has been closed.
2024-01-08 06:51:22,040 - distributed.nanny - INFO - Worker closed
2024-01-08 06:51:22,040 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-08 06:52:01,729 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:52:01,730 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:52:01,734 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:52:01,734 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34661
2024-01-08 06:52:01,734 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34661
2024-01-08 06:52:01,734 - distributed.worker - INFO -           Worker name:                          0
2024-01-08 06:52:01,735 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36365
2024-01-08 06:52:01,735 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42817
2024-01-08 06:52:01,735 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:01,735 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:52:01,735 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-08 06:52:01,735 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ooprbynz
2024-01-08 06:52:01,735 - distributed.worker - INFO - Starting Worker plugin PreImport-3571fc9a-eef0-4009-9bd0-7c939e3f59b3
2024-01-08 06:52:01,738 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-08 06:52:01,739 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-286f634b-3f03-4d8b-87ae-e0c9474d2578
2024-01-08 06:52:01,742 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd405e98-fe41-4802-a52a-9305cbf2711f
2024-01-08 06:52:01,744 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34661. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-08 06:52:01,744 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-08 06:52:01,748 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-08 06:52:06,235 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:52:06,235 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:52:06,312 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:52:06,312 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:52:06,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:52:06,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:52:06,416 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:52:06,416 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:52:06,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:52:06,436 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:52:06,452 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:52:06,452 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:52:06,498 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:52:06,498 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:52:06,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-08 06:52:06,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-08 06:52:06,933 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:52:06,934 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45855
2024-01-08 06:52:06,934 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45855
2024-01-08 06:52:06,934 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41183
2024-01-08 06:52:06,934 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41601
2024-01-08 06:52:06,934 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:06,934 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:52:06,934 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:52:06,934 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9a6s7bvn
2024-01-08 06:52:06,935 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8aac76e3-6502-43d3-ac9e-6056778ade34
2024-01-08 06:52:06,935 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a3830d7-a340-40e6-87ff-241a9494af61
2024-01-08 06:52:06,935 - distributed.worker - INFO - Starting Worker plugin PreImport-662a3c72-784e-4135-bf90-f82d3042ff44
2024-01-08 06:52:06,935 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:06,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:52:06,959 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34801
2024-01-08 06:52:06,959 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34801
2024-01-08 06:52:06,959 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34927
2024-01-08 06:52:06,959 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41601
2024-01-08 06:52:06,959 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:06,959 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:52:06,959 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:52:06,959 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7r2kdi83
2024-01-08 06:52:06,959 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ef2caa6-e6b0-42d7-9af8-658e229ddc04
2024-01-08 06:52:06,959 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e14870db-8a47-43b2-8fb0-ba30183bda1a
2024-01-08 06:52:06,960 - distributed.worker - INFO - Starting Worker plugin PreImport-e95309e5-c95f-4730-9ee6-d8433d630a4b
2024-01-08 06:52:06,960 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:06,988 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:52:06,989 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36343
2024-01-08 06:52:06,989 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36343
2024-01-08 06:52:06,989 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33387
2024-01-08 06:52:06,989 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41601
2024-01-08 06:52:06,989 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:06,989 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:52:06,989 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:52:06,989 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jvz721ll
2024-01-08 06:52:06,989 - distributed.worker - INFO - Starting Worker plugin PreImport-9c52bcca-af24-469b-ac65-b30bc201de7d
2024-01-08 06:52:06,990 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0d1d868-883e-4209-9130-e3b8aa8de21e
2024-01-08 06:52:06,990 - distributed.worker - INFO - Starting Worker plugin RMMSetup-617827b7-ab42-4204-8620-015fbc3576d1
2024-01-08 06:52:06,990 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,009 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:52:07,010 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,010 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,011 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41601
2024-01-08 06:52:07,047 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:52:07,048 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,048 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,050 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41601
2024-01-08 06:52:07,073 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:52:07,073 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42361
2024-01-08 06:52:07,073 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42361
2024-01-08 06:52:07,074 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39061
2024-01-08 06:52:07,074 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,074 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,074 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:52:07,074 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:52:07,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-25h8tm9w
2024-01-08 06:52:07,074 - distributed.worker - INFO - Starting Worker plugin PreImport-07cc218f-1460-4d03-80e0-578ce3dcb2a9
2024-01-08 06:52:07,074 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d3f03fd-8cec-4ae8-87d8-cadd5554a6fb
2024-01-08 06:52:07,074 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a851091-0f82-4d1b-a7e2-49f8f1cf7ef3
2024-01-08 06:52:07,074 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,089 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:52:07,090 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36849
2024-01-08 06:52:07,090 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36849
2024-01-08 06:52:07,090 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43555
2024-01-08 06:52:07,090 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,090 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,090 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:52:07,090 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:52:07,090 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3g54k165
2024-01-08 06:52:07,090 - distributed.worker - INFO - Starting Worker plugin PreImport-a766b829-b108-4129-98de-fff745a105ef
2024-01-08 06:52:07,090 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d8832ce-3eed-4585-b11b-2330056d0f4c
2024-01-08 06:52:07,091 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45f00f98-1096-4a80-8f86-c6cfc03a0d41
2024-01-08 06:52:07,091 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,094 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:52:07,095 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44541
2024-01-08 06:52:07,095 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44541
2024-01-08 06:52:07,095 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39467
2024-01-08 06:52:07,095 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,095 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,095 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:52:07,095 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:52:07,095 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gmqixu4u
2024-01-08 06:52:07,096 - distributed.worker - INFO - Starting Worker plugin PreImport-e5d8f43e-c6da-4d31-b32a-4a626220a2ef
2024-01-08 06:52:07,096 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ac11ab8f-604f-4964-8a9d-748e64934eba
2024-01-08 06:52:07,096 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-193d3852-5f67-43a3-9ed2-e5afb13465f0
2024-01-08 06:52:07,097 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,111 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:52:07,111 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,112 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,113 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41601
2024-01-08 06:52:07,135 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:52:07,136 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36017
2024-01-08 06:52:07,136 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36017
2024-01-08 06:52:07,136 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36021
2024-01-08 06:52:07,136 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,136 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,136 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:52:07,136 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:52:07,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-majkxgr9
2024-01-08 06:52:07,136 - distributed.worker - INFO - Starting Worker plugin PreImport-ac4806fc-114f-4ee8-97ea-cedef8cb8c4a
2024-01-08 06:52:07,136 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3ea42f71-2991-4b55-a915-4ef746b44550
2024-01-08 06:52:07,137 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2436f8d4-6d62-4533-a5ce-102a6c25dd69
2024-01-08 06:52:07,137 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-08 06:52:07,172 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41711
2024-01-08 06:52:07,172 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41711
2024-01-08 06:52:07,172 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45947
2024-01-08 06:52:07,173 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,173 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,173 - distributed.worker - INFO -               Threads:                          1
2024-01-08 06:52:07,173 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-08 06:52:07,173 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7504tprm
2024-01-08 06:52:07,173 - distributed.worker - INFO - Starting Worker plugin PreImport-1bfa0d9b-d34a-4158-9987-5265afa07a28
2024-01-08 06:52:07,173 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a265bdf-0fe9-4c48-b30a-feaae8927996
2024-01-08 06:52:07,174 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19d2a70d-432d-493a-836d-4bb671e3dd14
2024-01-08 06:52:07,174 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:52:07,184 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,184 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,185 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41601
2024-01-08 06:52:07,244 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:52:07,245 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,245 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41601
2024-01-08 06:52:07,248 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:52:07,249 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,249 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,251 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41601
2024-01-08 06:52:07,281 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:52:07,282 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,282 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,283 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41601
2024-01-08 06:52:07,295 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-08 06:52:07,295 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:41601
2024-01-08 06:52:07,295 - distributed.worker - INFO - -------------------------------------------------
2024-01-08 06:52:07,297 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:41601
2024-01-08 06:52:07,322 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45855. Reason: nanny-close
2024-01-08 06:52:07,323 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34801. Reason: nanny-close
2024-01-08 06:52:07,323 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36343. Reason: nanny-close
2024-01-08 06:52:07,324 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36849. Reason: nanny-close
2024-01-08 06:52:07,324 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42361. Reason: nanny-close
2024-01-08 06:52:07,325 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44541. Reason: nanny-close
2024-01-08 06:52:07,325 - distributed.core - INFO - Connection to tcp://127.0.0.1:41601 has been closed.
2024-01-08 06:52:07,325 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36017. Reason: nanny-close
2024-01-08 06:52:07,325 - distributed.core - INFO - Connection to tcp://127.0.0.1:41601 has been closed.
2024-01-08 06:52:07,325 - distributed.core - INFO - Connection to tcp://127.0.0.1:41601 has been closed.
2024-01-08 06:52:07,326 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41711. Reason: nanny-close
2024-01-08 06:52:07,326 - distributed.core - INFO - Connection to tcp://127.0.0.1:41601 has been closed.
2024-01-08 06:52:07,326 - distributed.nanny - INFO - Worker closed
2024-01-08 06:52:07,326 - distributed.core - INFO - Connection to tcp://127.0.0.1:41601 has been closed.
2024-01-08 06:52:07,327 - distributed.core - INFO - Connection to tcp://127.0.0.1:41601 has been closed.
2024-01-08 06:52:07,327 - distributed.nanny - INFO - Worker closed
2024-01-08 06:52:07,327 - distributed.nanny - INFO - Worker closed
2024-01-08 06:52:07,327 - distributed.core - INFO - Connection to tcp://127.0.0.1:41601 has been closed.
2024-01-08 06:52:07,327 - distributed.core - INFO - Connection to tcp://127.0.0.1:41601 has been closed.
2024-01-08 06:52:07,328 - distributed.nanny - INFO - Worker closed
2024-01-08 06:52:07,328 - distributed.nanny - INFO - Worker closed
2024-01-08 06:52:07,328 - distributed.nanny - INFO - Worker closed
2024-01-08 06:52:07,328 - distributed.nanny - INFO - Worker closed
2024-01-08 06:52:07,329 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk PASSED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-08 06:53:46,514 - distributed.worker - WARNING - RMM allocation of 1.00 MiB failed, spill-on-demand couldn't find any device memory to spill.
RMM allocs: 1.00 MiB, <ProxyManager dev_limit=25.60 GiB host_limit=0.98 TiB disk=0 B(0) host=0 B(0) dev=0 B(0)>, traceback:
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 937, in _bootstrap
    self._bootstrap_inner()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/threadpoolexecutor.py", line 57, in _worker
    task.run()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/_concurrent_futures_thread.py", line 65, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1541, in <lambda>
    executor, lambda: context.run(func, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2954, in apply_function
    msg = apply_function_simple(function, args, kwargs, time_delay)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2990, in apply_function_simple
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_proxify_host_file.py", line 467, in task
    rmm.DeviceBuffer(size=rmm_pool_size),  # Trigger OOM
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/proxify_host_file.py", line 617, in oom
    traceback.print_stack(file=f)


2024-01-08 06:53:46,821 - distributed.worker - WARNING - Compute Failed
Key:       task-5c302f54392a5ca98adb3c55c8357b0e
Function:  task
args:      ()
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_serializer PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[numpy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[cupy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_name PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj0] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj1] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[dask] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[pickle] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[disk] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-send_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-send_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucx-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucx-send_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucx-send_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucxx-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucxx-send_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucxx-send_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_disk_objects[True-tcp] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_disk_objects[True-ucx] [1704696903.516692] [dgx13:92699:0]            sock.c:470  UCX  ERROR bind(fd=164 addr=0.0.0.0:46032) failed: Address already in use
PASSED
dask_cuda/tests/test_proxy.py::test_communicating_disk_objects[True-ucxx] /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
