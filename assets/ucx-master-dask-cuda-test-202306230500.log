============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.2, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-06-23 05:45:06,366 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:06,371 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43583 instead
  warnings.warn(
2023-06-23 05:45:06,375 - distributed.scheduler - INFO - State start
2023-06-23 05:45:06,399 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:06,400 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-23 05:45:06,400 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43583/status
2023-06-23 05:45:06,614 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40871'
2023-06-23 05:45:06,635 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34555'
2023-06-23 05:45:06,638 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35943'
2023-06-23 05:45:06,647 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39427'
2023-06-23 05:45:08,457 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:08,458 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:08,466 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:08,498 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:08,498 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:08,506 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:08,513 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:08,513 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:08,521 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:08,593 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:08,593 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:08,601 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-06-23 05:45:08,702 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35199
2023-06-23 05:45:08,702 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35199
2023-06-23 05:45:08,702 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39917
2023-06-23 05:45:08,702 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:45:08,702 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:08,702 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:45:08,703 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:45:08,703 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x36poqt5
2023-06-23 05:45:08,703 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d5967b8-9cb1-46dc-990a-d298c2b1a76e
2023-06-23 05:45:08,703 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6b36b7c4-a30a-4ddb-8bab-e2d0bc718900
2023-06-23 05:45:08,703 - distributed.worker - INFO - Starting Worker plugin PreImport-8913f2d5-7a91-4c19-b0c8-b73b0be7ed36
2023-06-23 05:45:08,703 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:08,719 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35199', status: init, memory: 0, processing: 0>
2023-06-23 05:45:08,732 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35199
2023-06-23 05:45:08,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40964
2023-06-23 05:45:08,733 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:45:08,733 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:08,735 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:45:09,567 - distributed.scheduler - INFO - Receive client connection: Client-1a64f8d5-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:09,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40980
2023-06-23 05:45:09,944 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42479
2023-06-23 05:45:09,944 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42479
2023-06-23 05:45:09,944 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44605
2023-06-23 05:45:09,944 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:45:09,945 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:09,945 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:45:09,945 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:45:09,945 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5wan651c
2023-06-23 05:45:09,945 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-279d8ed2-2c7f-4b41-9333-a09e5e741214
2023-06-23 05:45:09,945 - distributed.worker - INFO - Starting Worker plugin RMMSetup-107ead33-ec21-43a4-8f04-236bd8f14cfe
2023-06-23 05:45:09,945 - distributed.worker - INFO - Starting Worker plugin PreImport-ff184925-078b-4f08-a6b2-c94a6c377b47
2023-06-23 05:45:09,946 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:09,977 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42479', status: init, memory: 0, processing: 0>
2023-06-23 05:45:09,978 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42479
2023-06-23 05:45:09,978 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58742
2023-06-23 05:45:09,979 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:45:09,979 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:09,981 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:45:10,109 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41535
2023-06-23 05:45:10,109 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41535
2023-06-23 05:45:10,109 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46193
2023-06-23 05:45:10,110 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:45:10,110 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:10,110 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:45:10,110 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:45:10,110 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_f0azgmj
2023-06-23 05:45:10,110 - distributed.worker - INFO - Starting Worker plugin PreImport-ac510678-33a6-4fb3-87c2-42a460cb800c
2023-06-23 05:45:10,110 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0acfba4d-9f41-4df7-95a6-7890d7ec10e1
2023-06-23 05:45:10,110 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-458130e0-264f-4edb-aa4e-68fc780ee99e
2023-06-23 05:45:10,111 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:10,137 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45813
2023-06-23 05:45:10,137 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45813
2023-06-23 05:45:10,137 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43857
2023-06-23 05:45:10,137 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:45:10,137 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:10,137 - distributed.worker - INFO -               Threads:                          4
2023-06-23 05:45:10,137 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-23 05:45:10,137 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-n8pihv01
2023-06-23 05:45:10,138 - distributed.worker - INFO - Starting Worker plugin RMMSetup-280c05da-870c-40b4-82dd-5b7012b02886
2023-06-23 05:45:10,138 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-09cc249c-96b9-461a-9830-b2fc3893686e
2023-06-23 05:45:10,138 - distributed.worker - INFO - Starting Worker plugin PreImport-a53b95e8-51aa-4060-bfa5-8605ae491590
2023-06-23 05:45:10,138 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:10,158 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45813', status: init, memory: 0, processing: 0>
2023-06-23 05:45:10,159 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45813
2023-06-23 05:45:10,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58756
2023-06-23 05:45:10,159 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:45:10,159 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:10,161 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:45:10,170 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41535', status: init, memory: 0, processing: 0>
2023-06-23 05:45:10,171 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41535
2023-06-23 05:45:10,171 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58744
2023-06-23 05:45:10,173 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:45:10,174 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:10,181 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:45:10,190 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-23 05:45:10,190 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-23 05:45:10,190 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-23 05:45:10,191 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-23 05:45:10,195 - distributed.scheduler - INFO - Remove client Client-1a64f8d5-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:10,196 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40980; closing.
2023-06-23 05:45:10,196 - distributed.scheduler - INFO - Remove client Client-1a64f8d5-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:10,196 - distributed.scheduler - INFO - Close client connection: Client-1a64f8d5-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:10,197 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40871'. Reason: nanny-close
2023-06-23 05:45:10,198 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:10,198 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34555'. Reason: nanny-close
2023-06-23 05:45:10,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:10,199 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35943'. Reason: nanny-close
2023-06-23 05:45:10,199 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42479. Reason: nanny-close
2023-06-23 05:45:10,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:10,200 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39427'. Reason: nanny-close
2023-06-23 05:45:10,200 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45813. Reason: nanny-close
2023-06-23 05:45:10,200 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:10,201 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41535. Reason: nanny-close
2023-06-23 05:45:10,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:45:10,201 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58742; closing.
2023-06-23 05:45:10,201 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35199. Reason: nanny-close
2023-06-23 05:45:10,201 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:45:10,202 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42479', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:10,202 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42479
2023-06-23 05:45:10,202 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:10,203 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:10,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:45:10,203 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:45:10,204 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:10,204 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58756; closing.
2023-06-23 05:45:10,205 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:10,205 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45813', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:10,205 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45813
2023-06-23 05:45:10,206 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40964; closing.
2023-06-23 05:45:10,206 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58744; closing.
2023-06-23 05:45:10,206 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35199', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:10,206 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35199
2023-06-23 05:45:10,207 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41535', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:10,207 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41535
2023-06-23 05:45:10,207 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:45:11,515 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:45:11,515 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:45:11,516 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:45:11,516 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-23 05:45:11,517 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-06-23 05:45:13,869 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:13,874 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34543 instead
  warnings.warn(
2023-06-23 05:45:13,879 - distributed.scheduler - INFO - State start
2023-06-23 05:45:13,903 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:13,904 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:45:13,905 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34543/status
2023-06-23 05:45:14,188 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45791'
2023-06-23 05:45:14,215 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33041'
2023-06-23 05:45:14,227 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42313'
2023-06-23 05:45:14,229 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44851'
2023-06-23 05:45:14,241 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44435'
2023-06-23 05:45:14,253 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39391'
2023-06-23 05:45:14,268 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34685'
2023-06-23 05:45:14,281 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39447'
2023-06-23 05:45:15,279 - distributed.scheduler - INFO - Receive client connection: Client-1ed0fb39-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:15,302 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40250
2023-06-23 05:45:16,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:16,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:16,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:16,098 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:16,098 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:16,099 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:16,111 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:16,131 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:16,131 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:16,135 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:16,135 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:16,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:16,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:16,158 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:16,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:16,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:16,167 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:16,177 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:16,195 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:16,195 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:16,232 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:16,234 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:16,261 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:16,265 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:20,264 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38391
2023-06-23 05:45:20,264 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38391
2023-06-23 05:45:20,264 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39435
2023-06-23 05:45:20,264 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:20,264 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:20,264 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:20,264 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:20,265 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i5kpwbrq
2023-06-23 05:45:20,265 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e10b0495-b4da-43b9-bc51-5dabfa7c3ed8
2023-06-23 05:45:20,269 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33865
2023-06-23 05:45:20,270 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33865
2023-06-23 05:45:20,270 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45835
2023-06-23 05:45:20,270 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:20,270 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:20,270 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:20,270 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:20,270 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-occ1ob4x
2023-06-23 05:45:20,271 - distributed.worker - INFO - Starting Worker plugin PreImport-69b48a56-490f-43d9-958c-2388782195c4
2023-06-23 05:45:20,271 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f3d22267-c89d-4793-b963-7ed50a9ebaa3
2023-06-23 05:45:20,555 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32829
2023-06-23 05:45:20,555 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32829
2023-06-23 05:45:20,555 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46127
2023-06-23 05:45:20,555 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:20,555 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:20,555 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:20,555 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:20,555 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-3do58e4u
2023-06-23 05:45:20,556 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b377fc86-426a-489a-9162-5304835944fd
2023-06-23 05:45:20,556 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ac9b60c4-412b-4c9e-acf9-24ce141c0728
2023-06-23 05:45:20,587 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35691
2023-06-23 05:45:20,588 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35691
2023-06-23 05:45:20,588 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33823
2023-06-23 05:45:20,588 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:20,588 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:20,588 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:20,588 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:20,588 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-slmwx9ee
2023-06-23 05:45:20,589 - distributed.worker - INFO - Starting Worker plugin PreImport-35134677-3feb-4a09-9148-f1adfea2b3f7
2023-06-23 05:45:20,589 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dea0a3fe-2fc5-40a4-a220-481ac5bd54d1
2023-06-23 05:45:20,854 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36149
2023-06-23 05:45:20,854 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36149
2023-06-23 05:45:20,854 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46241
2023-06-23 05:45:20,854 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:20,854 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:20,854 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:20,854 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:20,854 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-i_mtb8kl
2023-06-23 05:45:20,855 - distributed.worker - INFO - Starting Worker plugin PreImport-f3f54df1-bff6-4048-a34d-78c903d7daf3
2023-06-23 05:45:20,855 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5690c7e-33d2-424c-9de6-9fc9a47a96c0
2023-06-23 05:45:20,855 - distributed.worker - INFO - Starting Worker plugin RMMSetup-83a9a4b5-26cf-4532-b7a7-027c37fb5159
2023-06-23 05:45:20,864 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46249
2023-06-23 05:45:20,864 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46249
2023-06-23 05:45:20,864 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43593
2023-06-23 05:45:20,864 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:20,864 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:20,864 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:20,864 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:20,864 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-75934jpr
2023-06-23 05:45:20,865 - distributed.worker - INFO - Starting Worker plugin PreImport-1e2142ce-58b1-481e-b420-5c5b688850ec
2023-06-23 05:45:20,865 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8a9e86d2-1df0-43a2-bb2c-620bb538bde4
2023-06-23 05:45:20,870 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41701
2023-06-23 05:45:20,871 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41701
2023-06-23 05:45:20,871 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34011
2023-06-23 05:45:20,871 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:20,871 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:20,871 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:20,871 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:20,871 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pz3_6_el
2023-06-23 05:45:20,872 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f2b3e0e-d2f5-43d2-ba1b-749d92ada15e
2023-06-23 05:45:20,872 - distributed.worker - INFO - Starting Worker plugin RMMSetup-770d1889-3d95-4c8f-9824-e6cb8e180a98
2023-06-23 05:45:20,876 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37309
2023-06-23 05:45:20,877 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37309
2023-06-23 05:45:20,877 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38673
2023-06-23 05:45:20,877 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:20,877 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:20,877 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:20,877 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:20,877 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fahis4xc
2023-06-23 05:45:20,878 - distributed.worker - INFO - Starting Worker plugin PreImport-2128b1d9-d6ae-4e2c-915d-c286349615ba
2023-06-23 05:45:20,878 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7dceef16-2653-45c9-8b5c-399b0cebe57a
2023-06-23 05:45:21,051 - distributed.worker - INFO - Starting Worker plugin PreImport-e8341e81-e447-4562-b4d5-653d982bb6a4
2023-06-23 05:45:21,051 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3906f337-952a-4e1d-8358-a2b4efd80b50
2023-06-23 05:45:21,052 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,099 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38391', status: init, memory: 0, processing: 0>
2023-06-23 05:45:21,100 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38391
2023-06-23 05:45:21,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59396
2023-06-23 05:45:21,101 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:21,101 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:21,144 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db91ce7a-406e-4fd3-b1c0-358fe0bba23b
2023-06-23 05:45:21,145 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4cc7972f-24c9-430f-a1d4-ac30ff80c317
2023-06-23 05:45:21,145 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,145 - distributed.worker - INFO - Starting Worker plugin PreImport-8076e2ff-ab58-42ae-9e46-d08218e54cfb
2023-06-23 05:45:21,145 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,145 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,145 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9ad14f33-3adf-4e65-a10a-b2744266439c
2023-06-23 05:45:21,146 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,146 - distributed.worker - INFO - Starting Worker plugin PreImport-ff3d9005-d28d-4828-b9a9-c7ae691d912e
2023-06-23 05:45:21,146 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,146 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,185 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36149', status: init, memory: 0, processing: 0>
2023-06-23 05:45:21,186 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36149
2023-06-23 05:45:21,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59414
2023-06-23 05:45:21,186 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:21,187 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,188 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32829', status: init, memory: 0, processing: 0>
2023-06-23 05:45:21,188 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32829
2023-06-23 05:45:21,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59416
2023-06-23 05:45:21,188 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dcfd9bc1-0e4c-49e2-9afd-da8e2b89403b
2023-06-23 05:45:21,188 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:21,189 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:21,189 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41701', status: init, memory: 0, processing: 0>
2023-06-23 05:45:21,189 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,189 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41701
2023-06-23 05:45:21,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59398
2023-06-23 05:45:21,190 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:21,190 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,191 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:21,191 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33865', status: init, memory: 0, processing: 0>
2023-06-23 05:45:21,191 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33865
2023-06-23 05:45:21,192 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59428
2023-06-23 05:45:21,192 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:21,192 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:21,193 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35691', status: init, memory: 0, processing: 0>
2023-06-23 05:45:21,193 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35691
2023-06-23 05:45:21,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59444
2023-06-23 05:45:21,194 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46249', status: init, memory: 0, processing: 0>
2023-06-23 05:45:21,194 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:21,194 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,194 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46249
2023-06-23 05:45:21,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59450
2023-06-23 05:45:21,195 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:21,195 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:21,195 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:21,198 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:21,229 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37309', status: init, memory: 0, processing: 0>
2023-06-23 05:45:21,229 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37309
2023-06-23 05:45:21,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59466
2023-06-23 05:45:21,230 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:21,230 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:21,232 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:21,318 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:21,319 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:21,319 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:21,319 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:21,319 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:21,319 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:21,319 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:21,320 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:21,327 - distributed.scheduler - INFO - Remove client Client-1ed0fb39-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:21,327 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40250; closing.
2023-06-23 05:45:21,327 - distributed.scheduler - INFO - Remove client Client-1ed0fb39-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:21,328 - distributed.scheduler - INFO - Close client connection: Client-1ed0fb39-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:21,329 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45791'. Reason: nanny-close
2023-06-23 05:45:21,329 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:21,330 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33041'. Reason: nanny-close
2023-06-23 05:45:21,330 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:21,331 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42313'. Reason: nanny-close
2023-06-23 05:45:21,331 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32829. Reason: nanny-close
2023-06-23 05:45:21,331 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:21,331 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44851'. Reason: nanny-close
2023-06-23 05:45:21,331 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:21,331 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37309. Reason: nanny-close
2023-06-23 05:45:21,332 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44435'. Reason: nanny-close
2023-06-23 05:45:21,332 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33865. Reason: nanny-close
2023-06-23 05:45:21,332 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:21,332 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39391'. Reason: nanny-close
2023-06-23 05:45:21,332 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59416; closing.
2023-06-23 05:45:21,332 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35691. Reason: nanny-close
2023-06-23 05:45:21,332 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:21,332 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:21,333 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32829', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:21,333 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32829
2023-06-23 05:45:21,333 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34685'. Reason: nanny-close
2023-06-23 05:45:21,333 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38391. Reason: nanny-close
2023-06-23 05:45:21,333 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:21,333 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:21,333 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36149. Reason: nanny-close
2023-06-23 05:45:21,333 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39447'. Reason: nanny-close
2023-06-23 05:45:21,334 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:21,334 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:21,334 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32829
2023-06-23 05:45:21,334 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41701. Reason: nanny-close
2023-06-23 05:45:21,334 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59466; closing.
2023-06-23 05:45:21,334 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:21,334 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:21,334 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32829
2023-06-23 05:45:21,334 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32829
2023-06-23 05:45:21,335 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37309', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:21,335 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32829
2023-06-23 05:45:21,335 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32829
2023-06-23 05:45:21,335 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37309
2023-06-23 05:45:21,335 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:21,335 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:21,335 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:21,335 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59428; closing.
2023-06-23 05:45:21,336 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46249. Reason: nanny-close
2023-06-23 05:45:21,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59444; closing.
2023-06-23 05:45:21,336 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:21,336 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33865', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:21,336 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33865
2023-06-23 05:45:21,336 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32829
2023-06-23 05:45:21,336 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:21,336 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:21,336 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35691', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:21,337 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35691
2023-06-23 05:45:21,337 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:21,337 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:21,337 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59396; closing.
2023-06-23 05:45:21,337 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59414; closing.
2023-06-23 05:45:21,337 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38391', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:21,337 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38391
2023-06-23 05:45:21,338 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36149', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:21,338 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36149
2023-06-23 05:45:21,338 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:21,338 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59398; closing.
2023-06-23 05:45:21,339 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41701', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:21,339 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41701
2023-06-23 05:45:21,342 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59450; closing.
2023-06-23 05:45:21,342 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37309
2023-06-23 05:45:21,342 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46249', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:21,342 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33865
2023-06-23 05:45:21,342 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46249
2023-06-23 05:45:21,342 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35691
2023-06-23 05:45:21,342 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:45:21,342 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38391
2023-06-23 05:45:21,342 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36149
2023-06-23 05:45:21,343 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41701
2023-06-23 05:45:21,343 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:21,344 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:24,535 - distributed.nanny - WARNING - Worker process still alive after 3.1999987792968754 seconds, killing
2023-06-23 05:45:24,535 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2023-06-23 05:45:24,535 - distributed.nanny - WARNING - Worker process still alive after 3.1999992370605472 seconds, killing
2023-06-23 05:45:24,536 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-23 05:45:24,536 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-23 05:45:24,536 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-23 05:45:24,537 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-23 05:45:24,537 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-23 05:45:25,360 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 607, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 390, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 844, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-23 05:45:25,363 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 607, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 390, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 844, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-23 05:45:25,363 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 607, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 390, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 844, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-23 05:45:25,364 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 607, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 390, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 844, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-23 05:45:25,364 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 607, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 390, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 844, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-23 05:45:25,365 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 607, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 390, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 844, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-23 05:45:25,365 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 607, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 390, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 844, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-23 05:45:25,365 - distributed.nanny - ERROR - Error in Nanny killing Worker subprocess
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 490, in wait_for
    return fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 607, in close
    await self.kill(timeout=timeout, reason=reason)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 390, in kill
    await self.process.kill(reason=reason, timeout=0.8 * (deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 844, in kill
    await process.join(max(0, deadline - time()))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/process.py", line 330, in join
    await wait_for(asyncio.shield(self._exit_future), timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 492, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError
2023-06-23 05:45:25,369 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50321 parent=50131 started daemon>
2023-06-23 05:45:25,369 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50318 parent=50131 started daemon>
2023-06-23 05:45:25,370 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50315 parent=50131 started daemon>
2023-06-23 05:45:25,370 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50312 parent=50131 started daemon>
2023-06-23 05:45:25,370 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50308 parent=50131 started daemon>
2023-06-23 05:45:25,370 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50304 parent=50131 started daemon>
2023-06-23 05:45:25,370 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50300 parent=50131 started daemon>
2023-06-23 05:45:25,370 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=50297 parent=50131 started daemon>
2023-06-23 05:45:25,533 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 50318 exit status was already read will report exitcode 255
2023-06-23 05:45:25,729 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 50321 exit status was already read will report exitcode 255
2023-06-23 05:45:26,254 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:45:26,254 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:45:26,255 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:45:26,256 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:45:26,256 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-06-23 05:45:28,757 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:28,762 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34577 instead
  warnings.warn(
2023-06-23 05:45:28,767 - distributed.scheduler - INFO - State start
2023-06-23 05:45:29,929 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:29,931 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:45:29,932 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34577/status
2023-06-23 05:45:30,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43583'
2023-06-23 05:45:30,099 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34041'
2023-06-23 05:45:30,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34171'
2023-06-23 05:45:30,111 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42925'
2023-06-23 05:45:30,124 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43393'
2023-06-23 05:45:30,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45985'
2023-06-23 05:45:30,152 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45467'
2023-06-23 05:45:30,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38843'
2023-06-23 05:45:31,477 - distributed.scheduler - INFO - Receive client connection: Client-2796479b-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:31,491 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43142
2023-06-23 05:45:31,923 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:31,923 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:31,923 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:31,923 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:31,927 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:31,927 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:31,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:31,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:31,954 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:31,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:31,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:31,982 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:31,982 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:31,990 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:31,990 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:32,070 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:32,070 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:32,070 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:32,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:32,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:32,094 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:32,094 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:32,341 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:32,366 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:36,628 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43465
2023-06-23 05:45:36,628 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43465
2023-06-23 05:45:36,628 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36707
2023-06-23 05:45:36,628 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,628 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,628 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:36,628 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:36,628 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39071
2023-06-23 05:45:36,628 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7mfc0kpv
2023-06-23 05:45:36,628 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39071
2023-06-23 05:45:36,628 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42869
2023-06-23 05:45:36,628 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,629 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,629 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:36,629 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:36,629 - distributed.worker - INFO - Starting Worker plugin PreImport-a57510a9-b9d1-49ec-9ca3-05b595c3f02c
2023-06-23 05:45:36,629 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8j3suurv
2023-06-23 05:45:36,629 - distributed.worker - INFO - Starting Worker plugin RMMSetup-245b23c5-508b-4b3a-8c57-1012d3695b3c
2023-06-23 05:45:36,629 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3a40a962-44a9-4a1a-8273-37b108ca4bdd
2023-06-23 05:45:36,639 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41721
2023-06-23 05:45:36,639 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41721
2023-06-23 05:45:36,639 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32845
2023-06-23 05:45:36,639 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,639 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,639 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:36,639 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:36,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_ddtbw35
2023-06-23 05:45:36,640 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de0838c7-bc77-4a49-a0b5-744f386941d6
2023-06-23 05:45:36,640 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6ea1160-5954-49f0-9af8-b8b6119223a7
2023-06-23 05:45:36,653 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35003
2023-06-23 05:45:36,653 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35003
2023-06-23 05:45:36,653 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35417
2023-06-23 05:45:36,653 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,653 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,653 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:36,653 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:36,653 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mq6diwcs
2023-06-23 05:45:36,654 - distributed.worker - INFO - Starting Worker plugin PreImport-38dbd8b6-6b50-4f4a-b2eb-414d64dfb02d
2023-06-23 05:45:36,654 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a65dba4a-838b-44d1-b08b-feb295908dbe
2023-06-23 05:45:36,654 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33257
2023-06-23 05:45:36,655 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33257
2023-06-23 05:45:36,655 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38049
2023-06-23 05:45:36,655 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,655 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,655 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:36,655 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:36,655 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tthn1o1z
2023-06-23 05:45:36,656 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b0a9db44-507a-4880-af53-0e6abcc48e9f
2023-06-23 05:45:36,656 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dd6c4138-7613-4ce3-93d7-210c7986eaf8
2023-06-23 05:45:36,762 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41279
2023-06-23 05:45:36,762 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41279
2023-06-23 05:45:36,762 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42875
2023-06-23 05:45:36,762 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,762 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,762 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:36,762 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:36,762 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5wdjq6i6
2023-06-23 05:45:36,763 - distributed.worker - INFO - Starting Worker plugin PreImport-70d9f105-74d8-46d8-9df0-4d56c87f062c
2023-06-23 05:45:36,763 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e7fc8526-ff35-4a6a-9290-a3bef3ecebc4
2023-06-23 05:45:36,771 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39119
2023-06-23 05:45:36,772 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39119
2023-06-23 05:45:36,772 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33053
2023-06-23 05:45:36,772 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45625
2023-06-23 05:45:36,772 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45625
2023-06-23 05:45:36,772 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,772 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,772 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43177
2023-06-23 05:45:36,772 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:36,772 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,772 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,772 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:36,772 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:36,772 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-361pq_qn
2023-06-23 05:45:36,772 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:36,772 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-tczw5r2q
2023-06-23 05:45:36,773 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f330545-cc93-4833-8769-1fb994caacd5
2023-06-23 05:45:36,773 - distributed.worker - INFO - Starting Worker plugin PreImport-eeaf7783-a3b2-4ce3-825e-5a07acbf892a
2023-06-23 05:45:36,773 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11b798c0-4088-4155-80b2-d3c662639eff
2023-06-23 05:45:36,773 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b90fd4b8-895d-456e-a4fa-ff235b189c13
2023-06-23 05:45:36,792 - distributed.worker - INFO - Starting Worker plugin PreImport-d6e7ab38-c439-47e1-b74e-ed32fd80182a
2023-06-23 05:45:36,792 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,792 - distributed.worker - INFO - Starting Worker plugin PreImport-dfe304f7-7d65-4693-9d1a-9383c9b276c3
2023-06-23 05:45:36,793 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40c488e6-b200-43c3-8010-3dcc8b4679a5
2023-06-23 05:45:36,792 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54354937-9d02-4db2-bda8-e393211602f7
2023-06-23 05:45:36,793 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,793 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9259677-e8bc-4264-9c3a-1e863b2e69fd
2023-06-23 05:45:36,793 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,794 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,795 - distributed.worker - INFO - Starting Worker plugin PreImport-bde70b8e-3477-498f-8b03-b18c176b96b8
2023-06-23 05:45:36,797 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,817 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37bbaced-271f-4943-a76f-b71e87a77db6
2023-06-23 05:45:36,817 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,819 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ae3c7f43-2ba3-4c42-8ca9-ab8838195002
2023-06-23 05:45:36,819 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,822 - distributed.worker - INFO - Starting Worker plugin PreImport-5c2f8970-5df2-44f1-9541-485ce6e4be54
2023-06-23 05:45:36,821 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39071', status: init, memory: 0, processing: 0>
2023-06-23 05:45:36,822 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,823 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39071
2023-06-23 05:45:36,823 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43172
2023-06-23 05:45:36,823 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,823 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,824 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35003', status: init, memory: 0, processing: 0>
2023-06-23 05:45:36,824 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35003
2023-06-23 05:45:36,824 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43178
2023-06-23 05:45:36,825 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:36,827 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41721', status: init, memory: 0, processing: 0>
2023-06-23 05:45:36,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:36,827 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41721
2023-06-23 05:45:36,827 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43156
2023-06-23 05:45:36,828 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,828 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:36,831 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43465', status: init, memory: 0, processing: 0>
2023-06-23 05:45:36,831 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43465
2023-06-23 05:45:36,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43182
2023-06-23 05:45:36,832 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,832 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:36,836 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33257', status: init, memory: 0, processing: 0>
2023-06-23 05:45:36,836 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33257
2023-06-23 05:45:36,836 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43188
2023-06-23 05:45:36,837 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,837 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,840 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:36,851 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41279', status: init, memory: 0, processing: 0>
2023-06-23 05:45:36,851 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41279
2023-06-23 05:45:36,851 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43200
2023-06-23 05:45:36,852 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,852 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,855 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:36,855 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45625', status: init, memory: 0, processing: 0>
2023-06-23 05:45:36,856 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45625
2023-06-23 05:45:36,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43222
2023-06-23 05:45:36,857 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,857 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:36,862 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39119', status: init, memory: 0, processing: 0>
2023-06-23 05:45:36,862 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39119
2023-06-23 05:45:36,862 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43212
2023-06-23 05:45:36,863 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:36,863 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:36,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:36,913 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:36,913 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:36,914 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:36,914 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:36,914 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:36,914 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:36,914 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:36,914 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:36,922 - distributed.scheduler - INFO - Remove client Client-2796479b-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:36,922 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43142; closing.
2023-06-23 05:45:36,923 - distributed.scheduler - INFO - Remove client Client-2796479b-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:36,923 - distributed.scheduler - INFO - Close client connection: Client-2796479b-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:36,924 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42925'. Reason: nanny-close
2023-06-23 05:45:36,925 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:36,925 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43583'. Reason: nanny-close
2023-06-23 05:45:36,926 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:36,926 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41721. Reason: nanny-close
2023-06-23 05:45:36,926 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34041'. Reason: nanny-close
2023-06-23 05:45:36,927 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:36,927 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39071. Reason: nanny-close
2023-06-23 05:45:36,927 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34171'. Reason: nanny-close
2023-06-23 05:45:36,928 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:36,928 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45625. Reason: nanny-close
2023-06-23 05:45:36,928 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43393'. Reason: nanny-close
2023-06-23 05:45:36,928 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:36,928 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:36,928 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43156; closing.
2023-06-23 05:45:36,929 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45985'. Reason: nanny-close
2023-06-23 05:45:36,929 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41721', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:36,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33257. Reason: nanny-close
2023-06-23 05:45:36,929 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:36,929 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41721
2023-06-23 05:45:36,929 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:36,929 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45467'. Reason: nanny-close
2023-06-23 05:45:36,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39119. Reason: nanny-close
2023-06-23 05:45:36,930 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:36,930 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43172; closing.
2023-06-23 05:45:36,930 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:36,930 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35003. Reason: nanny-close
2023-06-23 05:45:36,930 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38843'. Reason: nanny-close
2023-06-23 05:45:36,930 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:36,930 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:36,930 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:36,931 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39071', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:36,931 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39071
2023-06-23 05:45:36,931 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41279. Reason: nanny-close
2023-06-23 05:45:36,931 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41721
2023-06-23 05:45:36,931 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41721
2023-06-23 05:45:36,931 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41721
2023-06-23 05:45:36,931 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43222; closing.
2023-06-23 05:45:36,932 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43188; closing.
2023-06-23 05:45:36,932 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:36,932 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:36,932 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:36,932 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43465. Reason: nanny-close
2023-06-23 05:45:36,932 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45625', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:36,932 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41721
2023-06-23 05:45:36,932 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45625
2023-06-23 05:45:36,932 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33257', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:36,933 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33257
2023-06-23 05:45:36,933 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:36,933 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:36,933 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43212; closing.
2023-06-23 05:45:36,933 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41721
2023-06-23 05:45:36,933 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:36,933 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39119', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:36,934 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39119
2023-06-23 05:45:36,934 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:36,934 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:36,934 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43178; closing.
2023-06-23 05:45:36,934 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:36,935 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35003', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:36,935 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35003
2023-06-23 05:45:36,935 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:36,935 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43200; closing.
2023-06-23 05:45:36,935 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:36,938 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41279', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:36,939 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41279
2023-06-23 05:45:36,939 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43182; closing.
2023-06-23 05:45:36,939 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43465', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:36,939 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43465
2023-06-23 05:45:36,939 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:45:38,642 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:45:38,642 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:45:38,643 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:45:38,644 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:45:38,644 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-06-23 05:45:40,868 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:40,873 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33437 instead
  warnings.warn(
2023-06-23 05:45:40,876 - distributed.scheduler - INFO - State start
2023-06-23 05:45:40,956 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:40,957 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:45:40,957 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33437/status
2023-06-23 05:45:41,016 - distributed.scheduler - INFO - Receive client connection: Client-2efb5625-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:41,027 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42430
2023-06-23 05:45:42,143 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44205'
2023-06-23 05:45:42,164 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41465'
2023-06-23 05:45:42,166 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34655'
2023-06-23 05:45:42,173 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39093'
2023-06-23 05:45:42,182 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44471'
2023-06-23 05:45:42,190 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37329'
2023-06-23 05:45:42,203 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33613'
2023-06-23 05:45:42,205 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32995'
2023-06-23 05:45:43,931 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:43,931 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:43,948 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:43,948 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:43,957 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:43,957 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:44,009 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:44,009 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:44,020 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:44,020 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:44,048 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:44,048 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:44,048 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:44,049 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:44,106 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:45:44,106 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:45:44,190 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:44,277 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:44,279 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:44,328 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:44,352 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:44,526 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:44,571 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:44,682 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:45:53,352 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35883
2023-06-23 05:45:53,352 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35883
2023-06-23 05:45:53,352 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43289
2023-06-23 05:45:53,352 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:53,352 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,352 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:53,352 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:53,352 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ypyk9uai
2023-06-23 05:45:53,353 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aae6fa3d-7583-434a-a463-fbec46a167bc
2023-06-23 05:45:53,353 - distributed.worker - INFO - Starting Worker plugin RMMSetup-26da34a1-b9ec-43cf-a73a-1c7f97d4f6f4
2023-06-23 05:45:53,559 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36887
2023-06-23 05:45:53,559 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36887
2023-06-23 05:45:53,559 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44707
2023-06-23 05:45:53,559 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:53,560 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,560 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:53,560 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:53,560 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6nx41h2p
2023-06-23 05:45:53,560 - distributed.worker - INFO - Starting Worker plugin PreImport-401f45b8-2ec0-412b-b79d-624338ad78c3
2023-06-23 05:45:53,561 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e1eb67c-795f-499e-bf37-bcc7b659d80e
2023-06-23 05:45:53,561 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5dfad65f-8907-4e1c-a30b-3ab13accb4c6
2023-06-23 05:45:53,591 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43967
2023-06-23 05:45:53,591 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43967
2023-06-23 05:45:53,591 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43203
2023-06-23 05:45:53,591 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:53,591 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,592 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:53,592 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:53,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-yzg8eqb0
2023-06-23 05:45:53,592 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4751bff9-f8bd-4428-8d83-eb0f983aa3a6
2023-06-23 05:45:53,593 - distributed.worker - INFO - Starting Worker plugin PreImport-7886bd7f-7a79-4946-812c-11c71918ea5e
2023-06-23 05:45:53,593 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ebcfd44-dca5-441f-abda-a8e6f8c67cd0
2023-06-23 05:45:53,611 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43937
2023-06-23 05:45:53,611 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43937
2023-06-23 05:45:53,612 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34153
2023-06-23 05:45:53,612 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:53,612 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,612 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:53,612 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:53,612 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-55ja6hbk
2023-06-23 05:45:53,613 - distributed.worker - INFO - Starting Worker plugin PreImport-bab3ad38-75e7-4779-b773-10d614b461c0
2023-06-23 05:45:53,613 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9c5391c9-9ebd-400f-bb9f-d9f15df94574
2023-06-23 05:45:53,670 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34229
2023-06-23 05:45:53,670 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34229
2023-06-23 05:45:53,670 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37417
2023-06-23 05:45:53,670 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:53,670 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,670 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:53,670 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:53,670 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9csloyrg
2023-06-23 05:45:53,671 - distributed.worker - INFO - Starting Worker plugin PreImport-23105b67-ad1a-4a5f-b885-171db87ff531
2023-06-23 05:45:53,671 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a10f9521-021c-404f-8447-c8383b242404
2023-06-23 05:45:53,674 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40553
2023-06-23 05:45:53,675 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40553
2023-06-23 05:45:53,675 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36665
2023-06-23 05:45:53,675 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:53,675 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,675 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:53,675 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:53,675 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-z29bj1_p
2023-06-23 05:45:53,676 - distributed.worker - INFO - Starting Worker plugin PreImport-ba928b4b-78da-4580-b513-8115a9cdb2c9
2023-06-23 05:45:53,676 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9859c2a7-6009-48cb-ab82-2b0149be48c2
2023-06-23 05:45:53,710 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45377
2023-06-23 05:45:53,710 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33491
2023-06-23 05:45:53,710 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45377
2023-06-23 05:45:53,710 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40803
2023-06-23 05:45:53,710 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33491
2023-06-23 05:45:53,710 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:53,710 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40955
2023-06-23 05:45:53,710 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,710 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:53,710 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:45:53,710 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,710 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:53,710 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:45:53,711 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q5695g6e
2023-06-23 05:45:53,711 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:45:53,711 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8d4nwf3i
2023-06-23 05:45:53,711 - distributed.worker - INFO - Starting Worker plugin PreImport-606b89e0-2281-4333-85f6-c7de0bf0fb7e
2023-06-23 05:45:53,711 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5501e5c5-bf86-4086-8869-73a63a5f66a8
2023-06-23 05:45:53,711 - distributed.worker - INFO - Starting Worker plugin PreImport-396c054b-0dc5-40e6-a26a-437af419aea4
2023-06-23 05:45:53,712 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c172ce8c-8f7b-4fb1-871c-f445507ca658
2023-06-23 05:45:53,712 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d1b042f-5eb5-4ec2-b8cc-13946d7afaf7
2023-06-23 05:45:53,766 - distributed.worker - INFO - Starting Worker plugin PreImport-920a32d6-0466-42c3-8275-2ee3ebd79ed7
2023-06-23 05:45:53,767 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,798 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35883', status: init, memory: 0, processing: 0>
2023-06-23 05:45:53,799 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35883
2023-06-23 05:45:53,799 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45862
2023-06-23 05:45:53,800 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:53,800 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,802 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:53,958 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,967 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59c6260d-8eee-4a84-98ec-d47da102b017
2023-06-23 05:45:53,968 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,968 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1eb0358d-ccc8-4843-accb-5cc9bfd1ab89
2023-06-23 05:45:53,968 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,970 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,971 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6340342f-02fa-4c19-9cf5-d6c9c5ae54e4
2023-06-23 05:45:53,972 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,978 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d684ebd1-6e8e-4b9e-b91b-29029302ff2b
2023-06-23 05:45:53,979 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,979 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,995 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36887', status: init, memory: 0, processing: 0>
2023-06-23 05:45:53,996 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36887
2023-06-23 05:45:53,996 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45874
2023-06-23 05:45:53,996 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:53,997 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:53,999 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:54,002 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34229', status: init, memory: 0, processing: 0>
2023-06-23 05:45:54,003 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34229
2023-06-23 05:45:54,003 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45886
2023-06-23 05:45:54,004 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:54,004 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:54,004 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40553', status: init, memory: 0, processing: 0>
2023-06-23 05:45:54,004 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40553
2023-06-23 05:45:54,005 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45920
2023-06-23 05:45:54,005 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:54,005 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:54,005 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43937', status: init, memory: 0, processing: 0>
2023-06-23 05:45:54,006 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43937
2023-06-23 05:45:54,006 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45898
2023-06-23 05:45:54,006 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:54,006 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:54,007 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:54,007 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43967', status: init, memory: 0, processing: 0>
2023-06-23 05:45:54,007 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43967
2023-06-23 05:45:54,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45910
2023-06-23 05:45:54,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:54,008 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:54,008 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:54,010 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:54,011 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:54,014 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45377', status: init, memory: 0, processing: 0>
2023-06-23 05:45:54,015 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45377
2023-06-23 05:45:54,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45936
2023-06-23 05:45:54,016 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:54,016 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:54,018 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:54,027 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33491', status: init, memory: 0, processing: 0>
2023-06-23 05:45:54,028 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33491
2023-06-23 05:45:54,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45950
2023-06-23 05:45:54,029 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:45:54,029 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:45:54,032 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:45:54,049 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:54,049 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:54,049 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:54,049 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:54,050 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:54,050 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:54,050 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:54,050 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:45:54,062 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:54,063 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:54,063 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:54,063 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:54,063 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:54,063 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:54,063 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:54,063 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:45:54,070 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:45:54,072 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:45:54,075 - distributed.scheduler - INFO - Remove client Client-2efb5625-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:54,075 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42430; closing.
2023-06-23 05:45:54,075 - distributed.scheduler - INFO - Remove client Client-2efb5625-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:54,076 - distributed.scheduler - INFO - Close client connection: Client-2efb5625-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:54,076 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39093'. Reason: nanny-close
2023-06-23 05:45:54,077 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:54,077 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44471'. Reason: nanny-close
2023-06-23 05:45:54,078 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:54,078 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44205'. Reason: nanny-close
2023-06-23 05:45:54,078 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43967. Reason: nanny-close
2023-06-23 05:45:54,079 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:54,079 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41465'. Reason: nanny-close
2023-06-23 05:45:54,079 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43937. Reason: nanny-close
2023-06-23 05:45:54,079 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:54,080 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34655'. Reason: nanny-close
2023-06-23 05:45:54,080 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45377. Reason: nanny-close
2023-06-23 05:45:54,080 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:54,080 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37329'. Reason: nanny-close
2023-06-23 05:45:54,080 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34229. Reason: nanny-close
2023-06-23 05:45:54,080 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:54,081 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33613'. Reason: nanny-close
2023-06-23 05:45:54,081 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:54,081 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36887. Reason: nanny-close
2023-06-23 05:45:54,081 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45910; closing.
2023-06-23 05:45:54,081 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:54,081 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:54,081 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43967', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:54,081 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32995'. Reason: nanny-close
2023-06-23 05:45:54,081 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43967
2023-06-23 05:45:54,081 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33491. Reason: nanny-close
2023-06-23 05:45:54,081 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:45:54,082 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35883. Reason: nanny-close
2023-06-23 05:45:54,082 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:54,082 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:54,082 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:54,082 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45898; closing.
2023-06-23 05:45:54,082 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:54,083 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:54,083 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40553. Reason: nanny-close
2023-06-23 05:45:54,083 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43937', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:54,083 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43937
2023-06-23 05:45:54,083 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:54,083 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43967
2023-06-23 05:45:54,083 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:54,084 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43967
2023-06-23 05:45:54,084 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45936; closing.
2023-06-23 05:45:54,084 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:54,084 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45886; closing.
2023-06-23 05:45:54,084 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:54,084 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:54,084 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43967
2023-06-23 05:45:54,085 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45377', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:54,085 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45377
2023-06-23 05:45:54,085 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:45:54,085 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34229', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:54,085 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34229
2023-06-23 05:45:54,085 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:54,085 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45874; closing.
2023-06-23 05:45:54,085 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:54,086 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36887', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:54,086 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36887
2023-06-23 05:45:54,086 - distributed.nanny - INFO - Worker closed
2023-06-23 05:45:54,086 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45950; closing.
2023-06-23 05:45:54,087 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45862; closing.
2023-06-23 05:45:54,087 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33491', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:54,087 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33491
2023-06-23 05:45:54,088 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35883', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:54,088 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35883
2023-06-23 05:45:54,088 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45920; closing.
2023-06-23 05:45:54,088 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40553', status: closing, memory: 0, processing: 0>
2023-06-23 05:45:54,088 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40553
2023-06-23 05:45:54,089 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:45:55,845 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:45:55,846 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:45:55,846 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:45:55,848 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:45:55,848 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-06-23 05:45:58,054 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:58,059 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44875 instead
  warnings.warn(
2023-06-23 05:45:58,063 - distributed.scheduler - INFO - State start
2023-06-23 05:45:58,504 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:45:58,505 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:45:58,505 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44875/status
2023-06-23 05:45:58,907 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38851'
2023-06-23 05:45:58,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43295'
2023-06-23 05:45:58,928 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39207'
2023-06-23 05:45:58,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44189'
2023-06-23 05:45:58,947 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40449'
2023-06-23 05:45:58,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39101'
2023-06-23 05:45:58,967 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33237'
2023-06-23 05:45:58,976 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39499'
2023-06-23 05:45:59,227 - distributed.scheduler - INFO - Receive client connection: Client-393298da-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:45:59,242 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46126
2023-06-23 05:46:00,703 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:00,703 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:00,703 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:00,703 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:00,714 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:00,715 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:00,774 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:00,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:00,819 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:00,819 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:00,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:00,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:00,838 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:00,838 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:00,862 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:00,862 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:01,715 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:01,717 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:01,718 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:01,772 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:01,832 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:01,882 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:01,884 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:01,917 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:11,079 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43637
2023-06-23 05:46:11,080 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43637
2023-06-23 05:46:11,080 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34887
2023-06-23 05:46:11,080 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,080 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,080 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:11,080 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:46:11,080 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2v2rv714
2023-06-23 05:46:11,080 - distributed.worker - INFO - Starting Worker plugin PreImport-33602e6a-4629-4d97-9c93-3ca000343804
2023-06-23 05:46:11,081 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4aa77225-2b46-4ba8-9bd5-014dbaa72953
2023-06-23 05:46:11,106 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37665
2023-06-23 05:46:11,106 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37665
2023-06-23 05:46:11,106 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38047
2023-06-23 05:46:11,106 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,106 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,106 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:11,106 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:46:11,106 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1fzxnwyb
2023-06-23 05:46:11,107 - distributed.worker - INFO - Starting Worker plugin PreImport-cb1c29ad-1fdd-475f-a8ae-2396c9a96b4f
2023-06-23 05:46:11,107 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9008b7db-2bbb-4c7f-acca-d9c9535700ac
2023-06-23 05:46:11,424 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-161dce08-614f-4a91-8636-9fc13997853a
2023-06-23 05:46:11,424 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,459 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42013
2023-06-23 05:46:11,459 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42013
2023-06-23 05:46:11,459 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44415
2023-06-23 05:46:11,459 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,459 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,459 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:11,459 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:46:11,460 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-wk3thx8y
2023-06-23 05:46:11,460 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be753513-ba79-4381-b5b4-9e1087e2e91e
2023-06-23 05:46:11,460 - distributed.worker - INFO - Starting Worker plugin PreImport-ee870c59-7110-49c4-a9c1-e09ba475ba22
2023-06-23 05:46:11,460 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5f570e84-4905-4518-90b0-8caa0a4c329b
2023-06-23 05:46:11,481 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-77d8b6e7-66ce-41da-87ca-c7284319abcf
2023-06-23 05:46:11,482 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,486 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37665', status: init, memory: 0, processing: 0>
2023-06-23 05:46:11,490 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37665
2023-06-23 05:46:11,490 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58740
2023-06-23 05:46:11,490 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,491 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,492 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:46:11,507 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38475
2023-06-23 05:46:11,507 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38475
2023-06-23 05:46:11,507 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39335
2023-06-23 05:46:11,507 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,508 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,508 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:11,508 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:46:11,508 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ljhxtkoi
2023-06-23 05:46:11,508 - distributed.worker - INFO - Starting Worker plugin PreImport-d7070a1b-9e55-4c6c-af00-3c37b29c6c33
2023-06-23 05:46:11,508 - distributed.worker - INFO - Starting Worker plugin RMMSetup-99862c78-f5f8-43a8-941b-ee3e7e94543b
2023-06-23 05:46:11,551 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36741
2023-06-23 05:46:11,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36741
2023-06-23 05:46:11,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32841
2023-06-23 05:46:11,552 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,552 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,552 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:11,552 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:46:11,552 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43637', status: init, memory: 0, processing: 0>
2023-06-23 05:46:11,552 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-7k65z8_k
2023-06-23 05:46:11,553 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43637
2023-06-23 05:46:11,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58742
2023-06-23 05:46:11,553 - distributed.worker - INFO - Starting Worker plugin PreImport-6a9a0fbb-6283-45c5-bd02-53a755b408e0
2023-06-23 05:46:11,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d7136517-54cd-4484-a5dc-20f928815669
2023-06-23 05:46:11,554 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,554 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,554 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f01f1303-0dbc-4a5f-ba2e-b36500856e2c
2023-06-23 05:46:11,557 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:46:11,564 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33975
2023-06-23 05:46:11,565 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33975
2023-06-23 05:46:11,565 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44969
2023-06-23 05:46:11,565 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,565 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,565 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:11,566 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:46:11,566 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vpg5r19h
2023-06-23 05:46:11,567 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-955bfbec-ae21-4572-9253-8de18ad43a79
2023-06-23 05:46:11,569 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4afb5ab7-8727-42f4-89a6-63f00a28c80b
2023-06-23 05:46:11,573 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46333
2023-06-23 05:46:11,573 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46333
2023-06-23 05:46:11,573 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40997
2023-06-23 05:46:11,573 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42489
2023-06-23 05:46:11,573 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,573 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40997
2023-06-23 05:46:11,573 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,574 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:11,574 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41537
2023-06-23 05:46:11,574 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:46:11,574 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,574 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nai0f06b
2023-06-23 05:46:11,574 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,574 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:11,574 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:46:11,574 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oo3kspsc
2023-06-23 05:46:11,575 - distributed.worker - INFO - Starting Worker plugin PreImport-51aa0aa1-7081-485f-ba3d-8607b8c83107
2023-06-23 05:46:11,575 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae0c62fe-8a5d-4da6-9fa1-fd290f7f3dfc
2023-06-23 05:46:11,576 - distributed.worker - INFO - Starting Worker plugin PreImport-b48d0a80-53f0-43a7-921f-ffb3202561c3
2023-06-23 05:46:11,576 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a95d008a-178f-4a0d-93b3-3d9e4cf0b6af
2023-06-23 05:46:11,576 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d7075a00-a47e-4730-a531-787519e12800
2023-06-23 05:46:11,742 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9d081d09-6e44-4389-8de4-f7f337f54b7c
2023-06-23 05:46:11,742 - distributed.worker - INFO - Starting Worker plugin PreImport-9d6d9634-2025-412b-b9f7-8708fc7bb01a
2023-06-23 05:46:11,742 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,742 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,742 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,742 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4a13eb96-914c-43d9-9277-01cebdde77a1
2023-06-23 05:46:11,742 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,743 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,743 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,772 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38475', status: init, memory: 0, processing: 0>
2023-06-23 05:46:11,773 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38475
2023-06-23 05:46:11,773 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58754
2023-06-23 05:46:11,774 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,774 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,774 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33975', status: init, memory: 0, processing: 0>
2023-06-23 05:46:11,775 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33975
2023-06-23 05:46:11,775 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58756
2023-06-23 05:46:11,775 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,775 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,776 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:46:11,777 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36741', status: init, memory: 0, processing: 0>
2023-06-23 05:46:11,777 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:46:11,778 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36741
2023-06-23 05:46:11,778 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58764
2023-06-23 05:46:11,778 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,778 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,779 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42013', status: init, memory: 0, processing: 0>
2023-06-23 05:46:11,779 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42013
2023-06-23 05:46:11,779 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58782
2023-06-23 05:46:11,780 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40997', status: init, memory: 0, processing: 0>
2023-06-23 05:46:11,780 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,780 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,781 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40997
2023-06-23 05:46:11,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58776
2023-06-23 05:46:11,781 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:46:11,781 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,781 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,781 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46333', status: init, memory: 0, processing: 0>
2023-06-23 05:46:11,782 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46333
2023-06-23 05:46:11,782 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58788
2023-06-23 05:46:11,782 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:46:11,783 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:11,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:46:11,784 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:46:11,785 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:46:11,812 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:46:11,812 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:46:11,812 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:46:11,812 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:46:11,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:46:11,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:46:11,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:46:11,813 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:46:11,818 - distributed.scheduler - INFO - Remove client Client-393298da-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:11,818 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46126; closing.
2023-06-23 05:46:11,818 - distributed.scheduler - INFO - Remove client Client-393298da-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:11,819 - distributed.scheduler - INFO - Close client connection: Client-393298da-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:11,820 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44189'. Reason: nanny-close
2023-06-23 05:46:11,820 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:11,821 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38851'. Reason: nanny-close
2023-06-23 05:46:11,821 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:11,821 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43295'. Reason: nanny-close
2023-06-23 05:46:11,822 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42013. Reason: nanny-close
2023-06-23 05:46:11,822 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:11,822 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39207'. Reason: nanny-close
2023-06-23 05:46:11,822 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46333. Reason: nanny-close
2023-06-23 05:46:11,822 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:11,823 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40449'. Reason: nanny-close
2023-06-23 05:46:11,823 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43637. Reason: nanny-close
2023-06-23 05:46:11,823 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:11,823 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37665. Reason: nanny-close
2023-06-23 05:46:11,823 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39101'. Reason: nanny-close
2023-06-23 05:46:11,823 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:11,824 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33237'. Reason: nanny-close
2023-06-23 05:46:11,824 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36741. Reason: nanny-close
2023-06-23 05:46:11,824 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:11,824 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:46:11,824 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58782; closing.
2023-06-23 05:46:11,824 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39499'. Reason: nanny-close
2023-06-23 05:46:11,824 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:46:11,824 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:46:11,824 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40997. Reason: nanny-close
2023-06-23 05:46:11,825 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42013', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:11,825 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:11,825 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42013
2023-06-23 05:46:11,825 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33975. Reason: nanny-close
2023-06-23 05:46:11,825 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:46:11,825 - distributed.nanny - INFO - Worker closed
2023-06-23 05:46:11,826 - distributed.nanny - INFO - Worker closed
2023-06-23 05:46:11,826 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38475. Reason: nanny-close
2023-06-23 05:46:11,826 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:46:11,826 - distributed.nanny - INFO - Worker closed
2023-06-23 05:46:11,826 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58788; closing.
2023-06-23 05:46:11,826 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58742; closing.
2023-06-23 05:46:11,826 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:46:11,827 - distributed.nanny - INFO - Worker closed
2023-06-23 05:46:11,827 - distributed.nanny - INFO - Worker closed
2023-06-23 05:46:11,827 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42013
2023-06-23 05:46:11,827 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42013
2023-06-23 05:46:11,828 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:46:11,828 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46333', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:11,828 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:46:11,828 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:46333
2023-06-23 05:46:11,828 - distributed.nanny - INFO - Worker closed
2023-06-23 05:46:11,828 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43637', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:11,828 - distributed.nanny - INFO - Worker closed
2023-06-23 05:46:11,828 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43637
2023-06-23 05:46:11,829 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58740; closing.
2023-06-23 05:46:11,829 - distributed.nanny - INFO - Worker closed
2023-06-23 05:46:11,830 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37665', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:11,830 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37665
2023-06-23 05:46:11,830 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58764; closing.
2023-06-23 05:46:11,830 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58776; closing.
2023-06-23 05:46:11,831 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36741', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:11,831 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36741
2023-06-23 05:46:11,832 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40997', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:11,832 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40997
2023-06-23 05:46:11,832 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58756; closing.
2023-06-23 05:46:11,832 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58754; closing.
2023-06-23 05:46:11,833 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33975', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:11,833 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33975
2023-06-23 05:46:11,833 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38475', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:11,833 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38475
2023-06-23 05:46:11,833 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:46:13,489 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:46:13,489 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:46:13,490 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:46:13,491 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:46:13,491 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-06-23 05:46:15,753 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:46:15,758 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44499 instead
  warnings.warn(
2023-06-23 05:46:15,762 - distributed.scheduler - INFO - State start
2023-06-23 05:46:15,851 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:46:15,852 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:46:15,853 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44499/status
2023-06-23 05:46:16,063 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33021'
2023-06-23 05:46:16,413 - distributed.scheduler - INFO - Receive client connection: Client-43c0f781-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:16,426 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58894
2023-06-23 05:46:17,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:17,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
2023-06-23 05:46:18,866 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:25,701 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32873
2023-06-23 05:46:25,701 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32873
2023-06-23 05:46:25,702 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-06-23 05:46:25,702 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:46:25,702 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:25,702 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:25,702 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:46:25,702 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-iwbr4vc6
2023-06-23 05:46:25,703 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9dd838d-d584-4383-a86e-f7fc4639bc04
2023-06-23 05:46:25,703 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aac75300-fe6f-4f47-9d76-3cf5988e29f9
2023-06-23 05:46:25,704 - distributed.worker - INFO - Starting Worker plugin PreImport-0b10c66e-8b3a-4884-a1e8-85bc15086bc4
2023-06-23 05:46:25,704 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:25,739 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32873', status: init, memory: 0, processing: 0>
2023-06-23 05:46:25,741 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32873
2023-06-23 05:46:25,741 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46810
2023-06-23 05:46:25,742 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:46:25,742 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:25,745 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:46:25,748 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:46:25,751 - distributed.scheduler - INFO - Remove client Client-43c0f781-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:25,752 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58894; closing.
2023-06-23 05:46:25,752 - distributed.scheduler - INFO - Remove client Client-43c0f781-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:25,752 - distributed.scheduler - INFO - Close client connection: Client-43c0f781-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:25,754 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33021'. Reason: nanny-close
2023-06-23 05:46:25,755 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:25,756 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32873. Reason: nanny-close
2023-06-23 05:46:25,759 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:46:25,759 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46810; closing.
2023-06-23 05:46:25,759 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32873', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:25,759 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32873
2023-06-23 05:46:25,759 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:46:25,760 - distributed.nanny - INFO - Worker closed
2023-06-23 05:46:27,172 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:46:27,173 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:46:27,173 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:46:27,174 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:46:27,174 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-06-23 05:46:31,750 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:46:31,755 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45435 instead
  warnings.warn(
2023-06-23 05:46:31,759 - distributed.scheduler - INFO - State start
2023-06-23 05:46:31,780 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:46:31,781 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:46:31,782 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45435/status
2023-06-23 05:46:31,927 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46341'
2023-06-23 05:46:32,715 - distributed.scheduler - INFO - Receive client connection: Client-4d3c41ff-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:32,733 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37404
2023-06-23 05:46:33,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:33,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
2023-06-23 05:46:34,821 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:41,609 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35501
2023-06-23 05:46:41,610 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35501
2023-06-23 05:46:41,610 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40757
2023-06-23 05:46:41,610 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:46:41,610 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:41,610 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:41,610 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:46:41,610 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bngxt2aj
2023-06-23 05:46:41,610 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0fa23bcc-4473-4866-acb5-ba04a9adc19c
2023-06-23 05:46:41,611 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8c62aaf8-282e-4b40-bfd1-f9946df9480a
2023-06-23 05:46:41,611 - distributed.worker - INFO - Starting Worker plugin PreImport-34bc0cc0-5b72-448d-b834-90378f9b25b3
2023-06-23 05:46:41,612 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:41,655 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35501', status: init, memory: 0, processing: 0>
2023-06-23 05:46:41,657 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35501
2023-06-23 05:46:41,657 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:35284
2023-06-23 05:46:41,657 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:46:41,658 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:41,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:46:41,691 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:46:41,696 - distributed.scheduler - INFO - Remove client Client-4d3c41ff-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:41,696 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37404; closing.
2023-06-23 05:46:41,697 - distributed.scheduler - INFO - Remove client Client-4d3c41ff-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:41,697 - distributed.scheduler - INFO - Close client connection: Client-4d3c41ff-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:41,698 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46341'. Reason: nanny-close
2023-06-23 05:46:41,699 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:41,700 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35501. Reason: nanny-close
2023-06-23 05:46:41,703 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:46:41,703 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:35284; closing.
2023-06-23 05:46:41,704 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35501', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:41,704 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35501
2023-06-23 05:46:41,704 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:46:41,704 - distributed.nanny - INFO - Worker closed
2023-06-23 05:46:43,217 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:46:43,217 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:46:43,218 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:46:43,218 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:46:43,219 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-06-23 05:46:45,467 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:46:45,472 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39067 instead
  warnings.warn(
2023-06-23 05:46:45,476 - distributed.scheduler - INFO - State start
2023-06-23 05:46:45,508 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:46:45,509 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:46:45,510 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39067/status
2023-06-23 05:46:51,239 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:46:51,239 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:46:51,239 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:46:51,240 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:46:51,240 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-06-23 05:46:53,531 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:46:53,536 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46721 instead
  warnings.warn(
2023-06-23 05:46:53,540 - distributed.scheduler - INFO - State start
2023-06-23 05:46:53,609 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:46:53,610 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-23 05:46:53,611 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46721/status
2023-06-23 05:46:53,818 - distributed.scheduler - INFO - Receive client connection: Client-5a41a7ba-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:53,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42940
2023-06-23 05:46:54,959 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39483'
2023-06-23 05:46:56,819 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:46:56,819 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:46:56,827 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:46:58,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43453
2023-06-23 05:46:58,753 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43453
2023-06-23 05:46:58,754 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37963
2023-06-23 05:46:58,754 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-23 05:46:58,754 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:58,754 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:46:58,754 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:46:58,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-sav150k7
2023-06-23 05:46:58,754 - distributed.worker - INFO - Starting Worker plugin PreImport-c860afda-ba19-4f9e-9340-ff7994b020ab
2023-06-23 05:46:58,755 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c850d5a3-03be-414e-83b9-2afa5f35f986
2023-06-23 05:46:58,755 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ea6db1d0-e7f2-47a0-b61b-a8df87eab006
2023-06-23 05:46:58,755 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:58,784 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43453', status: init, memory: 0, processing: 0>
2023-06-23 05:46:58,787 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43453
2023-06-23 05:46:58,787 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42962
2023-06-23 05:46:58,788 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-23 05:46:58,788 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:46:58,791 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-23 05:46:58,854 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:46:58,857 - distributed.scheduler - INFO - Remove client Client-5a41a7ba-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:58,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42940; closing.
2023-06-23 05:46:58,858 - distributed.scheduler - INFO - Remove client Client-5a41a7ba-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:58,858 - distributed.scheduler - INFO - Close client connection: Client-5a41a7ba-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:46:58,859 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39483'. Reason: nanny-close
2023-06-23 05:46:58,860 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:46:58,861 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43453. Reason: nanny-close
2023-06-23 05:46:58,863 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-23 05:46:58,863 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42962; closing.
2023-06-23 05:46:58,864 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43453', status: closing, memory: 0, processing: 0>
2023-06-23 05:46:58,864 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43453
2023-06-23 05:46:58,864 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:46:58,865 - distributed.nanny - INFO - Worker closed
2023-06-23 05:47:00,127 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:47:00,127 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:47:00,128 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:47:00,128 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-23 05:47:00,129 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-06-23 05:47:02,546 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:47:02,551 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41005 instead
  warnings.warn(
2023-06-23 05:47:02,555 - distributed.scheduler - INFO - State start
2023-06-23 05:47:02,581 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:47:02,583 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:47:02,583 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41005/status
2023-06-23 05:47:02,962 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45737'
2023-06-23 05:47:02,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42143'
2023-06-23 05:47:02,991 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32917'
2023-06-23 05:47:02,992 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37145'
2023-06-23 05:47:03,000 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39249'
2023-06-23 05:47:03,009 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42907'
2023-06-23 05:47:03,019 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40221'
2023-06-23 05:47:03,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38373'
2023-06-23 05:47:04,753 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:04,753 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:04,762 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:04,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:04,762 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:04,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:04,766 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:04,766 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:04,766 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:04,767 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:04,768 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:04,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:04,783 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:47:04,791 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:47:04,791 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:47:04,802 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:47:04,802 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:47:04,803 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:47:04,805 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:04,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:04,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:04,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:04,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:47:04,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:47:05,523 - distributed.scheduler - INFO - Receive client connection: Client-5fa77fcd-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:05,536 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36498
2023-06-23 05:47:09,544 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35913
2023-06-23 05:47:09,544 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35913
2023-06-23 05:47:09,545 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45739
2023-06-23 05:47:09,545 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:47:09,545 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:09,545 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:47:09,545 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:47:09,545 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-4gt90zlv
2023-06-23 05:47:09,545 - distributed.worker - INFO - Starting Worker plugin PreImport-83d2f7cd-149b-49c7-995e-171249647544
2023-06-23 05:47:09,545 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e35ed9c-5252-43d5-aa8e-5c04c8c019c5
2023-06-23 05:47:09,546 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09dd753e-9b46-47ca-baa9-736acbb2352f
2023-06-23 05:47:09,700 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33283
2023-06-23 05:47:09,700 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33283
2023-06-23 05:47:09,701 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42759
2023-06-23 05:47:09,701 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:47:09,701 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:09,701 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:47:09,701 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:47:09,701 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nv6u5p1o
2023-06-23 05:47:09,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-be38812a-d562-4d6e-a630-1710558c9c1a
2023-06-23 05:47:09,701 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3530243a-a1ff-49b7-9bf3-fe6e59492c74
2023-06-23 05:47:09,731 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45689
2023-06-23 05:47:09,732 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45689
2023-06-23 05:47:09,732 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45763
2023-06-23 05:47:09,732 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:47:09,732 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:09,732 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:47:09,732 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:47:09,732 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ofeh2i9u
2023-06-23 05:47:09,733 - distributed.worker - INFO - Starting Worker plugin PreImport-039e5317-1875-4de3-b796-35cb65f184ed
2023-06-23 05:47:09,733 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65290cd1-5646-4703-9cfd-61f6d235dd8c
2023-06-23 05:47:09,733 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1691b24c-d6af-4136-a94c-3ed0dd3cacb5
2023-06-23 05:47:09,736 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37861
2023-06-23 05:47:09,736 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37861
2023-06-23 05:47:09,736 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43609
2023-06-23 05:47:09,736 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:47:09,737 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:09,737 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:47:09,737 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:47:09,737 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-kl909oti
2023-06-23 05:47:09,737 - distributed.worker - INFO - Starting Worker plugin PreImport-50510386-871e-4ed3-9861-72a7ee8822ca
2023-06-23 05:47:09,737 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c94c73c3-b0dd-4de2-9663-8cd3cd762270
2023-06-23 05:47:09,742 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37373
2023-06-23 05:47:09,742 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37373
2023-06-23 05:47:09,742 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43975
2023-06-23 05:47:09,742 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:47:09,742 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:09,742 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:47:09,742 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:47:09,742 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1tk30btn
2023-06-23 05:47:09,743 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2424b097-4502-445a-bb1c-aa12b4196dcf
2023-06-23 05:47:09,743 - distributed.worker - INFO - Starting Worker plugin PreImport-ef0dd579-aba5-4c21-8b8a-e2d05ae1d183
2023-06-23 05:47:09,743 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bffcc0bc-a640-4f0c-a14f-e992cfdd20bc
2023-06-23 05:47:10,043 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,063 - distributed.worker - INFO - Starting Worker plugin PreImport-7651cdcd-3ef8-44f6-844e-85b47de26aa2
2023-06-23 05:47:10,063 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,064 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45323
2023-06-23 05:47:10,064 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45323
2023-06-23 05:47:10,064 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34439
2023-06-23 05:47:10,064 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:47:10,064 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,064 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:47:10,064 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:47:10,064 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ypdnh0fq
2023-06-23 05:47:10,065 - distributed.worker - INFO - Starting Worker plugin PreImport-99a11b0b-fb50-4262-b798-002edf652d00
2023-06-23 05:47:10,065 - distributed.worker - INFO - Starting Worker plugin RMMSetup-118f32c6-6650-443b-9fea-0fb04bd14564
2023-06-23 05:47:10,083 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a8c93af-ee14-4f3a-98a7-30f6d96d0935
2023-06-23 05:47:10,083 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,093 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,093 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35913', status: init, memory: 0, processing: 0>
2023-06-23 05:47:10,095 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35913
2023-06-23 05:47:10,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47738
2023-06-23 05:47:10,095 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:47:10,095 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,098 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:47:10,102 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33283', status: init, memory: 0, processing: 0>
2023-06-23 05:47:10,103 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33283
2023-06-23 05:47:10,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47750
2023-06-23 05:47:10,103 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:47:10,104 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:47:10,117 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,128 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37861', status: init, memory: 0, processing: 0>
2023-06-23 05:47:10,129 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37861
2023-06-23 05:47:10,129 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47766
2023-06-23 05:47:10,129 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:47:10,129 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,131 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:47:10,132 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45689', status: init, memory: 0, processing: 0>
2023-06-23 05:47:10,133 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45689
2023-06-23 05:47:10,133 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47772
2023-06-23 05:47:10,133 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:47:10,134 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,136 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:47:10,158 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37373', status: init, memory: 0, processing: 0>
2023-06-23 05:47:10,158 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37373
2023-06-23 05:47:10,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47784
2023-06-23 05:47:10,159 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:47:10,159 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,162 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:47:10,182 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44605
2023-06-23 05:47:10,182 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44605
2023-06-23 05:47:10,182 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44713
2023-06-23 05:47:10,182 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:47:10,182 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,182 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:47:10,182 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:47:10,182 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-adkc3ktw
2023-06-23 05:47:10,182 - distributed.worker - INFO - Starting Worker plugin PreImport-e01e6431-add0-420b-ac78-2ac873ecc492
2023-06-23 05:47:10,183 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4079fea2-2dfa-42eb-b4bc-c3c9ef969771
2023-06-23 05:47:10,551 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ad91497-063d-45ec-8ddb-e3f4daf60bd9
2023-06-23 05:47:10,551 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38017
2023-06-23 05:47:10,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38017
2023-06-23 05:47:10,552 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41335
2023-06-23 05:47:10,552 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:47:10,552 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,552 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:47:10,552 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,552 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-23 05:47:10,552 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-07lvjy6y
2023-06-23 05:47:10,553 - distributed.worker - INFO - Starting Worker plugin PreImport-9f25871d-e15f-4ba5-9e36-4c8f034f08fc
2023-06-23 05:47:10,553 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb175810-77b0-4754-b2a1-3ea4fa74f316
2023-06-23 05:47:10,559 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5f7dff42-fcb0-401c-85a9-1feb1c053ba4
2023-06-23 05:47:10,559 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,749 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44605', status: init, memory: 0, processing: 0>
2023-06-23 05:47:10,750 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44605
2023-06-23 05:47:10,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47802
2023-06-23 05:47:10,751 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:47:10,751 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,751 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45323', status: init, memory: 0, processing: 0>
2023-06-23 05:47:10,752 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45323
2023-06-23 05:47:10,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47790
2023-06-23 05:47:10,752 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:47:10,753 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:10,753 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:47:10,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:47:11,406 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-66e7f82a-c2e6-4602-8dfb-c7290510de1b
2023-06-23 05:47:11,406 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:11,447 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38017', status: init, memory: 0, processing: 0>
2023-06-23 05:47:11,448 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38017
2023-06-23 05:47:11,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47838
2023-06-23 05:47:11,448 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:47:11,449 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:11,451 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:47:11,527 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:47:11,527 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:47:11,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:47:11,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:47:11,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:47:11,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:47:11,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:47:11,528 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-23 05:47:11,541 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:47:11,542 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:47:11,542 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:47:11,542 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:47:11,542 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:47:11,542 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:47:11,542 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:47:11,542 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:47:11,547 - distributed.scheduler - INFO - Remove client Client-5fa77fcd-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:11,547 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36498; closing.
2023-06-23 05:47:11,547 - distributed.scheduler - INFO - Remove client Client-5fa77fcd-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:11,548 - distributed.scheduler - INFO - Close client connection: Client-5fa77fcd-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:11,549 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37145'. Reason: nanny-close
2023-06-23 05:47:11,549 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:47:11,549 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45737'. Reason: nanny-close
2023-06-23 05:47:11,550 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:47:11,550 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42143'. Reason: nanny-close
2023-06-23 05:47:11,550 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37373. Reason: nanny-close
2023-06-23 05:47:11,550 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:47:11,551 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32917'. Reason: nanny-close
2023-06-23 05:47:11,551 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38017. Reason: nanny-close
2023-06-23 05:47:11,551 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:47:11,551 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44605. Reason: nanny-close
2023-06-23 05:47:11,551 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39249'. Reason: nanny-close
2023-06-23 05:47:11,551 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:47:11,552 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37861. Reason: nanny-close
2023-06-23 05:47:11,552 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42907'. Reason: nanny-close
2023-06-23 05:47:11,552 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:47:11,552 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40221'. Reason: nanny-close
2023-06-23 05:47:11,552 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35913. Reason: nanny-close
2023-06-23 05:47:11,552 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47784; closing.
2023-06-23 05:47:11,552 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:47:11,553 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:47:11,553 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37373', status: closing, memory: 0, processing: 0>
2023-06-23 05:47:11,553 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:47:11,553 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:47:11,553 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38373'. Reason: nanny-close
2023-06-23 05:47:11,553 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37373
2023-06-23 05:47:11,553 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45689. Reason: nanny-close
2023-06-23 05:47:11,553 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:47:11,553 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:47:11,553 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33283. Reason: nanny-close
2023-06-23 05:47:11,554 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47802; closing.
2023-06-23 05:47:11,554 - distributed.nanny - INFO - Worker closed
2023-06-23 05:47:11,554 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47838; closing.
2023-06-23 05:47:11,554 - distributed.nanny - INFO - Worker closed
2023-06-23 05:47:11,554 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:47:11,554 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45323. Reason: nanny-close
2023-06-23 05:47:11,554 - distributed.nanny - INFO - Worker closed
2023-06-23 05:47:11,554 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:47:11,555 - distributed.nanny - INFO - Worker closed
2023-06-23 05:47:11,555 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37373
2023-06-23 05:47:11,555 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44605', status: closing, memory: 0, processing: 0>
2023-06-23 05:47:11,555 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44605
2023-06-23 05:47:11,555 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:47:11,555 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38017', status: closing, memory: 0, processing: 0>
2023-06-23 05:47:11,555 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38017
2023-06-23 05:47:11,556 - distributed.nanny - INFO - Worker closed
2023-06-23 05:47:11,556 - distributed.nanny - INFO - Worker closed
2023-06-23 05:47:11,556 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47766; closing.
2023-06-23 05:47:11,556 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37373
2023-06-23 05:47:11,556 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:47:11,556 - distributed.nanny - INFO - Worker closed
2023-06-23 05:47:11,557 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37861', status: closing, memory: 0, processing: 0>
2023-06-23 05:47:11,557 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37861
2023-06-23 05:47:11,557 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47738; closing.
2023-06-23 05:47:11,557 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47772; closing.
2023-06-23 05:47:11,558 - distributed.nanny - INFO - Worker closed
2023-06-23 05:47:11,558 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35913', status: closing, memory: 0, processing: 0>
2023-06-23 05:47:11,558 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35913
2023-06-23 05:47:11,558 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45689', status: closing, memory: 0, processing: 0>
2023-06-23 05:47:11,559 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45689
2023-06-23 05:47:11,559 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47750; closing.
2023-06-23 05:47:11,559 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47790; closing.
2023-06-23 05:47:11,560 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33283', status: closing, memory: 0, processing: 0>
2023-06-23 05:47:11,560 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33283
2023-06-23 05:47:11,560 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45323', status: closing, memory: 0, processing: 0>
2023-06-23 05:47:11,560 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45323
2023-06-23 05:47:11,560 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:47:14,754 - distributed.nanny - WARNING - Worker process still alive after 3.1999989318847657 seconds, killing
2023-06-23 05:47:14,754 - distributed.nanny - WARNING - Worker process still alive after 3.199999389648438 seconds, killing
2023-06-23 05:47:14,755 - distributed.nanny - WARNING - Worker process still alive after 3.1999995422363288 seconds, killing
2023-06-23 05:47:15,322 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:47:15,322 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:47:15,322 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:47:15,323 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:47:15,324 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-06-23 05:47:17,682 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:47:17,686 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40897 instead
  warnings.warn(
2023-06-23 05:47:17,690 - distributed.scheduler - INFO - State start
2023-06-23 05:47:18,415 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:47:18,416 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:47:18,416 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40897/status
2023-06-23 05:47:18,511 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35215'
2023-06-23 05:47:20,340 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:20,340 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:20,369 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:47:21,737 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45633
2023-06-23 05:47:21,737 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45633
2023-06-23 05:47:21,737 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37873
2023-06-23 05:47:21,737 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:47:21,737 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:21,737 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:47:21,737 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:47:21,737 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-mhx9u5lu
2023-06-23 05:47:21,738 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6322aba3-412a-40d5-844f-2ff3f8731b4c
2023-06-23 05:47:21,738 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e63a698-ebf4-4f0f-9c82-a5f2751a8d9f
2023-06-23 05:47:22,157 - distributed.worker - INFO - Starting Worker plugin PreImport-a359a103-75ce-4c69-9c4a-37876ad58fcd
2023-06-23 05:47:22,158 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:22,192 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45633', status: init, memory: 0, processing: 0>
2023-06-23 05:47:22,206 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45633
2023-06-23 05:47:22,206 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38216
2023-06-23 05:47:22,207 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-23 05:47:22,207 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:22,210 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-23 05:47:25,972 - distributed.scheduler - INFO - Receive client connection: Client-68a9353f-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:25,973 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38238
2023-06-23 05:47:25,980 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-23 05:47:25,984 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:47:25,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-23 05:47:25,988 - distributed.scheduler - INFO - Remove client Client-68a9353f-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:25,988 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38238; closing.
2023-06-23 05:47:25,989 - distributed.scheduler - INFO - Remove client Client-68a9353f-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:25,989 - distributed.scheduler - INFO - Close client connection: Client-68a9353f-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:25,990 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35215'. Reason: nanny-close
2023-06-23 05:47:25,990 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-23 05:47:25,992 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45633. Reason: nanny-close
2023-06-23 05:47:25,994 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38216; closing.
2023-06-23 05:47:25,994 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-23 05:47:25,994 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45633', status: closing, memory: 0, processing: 0>
2023-06-23 05:47:25,994 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45633
2023-06-23 05:47:25,994 - distributed.scheduler - INFO - Lost all workers
2023-06-23 05:47:25,995 - distributed.nanny - INFO - Worker closed
2023-06-23 05:47:27,207 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:47:27,207 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:47:27,208 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:47:27,208 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:47:27,209 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-06-23 05:47:29,461 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:47:29,466 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44161 instead
  warnings.warn(
2023-06-23 05:47:29,470 - distributed.scheduler - INFO - State start
2023-06-23 05:47:29,559 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-23 05:47:29,560 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-23 05:47:29,560 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44161/status
2023-06-23 05:47:29,930 - distributed.scheduler - INFO - Receive client connection: Client-6faed8f7-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:29,942 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46206
2023-06-23 05:47:30,313 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36211'
2023-06-23 05:47:32,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:32,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:33,134 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-23 05:47:38,748 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45703
2023-06-23 05:47:38,749 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45703
2023-06-23 05:47:38,749 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38665
2023-06-23 05:47:38,749 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-23 05:47:38,749 - distributed.worker - INFO - -------------------------------------------------
2023-06-23 05:47:38,749 - distributed.worker - INFO -               Threads:                          1
2023-06-23 05:47:38,749 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-23 05:47:38,749 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9pw43vzg
2023-06-23 05:47:38,750 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4f1606fd-0c58-40bc-8d79-da267422baed
2023-06-23 05:47:38,750 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fefe6082-234b-4405-ae7f-b37af7e11c4a
std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-23 05:47:39,093 - distributed.worker - INFO - Starting Worker plugin PreImport-6a558556-829b-4800-b79a-6c137dd1a2ce
2023-06-23 05:47:39,093 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45703. Reason: worker-close
2023-06-23 05:47:39,093 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2023-06-23 05:47:39,097 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:47:39,138 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-23 05:47:39,141 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36211'. Reason: nanny-instantiate-failed
2023-06-23 05:47:39,142 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2023-06-23 05:47:39,620 - distributed.nanny - INFO - Worker process 52788 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1478, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1881, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/utils.py", line 115, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 847, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 907, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 348, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 5, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 81, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1130, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1055, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 760, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 441, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 527, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 433, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 244, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-23 05:47:40,021 - distributed.scheduler - INFO - Remove client Client-6faed8f7-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:40,021 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46206; closing.
2023-06-23 05:47:40,022 - distributed.scheduler - INFO - Remove client Client-6faed8f7-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:40,022 - distributed.scheduler - INFO - Close client connection: Client-6faed8f7-1189-11ee-821e-d8c49764f6bb
2023-06-23 05:47:40,023 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-23 05:47:40,023 - distributed.scheduler - INFO - Scheduler closing...
2023-06-23 05:47:40,024 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-23 05:47:40,024 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-23 05:47:40,025 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43539 instead
  warnings.warn(
2023-06-23 05:47:51,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:51,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:51,303 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:51,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:51,330 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:51,330 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:51,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:51,355 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:51,356 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:51,356 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:51,357 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:51,357 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:51,358 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:51,358 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:47:51,359 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:47:51,359 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36671 instead
  warnings.warn(
2023-06-23 05:48:04,592 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:04,592 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:04,618 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:04,619 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:04,649 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:04,649 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:04,654 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:04,654 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:04,725 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:04,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:04,738 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:04,738 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:04,747 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:04,747 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:04,756 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:04,756 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45805 instead
  warnings.warn(
2023-06-23 05:48:16,630 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:16,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:16,642 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:16,642 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:16,646 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:16,646 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:16,648 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:16,649 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:16,709 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:16,709 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:16,710 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:16,710 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:16,767 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:16,768 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:16,773 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:16,773 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34961 instead
  warnings.warn(
2023-06-23 05:48:28,711 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:28,711 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:28,711 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:28,711 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:28,718 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:28,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:28,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:28,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:28,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:28,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:28,779 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:28,779 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:28,789 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:28,789 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:28,808 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:28,808 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39713 instead
  warnings.warn(
2023-06-23 05:48:42,291 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:42,291 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:42,299 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:42,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:42,304 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:42,304 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:42,331 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:42,331 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:42,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:42,339 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:42,340 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:42,340 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:42,406 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:42,406 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:42,411 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:42,411 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34981 instead
  warnings.warn(
2023-06-23 05:48:56,144 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:56,144 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:56,146 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:56,146 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:56,181 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:56,181 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:56,184 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:56,184 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:56,265 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:56,265 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:56,268 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:56,268 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:56,285 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:56,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:48:56,295 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:48:56,295 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35863 instead
  warnings.warn(
2023-06-23 05:49:11,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:11,040 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:11,045 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:11,045 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:11,111 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:11,111 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:11,114 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:11,114 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:11,118 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:11,118 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:11,119 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:11,119 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:11,120 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:11,120 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:11,127 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:11,127 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/dashboard/core.py:20: UserWarning: 
Dask needs bokeh >= 2.4.2, < 3 for the dashboard.
You have bokeh==3.1.1.
Continuing without the dashboard.
  warnings.warn(
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36203 instead
  warnings.warn(
2023-06-23 05:49:24,729 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:24,729 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:24,787 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:24,788 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:24,853 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:24,853 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:24,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:24,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:24,880 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:24,880 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:24,918 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:24,918 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:24,994 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:24,994 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:24,997 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-23 05:49:24,997 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-23 05:49:29,571 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 329, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1262, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35247 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39733 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44837 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44389 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39247 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35267 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35083 instead
  warnings.warn(
std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-23 05:50:56,607 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-23 05:50:56,616 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fc41902a580>>, <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-6' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:201> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 204, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1290, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 883, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 254, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 138, in _decode_default
    return merge_and_deserialize(
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 501, in merge_and_deserialize
    return deserialize(header, merged_frames, deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 428, in deserialize
    return loads(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 61, in dask_loads
    return loads(header["sub-header"], frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/comm/serialize.py", line 29, in dask_deserialize_cudf_object
    return Serializable.host_deserialize(header, frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 89, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-06-23 05:50:58,619 - distributed.nanny - ERROR - Worker process died unexpectedly
