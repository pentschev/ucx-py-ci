============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.2, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-10-15 05:37:06,809 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:37:06,813 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-15 05:37:06,817 - distributed.scheduler - INFO - State start
2023-10-15 05:37:06,838 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:37:06,839 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-15 05:37:06,840 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-15 05:37:06,840 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:37:06,956 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38085'
2023-10-15 05:37:06,975 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34787'
2023-10-15 05:37:06,977 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39267'
2023-10-15 05:37:06,984 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42749'
2023-10-15 05:37:07,785 - distributed.scheduler - INFO - Receive client connection: Client-dfaa908e-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:07,795 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44940
2023-10-15 05:37:08,665 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:08,665 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:08,665 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:08,665 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:08,665 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:08,665 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:08,669 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:08,669 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:08,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:08,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:08,670 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:08,674 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-10-15 05:37:08,686 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45047
2023-10-15 05:37:08,686 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45047
2023-10-15 05:37:08,686 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46253
2023-10-15 05:37:08,686 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-15 05:37:08,686 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:08,686 - distributed.worker - INFO -               Threads:                          4
2023-10-15 05:37:08,686 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-15 05:37:08,686 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ycd1reji
2023-10-15 05:37:08,687 - distributed.worker - INFO - Starting Worker plugin PreImport-aa45d984-4bdd-45a6-93f2-7d02241ede58
2023-10-15 05:37:08,687 - distributed.worker - INFO - Starting Worker plugin RMMSetup-52f71349-649b-4577-8aaa-d6e467b2cbf1
2023-10-15 05:37:08,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ee17a52b-a108-4d61-b95b-72b38bd72992
2023-10-15 05:37:08,687 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:09,121 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45047', status: init, memory: 0, processing: 0>
2023-10-15 05:37:09,122 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45047
2023-10-15 05:37:09,122 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44958
2023-10-15 05:37:09,123 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:09,124 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-15 05:37:09,124 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:09,125 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-15 05:37:10,188 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43881
2023-10-15 05:37:10,188 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43881
2023-10-15 05:37:10,189 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43953
2023-10-15 05:37:10,188 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37527
2023-10-15 05:37:10,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-15 05:37:10,189 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:10,189 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37527
2023-10-15 05:37:10,189 - distributed.worker - INFO -               Threads:                          4
2023-10-15 05:37:10,189 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46739
2023-10-15 05:37:10,189 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-15 05:37:10,189 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-qe8a9mx7
2023-10-15 05:37:10,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-15 05:37:10,189 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:10,189 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1bd42e4b-7415-4a4d-996e-95f93b3ce9b1
2023-10-15 05:37:10,189 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40173
2023-10-15 05:37:10,189 - distributed.worker - INFO -               Threads:                          4
2023-10-15 05:37:10,189 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40173
2023-10-15 05:37:10,189 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41001
2023-10-15 05:37:10,189 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-15 05:37:10,189 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-15 05:37:10,189 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:10,189 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-nr261rj4
2023-10-15 05:37:10,189 - distributed.worker - INFO - Starting Worker plugin PreImport-21d17615-e339-4aa4-984a-4c919e753f5a
2023-10-15 05:37:10,189 - distributed.worker - INFO -               Threads:                          4
2023-10-15 05:37:10,190 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-10-15 05:37:10,190 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-d7g8aijo
2023-10-15 05:37:10,190 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c5b8207-4b16-4fd6-bf04-c3636f55bf8d
2023-10-15 05:37:10,190 - distributed.worker - INFO - Starting Worker plugin PreImport-59cba722-ecb7-461e-bf23-839b13b8f402
2023-10-15 05:37:10,190 - distributed.worker - INFO - Starting Worker plugin PreImport-ce8e0063-50d5-48e4-9bc5-8c2c87a3d9a6
2023-10-15 05:37:10,190 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:10,190 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-23a5a20a-8e49-45c0-8f35-1c5ca05475f8
2023-10-15 05:37:10,190 - distributed.worker - INFO - Starting Worker plugin RMMSetup-423ea5ee-5945-4430-b4e1-14334f4e2e2f
2023-10-15 05:37:10,190 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3c0ab5f1-eabd-4875-b167-5b0271dfec00
2023-10-15 05:37:10,190 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:10,190 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c6c8244-aaba-4156-90ae-ceb66aaa8dfa
2023-10-15 05:37:10,191 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:10,210 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37527', status: init, memory: 0, processing: 0>
2023-10-15 05:37:10,211 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37527
2023-10-15 05:37:10,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56436
2023-10-15 05:37:10,212 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40173', status: init, memory: 0, processing: 0>
2023-10-15 05:37:10,212 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:10,212 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40173
2023-10-15 05:37:10,212 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56422
2023-10-15 05:37:10,212 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-15 05:37:10,213 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:10,213 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:10,214 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-15 05:37:10,214 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:10,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-15 05:37:10,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-15 05:37:10,224 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43881', status: init, memory: 0, processing: 0>
2023-10-15 05:37:10,225 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43881
2023-10-15 05:37:10,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56428
2023-10-15 05:37:10,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:10,228 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-15 05:37:10,228 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:10,230 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-15 05:37:10,266 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-15 05:37:10,266 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-15 05:37:10,267 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-15 05:37:10,267 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-10-15 05:37:10,272 - distributed.scheduler - INFO - Remove client Client-dfaa908e-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:10,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44940; closing.
2023-10-15 05:37:10,272 - distributed.scheduler - INFO - Remove client Client-dfaa908e-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:10,272 - distributed.scheduler - INFO - Close client connection: Client-dfaa908e-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:10,273 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38085'. Reason: nanny-close
2023-10-15 05:37:10,274 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:10,274 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34787'. Reason: nanny-close
2023-10-15 05:37:10,275 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:10,275 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40173. Reason: nanny-close
2023-10-15 05:37:10,275 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39267'. Reason: nanny-close
2023-10-15 05:37:10,275 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:10,276 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42749'. Reason: nanny-close
2023-10-15 05:37:10,276 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43881. Reason: nanny-close
2023-10-15 05:37:10,276 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:10,276 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37527. Reason: nanny-close
2023-10-15 05:37:10,277 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-15 05:37:10,277 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56422; closing.
2023-10-15 05:37:10,277 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45047. Reason: nanny-close
2023-10-15 05:37:10,277 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40173', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348230.2774649')
2023-10-15 05:37:10,278 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-15 05:37:10,278 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:10,278 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56436; closing.
2023-10-15 05:37:10,278 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-15 05:37:10,279 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37527', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348230.279462')
2023-10-15 05:37:10,279 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:10,279 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56428; closing.
2023-10-15 05:37:10,279 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-15 05:37:10,280 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348230.2803922')
2023-10-15 05:37:10,280 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44958; closing.
2023-10-15 05:37:10,280 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:10,281 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45047', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348230.2810502')
2023-10-15 05:37:10,281 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:10,281 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:37:11,390 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:37:11,390 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:37:11,391 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:37:11,392 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-15 05:37:11,392 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-10-15 05:37:13,512 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:37:13,517 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-15 05:37:13,521 - distributed.scheduler - INFO - State start
2023-10-15 05:37:13,548 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:37:13,549 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:37:13,550 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-15 05:37:13,550 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:37:13,705 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45513'
2023-10-15 05:37:13,721 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36769'
2023-10-15 05:37:13,730 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46501'
2023-10-15 05:37:13,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44957'
2023-10-15 05:37:13,756 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45871'
2023-10-15 05:37:13,758 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45215'
2023-10-15 05:37:13,767 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39067'
2023-10-15 05:37:13,771 - distributed.scheduler - INFO - Receive client connection: Client-e3a224b7-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:13,776 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35793'
2023-10-15 05:37:13,787 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34920
2023-10-15 05:37:15,605 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:15,605 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:15,605 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:15,605 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:15,607 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:15,607 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:15,609 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:15,609 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:15,611 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:15,630 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:15,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:15,634 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:15,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:15,644 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:15,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:15,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:15,648 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:15,649 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:15,651 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:15,652 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:15,656 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:15,664 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:15,664 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:15,668 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:19,329 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33889
2023-10-15 05:37:19,330 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33889
2023-10-15 05:37:19,330 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37697
2023-10-15 05:37:19,330 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:19,330 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:19,330 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:19,330 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:19,330 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1dl35r8b
2023-10-15 05:37:19,331 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7bfc5ef6-e005-48b4-8d76-b63a75e9d68f
2023-10-15 05:37:19,382 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39935
2023-10-15 05:37:19,382 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39935
2023-10-15 05:37:19,382 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40437
2023-10-15 05:37:19,382 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:19,383 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:19,383 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:19,383 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:19,383 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vl_e0vzf
2023-10-15 05:37:19,383 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ad5b43d4-e83f-4527-83eb-119b7f56aafc
2023-10-15 05:37:19,402 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43581
2023-10-15 05:37:19,403 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43581
2023-10-15 05:37:19,403 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39213
2023-10-15 05:37:19,403 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:19,403 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:19,403 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:19,404 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:19,404 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bhdqck9w
2023-10-15 05:37:19,404 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8122eba-aa3b-4f95-b539-dc200b5d638a
2023-10-15 05:37:19,408 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42597
2023-10-15 05:37:19,409 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42597
2023-10-15 05:37:19,409 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39497
2023-10-15 05:37:19,409 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:19,409 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:19,409 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:19,409 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:19,409 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rv1gc6fv
2023-10-15 05:37:19,410 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eacaa033-34fa-4cb7-bc4a-a5b8f7fbe139
2023-10-15 05:37:19,420 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45913
2023-10-15 05:37:19,420 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45913
2023-10-15 05:37:19,421 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40919
2023-10-15 05:37:19,421 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:19,421 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:19,421 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:19,421 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:19,421 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ksj3zqq7
2023-10-15 05:37:19,421 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35871
2023-10-15 05:37:19,421 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35871
2023-10-15 05:37:19,421 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46601
2023-10-15 05:37:19,421 - distributed.worker - INFO - Starting Worker plugin RMMSetup-190ce0cf-5a2d-47b8-b7d8-a5c0afe1cb3f
2023-10-15 05:37:19,421 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:19,422 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:19,422 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:19,422 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:19,422 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0j03oh5c
2023-10-15 05:37:19,422 - distributed.worker - INFO - Starting Worker plugin RMMSetup-579b198a-83f1-4237-ab87-ab48529617af
2023-10-15 05:37:19,422 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43191
2023-10-15 05:37:19,423 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43191
2023-10-15 05:37:19,423 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36115
2023-10-15 05:37:19,423 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:19,423 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:19,422 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41843
2023-10-15 05:37:19,423 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41843
2023-10-15 05:37:19,423 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:19,423 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:19,423 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41105
2023-10-15 05:37:19,423 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u7r9g8_d
2023-10-15 05:37:19,423 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:19,423 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:19,424 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:19,424 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:19,424 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8po8sggf
2023-10-15 05:37:19,424 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca2b23fe-4d31-4535-8d5a-b9d9b2952732
2023-10-15 05:37:19,424 - distributed.worker - INFO - Starting Worker plugin PreImport-1e11c24e-57db-4198-9fcd-530bcaaf3389
2023-10-15 05:37:19,424 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88cbe35b-1553-4852-a5f1-c3e99d737638
2023-10-15 05:37:19,425 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d909486-46d9-41e6-b5a3-e6e595b0d49b
2023-10-15 05:37:20,142 - distributed.worker - INFO - Starting Worker plugin PreImport-7418e0fc-7026-4bd0-ab44-717aa569a339
2023-10-15 05:37:20,142 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-11786d2e-d865-4caa-b70d-be072f21d606
2023-10-15 05:37:20,142 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,147 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-953bda81-57a1-4cfb-b509-9e060b7088c4
2023-10-15 05:37:20,148 - distributed.worker - INFO - Starting Worker plugin PreImport-121081a5-7e61-4437-8d24-23d89e33fb86
2023-10-15 05:37:20,149 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,166 - distributed.worker - INFO - Starting Worker plugin PreImport-66061f2d-90b5-4fd6-8951-08c87b660cef
2023-10-15 05:37:20,166 - distributed.worker - INFO - Starting Worker plugin PreImport-0487c44f-a723-43a9-ad7d-5a03b86387ce
2023-10-15 05:37:20,166 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fca7c1aa-f24d-4a8b-a1c0-6662b7b34de3
2023-10-15 05:37:20,166 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a621acf0-05ba-46d9-b7cd-2c7e3d7f9794
2023-10-15 05:37:20,166 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,166 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7241b2d3-a9ae-4419-8d7d-3d765cafb042
2023-10-15 05:37:20,167 - distributed.worker - INFO - Starting Worker plugin PreImport-013cedeb-4968-47cc-b02e-9be69a8d5836
2023-10-15 05:37:20,167 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,167 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,167 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,167 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-73231849-8da9-467f-a73c-ea5c7730cce2
2023-10-15 05:37:20,168 - distributed.worker - INFO - Starting Worker plugin PreImport-8a41f96e-1412-468f-a6bb-e4c2686e9a00
2023-10-15 05:37:20,169 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,171 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33889', status: init, memory: 0, processing: 0>
2023-10-15 05:37:20,173 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33889
2023-10-15 05:37:20,173 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46612
2023-10-15 05:37:20,174 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:20,175 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:20,175 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:20,187 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43191', status: init, memory: 0, processing: 0>
2023-10-15 05:37:20,188 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43191
2023-10-15 05:37:20,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46626
2023-10-15 05:37:20,189 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:20,191 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:20,191 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,193 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:20,197 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39935', status: init, memory: 0, processing: 0>
2023-10-15 05:37:20,198 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39935
2023-10-15 05:37:20,198 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46634
2023-10-15 05:37:20,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43581', status: init, memory: 0, processing: 0>
2023-10-15 05:37:20,199 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:20,199 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43581
2023-10-15 05:37:20,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46650
2023-10-15 05:37:20,199 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d71307c8-f428-4568-8b6e-8f0291f3a4d2
2023-10-15 05:37:20,199 - distributed.worker - INFO - Starting Worker plugin PreImport-3b4fc585-c705-4d71-9fc2-33de6ad2def0
2023-10-15 05:37:20,200 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,200 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:20,200 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:20,201 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:20,201 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:20,202 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:20,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42597', status: init, memory: 0, processing: 0>
2023-10-15 05:37:20,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42597
2023-10-15 05:37:20,208 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46666
2023-10-15 05:37:20,209 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:20,210 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:20,210 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,212 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45913', status: init, memory: 0, processing: 0>
2023-10-15 05:37:20,212 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:20,212 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45913
2023-10-15 05:37:20,213 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46674
2023-10-15 05:37:20,214 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41843', status: init, memory: 0, processing: 0>
2023-10-15 05:37:20,214 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:20,214 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41843
2023-10-15 05:37:20,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46670
2023-10-15 05:37:20,215 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:20,215 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,216 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:20,217 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:20,217 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:20,219 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:20,224 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35871', status: init, memory: 0, processing: 0>
2023-10-15 05:37:20,225 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35871
2023-10-15 05:37:20,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46686
2023-10-15 05:37:20,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:20,227 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:20,227 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:20,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:20,331 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:20,331 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:20,331 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:20,331 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:20,332 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:20,332 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:20,332 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:20,332 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:20,336 - distributed.scheduler - INFO - Remove client Client-e3a224b7-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:20,336 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34920; closing.
2023-10-15 05:37:20,336 - distributed.scheduler - INFO - Remove client Client-e3a224b7-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:20,337 - distributed.scheduler - INFO - Close client connection: Client-e3a224b7-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:20,338 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45513'. Reason: nanny-close
2023-10-15 05:37:20,338 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:20,339 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36769'. Reason: nanny-close
2023-10-15 05:37:20,339 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:20,339 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33889. Reason: nanny-close
2023-10-15 05:37:20,339 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46501'. Reason: nanny-close
2023-10-15 05:37:20,340 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:20,340 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39935. Reason: nanny-close
2023-10-15 05:37:20,340 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44957'. Reason: nanny-close
2023-10-15 05:37:20,340 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:20,340 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45913. Reason: nanny-close
2023-10-15 05:37:20,341 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45871'. Reason: nanny-close
2023-10-15 05:37:20,341 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:20,341 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:20,341 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46612; closing.
2023-10-15 05:37:20,341 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41843. Reason: nanny-close
2023-10-15 05:37:20,341 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45215'. Reason: nanny-close
2023-10-15 05:37:20,341 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348240.341694')
2023-10-15 05:37:20,341 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:20,341 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43581. Reason: nanny-close
2023-10-15 05:37:20,342 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39067'. Reason: nanny-close
2023-10-15 05:37:20,342 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:20,342 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:20,342 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35871. Reason: nanny-close
2023-10-15 05:37:20,342 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:20,342 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35793'. Reason: nanny-close
2023-10-15 05:37:20,343 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:20,343 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:20,343 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43191. Reason: nanny-close
2023-10-15 05:37:20,343 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46634; closing.
2023-10-15 05:37:20,343 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:20,344 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:20,344 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46674; closing.
2023-10-15 05:37:20,344 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:20,344 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42597. Reason: nanny-close
2023-10-15 05:37:20,344 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39935', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348240.344412')
2023-10-15 05:37:20,344 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:20,345 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45913', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348240.3450751')
2023-10-15 05:37:20,345 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:20,345 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46670; closing.
2023-10-15 05:37:20,345 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:20,345 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:20,346 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41843', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348240.3460705')
2023-10-15 05:37:20,346 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:20,346 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:20,346 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:20,346 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46650; closing.
2023-10-15 05:37:20,346 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46686; closing.
2023-10-15 05:37:20,347 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348240.3471043')
2023-10-15 05:37:20,347 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348240.3474839')
2023-10-15 05:37:20,347 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46626; closing.
2023-10-15 05:37:20,347 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:20,348 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:20,348 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43191', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348240.348348')
2023-10-15 05:37:20,348 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46666; closing.
2023-10-15 05:37:20,349 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42597', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348240.3490329')
2023-10-15 05:37:20,349 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:37:21,905 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:37:21,906 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:37:21,907 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:37:21,908 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:37:21,909 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-10-15 05:37:24,417 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:37:24,422 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41389 instead
  warnings.warn(
2023-10-15 05:37:24,426 - distributed.scheduler - INFO - State start
2023-10-15 05:37:24,492 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:37:24,493 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:37:24,494 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41389/status
2023-10-15 05:37:24,494 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:37:24,544 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39755'
2023-10-15 05:37:24,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46265'
2023-10-15 05:37:24,574 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42681'
2023-10-15 05:37:24,585 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37283'
2023-10-15 05:37:24,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44797'
2023-10-15 05:37:24,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41487'
2023-10-15 05:37:24,603 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41805'
2023-10-15 05:37:24,612 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35153'
2023-10-15 05:37:26,087 - distributed.scheduler - INFO - Receive client connection: Client-ea17a7fe-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:26,100 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46834
2023-10-15 05:37:26,326 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:26,326 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:26,330 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:26,399 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:26,399 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:26,403 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:26,457 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:26,458 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:26,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:26,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:26,462 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:26,462 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:26,462 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:26,466 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:26,466 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:26,477 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:26,477 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:26,481 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:26,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:26,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:26,510 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:26,520 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:26,520 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:26,524 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:28,328 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33009
2023-10-15 05:37:28,330 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33009
2023-10-15 05:37:28,331 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45093
2023-10-15 05:37:28,331 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:28,331 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:28,331 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:28,331 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:28,331 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j_vje3uy
2023-10-15 05:37:28,333 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e07204c-95ba-4cec-acdd-fce890b92dde
2023-10-15 05:37:28,433 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ad67dee-6ed1-4588-8b2e-9a5cec69831f
2023-10-15 05:37:28,434 - distributed.worker - INFO - Starting Worker plugin PreImport-f34d501b-bd62-4ae7-9ec8-d6046339cf2d
2023-10-15 05:37:28,434 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:28,462 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33009', status: init, memory: 0, processing: 0>
2023-10-15 05:37:28,464 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33009
2023-10-15 05:37:28,464 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46856
2023-10-15 05:37:28,465 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:28,466 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:28,466 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:28,468 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:29,358 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42835
2023-10-15 05:37:29,358 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42835
2023-10-15 05:37:29,359 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44673
2023-10-15 05:37:29,359 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,359 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,359 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:29,359 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:29,359 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-taxcqws0
2023-10-15 05:37:29,359 - distributed.worker - INFO - Starting Worker plugin PreImport-51c84958-a887-4822-9554-748acad35c3a
2023-10-15 05:37:29,360 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72e28ee0-17d1-46da-8787-71885fc6e21e
2023-10-15 05:37:29,360 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5873f512-eb97-4107-83d6-9cdd9c04eb20
2023-10-15 05:37:29,360 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43531
2023-10-15 05:37:29,361 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43531
2023-10-15 05:37:29,361 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37227
2023-10-15 05:37:29,361 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,361 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,361 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:29,362 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:29,362 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g4qzv2dl
2023-10-15 05:37:29,362 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8dccf4f-13ad-4e9f-a359-c065e5b70803
2023-10-15 05:37:29,488 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41403
2023-10-15 05:37:29,489 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41403
2023-10-15 05:37:29,489 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42229
2023-10-15 05:37:29,489 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,490 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,490 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:29,490 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:29,490 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cajt1agt
2023-10-15 05:37:29,490 - distributed.worker - INFO - Starting Worker plugin RMMSetup-86625dae-0051-47ff-8eeb-340eaf9daf7e
2023-10-15 05:37:29,495 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46231
2023-10-15 05:37:29,496 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46231
2023-10-15 05:37:29,496 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41375
2023-10-15 05:37:29,496 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,496 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,496 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:29,496 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:29,496 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j3xxoro3
2023-10-15 05:37:29,497 - distributed.worker - INFO - Starting Worker plugin PreImport-9f65707c-d938-49f2-ac16-665c9d2e2a48
2023-10-15 05:37:29,497 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2656ded3-5924-44d2-bbf6-1103ccc8e22d
2023-10-15 05:37:29,497 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44443
2023-10-15 05:37:29,498 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44443
2023-10-15 05:37:29,498 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39389
2023-10-15 05:37:29,498 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,498 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,498 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:29,499 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:29,499 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-358qgkyz
2023-10-15 05:37:29,499 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4014845b-f38e-4ec6-a4c7-751ae90bccb9
2023-10-15 05:37:29,499 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2d494ea3-1978-4f1c-8eea-13a1486da4fb
2023-10-15 05:37:29,501 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33943
2023-10-15 05:37:29,502 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33943
2023-10-15 05:37:29,502 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36551
2023-10-15 05:37:29,502 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,502 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,502 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:29,502 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:29,502 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-erxd3ia5
2023-10-15 05:37:29,503 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cb19f337-5954-4e96-809a-3ab7c9052720
2023-10-15 05:37:29,503 - distributed.worker - INFO - Starting Worker plugin PreImport-ad79e478-66f4-47a4-b2b0-d4af42c5cf84
2023-10-15 05:37:29,503 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5a8be475-fd07-4bab-bb5d-9bf3c751751f
2023-10-15 05:37:29,507 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39835
2023-10-15 05:37:29,507 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39835
2023-10-15 05:37:29,507 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33227
2023-10-15 05:37:29,507 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,508 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,508 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:29,508 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:29,508 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-aoicyo2m
2023-10-15 05:37:29,508 - distributed.worker - INFO - Starting Worker plugin PreImport-ae05ef21-1247-415f-b77d-51854f846360
2023-10-15 05:37:29,508 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7656684-3d8c-4b59-bc57-8e6d80d3a215
2023-10-15 05:37:29,509 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2272f2c6-9087-4518-944e-6b3d21c729a2
2023-10-15 05:37:29,517 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,519 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bdd00049-bc2c-4989-bdfa-b678120f7a12
2023-10-15 05:37:29,519 - distributed.worker - INFO - Starting Worker plugin PreImport-30de2fb5-17e0-4f1a-a782-0e27b11cead8
2023-10-15 05:37:29,519 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,520 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94710b9c-8cda-4550-bbbc-93a8cbb0c9c1
2023-10-15 05:37:29,520 - distributed.worker - INFO - Starting Worker plugin PreImport-4f99e6ba-0821-4dcd-996b-df0294af074c
2023-10-15 05:37:29,520 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,521 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,527 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-74cd7d2d-a024-4dd8-b307-a43ae4eea3dc
2023-10-15 05:37:29,527 - distributed.worker - INFO - Starting Worker plugin PreImport-be60ad72-9c85-471b-ba9b-883f5f15501d
2023-10-15 05:37:29,528 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,535 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,538 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,543 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44443', status: init, memory: 0, processing: 0>
2023-10-15 05:37:29,544 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44443
2023-10-15 05:37:29,544 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46880
2023-10-15 05:37:29,545 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43531', status: init, memory: 0, processing: 0>
2023-10-15 05:37:29,545 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43531
2023-10-15 05:37:29,545 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46876
2023-10-15 05:37:29,545 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:29,546 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33943', status: init, memory: 0, processing: 0>
2023-10-15 05:37:29,546 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,546 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:29,547 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33943
2023-10-15 05:37:29,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46892
2023-10-15 05:37:29,547 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:29,548 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,548 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:29,548 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,548 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,549 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:29,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:29,555 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42835', status: init, memory: 0, processing: 0>
2023-10-15 05:37:29,555 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42835
2023-10-15 05:37:29,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46866
2023-10-15 05:37:29,557 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:29,559 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,559 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,561 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:29,562 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39835', status: init, memory: 0, processing: 0>
2023-10-15 05:37:29,563 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39835
2023-10-15 05:37:29,563 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46924
2023-10-15 05:37:29,564 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41403', status: init, memory: 0, processing: 0>
2023-10-15 05:37:29,564 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:29,564 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41403
2023-10-15 05:37:29,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46896
2023-10-15 05:37:29,564 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,565 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,565 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:29,566 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:29,567 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,567 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,569 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46231', status: init, memory: 0, processing: 0>
2023-10-15 05:37:29,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:29,569 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46231
2023-10-15 05:37:29,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46908
2023-10-15 05:37:29,570 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:29,572 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:29,572 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:29,573 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:29,669 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:29,669 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:29,669 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:29,670 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:29,670 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:29,670 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:29,670 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:29,670 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:29,674 - distributed.scheduler - INFO - Remove client Client-ea17a7fe-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:29,675 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46834; closing.
2023-10-15 05:37:29,675 - distributed.scheduler - INFO - Remove client Client-ea17a7fe-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:29,676 - distributed.scheduler - INFO - Close client connection: Client-ea17a7fe-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:29,676 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39755'. Reason: nanny-close
2023-10-15 05:37:29,677 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:29,678 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46265'. Reason: nanny-close
2023-10-15 05:37:29,678 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:29,678 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42835. Reason: nanny-close
2023-10-15 05:37:29,679 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42681'. Reason: nanny-close
2023-10-15 05:37:29,679 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:29,679 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33009. Reason: nanny-close
2023-10-15 05:37:29,680 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37283'. Reason: nanny-close
2023-10-15 05:37:29,680 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:29,680 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39835. Reason: nanny-close
2023-10-15 05:37:29,680 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44797'. Reason: nanny-close
2023-10-15 05:37:29,681 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:29,681 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46866; closing.
2023-10-15 05:37:29,681 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:29,681 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43531. Reason: nanny-close
2023-10-15 05:37:29,681 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42835', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348249.681452')
2023-10-15 05:37:29,681 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41487'. Reason: nanny-close
2023-10-15 05:37:29,681 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:29,682 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:29,682 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:29,682 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46231. Reason: nanny-close
2023-10-15 05:37:29,682 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41805'. Reason: nanny-close
2023-10-15 05:37:29,682 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:29,682 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35153'. Reason: nanny-close
2023-10-15 05:37:29,682 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41403. Reason: nanny-close
2023-10-15 05:37:29,682 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:29,683 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44443. Reason: nanny-close
2023-10-15 05:37:29,683 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:29,683 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:29,683 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46924; closing.
2023-10-15 05:37:29,683 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:29,683 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46856; closing.
2023-10-15 05:37:29,683 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33943. Reason: nanny-close
2023-10-15 05:37:29,684 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:29,684 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39835', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348249.6844442')
2023-10-15 05:37:29,684 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:29,684 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33009', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348249.6849127')
2023-10-15 05:37:29,685 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:29,685 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46876; closing.
2023-10-15 05:37:29,685 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:29,685 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:29,685 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43531', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348249.6856833')
2023-10-15 05:37:29,685 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:29,686 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46908; closing.
2023-10-15 05:37:29,686 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46880; closing.
2023-10-15 05:37:29,687 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:29,687 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:29,687 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46896; closing.
2023-10-15 05:37:29,687 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:29,687 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46231', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348249.6875455')
2023-10-15 05:37:29,687 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:29,688 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44443', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348249.6879945')
2023-10-15 05:37:29,688 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41403', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348249.6884127')
2023-10-15 05:37:29,688 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46892; closing.
2023-10-15 05:37:29,689 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348249.689134')
2023-10-15 05:37:29,689 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:37:31,144 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:37:31,145 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:37:31,146 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:37:31,147 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:37:31,148 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-10-15 05:37:33,119 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:37:33,123 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32961 instead
  warnings.warn(
2023-10-15 05:37:44,979 - distributed.scheduler - INFO - State start
2023-10-15 05:37:45,000 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:37:45,001 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:37:45,002 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:32961/status
2023-10-15 05:37:45,002 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:37:45,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40237'
2023-10-15 05:37:45,694 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33805'
2023-10-15 05:37:47,008 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:47,008 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:47,012 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:47,296 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:47,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:47,299 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:47,843 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39303'
2023-10-15 05:37:47,940 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46519
2023-10-15 05:37:47,940 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46519
2023-10-15 05:37:47,940 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44195
2023-10-15 05:37:47,940 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:47,940 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:47,940 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:47,941 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:47,941 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-22aag0aw
2023-10-15 05:37:47,941 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e01de86-0b2e-4786-8ef4-dcbc2de81c6a
2023-10-15 05:37:48,062 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c7518a68-a73d-40b2-963d-d971e15a0d64
2023-10-15 05:37:48,062 - distributed.worker - INFO - Starting Worker plugin PreImport-ae377cb9-a1a5-4e4d-be3b-da2a5d887876
2023-10-15 05:37:48,062 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:48,090 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46519', status: init, memory: 0, processing: 0>
2023-10-15 05:37:48,101 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46519
2023-10-15 05:37:48,102 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47404
2023-10-15 05:37:48,102 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:48,104 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:48,104 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:48,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:48,152 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36175
2023-10-15 05:37:48,152 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36175
2023-10-15 05:37:48,152 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39883
2023-10-15 05:37:48,152 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:48,152 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:48,152 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:48,153 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:48,153 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i9gwncrg
2023-10-15 05:37:48,153 - distributed.worker - INFO - Starting Worker plugin RMMSetup-164056f4-29dd-4fb8-83fd-3c77ba65d491
2023-10-15 05:37:48,255 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5aadefb7-267d-49ba-a72e-dc396a2672b5
2023-10-15 05:37:48,255 - distributed.worker - INFO - Starting Worker plugin PreImport-d58cc029-4424-42b0-bd75-5e1ffc6de232
2023-10-15 05:37:48,255 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:48,277 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36175', status: init, memory: 0, processing: 0>
2023-10-15 05:37:48,277 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36175
2023-10-15 05:37:48,277 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47414
2023-10-15 05:37:48,278 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:48,279 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:48,279 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:48,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:49,401 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:49,401 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:49,404 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:49,902 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34465'
2023-10-15 05:37:50,146 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43103'
2023-10-15 05:37:50,296 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34721
2023-10-15 05:37:50,297 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34721
2023-10-15 05:37:50,298 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41981
2023-10-15 05:37:50,298 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:50,298 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:50,298 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:50,298 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:50,298 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1ryo7u7w
2023-10-15 05:37:50,299 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e3629008-e22d-4458-a3c3-afbb7e04776c
2023-10-15 05:37:50,436 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7170efb5-58df-43a0-a0ea-738df68bfdf6
2023-10-15 05:37:50,437 - distributed.worker - INFO - Starting Worker plugin PreImport-05e9d0b9-f309-42f6-b4da-46779571d26f
2023-10-15 05:37:50,437 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:50,461 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34721', status: init, memory: 0, processing: 0>
2023-10-15 05:37:50,461 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34721
2023-10-15 05:37:50,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54496
2023-10-15 05:37:50,462 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:50,464 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:50,464 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:50,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:51,189 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41721'
2023-10-15 05:37:51,519 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45287'
2023-10-15 05:37:51,548 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:51,549 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:51,552 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:51,830 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:51,830 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:51,834 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:52,472 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43011
2023-10-15 05:37:52,473 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43011
2023-10-15 05:37:52,473 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32925
2023-10-15 05:37:52,473 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:52,473 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:52,473 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:52,473 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:52,474 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lap_yj9w
2023-10-15 05:37:52,474 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3cf4f2d-79fe-404a-ab46-84772eec10c3
2023-10-15 05:37:52,597 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2fb4f9bc-2d07-4b32-9577-540216f26cbf
2023-10-15 05:37:52,598 - distributed.worker - INFO - Starting Worker plugin PreImport-2a242cf2-9b65-4ce9-90e2-4f7acf3ade75
2023-10-15 05:37:52,598 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:52,629 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43011', status: init, memory: 0, processing: 0>
2023-10-15 05:37:52,630 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43011
2023-10-15 05:37:52,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54534
2023-10-15 05:37:52,631 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:52,632 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:52,633 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:52,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:52,687 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43187
2023-10-15 05:37:52,687 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43187
2023-10-15 05:37:52,687 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34301
2023-10-15 05:37:52,687 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:52,687 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:52,687 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:52,688 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:52,688 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-75t6cix8
2023-10-15 05:37:52,688 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9c2327a-0b26-4561-ad63-19072570ae70
2023-10-15 05:37:52,804 - distributed.worker - INFO - Starting Worker plugin PreImport-7fe38202-70b0-4020-8e32-bce1efececa3
2023-10-15 05:37:52,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-67b16c64-9443-44ee-8615-88320ccb71ff
2023-10-15 05:37:52,806 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:52,835 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43187', status: init, memory: 0, processing: 0>
2023-10-15 05:37:52,836 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43187
2023-10-15 05:37:52,836 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54548
2023-10-15 05:37:52,837 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:52,838 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:52,838 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:52,839 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:52,851 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:52,851 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:52,854 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:53,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:53,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:53,193 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:53,775 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37969
2023-10-15 05:37:53,777 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37969
2023-10-15 05:37:53,777 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41711
2023-10-15 05:37:53,777 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:53,777 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:53,777 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:53,777 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:53,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bj4zehfb
2023-10-15 05:37:53,778 - distributed.worker - INFO - Starting Worker plugin PreImport-45d66636-5ada-49b3-bc38-18e643ba9ba2
2023-10-15 05:37:53,778 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7378ad5a-63ea-431d-936a-13f610e31166
2023-10-15 05:37:53,778 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b30f67a6-2db1-408f-9035-74300e1fae20
2023-10-15 05:37:53,904 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:53,934 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37969', status: init, memory: 0, processing: 0>
2023-10-15 05:37:53,935 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37969
2023-10-15 05:37:53,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54572
2023-10-15 05:37:53,936 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:53,937 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:53,937 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:53,939 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:53,987 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40797
2023-10-15 05:37:53,988 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40797
2023-10-15 05:37:53,988 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46837
2023-10-15 05:37:53,988 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:53,989 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:53,989 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:53,989 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:53,989 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q0dlt3g3
2023-10-15 05:37:53,989 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ab00be87-9a20-4b29-b8ad-f88374727907
2023-10-15 05:37:54,098 - distributed.worker - INFO - Starting Worker plugin PreImport-c753be21-1170-4e05-949d-eefdc19d69cc
2023-10-15 05:37:54,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f1d06f1d-d46b-4ca4-b4ae-e06d72c4d1e4
2023-10-15 05:37:54,099 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:54,122 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40797', status: init, memory: 0, processing: 0>
2023-10-15 05:37:54,123 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40797
2023-10-15 05:37:54,123 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54576
2023-10-15 05:37:54,124 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:54,124 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:54,124 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:54,126 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:55,509 - distributed.scheduler - INFO - Receive client connection: Client-ef69bc2d-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:55,509 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54598
2023-10-15 05:37:57,022 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38491'
2023-10-15 05:37:58,736 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:37:58,736 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:37:58,740 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:37:59,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36943
2023-10-15 05:37:59,628 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36943
2023-10-15 05:37:59,628 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34725
2023-10-15 05:37:59,628 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:37:59,628 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:59,628 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:37:59,628 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:37:59,628 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ag4jbjos
2023-10-15 05:37:59,628 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77b6a3a2-3aef-430f-88c4-ecf0711c6fb6
2023-10-15 05:37:59,754 - distributed.worker - INFO - Starting Worker plugin PreImport-48936142-3014-4046-b3dc-a8b235668c48
2023-10-15 05:37:59,754 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-807e78a5-9721-4701-a41e-fe48085410d9
2023-10-15 05:37:59,755 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:59,782 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36943', status: init, memory: 0, processing: 0>
2023-10-15 05:37:59,783 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36943
2023-10-15 05:37:59,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54624
2023-10-15 05:37:59,784 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:37:59,785 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:37:59,785 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:37:59,786 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:37:59,805 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:59,805 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:59,805 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:59,806 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:59,805 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:59,806 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:59,806 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:59,806 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:37:59,817 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:37:59,817 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:37:59,817 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:37:59,817 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:37:59,817 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:37:59,817 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:37:59,817 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:37:59,818 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:37:59,824 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:37:59,825 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:37:59,828 - distributed.scheduler - INFO - Remove client Client-ef69bc2d-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:59,828 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54598; closing.
2023-10-15 05:37:59,828 - distributed.scheduler - INFO - Remove client Client-ef69bc2d-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:59,828 - distributed.scheduler - INFO - Close client connection: Client-ef69bc2d-6b1c-11ee-b11b-d8c49764f6bb
2023-10-15 05:37:59,829 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45287'. Reason: nanny-close
2023-10-15 05:37:59,830 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:59,831 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43103'. Reason: nanny-close
2023-10-15 05:37:59,831 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:59,831 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40797. Reason: nanny-close
2023-10-15 05:37:59,832 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40237'. Reason: nanny-close
2023-10-15 05:37:59,832 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:59,832 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43187. Reason: nanny-close
2023-10-15 05:37:59,832 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41721'. Reason: nanny-close
2023-10-15 05:37:59,832 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:59,833 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33805'. Reason: nanny-close
2023-10-15 05:37:59,833 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:59,833 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46519. Reason: nanny-close
2023-10-15 05:37:59,833 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54576; closing.
2023-10-15 05:37:59,833 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:59,833 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34465'. Reason: nanny-close
2023-10-15 05:37:59,834 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40797', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348279.8339658')
2023-10-15 05:37:59,834 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37969. Reason: nanny-close
2023-10-15 05:37:59,834 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:59,834 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36175. Reason: nanny-close
2023-10-15 05:37:59,834 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39303'. Reason: nanny-close
2023-10-15 05:37:59,834 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:59,834 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:59,835 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38491'. Reason: nanny-close
2023-10-15 05:37:59,835 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:37:59,835 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43011. Reason: nanny-close
2023-10-15 05:37:59,835 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:59,835 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34721. Reason: nanny-close
2023-10-15 05:37:59,835 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54548; closing.
2023-10-15 05:37:59,835 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:59,836 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:59,836 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36943. Reason: nanny-close
2023-10-15 05:37:59,836 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:59,836 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:59,836 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43187', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348279.8365772')
2023-10-15 05:37:59,836 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47404; closing.
2023-10-15 05:37:59,837 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:59,837 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:59,837 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:59,837 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:59,837 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46519', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348279.8377762')
2023-10-15 05:37:59,838 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47414; closing.
2023-10-15 05:37:59,838 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:37:59,838 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54572; closing.
2023-10-15 05:37:59,838 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:59,839 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:59,839 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36175', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348279.8389993')
2023-10-15 05:37:59,839 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:59,839 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348279.8392875')
2023-10-15 05:37:59,840 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54534; closing.
2023-10-15 05:37:59,840 - distributed.nanny - INFO - Worker closed
2023-10-15 05:37:59,840 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54496; closing.
2023-10-15 05:37:59,840 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54624; closing.
2023-10-15 05:37:59,841 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43011', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348279.8410337')
2023-10-15 05:37:59,841 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34721', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348279.8414376')
2023-10-15 05:37:59,841 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36943', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348279.8418212')
2023-10-15 05:37:59,842 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:38:01,948 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:38:01,949 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:38:01,949 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:38:01,951 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:38:01,951 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-10-15 05:38:04,058 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:04,062 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38997 instead
  warnings.warn(
2023-10-15 05:38:04,066 - distributed.scheduler - INFO - State start
2023-10-15 05:38:04,088 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:04,089 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:38:04,089 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38997/status
2023-10-15 05:38:04,090 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:38:04,256 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34549'
2023-10-15 05:38:04,274 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40869'
2023-10-15 05:38:04,282 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37801'
2023-10-15 05:38:04,291 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37971'
2023-10-15 05:38:04,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39233'
2023-10-15 05:38:04,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36289'
2023-10-15 05:38:04,317 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40523'
2023-10-15 05:38:04,325 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37785'
2023-10-15 05:38:04,958 - distributed.scheduler - INFO - Receive client connection: Client-01c5bcea-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:04,973 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51440
2023-10-15 05:38:06,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:06,005 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:06,009 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:06,290 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:06,290 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:06,292 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:06,292 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:06,295 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:06,296 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:06,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:06,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:06,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:06,301 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:06,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:06,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:06,302 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:06,303 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:06,305 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:06,305 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:06,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:06,306 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:06,307 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:06,307 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:06,310 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:07,701 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37381
2023-10-15 05:38:07,702 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37381
2023-10-15 05:38:07,702 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42569
2023-10-15 05:38:07,702 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:07,702 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:07,703 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:07,703 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:07,703 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nyh7qg6i
2023-10-15 05:38:07,704 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8eedd567-0271-408b-9966-369deee7207a
2023-10-15 05:38:08,235 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e52e897-a249-4fe0-b767-c3c7d28d1155
2023-10-15 05:38:08,236 - distributed.worker - INFO - Starting Worker plugin PreImport-581d301a-a7da-484a-89e7-e89a5d321491
2023-10-15 05:38:08,237 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:08,272 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37381', status: init, memory: 0, processing: 0>
2023-10-15 05:38:08,274 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37381
2023-10-15 05:38:08,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51446
2023-10-15 05:38:08,276 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:08,279 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:08,280 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:08,281 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:09,174 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39629
2023-10-15 05:38:09,175 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39629
2023-10-15 05:38:09,175 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41507
2023-10-15 05:38:09,175 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,175 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,175 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:09,175 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:09,175 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0et2go15
2023-10-15 05:38:09,176 - distributed.worker - INFO - Starting Worker plugin RMMSetup-062e1b24-c910-47ca-8b87-c949422d9f97
2023-10-15 05:38:09,182 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43389
2023-10-15 05:38:09,183 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43389
2023-10-15 05:38:09,183 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35963
2023-10-15 05:38:09,182 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39983
2023-10-15 05:38:09,183 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,183 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39983
2023-10-15 05:38:09,183 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,183 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38277
2023-10-15 05:38:09,183 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,183 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:09,183 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,183 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:09,183 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-inndbtvc
2023-10-15 05:38:09,183 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:09,183 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:09,183 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tzoc6ao8
2023-10-15 05:38:09,184 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b89fdfd0-73d3-428c-b622-9f2f82b782c3
2023-10-15 05:38:09,184 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fce7615-8abe-45d2-9eba-e7d395a597e4
2023-10-15 05:38:09,190 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42513
2023-10-15 05:38:09,191 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42513
2023-10-15 05:38:09,191 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43095
2023-10-15 05:38:09,191 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,191 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,191 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:09,191 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:09,191 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lmcax4co
2023-10-15 05:38:09,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-299e04f0-c34c-4612-9ec7-b17f4390a2bc
2023-10-15 05:38:09,192 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44173
2023-10-15 05:38:09,193 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44173
2023-10-15 05:38:09,192 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34095
2023-10-15 05:38:09,193 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37641
2023-10-15 05:38:09,193 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34095
2023-10-15 05:38:09,193 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,193 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,193 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45351
2023-10-15 05:38:09,193 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,193 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:09,193 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,193 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:09,193 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-w1c2f764
2023-10-15 05:38:09,193 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:09,193 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:09,193 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bbzazqn0
2023-10-15 05:38:09,193 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c22053d1-a7f1-4c23-bcb0-96624e683e2b
2023-10-15 05:38:09,194 - distributed.worker - INFO - Starting Worker plugin PreImport-b41f617a-c015-445a-a097-6c1325c3b93c
2023-10-15 05:38:09,194 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d38f9131-1015-42e9-9dab-5c6b577bad8a
2023-10-15 05:38:09,194 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f00ce62d-230d-4857-8515-59cbcb949e3a
2023-10-15 05:38:09,195 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41753
2023-10-15 05:38:09,195 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41753
2023-10-15 05:38:09,196 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34651
2023-10-15 05:38:09,196 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,196 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,196 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:09,196 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:09,196 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-upw_ks9q
2023-10-15 05:38:09,196 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe8b18e0-f906-4ef0-87b9-0befc5095a79
2023-10-15 05:38:09,507 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,508 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6fdc379e-b01b-4c29-8525-3cfb071d4d90
2023-10-15 05:38:09,509 - distributed.worker - INFO - Starting Worker plugin PreImport-a00651d7-0985-4930-b7c3-1bee788bd950
2023-10-15 05:38:09,510 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,510 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3cd4286f-080c-40cf-bc8d-f81d45d32961
2023-10-15 05:38:09,510 - distributed.worker - INFO - Starting Worker plugin PreImport-c22763d2-ef00-4713-a7b1-a308c42ed0bd
2023-10-15 05:38:09,510 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-18f4d1d0-f07f-4b71-9359-26ed3b178161
2023-10-15 05:38:09,511 - distributed.worker - INFO - Starting Worker plugin PreImport-b71d4926-8f74-481b-9702-a869c6b60c99
2023-10-15 05:38:09,511 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,511 - distributed.worker - INFO - Starting Worker plugin PreImport-8cf40ca0-b4c2-4072-9483-bfbcc15ffc07
2023-10-15 05:38:09,511 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,511 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-22575b27-72fd-4c3a-bdea-d8011e9ffff6
2023-10-15 05:38:09,513 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,514 - distributed.worker - INFO - Starting Worker plugin PreImport-de21c7a3-c31e-499f-8d91-232302134c99
2023-10-15 05:38:09,514 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1cb1304d-c7a0-486a-8377-a16f73f21960
2023-10-15 05:38:09,516 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,516 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0bd826a5-98e7-4fd6-9d22-76a2129348ce
2023-10-15 05:38:09,517 - distributed.worker - INFO - Starting Worker plugin PreImport-532c47ae-12db-440f-8af4-d56bfd4f6534
2023-10-15 05:38:09,517 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,541 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42513', status: init, memory: 0, processing: 0>
2023-10-15 05:38:09,542 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42513
2023-10-15 05:38:09,542 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51480
2023-10-15 05:38:09,542 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34095', status: init, memory: 0, processing: 0>
2023-10-15 05:38:09,543 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:09,543 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34095
2023-10-15 05:38:09,543 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51462
2023-10-15 05:38:09,544 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,544 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39629', status: init, memory: 0, processing: 0>
2023-10-15 05:38:09,544 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,544 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:09,545 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39629
2023-10-15 05:38:09,545 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51478
2023-10-15 05:38:09,545 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,546 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,546 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:09,546 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43389', status: init, memory: 0, processing: 0>
2023-10-15 05:38:09,546 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:09,546 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43389
2023-10-15 05:38:09,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51484
2023-10-15 05:38:09,547 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:09,548 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,548 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,548 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:09,549 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,550 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:09,551 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:09,552 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41753', status: init, memory: 0, processing: 0>
2023-10-15 05:38:09,553 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41753
2023-10-15 05:38:09,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51524
2023-10-15 05:38:09,554 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:09,555 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44173', status: init, memory: 0, processing: 0>
2023-10-15 05:38:09,555 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,555 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,555 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44173
2023-10-15 05:38:09,555 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51512
2023-10-15 05:38:09,556 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:09,557 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:09,558 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,558 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,559 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39983', status: init, memory: 0, processing: 0>
2023-10-15 05:38:09,559 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39983
2023-10-15 05:38:09,559 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51498
2023-10-15 05:38:09,560 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:09,561 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:09,562 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:09,562 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:09,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:09,595 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:09,596 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:09,596 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:09,596 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:09,596 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:09,596 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:09,597 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:09,597 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:09,607 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:38:09,607 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:38:09,607 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:38:09,607 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:38:09,607 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:38:09,608 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:38:09,608 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:38:09,608 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:38:09,613 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:09,615 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:09,617 - distributed.scheduler - INFO - Remove client Client-01c5bcea-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:09,617 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51440; closing.
2023-10-15 05:38:09,618 - distributed.scheduler - INFO - Remove client Client-01c5bcea-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:09,618 - distributed.scheduler - INFO - Close client connection: Client-01c5bcea-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:09,619 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34549'. Reason: nanny-close
2023-10-15 05:38:09,619 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:09,620 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40869'. Reason: nanny-close
2023-10-15 05:38:09,621 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:09,621 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44173. Reason: nanny-close
2023-10-15 05:38:09,621 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37801'. Reason: nanny-close
2023-10-15 05:38:09,621 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:09,622 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39983. Reason: nanny-close
2023-10-15 05:38:09,622 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37971'. Reason: nanny-close
2023-10-15 05:38:09,622 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:09,622 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37381. Reason: nanny-close
2023-10-15 05:38:09,623 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39233'. Reason: nanny-close
2023-10-15 05:38:09,623 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:09,623 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:09,623 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51512; closing.
2023-10-15 05:38:09,623 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34095. Reason: nanny-close
2023-10-15 05:38:09,623 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44173', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348289.6237774')
2023-10-15 05:38:09,623 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36289'. Reason: nanny-close
2023-10-15 05:38:09,624 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:09,624 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:09,624 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39629. Reason: nanny-close
2023-10-15 05:38:09,624 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40523'. Reason: nanny-close
2023-10-15 05:38:09,625 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:09,625 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:09,625 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43389. Reason: nanny-close
2023-10-15 05:38:09,625 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:09,625 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:09,625 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37785'. Reason: nanny-close
2023-10-15 05:38:09,625 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:09,625 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51498; closing.
2023-10-15 05:38:09,626 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:09,626 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41753. Reason: nanny-close
2023-10-15 05:38:09,626 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51446; closing.
2023-10-15 05:38:09,626 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42513. Reason: nanny-close
2023-10-15 05:38:09,627 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39983', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348289.627005')
2023-10-15 05:38:09,627 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:09,627 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:09,627 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51462; closing.
2023-10-15 05:38:09,627 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:09,627 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37381', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348289.627768')
2023-10-15 05:38:09,627 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:09,628 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348289.6281056')
2023-10-15 05:38:09,628 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:09,629 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51478; closing.
2023-10-15 05:38:09,629 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:09,629 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51484; closing.
2023-10-15 05:38:09,629 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:09,629 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:09,630 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:09,630 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39629', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348289.629945')
2023-10-15 05:38:09,630 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43389', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348289.630261')
2023-10-15 05:38:09,630 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51524; closing.
2023-10-15 05:38:09,631 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41753', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348289.6310983')
2023-10-15 05:38:09,631 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51480; closing.
2023-10-15 05:38:09,631 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:09,632 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42513', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348289.6319714')
2023-10-15 05:38:09,632 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:38:09,632 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51480>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-10-15 05:38:11,237 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:38:11,237 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:38:11,238 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:38:11,239 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:38:11,239 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-10-15 05:38:13,561 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:13,565 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44863 instead
  warnings.warn(
2023-10-15 05:38:13,570 - distributed.scheduler - INFO - State start
2023-10-15 05:38:13,593 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:13,594 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:38:13,595 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44863/status
2023-10-15 05:38:13,595 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:38:13,596 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41963'
2023-10-15 05:38:13,623 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38779'
2023-10-15 05:38:13,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36995'
2023-10-15 05:38:13,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33231'
2023-10-15 05:38:13,642 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37187'
2023-10-15 05:38:13,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36649'
2023-10-15 05:38:13,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34109'
2023-10-15 05:38:13,670 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42811'
2023-10-15 05:38:14,215 - distributed.scheduler - INFO - Receive client connection: Client-074f331a-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:14,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48574
2023-10-15 05:38:15,499 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:15,499 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:15,503 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:15,503 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:15,503 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:15,505 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:15,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:15,507 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:15,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:15,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:15,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:15,565 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:15,565 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:15,568 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:15,568 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:15,568 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:15,570 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:15,573 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:15,575 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:15,576 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:15,578 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:15,578 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:15,580 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:15,583 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:18,744 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38379
2023-10-15 05:38:18,744 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35283
2023-10-15 05:38:18,744 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38379
2023-10-15 05:38:18,744 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35283
2023-10-15 05:38:18,745 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43343
2023-10-15 05:38:18,745 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38373
2023-10-15 05:38:18,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,745 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,745 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,745 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:18,745 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:18,745 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:18,745 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:18,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ab5rh4kc
2023-10-15 05:38:18,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_69kfhot
2023-10-15 05:38:18,745 - distributed.worker - INFO - Starting Worker plugin PreImport-8daae425-1c18-472c-aca5-a185253e5fa2
2023-10-15 05:38:18,746 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1a130fe-0653-4b37-8b1f-7835fab3ff1b
2023-10-15 05:38:18,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d5084f88-d4de-4fe0-a4cd-714c5e7d7685
2023-10-15 05:38:18,746 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fecd3901-51e6-41a2-9190-aa2dee8304fd
2023-10-15 05:38:18,745 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35169
2023-10-15 05:38:18,746 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35169
2023-10-15 05:38:18,746 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46765
2023-10-15 05:38:18,746 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,746 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,746 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:18,746 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:18,746 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kky0gzzs
2023-10-15 05:38:18,747 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d725dda1-b6aa-479a-9309-26ebf21d2437
2023-10-15 05:38:18,856 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46037
2023-10-15 05:38:18,857 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46037
2023-10-15 05:38:18,858 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46307
2023-10-15 05:38:18,858 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,858 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,858 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:18,858 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:18,858 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ypnh136c
2023-10-15 05:38:18,860 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f979b13e-97d7-47df-85e7-08fcb55cce60
2023-10-15 05:38:18,859 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35071
2023-10-15 05:38:18,860 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35071
2023-10-15 05:38:18,861 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38759
2023-10-15 05:38:18,861 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,861 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,861 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:18,861 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:18,861 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ryhfr1e
2023-10-15 05:38:18,861 - distributed.worker - INFO - Starting Worker plugin RMMSetup-290914a4-5398-4939-a773-3d899e386d9e
2023-10-15 05:38:18,869 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38771
2023-10-15 05:38:18,870 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38771
2023-10-15 05:38:18,870 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45385
2023-10-15 05:38:18,870 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,870 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,870 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:18,870 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:18,870 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0knd4_3x
2023-10-15 05:38:18,870 - distributed.worker - INFO - Starting Worker plugin RMMSetup-899a11ea-6601-485f-a9df-956789d0b65d
2023-10-15 05:38:18,872 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41567
2023-10-15 05:38:18,873 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41567
2023-10-15 05:38:18,873 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43945
2023-10-15 05:38:18,873 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,873 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,872 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37431
2023-10-15 05:38:18,873 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:18,873 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37431
2023-10-15 05:38:18,873 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:18,873 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35159
2023-10-15 05:38:18,873 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nkbikc5a
2023-10-15 05:38:18,873 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,873 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,874 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:18,874 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:18,874 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tpeclkkv
2023-10-15 05:38:18,874 - distributed.worker - INFO - Starting Worker plugin RMMSetup-30744dad-c712-43e0-8195-c107ea15795e
2023-10-15 05:38:18,874 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d9b501d-cab2-49f2-9e34-6a8f8ddda7f5
2023-10-15 05:38:18,942 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,953 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1ad292e-bf37-4f59-b21d-f14774dccfa6
2023-10-15 05:38:18,953 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69889a64-cd17-4b4d-a37e-17de4c3a5848
2023-10-15 05:38:18,953 - distributed.worker - INFO - Starting Worker plugin PreImport-1d6374b7-be6a-4970-984d-40f65a90db00
2023-10-15 05:38:18,953 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,953 - distributed.worker - INFO - Starting Worker plugin PreImport-6023f7e5-e251-46e1-80f9-39a676662e50
2023-10-15 05:38:18,954 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,972 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38379', status: init, memory: 0, processing: 0>
2023-10-15 05:38:18,973 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38379
2023-10-15 05:38:18,974 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48606
2023-10-15 05:38:18,975 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:18,976 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,976 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,977 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:18,982 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35283', status: init, memory: 0, processing: 0>
2023-10-15 05:38:18,984 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35283
2023-10-15 05:38:18,984 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48616
2023-10-15 05:38:18,985 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:18,986 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,986 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:18,994 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35169', status: init, memory: 0, processing: 0>
2023-10-15 05:38:18,995 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35169
2023-10-15 05:38:18,995 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48624
2023-10-15 05:38:18,996 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:18,997 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e4bf01e9-f3c4-454c-b17c-4bdde5799d1c
2023-10-15 05:38:18,997 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:18,997 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:18,998 - distributed.worker - INFO - Starting Worker plugin PreImport-127fa1bc-00d6-492f-b125-51cdffcff534
2023-10-15 05:38:18,998 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:19,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:19,009 - distributed.worker - INFO - Starting Worker plugin PreImport-067aeb75-2845-4b39-98b1-83f8c80d16a1
2023-10-15 05:38:19,009 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f75b29c1-4ba0-4311-82b6-eec01a7aea2b
2023-10-15 05:38:19,009 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4b98190-fa88-4a20-8522-797d41c639cb
2023-10-15 05:38:19,009 - distributed.worker - INFO - Starting Worker plugin PreImport-3daf9c05-eced-4daa-97a8-9f932160deec
2023-10-15 05:38:19,009 - distributed.worker - INFO - Starting Worker plugin PreImport-6536597b-08ee-4c88-8e6c-bb6ce8145b37
2023-10-15 05:38:19,009 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88e86115-15db-47fc-8c91-95b23c727f8e
2023-10-15 05:38:19,010 - distributed.worker - INFO - Starting Worker plugin PreImport-df2e6fe9-8e1c-4332-a4e9-82dad87e820a
2023-10-15 05:38:19,010 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a744fb8-c558-4fa4-9a2a-fdd93ddf6a68
2023-10-15 05:38:19,010 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:19,010 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:19,010 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:19,010 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:19,023 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46037', status: init, memory: 0, processing: 0>
2023-10-15 05:38:19,024 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46037
2023-10-15 05:38:19,025 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48640
2023-10-15 05:38:19,026 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:19,026 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:19,027 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:19,028 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:19,032 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37431', status: init, memory: 0, processing: 0>
2023-10-15 05:38:19,033 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37431
2023-10-15 05:38:19,033 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48650
2023-10-15 05:38:19,034 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:19,035 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:19,035 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:19,037 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:19,050 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41567', status: init, memory: 0, processing: 0>
2023-10-15 05:38:19,050 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41567
2023-10-15 05:38:19,050 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48680
2023-10-15 05:38:19,051 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35071', status: init, memory: 0, processing: 0>
2023-10-15 05:38:19,051 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35071
2023-10-15 05:38:19,052 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48664
2023-10-15 05:38:19,052 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:19,052 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38771', status: init, memory: 0, processing: 0>
2023-10-15 05:38:19,053 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38771
2023-10-15 05:38:19,053 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48652
2023-10-15 05:38:19,053 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:19,053 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:19,053 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:19,054 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:19,054 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:19,054 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:19,055 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:19,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:19,055 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:19,056 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:19,057 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:19,073 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:19,073 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:19,073 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:19,073 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:19,073 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:19,074 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:19,074 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:19,074 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:19,079 - distributed.scheduler - INFO - Remove client Client-074f331a-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:19,079 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48574; closing.
2023-10-15 05:38:19,080 - distributed.scheduler - INFO - Remove client Client-074f331a-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:19,080 - distributed.scheduler - INFO - Close client connection: Client-074f331a-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:19,081 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41963'. Reason: nanny-close
2023-10-15 05:38:19,082 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:19,083 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34109'. Reason: nanny-close
2023-10-15 05:38:19,083 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:19,083 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41567. Reason: nanny-close
2023-10-15 05:38:19,083 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36995'. Reason: nanny-close
2023-10-15 05:38:19,084 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:19,084 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35071. Reason: nanny-close
2023-10-15 05:38:19,084 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42811'. Reason: nanny-close
2023-10-15 05:38:19,084 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:19,084 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35283. Reason: nanny-close
2023-10-15 05:38:19,085 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36649'. Reason: nanny-close
2023-10-15 05:38:19,085 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:19,085 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38379. Reason: nanny-close
2023-10-15 05:38:19,085 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38779'. Reason: nanny-close
2023-10-15 05:38:19,085 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:19,086 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:19,086 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38771. Reason: nanny-close
2023-10-15 05:38:19,086 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48680; closing.
2023-10-15 05:38:19,086 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37187'. Reason: nanny-close
2023-10-15 05:38:19,086 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:19,086 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41567', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348299.0864625')
2023-10-15 05:38:19,086 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:19,086 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35169. Reason: nanny-close
2023-10-15 05:38:19,086 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33231'. Reason: nanny-close
2023-10-15 05:38:19,086 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:19,087 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:19,087 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46037. Reason: nanny-close
2023-10-15 05:38:19,087 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:19,087 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37431. Reason: nanny-close
2023-10-15 05:38:19,087 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:19,088 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:19,088 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:19,088 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48606; closing.
2023-10-15 05:38:19,088 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:19,088 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:19,088 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48616; closing.
2023-10-15 05:38:19,089 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48664; closing.
2023-10-15 05:38:19,089 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:19,089 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:19,090 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38379', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348299.09009')
2023-10-15 05:38:19,090 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:19,090 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:19,090 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35283', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348299.090491')
2023-10-15 05:38:19,090 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348299.0908554')
2023-10-15 05:38:19,091 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:19,091 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:19,091 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:19,091 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48652; closing.
2023-10-15 05:38:19,092 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38771', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348299.0929124')
2023-10-15 05:38:19,093 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48624; closing.
2023-10-15 05:38:19,093 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48640; closing.
2023-10-15 05:38:19,093 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48650; closing.
2023-10-15 05:38:19,094 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35169', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348299.0941849')
2023-10-15 05:38:19,094 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46037', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348299.0945973')
2023-10-15 05:38:19,095 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37431', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348299.09515')
2023-10-15 05:38:19,095 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:38:20,599 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:38:20,599 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:38:20,600 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:38:20,601 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:38:20,601 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-10-15 05:38:22,866 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:22,870 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-15 05:38:22,873 - distributed.scheduler - INFO - State start
2023-10-15 05:38:22,894 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:22,895 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:38:22,896 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-15 05:38:22,896 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:38:22,940 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38477'
2023-10-15 05:38:23,738 - distributed.scheduler - INFO - Receive client connection: Client-0ce83388-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:23,760 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38346
2023-10-15 05:38:24,622 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:24,623 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:25,158 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:26,004 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44067
2023-10-15 05:38:26,004 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44067
2023-10-15 05:38:26,004 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-10-15 05:38:26,004 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:26,005 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:26,005 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:26,005 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-15 05:38:26,005 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z1quvr19
2023-10-15 05:38:26,005 - distributed.worker - INFO - Starting Worker plugin PreImport-922f4e3e-d56e-43a1-a6a6-70879b467526
2023-10-15 05:38:26,005 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c42db4d4-ce12-4822-b558-acc7ab019fd2
2023-10-15 05:38:26,006 - distributed.worker - INFO - Starting Worker plugin RMMSetup-66bdef6d-0043-4206-823e-27c733bbb125
2023-10-15 05:38:26,006 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:26,048 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44067', status: init, memory: 0, processing: 0>
2023-10-15 05:38:26,050 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44067
2023-10-15 05:38:26,050 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38366
2023-10-15 05:38:26,052 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:26,053 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:26,053 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:26,056 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:26,079 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:26,082 - distributed.scheduler - INFO - Remove client Client-0ce83388-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:26,082 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38346; closing.
2023-10-15 05:38:26,082 - distributed.scheduler - INFO - Remove client Client-0ce83388-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:26,083 - distributed.scheduler - INFO - Close client connection: Client-0ce83388-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:26,083 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38477'. Reason: nanny-close
2023-10-15 05:38:26,084 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:26,085 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44067. Reason: nanny-close
2023-10-15 05:38:26,088 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38366; closing.
2023-10-15 05:38:26,088 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:26,088 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44067', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348306.0885942')
2023-10-15 05:38:26,088 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:38:26,090 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:27,401 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:38:27,401 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:38:27,402 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:38:27,403 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:38:27,403 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-10-15 05:38:31,884 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:31,888 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-15 05:38:31,892 - distributed.scheduler - INFO - State start
2023-10-15 05:38:32,077 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:32,078 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:38:32,079 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-15 05:38:32,079 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:38:32,165 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33065'
2023-10-15 05:38:33,043 - distributed.scheduler - INFO - Receive client connection: Client-1250d2ab-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:33,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34690
2023-10-15 05:38:34,021 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:34,021 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:34,619 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:35,500 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37327
2023-10-15 05:38:35,500 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37327
2023-10-15 05:38:35,500 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40551
2023-10-15 05:38:35,501 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:35,501 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:35,501 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:35,501 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-15 05:38:35,501 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ztxab2vn
2023-10-15 05:38:35,501 - distributed.worker - INFO - Starting Worker plugin PreImport-b147faf6-394e-4d73-bad0-811540d022f4
2023-10-15 05:38:35,503 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-905fd700-e525-4206-8d81-0bf13765015b
2023-10-15 05:38:35,503 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1e1c33a-0606-4337-afe7-a12711b12eb2
2023-10-15 05:38:35,503 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:35,529 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37327', status: init, memory: 0, processing: 0>
2023-10-15 05:38:35,530 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37327
2023-10-15 05:38:35,530 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34700
2023-10-15 05:38:35,531 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:35,532 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:35,532 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:35,533 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:35,606 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:35,609 - distributed.scheduler - INFO - Remove client Client-1250d2ab-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:35,610 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34690; closing.
2023-10-15 05:38:35,610 - distributed.scheduler - INFO - Remove client Client-1250d2ab-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:35,611 - distributed.scheduler - INFO - Close client connection: Client-1250d2ab-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:35,611 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33065'. Reason: nanny-close
2023-10-15 05:38:35,612 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:35,613 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37327. Reason: nanny-close
2023-10-15 05:38:35,615 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:35,615 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34700; closing.
2023-10-15 05:38:35,616 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348315.6162062')
2023-10-15 05:38:35,616 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:38:35,617 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:36,928 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:38:36,929 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:38:36,929 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:38:36,930 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:38:36,930 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-10-15 05:38:39,188 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:39,193 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-10-15 05:38:39,197 - distributed.scheduler - INFO - State start
2023-10-15 05:38:39,229 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:39,230 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:38:39,231 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-10-15 05:38:39,231 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:38:43,487 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:34710'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:34710>: Stream is closed
2023-10-15 05:38:43,716 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:38:43,716 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:38:43,717 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:38:43,717 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:38:43,718 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-10-15 05:38:45,611 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:45,615 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42681 instead
  warnings.warn(
2023-10-15 05:38:45,618 - distributed.scheduler - INFO - State start
2023-10-15 05:38:45,639 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:45,640 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-10-15 05:38:45,641 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42681/status
2023-10-15 05:38:45,641 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:38:45,810 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41171'
2023-10-15 05:38:47,318 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:47,319 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:47,323 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:47,421 - distributed.scheduler - INFO - Receive client connection: Client-1aa2b686-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:47,433 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55816
2023-10-15 05:38:48,273 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43615
2023-10-15 05:38:48,274 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43615
2023-10-15 05:38:48,274 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46241
2023-10-15 05:38:48,274 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-10-15 05:38:48,274 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:48,274 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:48,274 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-15 05:38:48,274 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-jt7s6j7h
2023-10-15 05:38:48,275 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ef321b4c-2565-43ca-9c1a-c1ad0d3b2f34
2023-10-15 05:38:48,275 - distributed.worker - INFO - Starting Worker plugin PreImport-d5cb5f0e-8a3c-4262-9c89-dbac1eaa3bbc
2023-10-15 05:38:48,275 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e832fcf9-dab1-4fcc-a166-b756312a8587
2023-10-15 05:38:48,275 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:48,296 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43615', status: init, memory: 0, processing: 0>
2023-10-15 05:38:48,297 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43615
2023-10-15 05:38:48,297 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55840
2023-10-15 05:38:48,298 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:48,299 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-10-15 05:38:48,299 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:48,301 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-10-15 05:38:48,354 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:48,356 - distributed.scheduler - INFO - Remove client Client-1aa2b686-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:48,356 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55816; closing.
2023-10-15 05:38:48,357 - distributed.scheduler - INFO - Remove client Client-1aa2b686-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:48,357 - distributed.scheduler - INFO - Close client connection: Client-1aa2b686-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:48,358 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41171'. Reason: nanny-close
2023-10-15 05:38:48,358 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:48,360 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43615. Reason: nanny-close
2023-10-15 05:38:48,362 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-10-15 05:38:48,362 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55840; closing.
2023-10-15 05:38:48,362 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43615', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348328.362496')
2023-10-15 05:38:48,362 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:38:48,363 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:49,524 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:38:49,524 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:38:49,525 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:38:49,526 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-10-15 05:38:49,526 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-10-15 05:38:51,477 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:51,481 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41449 instead
  warnings.warn(
2023-10-15 05:38:51,485 - distributed.scheduler - INFO - State start
2023-10-15 05:38:51,508 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:38:51,509 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:38:51,510 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41449/status
2023-10-15 05:38:51,510 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:38:51,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33515'
2023-10-15 05:38:51,741 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42117'
2023-10-15 05:38:51,749 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43405'
2023-10-15 05:38:51,765 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46075'
2023-10-15 05:38:51,766 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35609'
2023-10-15 05:38:51,774 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35341'
2023-10-15 05:38:51,783 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37213'
2023-10-15 05:38:51,794 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41561'
2023-10-15 05:38:53,479 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:53,479 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:53,483 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:53,545 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:53,546 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:53,550 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:53,795 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:53,795 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:53,796 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:53,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:53,799 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:53,801 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:53,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:53,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:53,801 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:53,803 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:53,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:53,804 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:53,805 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:38:53,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:38:53,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:53,807 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:53,809 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:53,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:38:55,593 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34941
2023-10-15 05:38:55,594 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34941
2023-10-15 05:38:55,594 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32791
2023-10-15 05:38:55,594 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:55,594 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:55,594 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:55,595 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:55,595 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9jesr5g0
2023-10-15 05:38:55,595 - distributed.worker - INFO - Starting Worker plugin PreImport-030bd29a-b3e9-480a-8767-39ed64a9af1e
2023-10-15 05:38:55,595 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3fce4014-67d6-47bd-9f66-d4dfaf8f7210
2023-10-15 05:38:55,597 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7aa4b3d-3e86-4fc1-ab6a-d1ef0185344a
2023-10-15 05:38:55,867 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:55,902 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34941', status: init, memory: 0, processing: 0>
2023-10-15 05:38:55,913 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34941
2023-10-15 05:38:55,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49914
2023-10-15 05:38:55,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:55,915 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:55,915 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:55,917 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:56,387 - distributed.scheduler - INFO - Receive client connection: Client-1e1c377f-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:56,388 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49926
2023-10-15 05:38:56,652 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41643
2023-10-15 05:38:56,653 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41643
2023-10-15 05:38:56,654 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37601
2023-10-15 05:38:56,654 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:56,654 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:56,654 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:56,654 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:56,654 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-341ejsjf
2023-10-15 05:38:56,655 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bc86d794-076d-490f-9b52-72d5613031ab
2023-10-15 05:38:56,757 - distributed.worker - INFO - Starting Worker plugin PreImport-dbf12940-3eb9-4f00-8ae7-5bea0982c0b1
2023-10-15 05:38:56,758 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce525e5a-e944-45b0-bf1d-f1412299d9b6
2023-10-15 05:38:56,758 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:56,791 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41643', status: init, memory: 0, processing: 0>
2023-10-15 05:38:56,791 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41643
2023-10-15 05:38:56,791 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49936
2023-10-15 05:38:56,792 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:56,793 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:56,793 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:56,795 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:56,902 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44821
2023-10-15 05:38:56,902 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44821
2023-10-15 05:38:56,902 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36361
2023-10-15 05:38:56,903 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:56,903 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:56,903 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:56,903 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:56,903 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-adaghhta
2023-10-15 05:38:56,903 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d2a3e895-6a20-4e06-80ca-e81da5aa99ab
2023-10-15 05:38:56,904 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35137
2023-10-15 05:38:56,905 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35137
2023-10-15 05:38:56,905 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42371
2023-10-15 05:38:56,905 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:56,905 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:56,905 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:56,905 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:56,905 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8k6mhuwm
2023-10-15 05:38:56,905 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45893
2023-10-15 05:38:56,906 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45893
2023-10-15 05:38:56,906 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17eed0ed-a019-48be-8ccd-891b702d29b9
2023-10-15 05:38:56,906 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41483
2023-10-15 05:38:56,906 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:56,906 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:56,906 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:56,906 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:56,906 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oluj8180
2023-10-15 05:38:56,907 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a41d6652-3d59-4e31-a9fe-b616e2855f7f
2023-10-15 05:38:56,914 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34375
2023-10-15 05:38:56,915 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34375
2023-10-15 05:38:56,915 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41969
2023-10-15 05:38:56,915 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:56,915 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:56,915 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:56,915 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:56,915 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u6mle85m
2023-10-15 05:38:56,916 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a566000a-1ebf-43e9-a3c2-7c82cb98f0a1
2023-10-15 05:38:56,916 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40349
2023-10-15 05:38:56,916 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40349
2023-10-15 05:38:56,916 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34935
2023-10-15 05:38:56,916 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:56,917 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:56,917 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:56,917 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:56,917 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b7mg_51u
2023-10-15 05:38:56,917 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d9ed4144-d7ee-4933-990f-5869a203ad6d
2023-10-15 05:38:56,923 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45721
2023-10-15 05:38:56,924 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45721
2023-10-15 05:38:56,924 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38199
2023-10-15 05:38:56,924 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:38:56,924 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:56,924 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:38:56,924 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-10-15 05:38:56,924 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5dzon00a
2023-10-15 05:38:56,925 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1fda644e-f537-47b9-913a-9ddc7847944c
2023-10-15 05:38:57,085 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e96b8f53-fe51-42a7-a19b-3abfdd6ff9ec
2023-10-15 05:38:57,086 - distributed.worker - INFO - Starting Worker plugin PreImport-75b2b039-dcbe-4562-933f-a0a3402d4dc2
2023-10-15 05:38:57,086 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,099 - distributed.worker - INFO - Starting Worker plugin PreImport-5d6e0ef7-a2a9-4291-b174-3f8613caa277
2023-10-15 05:38:57,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3ef871a4-0a93-434a-9d20-ea93229be8ec
2023-10-15 05:38:57,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb3c206f-611d-4405-9968-e7de789e8b12
2023-10-15 05:38:57,099 - distributed.worker - INFO - Starting Worker plugin PreImport-2a056af6-0916-4abb-b69b-8ca304271597
2023-10-15 05:38:57,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c994f9e8-a004-4f2e-83bd-35f6d5650533
2023-10-15 05:38:57,100 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,100 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,100 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e83d6579-6197-4250-b541-59a3c3bd1e3b
2023-10-15 05:38:57,100 - distributed.worker - INFO - Starting Worker plugin PreImport-dcadf9f0-c589-44c3-8e96-db9572bd7423
2023-10-15 05:38:57,100 - distributed.worker - INFO - Starting Worker plugin PreImport-c06e0c65-3830-4fdd-b29d-5c84043bad3e
2023-10-15 05:38:57,100 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9758b115-4fd6-47db-8e91-36fd20e4b11f
2023-10-15 05:38:57,100 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,101 - distributed.worker - INFO - Starting Worker plugin PreImport-6ea36564-ce06-4625-99ce-b1da13fd25da
2023-10-15 05:38:57,101 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,103 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,114 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40349', status: init, memory: 0, processing: 0>
2023-10-15 05:38:57,115 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40349
2023-10-15 05:38:57,115 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49960
2023-10-15 05:38:57,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:57,117 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:57,117 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,119 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:57,128 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45721', status: init, memory: 0, processing: 0>
2023-10-15 05:38:57,128 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45721
2023-10-15 05:38:57,128 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49980
2023-10-15 05:38:57,129 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35137', status: init, memory: 0, processing: 0>
2023-10-15 05:38:57,129 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:57,130 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35137
2023-10-15 05:38:57,130 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49988
2023-10-15 05:38:57,130 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:57,130 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,131 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34375', status: init, memory: 0, processing: 0>
2023-10-15 05:38:57,131 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:57,131 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34375
2023-10-15 05:38:57,131 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49976
2023-10-15 05:38:57,132 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:57,132 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:57,132 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,132 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:57,133 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:57,133 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,133 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:57,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:57,136 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45893', status: init, memory: 0, processing: 0>
2023-10-15 05:38:57,137 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45893
2023-10-15 05:38:57,137 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50004
2023-10-15 05:38:57,138 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:57,139 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:57,139 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,139 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44821', status: init, memory: 0, processing: 0>
2023-10-15 05:38:57,140 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44821
2023-10-15 05:38:57,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50012
2023-10-15 05:38:57,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:57,141 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:38:57,142 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:38:57,142 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:38:57,144 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:38:57,213 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:57,213 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:57,213 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:57,214 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:57,214 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:57,214 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:57,214 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:57,214 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-10-15 05:38:57,225 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:57,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:57,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:57,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:57,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:57,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:57,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:57,226 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:38:57,230 - distributed.scheduler - INFO - Remove client Client-1e1c377f-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:57,230 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49926; closing.
2023-10-15 05:38:57,230 - distributed.scheduler - INFO - Remove client Client-1e1c377f-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:57,231 - distributed.scheduler - INFO - Close client connection: Client-1e1c377f-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:38:57,232 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33515'. Reason: nanny-close
2023-10-15 05:38:57,232 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:57,233 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42117'. Reason: nanny-close
2023-10-15 05:38:57,233 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:57,233 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41643. Reason: nanny-close
2023-10-15 05:38:57,233 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43405'. Reason: nanny-close
2023-10-15 05:38:57,233 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:57,234 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44821. Reason: nanny-close
2023-10-15 05:38:57,234 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46075'. Reason: nanny-close
2023-10-15 05:38:57,234 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:57,234 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34375. Reason: nanny-close
2023-10-15 05:38:57,234 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35609'. Reason: nanny-close
2023-10-15 05:38:57,235 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:57,235 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35341'. Reason: nanny-close
2023-10-15 05:38:57,235 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34941. Reason: nanny-close
2023-10-15 05:38:57,235 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:57,236 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40349. Reason: nanny-close
2023-10-15 05:38:57,236 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37213'. Reason: nanny-close
2023-10-15 05:38:57,236 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:57,236 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49936; closing.
2023-10-15 05:38:57,236 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:57,236 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41561'. Reason: nanny-close
2023-10-15 05:38:57,236 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:57,236 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41643', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348337.2368112')
2023-10-15 05:38:57,236 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45893. Reason: nanny-close
2023-10-15 05:38:57,236 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:38:57,237 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35137. Reason: nanny-close
2023-10-15 05:38:57,237 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:57,237 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50012; closing.
2023-10-15 05:38:57,237 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45721. Reason: nanny-close
2023-10-15 05:38:57,238 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:57,239 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:57,239 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44821', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348337.2389703')
2023-10-15 05:38:57,239 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:57,239 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:57,239 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:57,239 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49976; closing.
2023-10-15 05:38:57,239 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:57,240 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:57,240 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:38:57,240 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:57,240 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49914; closing.
2023-10-15 05:38:57,241 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34375', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348337.2410145')
2023-10-15 05:38:57,241 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:57,241 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49960; closing.
2023-10-15 05:38:57,241 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:57,242 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34941', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348337.242122')
2023-10-15 05:38:57,242 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:57,242 - distributed.nanny - INFO - Worker closed
2023-10-15 05:38:57,242 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348337.2425764')
2023-10-15 05:38:57,242 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50004; closing.
2023-10-15 05:38:57,243 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49988; closing.
2023-10-15 05:38:57,243 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45893', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348337.243809')
2023-10-15 05:38:57,244 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35137', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348337.2442098')
2023-10-15 05:38:57,244 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49980; closing.
2023-10-15 05:38:57,245 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45721', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348337.2450712')
2023-10-15 05:38:57,245 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:38:58,899 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:38:58,899 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:38:58,900 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:38:58,901 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:38:58,901 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-10-15 05:39:01,030 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:39:01,034 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43967 instead
  warnings.warn(
2023-10-15 05:39:01,037 - distributed.scheduler - INFO - State start
2023-10-15 05:39:01,058 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:39:01,059 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:39:01,059 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43967/status
2023-10-15 05:39:01,060 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:39:01,249 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46841'
2023-10-15 05:39:01,633 - distributed.scheduler - INFO - Receive client connection: Client-23c13550-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:39:01,648 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46982
2023-10-15 05:39:02,810 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:39:02,811 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:39:02,815 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:39:03,786 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38587
2023-10-15 05:39:03,787 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38587
2023-10-15 05:39:03,787 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38847
2023-10-15 05:39:03,787 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:39:03,787 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:39:03,787 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:39:03,787 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-15 05:39:03,787 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p0dou9k7
2023-10-15 05:39:03,788 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b1357688-f5ab-40ec-9712-c96e1266da61
2023-10-15 05:39:03,876 - distributed.worker - INFO - Starting Worker plugin PreImport-472e479f-17d6-4fba-8c6d-d300bb86c040
2023-10-15 05:39:03,876 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-534b3465-4805-461d-981c-cb2140ea0a2c
2023-10-15 05:39:03,877 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:39:03,911 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38587', status: init, memory: 0, processing: 0>
2023-10-15 05:39:03,913 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38587
2023-10-15 05:39:03,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47000
2023-10-15 05:39:03,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:39:03,915 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:39:03,915 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:39:03,917 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:39:03,996 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:39:03,999 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:39:04,001 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:39:04,003 - distributed.scheduler - INFO - Remove client Client-23c13550-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:39:04,003 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46982; closing.
2023-10-15 05:39:04,004 - distributed.scheduler - INFO - Remove client Client-23c13550-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:39:04,004 - distributed.scheduler - INFO - Close client connection: Client-23c13550-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:39:04,005 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46841'. Reason: nanny-close
2023-10-15 05:39:04,005 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:39:04,007 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38587. Reason: nanny-close
2023-10-15 05:39:04,008 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47000; closing.
2023-10-15 05:39:04,009 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:39:04,009 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348344.0092669')
2023-10-15 05:39:04,009 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:39:04,010 - distributed.nanny - INFO - Worker closed
2023-10-15 05:39:05,122 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:39:05,122 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:39:05,123 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:39:05,123 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:39:05,124 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-10-15 05:39:07,205 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:39:07,210 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40257 instead
  warnings.warn(
2023-10-15 05:39:07,214 - distributed.scheduler - INFO - State start
2023-10-15 05:39:07,235 - distributed.scheduler - INFO - -----------------------------------------------
2023-10-15 05:39:07,236 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-10-15 05:39:07,237 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40257/status
2023-10-15 05:39:07,237 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-10-15 05:39:07,332 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39767'
2023-10-15 05:39:08,948 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-10-15 05:39:08,948 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-10-15 05:39:08,952 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-10-15 05:39:09,756 - distributed.scheduler - INFO - Receive client connection: Client-27728e34-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:39:09,766 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47134
2023-10-15 05:39:09,774 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46229
2023-10-15 05:39:09,774 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46229
2023-10-15 05:39:09,774 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38397
2023-10-15 05:39:09,774 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-10-15 05:39:09,775 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:39:09,775 - distributed.worker - INFO -               Threads:                          1
2023-10-15 05:39:09,775 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-10-15 05:39:09,775 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tkllbseg
2023-10-15 05:39:09,775 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ad4f077-faa0-4295-afa5-e59968ff8a56
2023-10-15 05:39:09,875 - distributed.worker - INFO - Starting Worker plugin PreImport-dd0f9dd1-546b-4b21-8b46-db0ec63de9f9
2023-10-15 05:39:09,875 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-610463c4-70d3-43d6-b3f7-02bbd3055208
2023-10-15 05:39:09,876 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:39:09,912 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46229', status: init, memory: 0, processing: 0>
2023-10-15 05:39:09,913 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46229
2023-10-15 05:39:09,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47156
2023-10-15 05:39:09,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-10-15 05:39:09,915 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-10-15 05:39:09,915 - distributed.worker - INFO - -------------------------------------------------
2023-10-15 05:39:09,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-10-15 05:39:09,977 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-10-15 05:39:09,982 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-10-15 05:39:09,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:39:09,987 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-10-15 05:39:09,990 - distributed.scheduler - INFO - Remove client Client-27728e34-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:39:09,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47134; closing.
2023-10-15 05:39:09,990 - distributed.scheduler - INFO - Remove client Client-27728e34-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:39:09,990 - distributed.scheduler - INFO - Close client connection: Client-27728e34-6b1d-11ee-b11b-d8c49764f6bb
2023-10-15 05:39:09,991 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39767'. Reason: nanny-close
2023-10-15 05:39:09,992 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-10-15 05:39:09,993 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46229. Reason: nanny-close
2023-10-15 05:39:09,995 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47156; closing.
2023-10-15 05:39:09,995 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-10-15 05:39:09,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46229', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1697348349.9956286')
2023-10-15 05:39:09,995 - distributed.scheduler - INFO - Lost all workers
2023-10-15 05:39:09,997 - distributed.nanny - INFO - Worker closed
2023-10-15 05:39:11,007 - distributed._signals - INFO - Received signal SIGINT (2)
2023-10-15 05:39:11,007 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-10-15 05:39:11,008 - distributed.scheduler - INFO - Scheduler closing all comms
2023-10-15 05:39:11,009 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-10-15 05:39:11,009 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43461 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46645 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43383 instead
  warnings.warn(
2023-10-15 05:40:07,528 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-15 05:40:07,550 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://10.33.225.163:44991', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35061 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35385 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41069 instead
  warnings.warn(
2023-10-15 05:40:50,799 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-15 05:40:50,802 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-15 05:40:50,802 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2023-10-15 05:40:50,802 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1516, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1920, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1626, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1628, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
Task exception was never retrieved
future: <Task finished name='Task-1394' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1395' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1393' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-1392' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35605 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39699 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41647 instead
  warnings.warn(
[1697348479.259167] [dgx13:67187:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:41451) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39773 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46661 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33319 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33819 instead
  warnings.warn(
2023-10-15 05:42:21,160 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-15 05:42:21,161 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:58806 remote=tcp://127.0.0.1:35815>: Stream is closed
2023-10-15 05:42:21,161 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1347, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1106, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:58812 remote=tcp://127.0.0.1:35815>: Stream is closed
2023-10-15 05:42:21,164 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'tcp://127.0.0.1:40457', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43377 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42521 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38549 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42693 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45371 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46373 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41781 instead
  warnings.warn(
[1697348658.558533] [dgx13:70973:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:45412) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40169 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38919 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43953 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34773 instead
  warnings.warn(
2023-10-15 05:45:50,251 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-dd1602bb-6609-41db-8ecc-fd35a4cfe1f0
Function:  _run_coroutine_on_worker
args:      (75703017827472276591946667550391813204, <function shuffle_task at 0x7f9c242d2430>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set()}, {0: {0}, 1: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-15 05:45:50,267 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-f694975b-fc99-42a4-b0a3-886908c2d4ca
Function:  _run_coroutine_on_worker
args:      (75703017827472276591946667550391813204, <function shuffle_task at 0x7f0e69953820>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set()}, {0: {0}, 1: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

Process SpawnProcess-25:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 146, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 342, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 372, in shuffle_task
    no_comm_postprocess = get_no_comm_postprocess(stage, num_rounds, batchsize, proxify)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 76, in get_no_comm_postprocess
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35703 instead
  warnings.warn(
2023-10-15 05:46:00,215 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-f0962b14-ee18-4774-b060-925db1289bcc
Function:  _run_coroutine_on_worker
args:      (155569062030531810650299296177399713392, <function shuffle_task at 0x7ff5852d15e0>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set(), 2: set()}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-15 05:46:00,222 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-898b8929-4a88-47ec-a2bb-852d71b73b15
Function:  _run_coroutine_on_worker
args:      (155569062030531810650299296177399713392, <function shuffle_task at 0x7f7d0c31b670>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set(), 2: set()}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-15 05:46:00,267 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-6370a92c-9d34-43ba-8a42-85cbd869693d
Function:  _run_coroutine_on_worker
args:      (155569062030531810650299296177399713392, <function shuffle_task at 0x7f971d6c4430>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set(), 2: set()}, {0: {0}, 1: set(), 2: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

Process SpawnProcess-26:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 146, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 342, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 372, in shuffle_task
    no_comm_postprocess = get_no_comm_postprocess(stage, num_rounds, batchsize, proxify)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 76, in get_no_comm_postprocess
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 98, in _test_dataframe_shuffle
    cudf = pytest.importorskip("cudf")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/outcomes.py", line 292, in importorskip
    __import__(modname)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] Process SpawnProcess-28:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 98, in _test_dataframe_shuffle
    cudf = pytest.importorskip("cudf")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/outcomes.py", line 292, in importorskip
    __import__(modname)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] Process SpawnProcess-29:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 98, in _test_dataframe_shuffle
    cudf = pytest.importorskip("cudf")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/_pytest/outcomes.py", line 292, in importorskip
    __import__(modname)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38467 instead
  warnings.warn(
2023-10-15 05:46:24,649 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-d73e8ab7-9abc-4fcb-a04b-5709516e19f4
Function:  _run_coroutine_on_worker
args:      (196073023245175474099218897782563309800, <function shuffle_task at 0x7fafcd9d1a60>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}}, {0: {0}}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

Process SpawnProcess-30:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 146, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 342, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 372, in shuffle_task
    no_comm_postprocess = get_no_comm_postprocess(stage, num_rounds, batchsize, proxify)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 76, in get_no_comm_postprocess
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33765 instead
  warnings.warn(
2023-10-15 05:46:38,258 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-efc12129-0dc5-4967-b419-dca1c46a6318
Function:  _run_coroutine_on_worker
args:      (50521955110825593688489726466349765443, <function shuffle_task at 0x7f25de845700>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set()}, {0: {0}, 1: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-15 05:46:38,271 - distributed.worker - WARNING - Compute Failed
Key:       _run_coroutine_on_worker-6f57ec5d-614a-41b6-b9c9-1400105ecbb7
Function:  _run_coroutine_on_worker
args:      (50521955110825593688489726466349765443, <function shuffle_task at 0x7f56d855a310>, ('explicit-comms-shuffle-0291ace6a7b56bee3e00a5626eca6654', {0: {('explicit-comms-shuffle-9d873a5027d9898eaccb97417f61727f', 0)}, 1: set()}, {0: {0}, 1: set()}, ['key'], 1, False, 1, 2))
kwargs:    {}
Exception: "CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')"

2023-10-15 05:46:38,371 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2023-10-15 05:46:38,372 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:47674', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1253, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 454, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 433, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1344, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1543, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
Process SpawnProcess-31:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 146, in _test_dataframe_shuffle
    result = ddf.map_partitions(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 342, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 628, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 101, in _run_coroutine_on_worker
    return executor.submit(_run).result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/comms.py", line 98, in _run
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 372, in shuffle_task
    no_comm_postprocess = get_no_comm_postprocess(stage, num_rounds, batchsize, proxify)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/explicit_comms/dataframe/shuffle.py", line 76, in get_no_comm_postprocess
    import cudf
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35249 instead
  warnings.warn(
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f09edb74180, tag: 0x6ab3d999e9f37a87>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f09edb74180, tag: 0x6ab3d999e9f37a87>: 
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-4358' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44039 instead
  warnings.warn(
2023-10-15 05:47:07,815 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-10-15 05:47:08,001 - distributed.core - ERROR - [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-10-15 05:47:08,036 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'ucx://127.0.0.1:60068'.
2023-10-15 05:47:08,037 - distributed.worker - ERROR - Scheduler was unaware of this worker 'ucx://127.0.0.1:60068'. Shutting down.
2023-10-15 05:47:08,064 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f5e1e9017c0>>, <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-16' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 803, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 407, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 26, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 65, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 34, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-10-15 05:47:10,068 - distributed.nanny - ERROR - Worker process died unexpectedly
