============================= test session starts ==============================
platform linux -- Python 3.9.16, pytest-7.3.1, pluggy-1.0.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1181 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-06-03 05:40:02,375 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:02,379 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-03 05:40:02,383 - distributed.scheduler - INFO - State start
2023-06-03 05:40:02,411 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:02,412 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-03 05:40:02,413 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-03 05:40:02,634 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35123'
2023-06-03 05:40:02,650 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41395'
2023-06-03 05:40:02,653 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39733'
2023-06-03 05:40:02,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41317'
2023-06-03 05:40:02,813 - distributed.scheduler - INFO - Receive client connection: Client-15149a83-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:02,825 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36346
2023-06-03 05:40:04,204 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:04,204 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:04,208 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:04,208 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:04,208 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:04,208 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:04,211 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:04,211 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:04,211 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:04,215 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:04,215 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:04,218 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-06-03 05:40:04,328 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34807
2023-06-03 05:40:04,328 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34807
2023-06-03 05:40:04,329 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36339
2023-06-03 05:40:04,329 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-03 05:40:04,329 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:04,329 - distributed.worker - INFO -               Threads:                          4
2023-06-03 05:40:04,329 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-03 05:40:04,329 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zpl7o8b5
2023-06-03 05:40:04,329 - distributed.worker - INFO - Starting Worker plugin PreImport-b8d69da1-6fc3-4b04-9f71-e2ddcea53c06
2023-06-03 05:40:04,329 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9eccdcc1-f5b3-45c6-a824-c6c115886dd5
2023-06-03 05:40:04,329 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-88dd7022-6673-4d30-b291-45eaaae774c2
2023-06-03 05:40:04,329 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:04,343 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34807', status: init, memory: 0, processing: 0>
2023-06-03 05:40:04,344 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34807
2023-06-03 05:40:04,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36368
2023-06-03 05:40:04,345 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-03 05:40:04,345 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:04,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-03 05:40:07,825 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35235
2023-06-03 05:40:07,825 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35235
2023-06-03 05:40:07,825 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46849
2023-06-03 05:40:07,825 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-03 05:40:07,825 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36829
2023-06-03 05:40:07,825 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:07,826 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36829
2023-06-03 05:40:07,826 - distributed.worker - INFO -               Threads:                          4
2023-06-03 05:40:07,826 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46777
2023-06-03 05:40:07,826 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-03 05:40:07,826 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-03 05:40:07,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8wqvufnc
2023-06-03 05:40:07,826 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:07,826 - distributed.worker - INFO -               Threads:                          4
2023-06-03 05:40:07,826 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-03 05:40:07,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jnut_sw0
2023-06-03 05:40:07,826 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-70327737-d577-4c96-a88b-c778f962fa00
2023-06-03 05:40:07,826 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-274c8bcf-5883-47a0-989e-23a032ba66f8
2023-06-03 05:40:07,826 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42021
2023-06-03 05:40:07,826 - distributed.worker - INFO - Starting Worker plugin PreImport-dc384a88-b4e8-41e2-8985-28c60297af5f
2023-06-03 05:40:07,826 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42021
2023-06-03 05:40:07,826 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34699
2023-06-03 05:40:07,826 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c34eb908-470a-4afc-aca8-cbd0315204b4
2023-06-03 05:40:07,826 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-03 05:40:07,826 - distributed.worker - INFO - Starting Worker plugin PreImport-b92ddd18-c46a-4b18-974a-1dedd0d6b806
2023-06-03 05:40:07,827 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:07,827 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:07,827 - distributed.worker - INFO -               Threads:                          4
2023-06-03 05:40:07,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d3479e1-b051-451a-a1ca-62b9b6211395
2023-06-03 05:40:07,827 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-06-03 05:40:07,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-svd3mbq_
2023-06-03 05:40:07,827 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:07,827 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a553250e-6fd2-49c8-aa53-a2ef730e50a2
2023-06-03 05:40:07,828 - distributed.worker - INFO - Starting Worker plugin PreImport-618b9097-e248-4200-8cd5-832029934db5
2023-06-03 05:40:07,828 - distributed.worker - INFO - Starting Worker plugin RMMSetup-96a756c4-db4a-43a0-9b21-7f703654a330
2023-06-03 05:40:07,828 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:07,847 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36829', status: init, memory: 0, processing: 0>
2023-06-03 05:40:07,847 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36829
2023-06-03 05:40:07,847 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36386
2023-06-03 05:40:07,848 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-03 05:40:07,848 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:07,850 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-03 05:40:07,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35235', status: init, memory: 0, processing: 0>
2023-06-03 05:40:07,861 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35235
2023-06-03 05:40:07,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36390
2023-06-03 05:40:07,862 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-03 05:40:07,862 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:07,862 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42021', status: init, memory: 0, processing: 0>
2023-06-03 05:40:07,862 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42021
2023-06-03 05:40:07,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36400
2023-06-03 05:40:07,863 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-03 05:40:07,863 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:07,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-03 05:40:07,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-03 05:40:08,136 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-03 05:40:08,136 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-03 05:40:08,137 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-03 05:40:08,137 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-06-03 05:40:08,142 - distributed.scheduler - INFO - Remove client Client-15149a83-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:08,142 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36346; closing.
2023-06-03 05:40:08,142 - distributed.scheduler - INFO - Remove client Client-15149a83-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:08,143 - distributed.scheduler - INFO - Close client connection: Client-15149a83-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:08,143 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35123'. Reason: nanny-close
2023-06-03 05:40:08,144 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:08,144 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41395'. Reason: nanny-close
2023-06-03 05:40:08,145 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:08,145 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39733'. Reason: nanny-close
2023-06-03 05:40:08,145 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42021. Reason: nanny-close
2023-06-03 05:40:08,145 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:08,145 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41317'. Reason: nanny-close
2023-06-03 05:40:08,145 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36829. Reason: nanny-close
2023-06-03 05:40:08,146 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:08,146 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35235. Reason: nanny-close
2023-06-03 05:40:08,147 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34807. Reason: nanny-close
2023-06-03 05:40:08,147 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36386; closing.
2023-06-03 05:40:08,147 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-03 05:40:08,147 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-03 05:40:08,147 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36829', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:08,148 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36829
2023-06-03 05:40:08,148 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-03 05:40:08,148 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:08,148 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36400; closing.
2023-06-03 05:40:08,149 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-03 05:40:08,149 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42021', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:08,149 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:08,149 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42021
2023-06-03 05:40:08,149 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:08,150 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36368; closing.
2023-06-03 05:40:08,150 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34807', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:08,150 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34807
2023-06-03 05:40:08,150 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:08,151 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36390; closing.
2023-06-03 05:40:08,151 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35235', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:08,151 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35235
2023-06-03 05:40:08,151 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:40:09,561 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:40:09,561 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:40:09,561 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:40:09,562 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-03 05:40:09,563 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-06-03 05:40:11,642 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:11,646 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-03 05:40:11,649 - distributed.scheduler - INFO - State start
2023-06-03 05:40:11,669 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:11,670 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:40:11,671 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-03 05:40:11,901 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41129'
2023-06-03 05:40:11,919 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46165'
2023-06-03 05:40:11,921 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42193'
2023-06-03 05:40:11,928 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43267'
2023-06-03 05:40:11,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46217'
2023-06-03 05:40:11,945 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37721'
2023-06-03 05:40:11,953 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35495'
2023-06-03 05:40:11,958 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38603'
2023-06-03 05:40:13,209 - distributed.scheduler - INFO - Receive client connection: Client-1a87e5ef-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:13,221 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50424
2023-06-03 05:40:13,644 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:13,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:13,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:13,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:13,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:13,645 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:13,645 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:13,646 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:13,673 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:13,675 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:13,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:13,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:13,675 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:13,675 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:13,675 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:13,676 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:13,685 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:13,685 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:13,710 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:13,710 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:13,713 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:13,716 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:13,727 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:13,763 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:18,743 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37999
2023-06-03 05:40:18,743 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37999
2023-06-03 05:40:18,743 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46389
2023-06-03 05:40:18,743 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:18,743 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:18,744 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:18,744 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:18,744 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8yq8_jc8
2023-06-03 05:40:18,744 - distributed.worker - INFO - Starting Worker plugin RMMSetup-207ac608-69dd-4451-803b-9b4e88a2ed6b
2023-06-03 05:40:19,145 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6a7fb07-70da-44b2-87a6-2dc8b2b96ed1
2023-06-03 05:40:19,146 - distributed.worker - INFO - Starting Worker plugin PreImport-857c356f-0fcd-47dc-a5a3-d4ffa4503b24
2023-06-03 05:40:19,146 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,178 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37999', status: init, memory: 0, processing: 0>
2023-06-03 05:40:19,179 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37999
2023-06-03 05:40:19,180 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50432
2023-06-03 05:40:19,180 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,180 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:19,259 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32995
2023-06-03 05:40:19,259 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32995
2023-06-03 05:40:19,259 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45713
2023-06-03 05:40:19,259 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,259 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,259 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:19,259 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:19,259 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rifsf71y
2023-06-03 05:40:19,259 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39655
2023-06-03 05:40:19,260 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39655
2023-06-03 05:40:19,260 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41559
2023-06-03 05:40:19,260 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,260 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,260 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:19,260 - distributed.worker - INFO - Starting Worker plugin PreImport-b82da84f-00bb-420f-820c-23039b36b04e
2023-06-03 05:40:19,260 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:19,260 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5ea8izv5
2023-06-03 05:40:19,260 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-da633a9c-be98-43e1-b2ca-fd8204f86377
2023-06-03 05:40:19,260 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eab00354-b0a6-460f-88c5-6f1ed0dbe996
2023-06-03 05:40:19,260 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20b20161-16cf-4ff3-962a-b41282dfec2a
2023-06-03 05:40:19,261 - distributed.worker - INFO - Starting Worker plugin PreImport-5593f9e5-f622-4c3a-b5ae-7d9be0c85c4a
2023-06-03 05:40:19,261 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2cafdfab-0357-4bfa-8154-223c6c557be2
2023-06-03 05:40:19,278 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42433
2023-06-03 05:40:19,278 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42433
2023-06-03 05:40:19,278 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45441
2023-06-03 05:40:19,278 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,278 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,278 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:19,278 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:19,279 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-jnpab1ta
2023-06-03 05:40:19,279 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cacf4eb2-ce81-4e25-b9b3-e5d9526448f9
2023-06-03 05:40:19,442 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32839
2023-06-03 05:40:19,442 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32839
2023-06-03 05:40:19,442 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39351
2023-06-03 05:40:19,442 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,442 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,442 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:19,442 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:19,442 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-500ma1fg
2023-06-03 05:40:19,443 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb5dd1ea-1e88-4757-8ba2-fd0783d5dbd7
2023-06-03 05:40:19,525 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35837
2023-06-03 05:40:19,526 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35837
2023-06-03 05:40:19,526 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32847
2023-06-03 05:40:19,526 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,526 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,526 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:19,526 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:19,526 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-u55qw5ga
2023-06-03 05:40:19,527 - distributed.worker - INFO - Starting Worker plugin RMMSetup-edce4acc-6ba1-460d-98c6-edee59fd2ee9
2023-06-03 05:40:19,743 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34797
2023-06-03 05:40:19,743 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34797
2023-06-03 05:40:19,743 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33433
2023-06-03 05:40:19,743 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,743 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,743 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:19,743 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:19,743 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-soqc2xbf
2023-06-03 05:40:19,744 - distributed.worker - INFO - Starting Worker plugin PreImport-cec42383-4f49-44cb-a02b-081461b2f6d3
2023-06-03 05:40:19,744 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5ac8cee-6ad1-4d1f-872f-b0295506ba1e
2023-06-03 05:40:19,745 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0fde9ec0-e883-4860-bf19-e9e2d00af1df
2023-06-03 05:40:19,745 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34911
2023-06-03 05:40:19,745 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34911
2023-06-03 05:40:19,745 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46163
2023-06-03 05:40:19,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,745 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,746 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:19,746 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:19,746 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bwwn8bda
2023-06-03 05:40:19,746 - distributed.worker - INFO - Starting Worker plugin PreImport-ded0c330-a5de-4663-b94b-89e22e1b8914
2023-06-03 05:40:19,746 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aad6783a-490a-4dbd-94f8-5b825fffa97d
2023-06-03 05:40:19,746 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1287e6c-a8ae-471d-9e17-bd1864562e60
2023-06-03 05:40:19,854 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,857 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,893 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32995', status: init, memory: 0, processing: 0>
2023-06-03 05:40:19,894 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32995
2023-06-03 05:40:19,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50448
2023-06-03 05:40:19,894 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,895 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,896 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:19,907 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-24b44078-9afd-46d5-a7eb-b4b99473bd9e
2023-06-03 05:40:19,910 - distributed.worker - INFO - Starting Worker plugin PreImport-ed9f26f6-653a-4f1e-a670-02fdd6806f2b
2023-06-03 05:40:19,911 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,914 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39655', status: init, memory: 0, processing: 0>
2023-06-03 05:40:19,915 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39655
2023-06-03 05:40:19,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53976
2023-06-03 05:40:19,916 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,916 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:19,966 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42433', status: init, memory: 0, processing: 0>
2023-06-03 05:40:19,967 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42433
2023-06-03 05:40:19,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53986
2023-06-03 05:40:19,967 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:19,967 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:19,970 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:20,076 - distributed.worker - INFO - Starting Worker plugin PreImport-bc675441-f307-4ced-a3cf-8e4245e561e3
2023-06-03 05:40:20,077 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-416bd6cf-6c17-4450-b727-ef6af3d8de20
2023-06-03 05:40:20,077 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:20,102 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32839', status: init, memory: 0, processing: 0>
2023-06-03 05:40:20,103 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32839
2023-06-03 05:40:20,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53998
2023-06-03 05:40:20,103 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:20,103 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:20,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:20,106 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3abdad2d-dfee-4f65-a604-c8e5a592d320
2023-06-03 05:40:20,107 - distributed.worker - INFO - Starting Worker plugin PreImport-9a15d186-adc4-461c-bcea-3fd46ca820c7
2023-06-03 05:40:20,107 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:20,148 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35837', status: init, memory: 0, processing: 0>
2023-06-03 05:40:20,148 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35837
2023-06-03 05:40:20,148 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54002
2023-06-03 05:40:20,149 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:20,149 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:20,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:20,188 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:20,188 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:20,214 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34911', status: init, memory: 0, processing: 0>
2023-06-03 05:40:20,215 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34911
2023-06-03 05:40:20,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54016
2023-06-03 05:40:20,215 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:20,215 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:20,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:20,224 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34797', status: init, memory: 0, processing: 0>
2023-06-03 05:40:20,225 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34797
2023-06-03 05:40:20,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54036
2023-06-03 05:40:20,226 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:20,226 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:20,229 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:20,289 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:20,289 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:20,290 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:20,290 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:20,290 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:20,290 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:20,291 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:20,293 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:20,297 - distributed.scheduler - INFO - Remove client Client-1a87e5ef-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:20,297 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50424; closing.
2023-06-03 05:40:20,298 - distributed.scheduler - INFO - Remove client Client-1a87e5ef-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:20,298 - distributed.scheduler - INFO - Close client connection: Client-1a87e5ef-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:20,299 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42193'. Reason: nanny-close
2023-06-03 05:40:20,299 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:20,300 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46217'. Reason: nanny-close
2023-06-03 05:40:20,300 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:20,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41129'. Reason: nanny-close
2023-06-03 05:40:20,301 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39655. Reason: nanny-close
2023-06-03 05:40:20,301 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:20,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46165'. Reason: nanny-close
2023-06-03 05:40:20,301 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35837. Reason: nanny-close
2023-06-03 05:40:20,301 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:20,302 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34911. Reason: nanny-close
2023-06-03 05:40:20,302 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43267'. Reason: nanny-close
2023-06-03 05:40:20,302 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:20,302 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37999. Reason: nanny-close
2023-06-03 05:40:20,302 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37721'. Reason: nanny-close
2023-06-03 05:40:20,303 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:20,303 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54002; closing.
2023-06-03 05:40:20,303 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:20,303 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35495'. Reason: nanny-close
2023-06-03 05:40:20,303 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:20,303 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42433. Reason: nanny-close
2023-06-03 05:40:20,303 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35837', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:20,303 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35837
2023-06-03 05:40:20,303 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:20,304 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:20,304 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:20,304 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34797. Reason: nanny-close
2023-06-03 05:40:20,304 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38603'. Reason: nanny-close
2023-06-03 05:40:20,304 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53976; closing.
2023-06-03 05:40:20,304 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:20,304 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:20,304 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32995. Reason: nanny-close
2023-06-03 05:40:20,304 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39655', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:20,305 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39655
2023-06-03 05:40:20,305 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:20,305 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:20,305 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:20,306 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54016; closing.
2023-06-03 05:40:20,306 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32839. Reason: nanny-close
2023-06-03 05:40:20,306 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50432; closing.
2023-06-03 05:40:20,306 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35837
2023-06-03 05:40:20,306 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35837
2023-06-03 05:40:20,306 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35837
2023-06-03 05:40:20,306 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:20,306 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:20,307 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:20,307 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35837
2023-06-03 05:40:20,306 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53976>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-03 05:40:20,308 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:20,308 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:20,308 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34911', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:20,308 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34911
2023-06-03 05:40:20,308 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:20,308 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:20,308 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37999', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:20,308 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37999
2023-06-03 05:40:20,309 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53986; closing.
2023-06-03 05:40:20,309 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54036; closing.
2023-06-03 05:40:20,309 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50448; closing.
2023-06-03 05:40:20,310 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:20,310 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42433', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:20,310 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42433
2023-06-03 05:40:20,310 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34797', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:20,310 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34797
2023-06-03 05:40:20,311 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32995', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:20,311 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32995
2023-06-03 05:40:20,311 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53998; closing.
2023-06-03 05:40:20,311 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32839', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:20,311 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:32839
2023-06-03 05:40:20,311 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:40:21,817 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:40:21,818 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:40:21,819 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:40:21,820 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:40:21,820 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-06-03 05:40:23,868 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:23,872 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-03 05:40:23,876 - distributed.scheduler - INFO - State start
2023-06-03 05:40:23,896 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:23,897 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:40:23,897 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-03 05:40:24,101 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39769'
2023-06-03 05:40:24,120 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36891'
2023-06-03 05:40:24,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36297'
2023-06-03 05:40:24,129 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45367'
2023-06-03 05:40:24,137 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32841'
2023-06-03 05:40:24,145 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34811'
2023-06-03 05:40:24,153 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35967'
2023-06-03 05:40:24,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42933'
2023-06-03 05:40:25,801 - distributed.scheduler - INFO - Receive client connection: Client-21cc9679-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:25,813 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54206
2023-06-03 05:40:25,813 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:25,813 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:25,813 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:25,813 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:25,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:25,841 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:25,859 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:25,859 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:25,862 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:25,862 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:25,891 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:25,891 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:25,893 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:25,894 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:25,893 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:25,894 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:25,894 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:25,894 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:25,895 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:25,895 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:25,941 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:25,947 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:25,952 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:25,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:29,732 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34011
2023-06-03 05:40:29,732 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34011
2023-06-03 05:40:29,732 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45837
2023-06-03 05:40:29,732 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:29,732 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:29,732 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:29,732 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:29,733 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-bx1wjtmg
2023-06-03 05:40:29,733 - distributed.worker - INFO - Starting Worker plugin RMMSetup-81218c12-ac41-42e5-a85f-8cf442a7affe
2023-06-03 05:40:30,100 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44201
2023-06-03 05:40:30,100 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44201
2023-06-03 05:40:30,100 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38897
2023-06-03 05:40:30,100 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,100 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,100 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:30,100 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:30,100 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-j8e4jbww
2023-06-03 05:40:30,101 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f7bf5738-d43f-4a01-ad2d-369438909e5e
2023-06-03 05:40:30,420 - distributed.worker - INFO - Starting Worker plugin PreImport-7dd59709-6d44-443d-a1e3-c3e2fce301fa
2023-06-03 05:40:30,421 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5384ac11-b6dc-4dab-8e41-becfe74230ca
2023-06-03 05:40:30,421 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,447 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44201', status: init, memory: 0, processing: 0>
2023-06-03 05:40:30,448 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44201
2023-06-03 05:40:30,448 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48544
2023-06-03 05:40:30,449 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,449 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,451 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:30,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42829
2023-06-03 05:40:30,506 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42829
2023-06-03 05:40:30,506 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41325
2023-06-03 05:40:30,506 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,506 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,506 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:30,506 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:30,506 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-y__4kuy1
2023-06-03 05:40:30,507 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4fadf8d-d2dc-4426-b4d9-7452db2ceae0
2023-06-03 05:40:30,507 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40315
2023-06-03 05:40:30,508 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40315
2023-06-03 05:40:30,508 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37665
2023-06-03 05:40:30,508 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,508 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,508 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:30,508 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:30,508 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-fyar_96l
2023-06-03 05:40:30,508 - distributed.worker - INFO - Starting Worker plugin PreImport-b337a156-2f43-434f-8103-8183be8cb8af
2023-06-03 05:40:30,509 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cc525fab-0b9b-4785-b84c-0cfc4eecc39b
2023-06-03 05:40:30,509 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42825
2023-06-03 05:40:30,509 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42825
2023-06-03 05:40:30,509 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45717
2023-06-03 05:40:30,509 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,509 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a9d3c980-5732-4412-92f6-d4e393f335a7
2023-06-03 05:40:30,509 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,509 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:30,509 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:30,509 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-nuserw7p
2023-06-03 05:40:30,510 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2b1440a-17fa-43d8-a999-50abd670055e
2023-06-03 05:40:30,510 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43499
2023-06-03 05:40:30,510 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43499
2023-06-03 05:40:30,510 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40195
2023-06-03 05:40:30,510 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,511 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,511 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:30,511 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:30,511 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-vsy5jysr
2023-06-03 05:40:30,511 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72036b6d-8b23-4b7c-b465-7071ad11a58f
2023-06-03 05:40:30,512 - distributed.worker - INFO - Starting Worker plugin PreImport-bab6d550-d075-4764-a3e6-6fd118a60d08
2023-06-03 05:40:30,513 - distributed.worker - INFO - Starting Worker plugin RMMSetup-58d6641c-0472-4e19-bb30-82c8ef4d313b
2023-06-03 05:40:30,514 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39153
2023-06-03 05:40:30,514 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39153
2023-06-03 05:40:30,514 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38993
2023-06-03 05:40:30,514 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,514 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,514 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:30,514 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:30,514 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-zoqj6sxr
2023-06-03 05:40:30,515 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2123fa4a-40c0-435f-83f5-d635ce1d2888
2023-06-03 05:40:30,519 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41859
2023-06-03 05:40:30,519 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41859
2023-06-03 05:40:30,519 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44087
2023-06-03 05:40:30,519 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,519 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,519 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:30,519 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:30,519 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-9tir_3hb
2023-06-03 05:40:30,520 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9f960a9-5949-4626-bb7d-b60664fad689
2023-06-03 05:40:30,688 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59960937-fb85-4aeb-a81b-45efe395bd29
2023-06-03 05:40:30,688 - distributed.worker - INFO - Starting Worker plugin PreImport-9a651f53-7e2a-4de4-9c49-da984ea6b805
2023-06-03 05:40:30,688 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,721 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34011', status: init, memory: 0, processing: 0>
2023-06-03 05:40:30,722 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34011
2023-06-03 05:40:30,722 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48552
2023-06-03 05:40:30,722 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,722 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:30,808 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-15756e16-f598-4cf1-b2d0-be58bdc651cd
2023-06-03 05:40:30,808 - distributed.worker - INFO - Starting Worker plugin PreImport-29495c43-e70f-4ac7-9286-b01157443319
2023-06-03 05:40:30,809 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,810 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0fa5920a-f7fb-4bfe-b8d6-106ad21ca3d1
2023-06-03 05:40:30,810 - distributed.worker - INFO - Starting Worker plugin PreImport-c98dc5dc-ec82-4c7e-a45b-baacbb472485
2023-06-03 05:40:30,810 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,811 - distributed.worker - INFO - Starting Worker plugin PreImport-83591c24-e618-4b5a-8cf5-e1e47ac6f64c
2023-06-03 05:40:30,812 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65d73c56-93a1-4f1b-8c59-2d4cfc51aec3
2023-06-03 05:40:30,812 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,817 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,818 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-290c8877-305b-4178-b19c-ac46af52c058
2023-06-03 05:40:30,819 - distributed.worker - INFO - Starting Worker plugin PreImport-80a412a1-b6f9-45f4-9987-2db186b1f5f1
2023-06-03 05:40:30,819 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,824 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,845 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39153', status: init, memory: 0, processing: 0>
2023-06-03 05:40:30,846 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39153
2023-06-03 05:40:30,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48576
2023-06-03 05:40:30,847 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,847 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,847 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42829', status: init, memory: 0, processing: 0>
2023-06-03 05:40:30,848 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42829
2023-06-03 05:40:30,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48562
2023-06-03 05:40:30,848 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,849 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,849 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:30,850 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:30,855 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42825', status: init, memory: 0, processing: 0>
2023-06-03 05:40:30,856 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42825
2023-06-03 05:40:30,856 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48578
2023-06-03 05:40:30,856 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,856 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40315', status: init, memory: 0, processing: 0>
2023-06-03 05:40:30,856 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,857 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40315
2023-06-03 05:40:30,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48584
2023-06-03 05:40:30,858 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,858 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,858 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41859', status: init, memory: 0, processing: 0>
2023-06-03 05:40:30,859 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41859
2023-06-03 05:40:30,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48594
2023-06-03 05:40:30,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:30,860 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,860 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,861 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:30,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:30,867 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43499', status: init, memory: 0, processing: 0>
2023-06-03 05:40:30,868 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43499
2023-06-03 05:40:30,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48602
2023-06-03 05:40:30,868 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:30,869 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:30,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:30,969 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:30,969 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:30,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:30,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:30,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:30,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:30,970 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:30,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:30,976 - distributed.scheduler - INFO - Remove client Client-21cc9679-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:30,976 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54206; closing.
2023-06-03 05:40:30,976 - distributed.scheduler - INFO - Remove client Client-21cc9679-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:30,976 - distributed.scheduler - INFO - Close client connection: Client-21cc9679-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:30,977 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45367'. Reason: nanny-close
2023-06-03 05:40:30,978 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:30,978 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39769'. Reason: nanny-close
2023-06-03 05:40:30,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:30,979 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43499. Reason: nanny-close
2023-06-03 05:40:30,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36891'. Reason: nanny-close
2023-06-03 05:40:30,979 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:30,980 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36297'. Reason: nanny-close
2023-06-03 05:40:30,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40315. Reason: nanny-close
2023-06-03 05:40:30,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:30,980 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34011. Reason: nanny-close
2023-06-03 05:40:30,980 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32841'. Reason: nanny-close
2023-06-03 05:40:30,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:30,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44201. Reason: nanny-close
2023-06-03 05:40:30,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34811'. Reason: nanny-close
2023-06-03 05:40:30,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:30,981 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48602; closing.
2023-06-03 05:40:30,981 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:30,981 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43499', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:30,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42825. Reason: nanny-close
2023-06-03 05:40:30,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35967'. Reason: nanny-close
2023-06-03 05:40:30,981 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43499
2023-06-03 05:40:30,982 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:30,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:30,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:30,982 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42933'. Reason: nanny-close
2023-06-03 05:40:30,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41859. Reason: nanny-close
2023-06-03 05:40:30,982 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:30,982 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:30,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42829. Reason: nanny-close
2023-06-03 05:40:30,982 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:30,983 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43499
2023-06-03 05:40:30,983 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:30,983 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48584; closing.
2023-06-03 05:40:30,983 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:30,983 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48552; closing.
2023-06-03 05:40:30,983 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43499
2023-06-03 05:40:30,983 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39153. Reason: nanny-close
2023-06-03 05:40:30,984 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:30,984 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40315', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:30,984 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40315
2023-06-03 05:40:30,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:30,984 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43499
2023-06-03 05:40:30,984 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43499
2023-06-03 05:40:30,984 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34011', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:30,984 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34011
2023-06-03 05:40:30,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:30,985 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:30,985 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48544; closing.
2023-06-03 05:40:30,985 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:30,985 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44201', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:30,985 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44201
2023-06-03 05:40:30,985 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:30,985 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48578; closing.
2023-06-03 05:40:30,986 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:30,986 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:30,986 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42825', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:30,986 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42825
2023-06-03 05:40:30,986 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:30,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48562; closing.
2023-06-03 05:40:30,987 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48594; closing.
2023-06-03 05:40:30,987 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48576; closing.
2023-06-03 05:40:30,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42829', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:30,987 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42829
2023-06-03 05:40:30,988 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41859', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:30,988 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41859
2023-06-03 05:40:30,988 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39153', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:30,988 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:39153
2023-06-03 05:40:30,988 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:40:32,897 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:40:32,897 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:40:32,897 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:40:32,898 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:40:32,899 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-06-03 05:40:34,993 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:34,997 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-03 05:40:35,000 - distributed.scheduler - INFO - State start
2023-06-03 05:40:35,051 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:35,052 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:40:35,052 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-03 05:40:35,325 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38155'
2023-06-03 05:40:35,345 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38263'
2023-06-03 05:40:35,356 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33073'
2023-06-03 05:40:35,366 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35155'
2023-06-03 05:40:35,385 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40447'
2023-06-03 05:40:35,388 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41923'
2023-06-03 05:40:35,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42093'
2023-06-03 05:40:35,417 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35503'
2023-06-03 05:40:35,426 - distributed.scheduler - INFO - Receive client connection: Client-286cbdf2-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:35,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48754
2023-06-03 05:40:37,095 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:37,095 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:37,119 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:37,142 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:37,142 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:37,142 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:37,142 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:37,171 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:37,171 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:37,172 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:37,172 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:37,181 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:37,182 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:37,185 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:37,185 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:37,191 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:37,191 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:37,199 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:37,199 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:37,209 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:37,225 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:37,234 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:37,235 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:37,253 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:40,621 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34579
2023-06-03 05:40:40,621 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34579
2023-06-03 05:40:40,621 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43661
2023-06-03 05:40:40,621 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:40,621 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:40,621 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:40,621 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:40,621 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-44855wo8
2023-06-03 05:40:40,622 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c467b8bd-1726-4d98-acbc-0a2f8300f909
2023-06-03 05:40:40,901 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33143
2023-06-03 05:40:40,901 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33143
2023-06-03 05:40:40,901 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36393
2023-06-03 05:40:40,901 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:40,901 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:40,901 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:40,901 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:40,901 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ceenpb73
2023-06-03 05:40:40,902 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5b2226e-2b2a-40cc-b997-26ac6e33c20d
2023-06-03 05:40:40,905 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35331
2023-06-03 05:40:40,905 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35331
2023-06-03 05:40:40,905 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38883
2023-06-03 05:40:40,905 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:40,905 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:40,905 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:40,905 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:40,905 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-lqrvs6bz
2023-06-03 05:40:40,906 - distributed.worker - INFO - Starting Worker plugin PreImport-c6051459-6ec8-428e-aba4-821756c9bd46
2023-06-03 05:40:40,906 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-535f0aa1-664b-4c5a-96a6-36dc3d27a5ca
2023-06-03 05:40:40,906 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c0a1ec3-54a0-4fd0-a964-6fcf8e84d6e4
2023-06-03 05:40:40,918 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42217
2023-06-03 05:40:40,918 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42217
2023-06-03 05:40:40,918 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46003
2023-06-03 05:40:40,918 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:40,918 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:40,918 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:40,918 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:40,918 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-xa3r5qoy
2023-06-03 05:40:40,919 - distributed.worker - INFO - Starting Worker plugin PreImport-1815e586-5a41-4734-8bf0-a4d15a966c01
2023-06-03 05:40:40,919 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3d494f4d-6cc1-4b06-9426-5dc798dc82ed
2023-06-03 05:40:40,919 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fbf9338-6881-41ce-8249-39534a667e09
2023-06-03 05:40:40,919 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42209
2023-06-03 05:40:40,919 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42209
2023-06-03 05:40:40,920 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32893
2023-06-03 05:40:40,920 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:40,920 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:40,920 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:40,920 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:40,920 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-oax1v2jx
2023-06-03 05:40:40,920 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b3838c9c-4561-4f08-a423-7c3ddb506b56
2023-06-03 05:40:40,930 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33745
2023-06-03 05:40:40,931 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33745
2023-06-03 05:40:40,931 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43657
2023-06-03 05:40:40,931 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:40,931 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:40,931 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:40,931 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:40,931 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-q6t2u8s5
2023-06-03 05:40:40,932 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6ded467-9374-41ea-a5ef-ef0641a86692
2023-06-03 05:40:40,932 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38945
2023-06-03 05:40:40,933 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38945
2023-06-03 05:40:40,933 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35009
2023-06-03 05:40:40,933 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:40,933 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:40,933 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:40,933 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:40,933 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-5fntg1ou
2023-06-03 05:40:40,934 - distributed.worker - INFO - Starting Worker plugin PreImport-689667e4-2927-43f9-ba71-45f611113eef
2023-06-03 05:40:40,934 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95201a72-f401-41b7-a3d4-4556cf70d31d
2023-06-03 05:40:40,934 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e97b2c26-eb00-41d2-bcf0-c1bf7aaefc62
2023-06-03 05:40:40,954 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44133
2023-06-03 05:40:40,954 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44133
2023-06-03 05:40:40,954 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35011
2023-06-03 05:40:40,954 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:40,954 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:40,954 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:40,954 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:40,954 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2l0ljddy
2023-06-03 05:40:40,955 - distributed.worker - INFO - Starting Worker plugin RMMSetup-16c94e7f-870b-405e-b318-d80251321abd
2023-06-03 05:40:40,968 - distributed.worker - INFO - Starting Worker plugin PreImport-2aebacf6-3c7a-42e5-a805-d24835856333
2023-06-03 05:40:40,968 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b2f6d4f1-0bca-4b3a-86de-86c4495b1238
2023-06-03 05:40:40,969 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,005 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34579', status: init, memory: 0, processing: 0>
2023-06-03 05:40:41,007 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34579
2023-06-03 05:40:41,007 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38592
2023-06-03 05:40:41,008 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:41,008 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,011 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:41,167 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,171 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4a3d3d8-e39c-40e9-aa6a-a7e6ed4e32bc
2023-06-03 05:40:41,171 - distributed.worker - INFO - Starting Worker plugin PreImport-f97bdec7-4216-4aef-9602-870d81c7298d
2023-06-03 05:40:41,172 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,175 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c25cf998-5817-43a4-afe3-f820a0237e41
2023-06-03 05:40:41,176 - distributed.worker - INFO - Starting Worker plugin PreImport-c3e9a81d-5c83-4565-806b-55621b34c2ee
2023-06-03 05:40:41,176 - distributed.worker - INFO - Starting Worker plugin PreImport-e7ed6e04-eb6b-49af-81b2-f09f38a4d6a9
2023-06-03 05:40:41,176 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b7a9d58-a127-4ced-801c-d74cb717d5c2
2023-06-03 05:40:41,176 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,176 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,178 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,178 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,180 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c69cba0e-81a4-4b3f-beea-6c14df33e52b
2023-06-03 05:40:41,180 - distributed.worker - INFO - Starting Worker plugin PreImport-598e2f4e-a3bb-4ac8-84da-55c91c00779c
2023-06-03 05:40:41,181 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,194 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35331', status: init, memory: 0, processing: 0>
2023-06-03 05:40:41,194 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35331
2023-06-03 05:40:41,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38594
2023-06-03 05:40:41,195 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:41,195 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,197 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:41,207 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42217', status: init, memory: 0, processing: 0>
2023-06-03 05:40:41,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42217
2023-06-03 05:40:41,207 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38628
2023-06-03 05:40:41,208 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:41,208 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,210 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44133', status: init, memory: 0, processing: 0>
2023-06-03 05:40:41,210 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:41,210 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44133
2023-06-03 05:40:41,210 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38654
2023-06-03 05:40:41,210 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:41,211 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,211 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38945', status: init, memory: 0, processing: 0>
2023-06-03 05:40:41,211 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38945
2023-06-03 05:40:41,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38644
2023-06-03 05:40:41,212 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:41,212 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,212 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33745', status: init, memory: 0, processing: 0>
2023-06-03 05:40:41,212 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:41,213 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33745
2023-06-03 05:40:41,213 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38608
2023-06-03 05:40:41,213 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33143', status: init, memory: 0, processing: 0>
2023-06-03 05:40:41,213 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:41,214 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,214 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33143
2023-06-03 05:40:41,214 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38598
2023-06-03 05:40:41,215 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:41,215 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:41,215 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:41,216 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42209', status: init, memory: 0, processing: 0>
2023-06-03 05:40:41,217 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42209
2023-06-03 05:40:41,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38614
2023-06-03 05:40:41,217 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:41,218 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:41,218 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:41,220 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:41,261 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:41,261 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:41,261 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:41,262 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:41,262 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:41,262 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:41,262 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:41,262 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:41,273 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:40:41,273 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:40:41,274 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:40:41,274 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:40:41,274 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:40:41,274 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:40:41,274 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:40:41,274 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:40:41,280 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:40:41,282 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:40:41,285 - distributed.scheduler - INFO - Remove client Client-286cbdf2-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:41,285 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48754; closing.
2023-06-03 05:40:41,285 - distributed.scheduler - INFO - Remove client Client-286cbdf2-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:41,285 - distributed.scheduler - INFO - Close client connection: Client-286cbdf2-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:41,286 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38155'. Reason: nanny-close
2023-06-03 05:40:41,287 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:41,288 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38263'. Reason: nanny-close
2023-06-03 05:40:41,288 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:41,289 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33073'. Reason: nanny-close
2023-06-03 05:40:41,289 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34579. Reason: nanny-close
2023-06-03 05:40:41,289 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:41,289 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35155'. Reason: nanny-close
2023-06-03 05:40:41,289 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33143. Reason: nanny-close
2023-06-03 05:40:41,290 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:41,290 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35331. Reason: nanny-close
2023-06-03 05:40:41,290 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40447'. Reason: nanny-close
2023-06-03 05:40:41,290 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:41,291 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44133. Reason: nanny-close
2023-06-03 05:40:41,291 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41923'. Reason: nanny-close
2023-06-03 05:40:41,291 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:41,291 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38592; closing.
2023-06-03 05:40:41,291 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:41,291 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42093'. Reason: nanny-close
2023-06-03 05:40:41,291 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34579', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:41,291 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42209. Reason: nanny-close
2023-06-03 05:40:41,291 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34579
2023-06-03 05:40:41,291 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:41,291 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:41,292 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:41,292 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35503'. Reason: nanny-close
2023-06-03 05:40:41,292 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38945. Reason: nanny-close
2023-06-03 05:40:41,292 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:41,292 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:41,292 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42217. Reason: nanny-close
2023-06-03 05:40:41,292 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:41,293 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:41,293 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34579
2023-06-03 05:40:41,293 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:41,293 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38598; closing.
2023-06-03 05:40:41,293 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38594; closing.
2023-06-03 05:40:41,293 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:41,293 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33745. Reason: nanny-close
2023-06-03 05:40:41,293 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34579
2023-06-03 05:40:41,294 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34579
2023-06-03 05:40:41,294 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34579
2023-06-03 05:40:41,294 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33143', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:41,294 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:41,294 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33143
2023-06-03 05:40:41,294 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:41,294 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35331', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:41,294 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35331
2023-06-03 05:40:41,294 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:41,295 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38654; closing.
2023-06-03 05:40:41,295 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:41,295 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44133', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:41,295 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44133
2023-06-03 05:40:41,295 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:41,295 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:41,296 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38614; closing.
2023-06-03 05:40:41,296 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:41,296 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42209', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:41,296 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42209
2023-06-03 05:40:41,296 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:41,297 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38644; closing.
2023-06-03 05:40:41,297 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38628; closing.
2023-06-03 05:40:41,297 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38608; closing.
2023-06-03 05:40:41,297 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38945', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:41,297 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38945
2023-06-03 05:40:41,298 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42217', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:41,298 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42217
2023-06-03 05:40:41,298 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33745', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:41,298 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33745
2023-06-03 05:40:41,299 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:40:42,905 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:40:42,905 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:40:42,906 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:40:42,907 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:40:42,907 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-06-03 05:40:45,208 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:45,214 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2023-06-03 05:40:45,218 - distributed.scheduler - INFO - State start
2023-06-03 05:40:45,360 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:45,362 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:40:45,362 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2023-06-03 05:40:45,505 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39189'
2023-06-03 05:40:45,530 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37951'
2023-06-03 05:40:45,533 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42219'
2023-06-03 05:40:45,542 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34089'
2023-06-03 05:40:45,551 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35755'
2023-06-03 05:40:45,559 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41211'
2023-06-03 05:40:45,568 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38345'
2023-06-03 05:40:45,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33747'
2023-06-03 05:40:46,091 - distributed.scheduler - INFO - Receive client connection: Client-2e6acff2-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:46,103 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38792
2023-06-03 05:40:47,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:47,253 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:47,263 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:47,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:47,278 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:47,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:47,282 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:47,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:47,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:47,284 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:47,285 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:47,289 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:47,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:47,322 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:47,324 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:47,350 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:47,350 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:47,371 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:47,372 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:47,382 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:47,382 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:47,413 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:47,434 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:47,445 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:40:51,140 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41019
2023-06-03 05:40:51,140 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41019
2023-06-03 05:40:51,140 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40123
2023-06-03 05:40:51,140 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,140 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,140 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:51,141 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:51,141 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-hfgizdxr
2023-06-03 05:40:51,141 - distributed.worker - INFO - Starting Worker plugin PreImport-628aab3e-ec30-467e-a115-2079b9d5cd59
2023-06-03 05:40:51,141 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0f2d1415-d3f9-4a20-bd47-139dcbed9df0
2023-06-03 05:40:51,141 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7208caad-a8f5-42fe-995f-3621b9340c5f
2023-06-03 05:40:51,160 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40339
2023-06-03 05:40:51,160 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40339
2023-06-03 05:40:51,160 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35277
2023-06-03 05:40:51,160 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,160 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,160 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:51,160 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:51,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-emnf1rhy
2023-06-03 05:40:51,161 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7ff60fd9-71df-4f2d-8b5c-f5ed8c7ddf8a
2023-06-03 05:40:51,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37901
2023-06-03 05:40:51,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37901
2023-06-03 05:40:51,163 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33025
2023-06-03 05:40:51,163 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,163 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,163 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:51,164 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:51,164 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-rn8kqn1h
2023-06-03 05:40:51,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37623
2023-06-03 05:40:51,164 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37623
2023-06-03 05:40:51,164 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44299
2023-06-03 05:40:51,164 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,164 - distributed.worker - INFO - Starting Worker plugin PreImport-5c511915-7461-42e5-80ad-89ff16a6920c
2023-06-03 05:40:51,164 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,164 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:51,164 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b860b8e8-5fcd-40da-b119-7f2637e39e38
2023-06-03 05:40:51,164 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:51,164 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-2ktmxmmf
2023-06-03 05:40:51,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65cab843-baa9-4665-bd34-41fd98b4dffa
2023-06-03 05:40:51,165 - distributed.worker - INFO - Starting Worker plugin PreImport-68ed939d-5a2c-4de1-97c0-93ae13923905
2023-06-03 05:40:51,165 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61994890-4d0a-47fe-b39d-2d953e1c5bb6
2023-06-03 05:40:51,166 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d471607-3934-4346-829b-5dafa0729c99
2023-06-03 05:40:51,169 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44707
2023-06-03 05:40:51,169 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44707
2023-06-03 05:40:51,169 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41627
2023-06-03 05:40:51,169 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,169 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,169 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:51,169 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:51,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-8s_22upd
2023-06-03 05:40:51,170 - distributed.worker - INFO - Starting Worker plugin RMMSetup-41435785-3e05-4bb9-99d3-bd05445b434e
2023-06-03 05:40:51,177 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33729
2023-06-03 05:40:51,177 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33729
2023-06-03 05:40:51,177 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41907
2023-06-03 05:40:51,177 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,177 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,177 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:51,177 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:51,177 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-h_6q08m1
2023-06-03 05:40:51,178 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee1fa98b-1c03-4a32-af03-b6df0107830b
2023-06-03 05:40:51,179 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45869
2023-06-03 05:40:51,179 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45869
2023-06-03 05:40:51,179 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34341
2023-06-03 05:40:51,179 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,179 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,179 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:51,179 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:51,179 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-dlnnu58u
2023-06-03 05:40:51,180 - distributed.worker - INFO - Starting Worker plugin RMMSetup-70c09024-eb26-4682-bf8e-41457d7bf772
2023-06-03 05:40:51,183 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43707
2023-06-03 05:40:51,183 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43707
2023-06-03 05:40:51,183 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38831
2023-06-03 05:40:51,184 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,184 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,184 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:40:51,184 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:40:51,184 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-468cwwfl
2023-06-03 05:40:51,184 - distributed.worker - INFO - Starting Worker plugin RMMSetup-636a141d-d6b8-4d91-b0cb-f4b14d8cfdaa
2023-06-03 05:40:51,313 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,342 - distributed.worker - INFO - Starting Worker plugin PreImport-be8991d3-47e3-4534-a2a1-032d11eda556
2023-06-03 05:40:51,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e8673c12-ace0-4869-9724-dc81f37fd539
2023-06-03 05:40:51,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-449ba04e-140b-48c8-8810-f0b8d6511965
2023-06-03 05:40:51,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34cafe6b-d03f-4012-8344-85f782f53a21
2023-06-03 05:40:51,342 - distributed.worker - INFO - Starting Worker plugin PreImport-e6af5d2e-f58c-45f3-bae5-cd522caaf6c8
2023-06-03 05:40:51,342 - distributed.worker - INFO - Starting Worker plugin PreImport-e84590c5-735e-4b5f-908e-09486ff43b3f
2023-06-03 05:40:51,342 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,342 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-862cd0dd-6bfb-41af-9399-d756ab71c565
2023-06-03 05:40:51,342 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,343 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,343 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-849f5ccb-e73f-4ec6-89cc-c724c9262741
2023-06-03 05:40:51,343 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,343 - distributed.worker - INFO - Starting Worker plugin PreImport-ccd59f94-fe85-4391-af6a-e8ea076131ba
2023-06-03 05:40:51,343 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,343 - distributed.worker - INFO - Starting Worker plugin PreImport-ad7d1621-78b3-41d3-b911-b164d0bb6344
2023-06-03 05:40:51,343 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,344 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,347 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41019', status: init, memory: 0, processing: 0>
2023-06-03 05:40:51,348 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41019
2023-06-03 05:40:51,348 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48812
2023-06-03 05:40:51,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,349 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,351 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:51,370 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44707', status: init, memory: 0, processing: 0>
2023-06-03 05:40:51,371 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44707
2023-06-03 05:40:51,371 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48824
2023-06-03 05:40:51,372 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,372 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,372 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45869', status: init, memory: 0, processing: 0>
2023-06-03 05:40:51,373 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45869
2023-06-03 05:40:51,373 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48816
2023-06-03 05:40:51,373 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,373 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,374 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37901', status: init, memory: 0, processing: 0>
2023-06-03 05:40:51,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:51,374 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37901
2023-06-03 05:40:51,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48818
2023-06-03 05:40:51,375 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,375 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,375 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:51,377 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:51,380 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43707', status: init, memory: 0, processing: 0>
2023-06-03 05:40:51,380 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43707
2023-06-03 05:40:51,380 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48846
2023-06-03 05:40:51,381 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,381 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,381 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33729', status: init, memory: 0, processing: 0>
2023-06-03 05:40:51,382 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33729
2023-06-03 05:40:51,382 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48832
2023-06-03 05:40:51,383 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,383 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,383 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37623', status: init, memory: 0, processing: 0>
2023-06-03 05:40:51,383 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37623
2023-06-03 05:40:51,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48848
2023-06-03 05:40:51,384 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:51,384 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,384 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,385 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40339', status: init, memory: 0, processing: 0>
2023-06-03 05:40:51,385 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40339
2023-06-03 05:40:51,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48828
2023-06-03 05:40:51,386 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:51,386 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:40:51,386 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:40:51,387 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:51,389 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:40:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:51,491 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:51,492 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:40:51,496 - distributed.scheduler - INFO - Remove client Client-2e6acff2-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:51,496 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38792; closing.
2023-06-03 05:40:51,497 - distributed.scheduler - INFO - Remove client Client-2e6acff2-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:51,497 - distributed.scheduler - INFO - Close client connection: Client-2e6acff2-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:51,498 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42219'. Reason: nanny-close
2023-06-03 05:40:51,499 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:51,499 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39189'. Reason: nanny-close
2023-06-03 05:40:51,500 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:51,500 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37951'. Reason: nanny-close
2023-06-03 05:40:51,500 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40339. Reason: nanny-close
2023-06-03 05:40:51,501 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:51,501 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34089'. Reason: nanny-close
2023-06-03 05:40:51,501 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33729. Reason: nanny-close
2023-06-03 05:40:51,501 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:51,502 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37901. Reason: nanny-close
2023-06-03 05:40:51,502 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35755'. Reason: nanny-close
2023-06-03 05:40:51,502 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:51,502 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45869. Reason: nanny-close
2023-06-03 05:40:51,502 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41211'. Reason: nanny-close
2023-06-03 05:40:51,502 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:51,503 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48828; closing.
2023-06-03 05:40:51,503 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:51,503 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38345'. Reason: nanny-close
2023-06-03 05:40:51,503 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43707. Reason: nanny-close
2023-06-03 05:40:51,503 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40339', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:51,503 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:51,503 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40339
2023-06-03 05:40:51,503 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:51,503 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:51,503 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33747'. Reason: nanny-close
2023-06-03 05:40:51,503 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41019. Reason: nanny-close
2023-06-03 05:40:51,503 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:40:51,503 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:51,504 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37623. Reason: nanny-close
2023-06-03 05:40:51,504 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:51,504 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:51,504 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44707. Reason: nanny-close
2023-06-03 05:40:51,505 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48832; closing.
2023-06-03 05:40:51,505 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:51,505 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:51,505 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40339
2023-06-03 05:40:51,505 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48818; closing.
2023-06-03 05:40:51,505 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48816; closing.
2023-06-03 05:40:51,505 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40339
2023-06-03 05:40:51,505 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:51,505 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40339
2023-06-03 05:40:51,506 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40339
2023-06-03 05:40:51,506 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33729', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:51,506 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33729
2023-06-03 05:40:51,506 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:51,506 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:51,506 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37901', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:51,506 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37901
2023-06-03 05:40:51,506 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:40:51,506 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45869', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:51,506 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:45869
2023-06-03 05:40:51,507 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:51,507 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:51,507 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:51,507 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48846; closing.
2023-06-03 05:40:51,507 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48812; closing.
2023-06-03 05:40:51,507 - distributed.nanny - INFO - Worker closed
2023-06-03 05:40:51,508 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43707', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:51,508 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43707
2023-06-03 05:40:51,508 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41019', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:51,508 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:41019
2023-06-03 05:40:51,508 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48848; closing.
2023-06-03 05:40:51,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48824; closing.
2023-06-03 05:40:51,509 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37623', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:51,509 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:37623
2023-06-03 05:40:51,510 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44707', status: closing, memory: 0, processing: 0>
2023-06-03 05:40:51,510 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44707
2023-06-03 05:40:51,510 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:40:53,167 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:40:53,168 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:40:53,168 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:40:53,169 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:40:53,169 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-06-03 05:40:55,377 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:55,381 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44609 instead
  warnings.warn(
2023-06-03 05:40:55,386 - distributed.scheduler - INFO - State start
2023-06-03 05:40:55,407 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:40:55,408 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:40:55,409 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:44609/status
2023-06-03 05:40:55,497 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33733'
2023-06-03 05:40:56,818 - distributed.scheduler - INFO - Receive client connection: Client-34805396-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:40:56,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48978
2023-06-03 05:40:57,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:40:57,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:40:57,622 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:03,261 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44243
2023-06-03 05:41:03,261 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44243
2023-06-03 05:41:03,261 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-06-03 05:41:03,261 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:03,261 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:03,261 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:03,261 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-03 05:41:03,261 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-pfyxygjk
2023-06-03 05:41:03,262 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9288daf0-c757-4c29-adfa-9dfc2e063f72
2023-06-03 05:41:03,262 - distributed.worker - INFO - Starting Worker plugin PreImport-f961b308-0f30-4db5-9b97-f9dec783a757
2023-06-03 05:41:03,262 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5067e66-e540-4b0c-a538-c671dd44190d
2023-06-03 05:41:03,263 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:03,291 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44243', status: init, memory: 0, processing: 0>
2023-06-03 05:41:03,293 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44243
2023-06-03 05:41:03,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45520
2023-06-03 05:41:03,293 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:03,294 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:03,296 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:03,349 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:03,352 - distributed.scheduler - INFO - Remove client Client-34805396-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:03,353 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48978; closing.
2023-06-03 05:41:03,353 - distributed.scheduler - INFO - Remove client Client-34805396-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:03,353 - distributed.scheduler - INFO - Close client connection: Client-34805396-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:03,355 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33733'. Reason: nanny-close
2023-06-03 05:41:03,355 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:03,357 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44243. Reason: nanny-close
2023-06-03 05:41:03,359 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:03,359 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45520; closing.
2023-06-03 05:41:03,359 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44243', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:03,359 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:44243
2023-06-03 05:41:03,360 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:41:03,360 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:04,723 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:41:04,723 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:41:04,724 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:41:04,724 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:41:04,725 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-06-03 05:41:09,157 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:09,162 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43901 instead
  warnings.warn(
2023-06-03 05:41:09,166 - distributed.scheduler - INFO - State start
2023-06-03 05:41:09,187 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:09,188 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:41:09,189 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:43901/status
2023-06-03 05:41:09,312 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33029'
2023-06-03 05:41:09,452 - distributed.scheduler - INFO - Receive client connection: Client-3cc07b56-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:09,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45612
2023-06-03 05:41:11,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:11,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:11,412 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:16,521 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40747
2023-06-03 05:41:16,521 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40747
2023-06-03 05:41:16,521 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45137
2023-06-03 05:41:16,521 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:16,522 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:16,522 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:16,522 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-03 05:41:16,522 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-63jdgr2l
2023-06-03 05:41:16,522 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c81484b8-8075-4da8-bc13-6d96f5d255fa
2023-06-03 05:41:16,522 - distributed.worker - INFO - Starting Worker plugin PreImport-40a8bc84-3903-4c9b-a524-c894ac802d1d
2023-06-03 05:41:16,524 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8af46668-b4c3-44da-bfe7-42ca6e1eecd8
2023-06-03 05:41:16,524 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:16,556 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40747', status: init, memory: 0, processing: 0>
2023-06-03 05:41:16,559 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40747
2023-06-03 05:41:16,559 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38610
2023-06-03 05:41:16,560 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:16,560 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:16,562 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:16,629 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:16,633 - distributed.scheduler - INFO - Remove client Client-3cc07b56-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:16,633 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45612; closing.
2023-06-03 05:41:16,633 - distributed.scheduler - INFO - Remove client Client-3cc07b56-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:16,634 - distributed.scheduler - INFO - Close client connection: Client-3cc07b56-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:16,634 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33029'. Reason: nanny-close
2023-06-03 05:41:16,635 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:16,636 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40747. Reason: nanny-close
2023-06-03 05:41:16,638 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:16,638 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38610; closing.
2023-06-03 05:41:16,639 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40747', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:16,639 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40747
2023-06-03 05:41:16,639 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:41:16,640 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:18,052 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:41:18,053 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:41:18,053 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:41:18,054 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:41:18,055 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-06-03 05:41:20,427 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:20,432 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37581 instead
  warnings.warn(
2023-06-03 05:41:20,436 - distributed.scheduler - INFO - State start
2023-06-03 05:41:20,945 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:20,947 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:41:20,947 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37581/status
2023-06-03 05:41:25,206 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:41:25,206 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:41:25,206 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:41:25,207 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:41:25,207 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-06-03 05:41:27,538 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:27,543 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37289 instead
  warnings.warn(
2023-06-03 05:41:27,547 - distributed.scheduler - INFO - State start
2023-06-03 05:41:27,784 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:27,786 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-06-03 05:41:27,787 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37289/status
2023-06-03 05:41:27,832 - distributed.scheduler - INFO - Receive client connection: Client-47a707ac-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:27,848 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49664
2023-06-03 05:41:27,850 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36633'
2023-06-03 05:41:29,725 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:29,725 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:29,733 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:32,995 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35969
2023-06-03 05:41:32,996 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35969
2023-06-03 05:41:32,996 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39637
2023-06-03 05:41:32,996 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-06-03 05:41:32,996 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:32,996 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:32,996 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-03 05:41:32,996 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-uwb4nujp
2023-06-03 05:41:32,996 - distributed.worker - INFO - Starting Worker plugin RMMSetup-13152f65-8676-463c-aca7-2011237b5a3c
2023-06-03 05:41:32,997 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-695e1c5a-2da0-4829-a58c-8322cbe14634
2023-06-03 05:41:32,997 - distributed.worker - INFO - Starting Worker plugin PreImport-480cb457-af04-4307-91a5-fdf9731524c2
2023-06-03 05:41:32,997 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:33,028 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35969', status: init, memory: 0, processing: 0>
2023-06-03 05:41:33,031 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35969
2023-06-03 05:41:33,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58978
2023-06-03 05:41:33,031 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-06-03 05:41:33,031 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:33,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-06-03 05:41:33,090 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:33,094 - distributed.scheduler - INFO - Remove client Client-47a707ac-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:33,094 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49664; closing.
2023-06-03 05:41:33,094 - distributed.scheduler - INFO - Remove client Client-47a707ac-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:33,095 - distributed.scheduler - INFO - Close client connection: Client-47a707ac-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:33,095 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36633'. Reason: nanny-close
2023-06-03 05:41:33,096 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:33,097 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35969. Reason: nanny-close
2023-06-03 05:41:33,100 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58978; closing.
2023-06-03 05:41:33,100 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-06-03 05:41:33,100 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35969', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:33,100 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35969
2023-06-03 05:41:33,100 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:41:33,102 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:34,212 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:41:34,212 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:41:34,213 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:41:34,213 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-06-03 05:41:34,214 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-06-03 05:41:36,545 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:36,550 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34317 instead
  warnings.warn(
2023-06-03 05:41:36,555 - distributed.scheduler - INFO - State start
2023-06-03 05:41:36,576 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:36,577 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:41:36,578 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34317/status
2023-06-03 05:41:36,826 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34195'
2023-06-03 05:41:36,845 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37815'
2023-06-03 05:41:36,847 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33931'
2023-06-03 05:41:36,855 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46425'
2023-06-03 05:41:36,864 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39229'
2023-06-03 05:41:36,872 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34387'
2023-06-03 05:41:36,882 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33489'
2023-06-03 05:41:36,895 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33887'
2023-06-03 05:41:38,580 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:38,580 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:38,580 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:38,580 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:38,582 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:38,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:38,639 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:38,639 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:38,664 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:38,665 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:38,682 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:38,682 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:38,682 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:38,682 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:38,695 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:38,695 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:38,866 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:38,891 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:38,895 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:38,923 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:38,928 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:38,933 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:38,934 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:38,940 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:39,323 - distributed.scheduler - INFO - Receive client connection: Client-4d0cb8f7-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:39,339 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36052
2023-06-03 05:41:42,887 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38281
2023-06-03 05:41:42,888 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38281
2023-06-03 05:41:42,888 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38209
2023-06-03 05:41:42,888 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:42,888 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:42,888 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:42,888 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:41:42,888 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-aq06jn71
2023-06-03 05:41:42,888 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2b2010a6-e68e-4c15-b5ca-f84b458f3140
2023-06-03 05:41:42,895 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42201
2023-06-03 05:41:42,895 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42201
2023-06-03 05:41:42,895 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45797
2023-06-03 05:41:42,896 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:42,896 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:42,896 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:42,896 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:41:42,896 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ih7q1ksl
2023-06-03 05:41:42,896 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43735
2023-06-03 05:41:42,896 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43735
2023-06-03 05:41:42,896 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33063
2023-06-03 05:41:42,896 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:42,896 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:42,896 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:42,896 - distributed.worker - INFO - Starting Worker plugin RMMSetup-77305bc0-322d-476d-8c61-f9af0ad04de5
2023-06-03 05:41:42,896 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:41:42,896 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1t20y9au
2023-06-03 05:41:42,897 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aaec95e2-544c-42b0-b3ca-bec4234df76b
2023-06-03 05:41:42,923 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35799
2023-06-03 05:41:42,923 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35799
2023-06-03 05:41:42,923 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34571
2023-06-03 05:41:42,923 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:42,923 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:42,923 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:42,923 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:41:42,923 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-1o_1twe3
2023-06-03 05:41:42,924 - distributed.worker - INFO - Starting Worker plugin PreImport-0153d072-a3f6-4290-95cd-3bfd16ae0482
2023-06-03 05:41:42,924 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-346fd097-c739-43b5-b205-7ac3085c50c3
2023-06-03 05:41:42,924 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6cecded8-a2e4-44ca-bc52-1b27affbb369
2023-06-03 05:41:42,949 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40781
2023-06-03 05:41:42,949 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40781
2023-06-03 05:41:42,949 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35055
2023-06-03 05:41:42,949 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:42,949 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:42,949 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:42,949 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:41:42,949 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-6vcccgbc
2023-06-03 05:41:42,950 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2745ba29-9ff4-4f33-8e45-6e92376134a6
2023-06-03 05:41:42,950 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35659
2023-06-03 05:41:42,951 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35659
2023-06-03 05:41:42,951 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37127
2023-06-03 05:41:42,951 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:42,951 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:42,951 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:42,951 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:41:42,951 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-_b3ztrrc
2023-06-03 05:41:42,951 - distributed.worker - INFO - Starting Worker plugin PreImport-b6ba41dd-ea34-437a-8bcd-fb3c9e27d018
2023-06-03 05:41:42,951 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34197
2023-06-03 05:41:42,952 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34197
2023-06-03 05:41:42,952 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a18a7703-84ce-413f-a563-19c0975b5bfd
2023-06-03 05:41:42,952 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33195
2023-06-03 05:41:42,952 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5926e430-edc4-4ce1-abd5-79e5b0312fcb
2023-06-03 05:41:42,952 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:42,952 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:42,952 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:42,952 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:41:42,952 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-x9iqh8hg
2023-06-03 05:41:42,952 - distributed.worker - INFO - Starting Worker plugin PreImport-060b27cb-53f4-4acb-806c-4644a4805fd9
2023-06-03 05:41:42,952 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a7dfe25-222b-4dad-ac58-a185d2c59f42
2023-06-03 05:41:42,953 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2038c4ff-6664-4504-8421-bdfa6c052fd5
2023-06-03 05:41:42,955 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33219
2023-06-03 05:41:42,955 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33219
2023-06-03 05:41:42,955 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45219
2023-06-03 05:41:42,955 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:42,955 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:42,955 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:42,956 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-06-03 05:41:42,956 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-0s_q6kr1
2023-06-03 05:41:42,956 - distributed.worker - INFO - Starting Worker plugin RMMSetup-407fda2a-a23a-4581-ab2f-06632372341a
2023-06-03 05:41:43,113 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ce4792f-e901-4bbb-bbf2-ee2018d5e89b
2023-06-03 05:41:43,113 - distributed.worker - INFO - Starting Worker plugin PreImport-ded592a1-1598-4db8-93f6-2d2ea980597e
2023-06-03 05:41:43,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,114 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c90fafd8-01f1-4d7e-b0ce-03bf274bd3d1
2023-06-03 05:41:43,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,114 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0e83fc09-8624-40fe-874c-c86769c38d98
2023-06-03 05:41:43,114 - distributed.worker - INFO - Starting Worker plugin PreImport-60e4aad8-8796-4d42-9d1c-55ed074b4c35
2023-06-03 05:41:43,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,114 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2be64c1a-b27b-42f2-8609-2f9795ddd6e5
2023-06-03 05:41:43,114 - distributed.worker - INFO - Starting Worker plugin PreImport-298e607a-c9d9-494e-87d9-7e3cc48a0f83
2023-06-03 05:41:43,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,114 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b946abc6-0f99-4db0-93e6-80e2cacbe87e
2023-06-03 05:41:43,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,114 - distributed.worker - INFO - Starting Worker plugin PreImport-c2b6d5c0-89d5-43bc-a9ab-bb2c74aac3e2
2023-06-03 05:41:43,114 - distributed.worker - INFO - Starting Worker plugin PreImport-a756b0ae-d12f-4627-9d5b-6f720ac65dd6
2023-06-03 05:41:43,114 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,115 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,115 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,147 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33219', status: init, memory: 0, processing: 0>
2023-06-03 05:41:43,149 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33219
2023-06-03 05:41:43,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55366
2023-06-03 05:41:43,150 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:43,150 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,151 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35659', status: init, memory: 0, processing: 0>
2023-06-03 05:41:43,151 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35659
2023-06-03 05:41:43,151 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55350
2023-06-03 05:41:43,152 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:43,152 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,152 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34197', status: init, memory: 0, processing: 0>
2023-06-03 05:41:43,152 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:43,152 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34197
2023-06-03 05:41:43,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55354
2023-06-03 05:41:43,153 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:43,153 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38281', status: init, memory: 0, processing: 0>
2023-06-03 05:41:43,153 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,154 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38281
2023-06-03 05:41:43,154 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55348
2023-06-03 05:41:43,154 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:43,154 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:43,154 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,155 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35799', status: init, memory: 0, processing: 0>
2023-06-03 05:41:43,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:43,156 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35799
2023-06-03 05:41:43,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55372
2023-06-03 05:41:43,156 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:43,156 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:43,157 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43735', status: init, memory: 0, processing: 0>
2023-06-03 05:41:43,157 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43735
2023-06-03 05:41:43,158 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55390
2023-06-03 05:41:43,158 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:43,158 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,158 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42201', status: init, memory: 0, processing: 0>
2023-06-03 05:41:43,159 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42201
2023-06-03 05:41:43,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55384
2023-06-03 05:41:43,159 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:43,160 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:43,160 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,160 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40781', status: init, memory: 0, processing: 0>
2023-06-03 05:41:43,161 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40781
2023-06-03 05:41:43,161 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:55376
2023-06-03 05:41:43,161 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:43,161 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:43,162 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:43,163 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:43,165 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:43,254 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:41:43,254 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:41:43,255 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:41:43,255 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:41:43,255 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:41:43,255 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:41:43,256 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:41:43,256 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-06-03 05:41:43,271 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:43,271 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:43,271 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:43,272 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:43,272 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:43,272 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:43,272 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:43,272 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:43,278 - distributed.scheduler - INFO - Remove client Client-4d0cb8f7-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:43,278 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36052; closing.
2023-06-03 05:41:43,278 - distributed.scheduler - INFO - Remove client Client-4d0cb8f7-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:43,279 - distributed.scheduler - INFO - Close client connection: Client-4d0cb8f7-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:43,279 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33931'. Reason: nanny-close
2023-06-03 05:41:43,280 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:43,280 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34387'. Reason: nanny-close
2023-06-03 05:41:43,281 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:43,281 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34195'. Reason: nanny-close
2023-06-03 05:41:43,282 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:43,282 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40781. Reason: nanny-close
2023-06-03 05:41:43,282 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37815'. Reason: nanny-close
2023-06-03 05:41:43,282 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43735. Reason: nanny-close
2023-06-03 05:41:43,282 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:43,283 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46425'. Reason: nanny-close
2023-06-03 05:41:43,283 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:43,283 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35659. Reason: nanny-close
2023-06-03 05:41:43,283 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39229'. Reason: nanny-close
2023-06-03 05:41:43,283 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38281. Reason: nanny-close
2023-06-03 05:41:43,283 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:43,284 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33489'. Reason: nanny-close
2023-06-03 05:41:43,284 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:43,284 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42201. Reason: nanny-close
2023-06-03 05:41:43,284 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33887'. Reason: nanny-close
2023-06-03 05:41:43,284 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:43,284 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35799. Reason: nanny-close
2023-06-03 05:41:43,285 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:43,285 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:43,285 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55376; closing.
2023-06-03 05:41:43,285 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:43,285 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34197. Reason: nanny-close
2023-06-03 05:41:43,285 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:43,285 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40781', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:43,285 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40781
2023-06-03 05:41:43,286 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33219. Reason: nanny-close
2023-06-03 05:41:43,286 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55390; closing.
2023-06-03 05:41:43,286 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:43,286 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:43,286 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:43,287 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:43,287 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:43,287 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43735', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:43,287 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:43735
2023-06-03 05:41:43,287 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:43,287 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55350; closing.
2023-06-03 05:41:43,287 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:43,287 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55348; closing.
2023-06-03 05:41:43,288 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:43,288 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:43,288 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:43,288 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35659', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:43,289 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35659
2023-06-03 05:41:43,289 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:43,289 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:43,289 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38281', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:43,289 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:38281
2023-06-03 05:41:43,289 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55384; closing.
2023-06-03 05:41:43,290 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:55390>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 269, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-06-03 05:41:43,292 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42201', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:43,292 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:42201
2023-06-03 05:41:43,293 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55354; closing.
2023-06-03 05:41:43,293 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55372; closing.
2023-06-03 05:41:43,293 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:55366; closing.
2023-06-03 05:41:43,293 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34197', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:43,294 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:34197
2023-06-03 05:41:43,294 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35799', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:43,294 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:35799
2023-06-03 05:41:43,294 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33219', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:43,294 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:33219
2023-06-03 05:41:43,294 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:41:45,099 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:41:45,100 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:41:45,101 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:41:45,102 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:41:45,103 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-06-03 05:41:47,401 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:47,406 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40487 instead
  warnings.warn(
2023-06-03 05:41:47,410 - distributed.scheduler - INFO - State start
2023-06-03 05:41:47,433 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:47,434 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:41:47,434 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40487/status
2023-06-03 05:41:47,674 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44883'
2023-06-03 05:41:49,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:49,347 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:49,740 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:50,084 - distributed.scheduler - INFO - Receive client connection: Client-538f93c7-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:50,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57680
2023-06-03 05:41:52,199 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36503
2023-06-03 05:41:52,200 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36503
2023-06-03 05:41:52,200 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45193
2023-06-03 05:41:52,200 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:52,200 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:52,200 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:52,200 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-03 05:41:52,200 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-ot40rzfs
2023-06-03 05:41:52,201 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eab2527f-03d4-4fe4-a752-b8e0add78060
2023-06-03 05:41:52,321 - distributed.worker - INFO - Starting Worker plugin PreImport-df80f561-85d5-456d-874f-a812da1c086d
2023-06-03 05:41:52,321 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-227a19bf-0e94-4874-bc48-ed9de702ce7e
2023-06-03 05:41:52,322 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:52,350 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36503', status: init, memory: 0, processing: 0>
2023-06-03 05:41:52,352 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36503
2023-06-03 05:41:52,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57702
2023-06-03 05:41:52,352 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:52,352 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:52,355 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:52,390 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:41:52,394 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:52,396 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:52,398 - distributed.scheduler - INFO - Remove client Client-538f93c7-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:52,398 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57680; closing.
2023-06-03 05:41:52,398 - distributed.scheduler - INFO - Remove client Client-538f93c7-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:52,399 - distributed.scheduler - INFO - Close client connection: Client-538f93c7-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:52,400 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44883'. Reason: nanny-close
2023-06-03 05:41:52,400 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:52,402 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36503. Reason: nanny-close
2023-06-03 05:41:52,404 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57702; closing.
2023-06-03 05:41:52,404 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:52,404 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36503', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:52,404 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:36503
2023-06-03 05:41:52,405 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:41:52,405 - distributed.nanny - INFO - Worker closed
2023-06-03 05:41:53,517 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:41:53,517 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:41:53,518 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:41:53,518 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:41:53,519 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-06-03 05:41:55,765 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:55,769 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40345 instead
  warnings.warn(
2023-06-03 05:41:55,774 - distributed.scheduler - INFO - State start
2023-06-03 05:41:55,794 - distributed.scheduler - INFO - -----------------------------------------------
2023-06-03 05:41:55,796 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-06-03 05:41:55,796 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40345/status
2023-06-03 05:41:55,932 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43201'
2023-06-03 05:41:56,437 - distributed.scheduler - INFO - Receive client connection: Client-587c338e-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:56,452 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57810
2023-06-03 05:41:57,672 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:41:57,672 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:41:57,698 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-06-03 05:41:58,818 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40619
2023-06-03 05:41:58,818 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40619
2023-06-03 05:41:58,818 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44245
2023-06-03 05:41:58,818 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-06-03 05:41:58,818 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:58,818 - distributed.worker - INFO -               Threads:                          1
2023-06-03 05:41:58,818 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-06-03 05:41:58,818 - distributed.worker - INFO -       Local Directory: /tmp/dask-worker-space/worker-30os6m6o
2023-06-03 05:41:58,819 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4a11de76-2061-4eb8-9cdf-4f112881b5a4
2023-06-03 05:41:58,944 - distributed.worker - INFO - Starting Worker plugin PreImport-8291d65a-4cdf-475d-9046-68c40c510066
2023-06-03 05:41:58,944 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d0d96299-a5e3-4167-9574-aed0b44b2363
2023-06-03 05:41:58,945 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:58,981 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40619', status: init, memory: 0, processing: 0>
2023-06-03 05:41:58,982 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40619
2023-06-03 05:41:58,982 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:57820
2023-06-03 05:41:58,983 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-06-03 05:41:58,983 - distributed.worker - INFO - -------------------------------------------------
2023-06-03 05:41:58,986 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-06-03 05:41:59,010 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-06-03 05:41:59,016 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-06-03 05:41:59,021 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:59,023 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-06-03 05:41:59,025 - distributed.scheduler - INFO - Remove client Client-587c338e-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:59,026 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57810; closing.
2023-06-03 05:41:59,026 - distributed.scheduler - INFO - Remove client Client-587c338e-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:59,027 - distributed.scheduler - INFO - Close client connection: Client-587c338e-01d1-11ee-8dad-d8c49764f6bb
2023-06-03 05:41:59,027 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43201'. Reason: nanny-close
2023-06-03 05:41:59,028 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-06-03 05:41:59,030 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40619. Reason: nanny-close
2023-06-03 05:41:59,032 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-06-03 05:41:59,032 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:57820; closing.
2023-06-03 05:41:59,032 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40619', status: closing, memory: 0, processing: 0>
2023-06-03 05:41:59,032 - distributed.core - INFO - Removing comms to tcp://127.0.0.1:40619
2023-06-03 05:41:59,033 - distributed.scheduler - INFO - Lost all workers
2023-06-03 05:41:59,033 - distributed.nanny - INFO - Worker closed
2023-06-03 05:42:00,395 - distributed._signals - INFO - Received signal SIGINT (2)
2023-06-03 05:42:00,395 - distributed.scheduler - INFO - Scheduler closing...
2023-06-03 05:42:00,396 - distributed.scheduler - INFO - Scheduler closing all comms
2023-06-03 05:42:00,397 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-06-03 05:42:00,397 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42537 instead
  warnings.warn(
2023-06-03 05:42:11,058 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:11,058 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:11,119 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:11,119 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:11,123 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:11,123 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:11,126 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:11,126 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:11,169 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:11,169 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:11,171 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:11,171 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:11,188 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:11,188 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:11,219 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:11,219 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36573 instead
  warnings.warn(
2023-06-03 05:42:22,757 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:22,757 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:22,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:22,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:22,760 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:22,761 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:22,770 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:22,770 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:22,798 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:22,798 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:22,800 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:22,800 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:22,800 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:22,800 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:22,900 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:22,900 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42487 instead
  warnings.warn(
2023-06-03 05:42:32,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:32,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:32,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:32,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:32,991 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:32,991 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:32,992 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:32,992 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:33,026 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:33,026 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:33,026 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:33,027 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:33,044 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:33,045 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:33,112 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:33,112 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45577 instead
  warnings.warn(
2023-06-03 05:42:43,722 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:43,722 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:43,727 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:43,728 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:43,741 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:43,741 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:43,744 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:43,744 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:43,801 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:43,801 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:43,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:43,805 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:43,810 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:43,811 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:43,817 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:43,817 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37353 instead
  warnings.warn(
2023-06-03 05:42:56,987 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:56,987 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:57,004 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:57,004 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:57,005 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:57,006 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:57,016 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:57,016 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:57,063 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:57,063 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:57,083 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:57,083 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:57,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:57,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:42:57,267 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:42:57,268 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:04,081 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #015] ep: 0x7fd54081c0c0, tag: 0x1a00afb32fcee4a3, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1265, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #015] ep: 0x7fd54081c0c0, tag: 0x1a00afb32fcee4a3, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-03 05:43:04,088 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #015] ep: 0x7f13c00c00c0, tag: 0xd9384b74778b0d32, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1244, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 413, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1265, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #015] ep: 0x7f13c00c00c0, tag: 0xd9384b74778b0d32, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40111 instead
  warnings.warn(
2023-06-03 05:43:12,147 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:12,147 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:12,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:12,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:12,205 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:12,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:12,216 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:12,217 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:12,232 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:12,232 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:12,255 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:12,255 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:12,261 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:12,262 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:12,313 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:12,313 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:15,642 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1389, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 719, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 213, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 525, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 142, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 684, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 340, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 408, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-03 05:43:15,668 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-50' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:118> exception=RuntimeError('Nanny failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 119, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2023-06-03 05:43:16,802 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOLoop object at 0x7fc84260e970>>, <Task finished name='Task-49' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:341> exception=RuntimeError('Worker failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 382, in _correct_state_internal
    await w  # for tornado gen.coroutine support
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 525, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 536, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 368, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 442, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 718, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 866, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 930, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 549, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 544, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2023-06-03 05:43:18,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:18,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:18,781 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:18,781 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:18,796 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:18,796 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:18,800 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:18,800 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:18,860 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:18,860 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:18,904 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:18,904 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:18,918 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:18,919 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:18,942 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-03 05:43:18,942 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-03 05:43:22,210 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 1
2023-06-03 05:43:22,212 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 1
2023-06-03 05:43:22,256 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 7
2023-06-03 05:43:22,257 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 7
2023-06-03 05:43:22,257 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 6
2023-06-03 05:43:22,258 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 6
2023-06-03 05:43:22,260 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 4
2023-06-03 05:43:22,260 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 4
2023-06-03 05:43:22,263 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 3
2023-06-03 05:43:22,264 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 3
2023-06-03 05:43:22,272 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 2
2023-06-03 05:43:22,273 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 2
2023-06-03 05:43:22,390 - distributed.scheduler - WARNING - Worker tried to connect with a duplicate name: 5
2023-06-03 05:43:22,391 - distributed.worker - ERROR - Unable to connect to scheduler: name taken, 5
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 93 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
