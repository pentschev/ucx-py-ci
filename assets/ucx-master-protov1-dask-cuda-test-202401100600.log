============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-10 06:26:29,800 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:29,805 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:26:29,808 - distributed.scheduler - INFO - State start
2024-01-10 06:26:29,930 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:29,931 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-10 06:26:29,932 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:26:29,932 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:26:30,019 - distributed.scheduler - INFO - Receive client connection: Client-2fa50626-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:30,032 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45190
2024-01-10 06:26:30,103 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36449'
2024-01-10 06:26:30,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42217'
2024-01-10 06:26:30,125 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37181'
2024-01-10 06:26:30,133 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34143'
2024-01-10 06:26:31,931 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:31,932 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:31,936 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:31,937 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37639
2024-01-10 06:26:31,937 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37639
2024-01-10 06:26:31,937 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35831
2024-01-10 06:26:31,937 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:26:31,937 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:31,937 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:26:31,937 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:26:31,937 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-zbyusd7g
2024-01-10 06:26:31,937 - distributed.worker - INFO - Starting Worker plugin PreImport-f666e420-8b6c-4388-bd5d-ffd5b1076a11
2024-01-10 06:26:31,938 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-388afaef-dd1f-467d-97c7-ac387f22a783
2024-01-10 06:26:31,938 - distributed.worker - INFO - Starting Worker plugin RMMSetup-be6f9638-f012-4b0b-9826-6c33a46466db
2024-01-10 06:26:31,938 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:31,946 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:31,946 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:31,948 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:31,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:31,950 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:31,950 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:31,950 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:31,950 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42275
2024-01-10 06:26:31,950 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42275
2024-01-10 06:26:31,951 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44973
2024-01-10 06:26:31,951 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:26:31,951 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:31,951 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:26:31,951 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:26:31,951 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ek0n4smy
2024-01-10 06:26:31,951 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1be17be7-26fb-4e2a-a470-37b52bc7f54a
2024-01-10 06:26:31,951 - distributed.worker - INFO - Starting Worker plugin PreImport-439d8274-db7c-4d1a-b725-1aaed0a5ff03
2024-01-10 06:26:31,951 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fe62fa55-8f4a-4d2b-9882-449a615fb5e6
2024-01-10 06:26:31,951 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:31,953 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:31,953 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36365
2024-01-10 06:26:31,953 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36365
2024-01-10 06:26:31,953 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34011
2024-01-10 06:26:31,954 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:26:31,954 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:31,954 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:26:31,954 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:26:31,954 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-9y75j28y
2024-01-10 06:26:31,954 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:31,954 - distributed.worker - INFO - Starting Worker plugin PreImport-62734191-9af4-4af1-8c79-a6c3cc67f8cf
2024-01-10 06:26:31,954 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-84283be1-52cf-4311-a683-71b7b51415f0
2024-01-10 06:26:31,954 - distributed.worker - INFO - Starting Worker plugin RMMSetup-19c215dc-29e0-4092-8080-9a6c095368b0
2024-01-10 06:26:31,955 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:31,955 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32861
2024-01-10 06:26:31,955 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32861
2024-01-10 06:26:31,955 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39135
2024-01-10 06:26:31,955 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:26:31,955 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:31,955 - distributed.worker - INFO -               Threads:                          4
2024-01-10 06:26:31,955 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-10 06:26:31,955 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-0j05tuwz
2024-01-10 06:26:31,955 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3956de64-e189-4ec8-9dd5-d91c2ebcd32d
2024-01-10 06:26:31,956 - distributed.worker - INFO - Starting Worker plugin PreImport-6f2aaa87-d20a-4ffa-985b-2c75dbc4a63f
2024-01-10 06:26:31,956 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-00068352-6068-48bd-8e09-3ebc85b3ba29
2024-01-10 06:26:31,956 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:32,042 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37639', status: init, memory: 0, processing: 0>
2024-01-10 06:26:32,045 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37639
2024-01-10 06:26:32,045 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45258
2024-01-10 06:26:32,046 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:32,047 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:26:32,047 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:32,048 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:26:32,072 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42275', status: init, memory: 0, processing: 0>
2024-01-10 06:26:32,072 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42275
2024-01-10 06:26:32,072 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45272
2024-01-10 06:26:32,073 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:32,074 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:26:32,074 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:32,075 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:26:32,083 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32861', status: init, memory: 0, processing: 0>
2024-01-10 06:26:32,083 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32861
2024-01-10 06:26:32,084 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45282
2024-01-10 06:26:32,084 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36365', status: init, memory: 0, processing: 0>
2024-01-10 06:26:32,084 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:32,085 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36365
2024-01-10 06:26:32,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45274
2024-01-10 06:26:32,085 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:26:32,085 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:32,086 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:32,086 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:26:32,087 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:26:32,087 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:32,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:26:32,176 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:26:32,177 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:26:32,177 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:26:32,177 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-10 06:26:32,182 - distributed.scheduler - INFO - Remove client Client-2fa50626-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:32,182 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45190; closing.
2024-01-10 06:26:32,182 - distributed.scheduler - INFO - Remove client Client-2fa50626-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:32,182 - distributed.scheduler - INFO - Close client connection: Client-2fa50626-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:32,183 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36449'. Reason: nanny-close
2024-01-10 06:26:32,184 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:32,184 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42217'. Reason: nanny-close
2024-01-10 06:26:32,185 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:32,185 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37181'. Reason: nanny-close
2024-01-10 06:26:32,185 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37639. Reason: nanny-close
2024-01-10 06:26:32,185 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:32,186 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34143'. Reason: nanny-close
2024-01-10 06:26:32,186 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:32,186 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32861. Reason: nanny-close
2024-01-10 06:26:32,186 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36365. Reason: nanny-close
2024-01-10 06:26:32,187 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42275. Reason: nanny-close
2024-01-10 06:26:32,188 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:26:32,188 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:26:32,188 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45258; closing.
2024-01-10 06:26:32,188 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:26:32,188 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37639', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867992.1888115')
2024-01-10 06:26:32,189 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:26:32,189 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45282; closing.
2024-01-10 06:26:32,189 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:32,189 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:32,190 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32861', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867992.190112')
2024-01-10 06:26:32,190 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:32,190 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45272; closing.
2024-01-10 06:26:32,190 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:32,191 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42275', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867992.1912751')
2024-01-10 06:26:32,191 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45274; closing.
2024-01-10 06:26:32,191 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:45282>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:26:32,193 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36365', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867992.193175')
2024-01-10 06:26:32,193 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:26:32,899 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:26:32,899 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:26:32,900 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:26:32,901 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-10 06:26:32,901 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-10 06:26:35,039 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:35,044 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:26:35,047 - distributed.scheduler - INFO - State start
2024-01-10 06:26:35,069 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:35,070 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:26:35,070 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:26:35,070 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:26:35,221 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34065'
2024-01-10 06:26:35,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34163'
2024-01-10 06:26:35,249 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43921'
2024-01-10 06:26:35,259 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41935'
2024-01-10 06:26:35,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39003'
2024-01-10 06:26:35,271 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37807'
2024-01-10 06:26:35,281 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33851'
2024-01-10 06:26:35,290 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42973'
2024-01-10 06:26:37,130 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:37,130 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:37,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:37,130 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:37,135 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:37,135 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:37,136 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36313
2024-01-10 06:26:37,136 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44511
2024-01-10 06:26:37,136 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36313
2024-01-10 06:26:37,136 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44511
2024-01-10 06:26:37,136 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41493
2024-01-10 06:26:37,136 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:37,136 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43773
2024-01-10 06:26:37,136 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:37,136 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:37,136 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:37,136 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:37,136 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:37,136 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:37,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7kb1qs4k
2024-01-10 06:26:37,136 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:37,136 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-51bdynuq
2024-01-10 06:26:37,136 - distributed.worker - INFO - Starting Worker plugin PreImport-b0cd9884-fefc-4149-b983-8eae4c899e74
2024-01-10 06:26:37,136 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-147af262-f344-4fd0-86b9-eb52543ac83d
2024-01-10 06:26:37,136 - distributed.worker - INFO - Starting Worker plugin PreImport-13f210cb-c030-4bcc-a826-4ef6d160239b
2024-01-10 06:26:37,137 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3fda43b9-c9d3-4ddd-8d34-d25f3f99cc7b
2024-01-10 06:26:37,137 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2f3d293a-b2f3-4864-b6b6-9fb2ef7d65ac
2024-01-10 06:26:37,137 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7de4091c-9acf-435b-b938-5756501e7005
2024-01-10 06:26:37,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:37,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:37,186 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:37,186 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:37,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:37,191 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:37,191 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44819
2024-01-10 06:26:37,191 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38733
2024-01-10 06:26:37,192 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44819
2024-01-10 06:26:37,192 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38733
2024-01-10 06:26:37,192 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34465
2024-01-10 06:26:37,192 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40987
2024-01-10 06:26:37,192 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:37,192 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:37,192 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:37,192 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:37,192 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:37,192 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:37,192 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:37,192 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:37,192 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6q8h69ap
2024-01-10 06:26:37,192 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1tw8vpkq
2024-01-10 06:26:37,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d76d3f2-3e8f-4df1-9bd0-0d99862bd983
2024-01-10 06:26:37,192 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0622e4d3-e159-4765-81df-8e986973494f
2024-01-10 06:26:37,195 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:37,195 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:37,200 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:37,201 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33427
2024-01-10 06:26:37,201 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33427
2024-01-10 06:26:37,201 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45993
2024-01-10 06:26:37,202 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:37,202 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:37,202 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:37,202 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:37,202 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xmbthaad
2024-01-10 06:26:37,202 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cefd9498-3c24-43dc-85a2-f9638cf8252c
2024-01-10 06:26:37,229 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:37,229 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:37,234 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:37,235 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41757
2024-01-10 06:26:37,235 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41757
2024-01-10 06:26:37,235 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35729
2024-01-10 06:26:37,235 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:37,235 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:37,235 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:37,235 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:37,235 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wksd48j2
2024-01-10 06:26:37,235 - distributed.worker - INFO - Starting Worker plugin RMMSetup-57b1e35e-620e-4649-8150-b5c80ac33050
2024-01-10 06:26:37,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:37,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:37,252 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:37,253 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33537
2024-01-10 06:26:37,253 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33537
2024-01-10 06:26:37,253 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44643
2024-01-10 06:26:37,253 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:37,253 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:37,253 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:37,253 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:37,254 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-a7wl3se6
2024-01-10 06:26:37,254 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d1641032-50ca-455d-a996-c9469104d64c
2024-01-10 06:26:37,262 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:37,262 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:37,266 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:37,267 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45443
2024-01-10 06:26:37,267 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45443
2024-01-10 06:26:37,267 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37701
2024-01-10 06:26:37,267 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:37,267 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:37,267 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:37,267 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:37,267 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m76wsljm
2024-01-10 06:26:37,267 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2aee4f24-81af-460f-b401-3375ff4467f3
2024-01-10 06:26:37,999 - distributed.scheduler - INFO - Receive client connection: Client-32ce627c-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:38,013 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42336
2024-01-10 06:26:39,123 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,147 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44511', status: init, memory: 0, processing: 0>
2024-01-10 06:26:39,148 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44511
2024-01-10 06:26:39,148 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42342
2024-01-10 06:26:39,149 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:39,150 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:39,150 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,151 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:39,236 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,268 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36313', status: init, memory: 0, processing: 0>
2024-01-10 06:26:39,269 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36313
2024-01-10 06:26:39,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42354
2024-01-10 06:26:39,270 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:39,272 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:39,272 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:39,316 - distributed.worker - INFO - Starting Worker plugin PreImport-2b262b0c-8d31-4541-b3c9-a284409bc4c5
2024-01-10 06:26:39,317 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-964b00da-6490-46f1-a2bb-35d798a36989
2024-01-10 06:26:39,318 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,340 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38733', status: init, memory: 0, processing: 0>
2024-01-10 06:26:39,341 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38733
2024-01-10 06:26:39,341 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42356
2024-01-10 06:26:39,342 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:39,343 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:39,343 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,344 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:39,426 - distributed.worker - INFO - Starting Worker plugin PreImport-aaaa12bc-53a1-4558-97ee-cdc090276218
2024-01-10 06:26:39,426 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f994b36-3d17-4c03-a7bf-b65085092985
2024-01-10 06:26:39,426 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,448 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44819', status: init, memory: 0, processing: 0>
2024-01-10 06:26:39,449 - distributed.worker - INFO - Starting Worker plugin PreImport-b6664d92-3e7f-414e-828a-b101abfe9ea7
2024-01-10 06:26:39,449 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44819
2024-01-10 06:26:39,449 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b523ba8-81d5-4b88-acd8-f4c59d59239b
2024-01-10 06:26:39,449 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42366
2024-01-10 06:26:39,450 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:39,450 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,451 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:39,451 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,452 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:39,453 - distributed.worker - INFO - Starting Worker plugin PreImport-d1337ce9-5617-4036-9ddc-5c36cda794a1
2024-01-10 06:26:39,454 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-34838645-ad88-48d0-a369-cd4e34d1db1d
2024-01-10 06:26:39,454 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,465 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-03aa6bbc-46fd-4f7e-b88b-ea4125b4fc90
2024-01-10 06:26:39,466 - distributed.worker - INFO - Starting Worker plugin PreImport-3e641f66-d5e0-4b1b-9ee7-d6177daa43bb
2024-01-10 06:26:39,467 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,468 - distributed.worker - INFO - Starting Worker plugin PreImport-492dc4c1-5d80-4c5d-a5de-b31083454ab9
2024-01-10 06:26:39,469 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5bb30f12-4234-46a9-b9c2-788dba920f99
2024-01-10 06:26:39,470 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,476 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33537', status: init, memory: 0, processing: 0>
2024-01-10 06:26:39,476 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33537
2024-01-10 06:26:39,476 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42372
2024-01-10 06:26:39,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:39,478 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:39,478 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,479 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:39,487 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41757', status: init, memory: 0, processing: 0>
2024-01-10 06:26:39,487 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41757
2024-01-10 06:26:39,487 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42368
2024-01-10 06:26:39,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:39,490 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:39,490 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,493 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:39,507 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33427', status: init, memory: 0, processing: 0>
2024-01-10 06:26:39,507 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33427
2024-01-10 06:26:39,508 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42382
2024-01-10 06:26:39,509 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:39,511 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:39,511 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,514 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:39,516 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45443', status: init, memory: 0, processing: 0>
2024-01-10 06:26:39,516 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45443
2024-01-10 06:26:39,516 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42396
2024-01-10 06:26:39,518 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:39,520 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:39,520 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:39,522 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:39,624 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:39,624 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:39,625 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:39,625 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:39,625 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:39,625 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:39,625 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:39,626 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:39,630 - distributed.scheduler - INFO - Remove client Client-32ce627c-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:39,630 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42336; closing.
2024-01-10 06:26:39,630 - distributed.scheduler - INFO - Remove client Client-32ce627c-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:39,631 - distributed.scheduler - INFO - Close client connection: Client-32ce627c-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:39,632 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34065'. Reason: nanny-close
2024-01-10 06:26:39,632 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:39,632 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34163'. Reason: nanny-close
2024-01-10 06:26:39,633 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:39,633 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43921'. Reason: nanny-close
2024-01-10 06:26:39,634 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33427. Reason: nanny-close
2024-01-10 06:26:39,634 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:39,634 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41935'. Reason: nanny-close
2024-01-10 06:26:39,634 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:39,634 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36313. Reason: nanny-close
2024-01-10 06:26:39,634 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39003'. Reason: nanny-close
2024-01-10 06:26:39,635 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44511. Reason: nanny-close
2024-01-10 06:26:39,635 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:39,635 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37807'. Reason: nanny-close
2024-01-10 06:26:39,635 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44819. Reason: nanny-close
2024-01-10 06:26:39,635 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:39,635 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33851'. Reason: nanny-close
2024-01-10 06:26:39,636 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41757. Reason: nanny-close
2024-01-10 06:26:39,636 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:39,636 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42973'. Reason: nanny-close
2024-01-10 06:26:39,636 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:39,636 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45443. Reason: nanny-close
2024-01-10 06:26:39,637 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38733. Reason: nanny-close
2024-01-10 06:26:39,637 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42382; closing.
2024-01-10 06:26:39,637 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:39,637 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:39,637 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33427', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867999.6374142')
2024-01-10 06:26:39,637 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:39,637 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33537. Reason: nanny-close
2024-01-10 06:26:39,638 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42342; closing.
2024-01-10 06:26:39,638 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:39,638 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44511', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867999.6385717')
2024-01-10 06:26:39,638 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:39,638 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:39,639 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:39,639 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:39,639 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:39,639 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:39,639 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42354; closing.
2024-01-10 06:26:39,639 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:39,640 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42366; closing.
2024-01-10 06:26:39,640 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:39,640 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:39,641 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:39,641 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:39,640 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42342>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:26:39,642 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42368; closing.
2024-01-10 06:26:39,642 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36313', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867999.6426427')
2024-01-10 06:26:39,643 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44819', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867999.6430898')
2024-01-10 06:26:39,643 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:39,643 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42356; closing.
2024-01-10 06:26:39,644 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867999.6439967')
2024-01-10 06:26:39,644 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38733', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867999.644514')
2024-01-10 06:26:39,644 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42396; closing.
2024-01-10 06:26:39,645 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42372; closing.
2024-01-10 06:26:39,645 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45443', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867999.6455271')
2024-01-10 06:26:39,646 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33537', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704867999.6459332')
2024-01-10 06:26:39,646 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:26:40,648 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:26:40,648 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:26:40,649 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:26:40,650 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:26:40,650 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-10 06:26:42,918 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:42,923 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:26:42,926 - distributed.scheduler - INFO - State start
2024-01-10 06:26:42,950 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:42,951 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:26:42,952 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:26:42,952 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:26:43,111 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38287'
2024-01-10 06:26:43,126 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41865'
2024-01-10 06:26:43,141 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36077'
2024-01-10 06:26:43,144 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34769'
2024-01-10 06:26:43,153 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39555'
2024-01-10 06:26:43,163 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33697'
2024-01-10 06:26:43,175 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36339'
2024-01-10 06:26:43,185 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45259'
2024-01-10 06:26:43,904 - distributed.scheduler - INFO - Receive client connection: Client-377d247e-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:43,920 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38882
2024-01-10 06:26:45,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:45,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:45,045 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:45,046 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44721
2024-01-10 06:26:45,046 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44721
2024-01-10 06:26:45,046 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43051
2024-01-10 06:26:45,046 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:45,046 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:45,046 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:45,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:45,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hay68_u6
2024-01-10 06:26:45,046 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c7d68ec2-afd1-40db-adab-65cbba3c33af
2024-01-10 06:26:45,051 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:45,051 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:45,055 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:45,056 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46749
2024-01-10 06:26:45,056 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46749
2024-01-10 06:26:45,056 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46295
2024-01-10 06:26:45,056 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:45,056 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:45,056 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:45,056 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:45,056 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tfel2mpf
2024-01-10 06:26:45,056 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a4c94744-3aa9-4884-8baf-fc1d3aa208f6
2024-01-10 06:26:45,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:45,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:45,084 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:45,084 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:45,086 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:45,086 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:45,088 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:45,088 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:45,088 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:45,088 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:45,088 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:45,088 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:45,089 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37197
2024-01-10 06:26:45,089 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37197
2024-01-10 06:26:45,089 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36463
2024-01-10 06:26:45,089 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:45,089 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:45,089 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:45,089 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34535
2024-01-10 06:26:45,089 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:45,089 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iit0s_dl
2024-01-10 06:26:45,089 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34535
2024-01-10 06:26:45,089 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43801
2024-01-10 06:26:45,089 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:45,089 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:45,089 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8b89920-96cc-4c7b-9ccd-db5afcd0c59b
2024-01-10 06:26:45,089 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:45,089 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:45,090 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i9zheg41
2024-01-10 06:26:45,090 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a1fdb70d-670c-42ca-ace4-9b248b895434
2024-01-10 06:26:45,090 - distributed.worker - INFO - Starting Worker plugin PreImport-5fd49fe9-372e-407f-92e4-defa51bbf553
2024-01-10 06:26:45,090 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7d84421d-3bfe-4b6e-aafd-224bb79e4eac
2024-01-10 06:26:45,090 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:45,091 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34477
2024-01-10 06:26:45,091 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34477
2024-01-10 06:26:45,091 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33663
2024-01-10 06:26:45,092 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:45,092 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:45,092 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:45,092 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:45,092 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5sp8yrl2
2024-01-10 06:26:45,092 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b5d5b551-d4da-4aa4-958d-3afe321a7d2e
2024-01-10 06:26:45,092 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:45,092 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:45,093 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34323
2024-01-10 06:26:45,093 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43803
2024-01-10 06:26:45,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34323
2024-01-10 06:26:45,093 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43803
2024-01-10 06:26:45,093 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45811
2024-01-10 06:26:45,093 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:45,093 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42321
2024-01-10 06:26:45,093 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:45,093 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:45,093 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:45,093 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:45,093 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:45,093 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:45,093 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:45,093 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5fzc36rk
2024-01-10 06:26:45,093 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_bqee3ls
2024-01-10 06:26:45,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f3bac64-b27b-413c-bd97-a8e04c2d69da
2024-01-10 06:26:45,094 - distributed.worker - INFO - Starting Worker plugin PreImport-7a3e1970-d732-493e-9f1e-07458631becb
2024-01-10 06:26:45,094 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6dfb3260-6e89-44d4-a61c-0b81fe46f48b
2024-01-10 06:26:45,094 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fee1c8af-0fd6-4f59-a6da-543d6da3642f
2024-01-10 06:26:45,143 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:45,143 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:45,148 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:45,148 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34601
2024-01-10 06:26:45,149 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34601
2024-01-10 06:26:45,149 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33529
2024-01-10 06:26:45,149 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:45,149 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:45,149 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:45,149 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:45,149 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vrdd2rs8
2024-01-10 06:26:45,149 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ff9ceaa0-dfdd-4dd3-a65c-504a9adc9bd7
2024-01-10 06:26:46,860 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ab5cd5c-59f4-4392-bf3a-63ff44b944b8
2024-01-10 06:26:46,861 - distributed.worker - INFO - Starting Worker plugin PreImport-d8171ac3-5703-46d0-b1ec-92a403f5c737
2024-01-10 06:26:46,862 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:46,899 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44721', status: init, memory: 0, processing: 0>
2024-01-10 06:26:46,900 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44721
2024-01-10 06:26:46,900 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38904
2024-01-10 06:26:46,901 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:46,902 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:46,902 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:46,904 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:46,933 - distributed.worker - INFO - Starting Worker plugin PreImport-a10b99d5-361a-4641-9ef3-5a6ee48f8079
2024-01-10 06:26:46,933 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a7f8b5e-72ea-4d17-8d3c-2b950470aad5
2024-01-10 06:26:46,934 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:46,957 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46749', status: init, memory: 0, processing: 0>
2024-01-10 06:26:46,958 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46749
2024-01-10 06:26:46,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38918
2024-01-10 06:26:46,959 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:46,959 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:46,959 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:46,961 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:47,024 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,031 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,058 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43803', status: init, memory: 0, processing: 0>
2024-01-10 06:26:47,059 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43803
2024-01-10 06:26:47,059 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38934
2024-01-10 06:26:47,060 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34535', status: init, memory: 0, processing: 0>
2024-01-10 06:26:47,060 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:47,060 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34535
2024-01-10 06:26:47,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38926
2024-01-10 06:26:47,061 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:47,061 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,062 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:47,062 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:47,063 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:47,063 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,065 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:47,079 - distributed.worker - INFO - Starting Worker plugin PreImport-a07ddabe-4e37-4ec0-8d5b-92fc67898c55
2024-01-10 06:26:47,079 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6157f79-cccb-47b4-af78-5206a100c09b
2024-01-10 06:26:47,081 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,105 - distributed.worker - INFO - Starting Worker plugin PreImport-60fd0166-ef99-4f0a-8471-b724b17264cd
2024-01-10 06:26:47,106 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7ca0ef31-f921-470e-a7cc-9417931e3eef
2024-01-10 06:26:47,107 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,108 - distributed.worker - INFO - Starting Worker plugin PreImport-47d15980-e4d6-427e-8e3c-a932a290ee60
2024-01-10 06:26:47,109 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60966397-1c89-46af-8d2f-e54f32c02021
2024-01-10 06:26:47,111 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,110 - distributed.worker - INFO - Starting Worker plugin PreImport-a0daa7be-4fcb-42b8-a1a5-467521506d01
2024-01-10 06:26:47,111 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bf846b3b-8ee0-4b36-8d43-c8b74fe258b2
2024-01-10 06:26:47,111 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,116 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37197', status: init, memory: 0, processing: 0>
2024-01-10 06:26:47,117 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37197
2024-01-10 06:26:47,117 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38946
2024-01-10 06:26:47,118 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:47,120 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:47,120 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,122 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:47,131 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34477', status: init, memory: 0, processing: 0>
2024-01-10 06:26:47,131 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34477
2024-01-10 06:26:47,131 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38960
2024-01-10 06:26:47,132 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:47,133 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:47,133 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:47,136 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34601', status: init, memory: 0, processing: 0>
2024-01-10 06:26:47,136 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34601
2024-01-10 06:26:47,136 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38966
2024-01-10 06:26:47,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:47,138 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:47,138 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:47,149 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34323', status: init, memory: 0, processing: 0>
2024-01-10 06:26:47,150 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34323
2024-01-10 06:26:47,150 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38968
2024-01-10 06:26:47,152 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:47,153 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:47,153 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:47,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:47,206 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:47,206 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:47,206 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:47,206 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:47,206 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:47,206 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:47,207 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:47,207 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:47,211 - distributed.scheduler - INFO - Remove client Client-377d247e-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:47,211 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38882; closing.
2024-01-10 06:26:47,212 - distributed.scheduler - INFO - Remove client Client-377d247e-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:47,212 - distributed.scheduler - INFO - Close client connection: Client-377d247e-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:47,213 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38287'. Reason: nanny-close
2024-01-10 06:26:47,213 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:47,213 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41865'. Reason: nanny-close
2024-01-10 06:26:47,214 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:47,214 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36077'. Reason: nanny-close
2024-01-10 06:26:47,215 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44721. Reason: nanny-close
2024-01-10 06:26:47,215 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:47,215 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34769'. Reason: nanny-close
2024-01-10 06:26:47,215 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:47,215 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39555'. Reason: nanny-close
2024-01-10 06:26:47,215 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34323. Reason: nanny-close
2024-01-10 06:26:47,215 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46749. Reason: nanny-close
2024-01-10 06:26:47,216 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:47,216 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33697'. Reason: nanny-close
2024-01-10 06:26:47,216 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43803. Reason: nanny-close
2024-01-10 06:26:47,216 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:47,216 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36339'. Reason: nanny-close
2024-01-10 06:26:47,216 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:47,216 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34535. Reason: nanny-close
2024-01-10 06:26:47,217 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45259'. Reason: nanny-close
2024-01-10 06:26:47,217 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:47,217 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37197. Reason: nanny-close
2024-01-10 06:26:47,217 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:47,217 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38904; closing.
2024-01-10 06:26:47,217 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34477. Reason: nanny-close
2024-01-10 06:26:47,217 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:47,217 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44721', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868007.2178156')
2024-01-10 06:26:47,218 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34601. Reason: nanny-close
2024-01-10 06:26:47,218 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:47,218 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:47,218 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38918; closing.
2024-01-10 06:26:47,219 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:47,219 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:47,219 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:47,219 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:47,219 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:47,219 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46749', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868007.2198558')
2024-01-10 06:26:47,220 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38934; closing.
2024-01-10 06:26:47,220 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:47,220 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38968; closing.
2024-01-10 06:26:47,220 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:47,220 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:47,221 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:47,221 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43803', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868007.2214746')
2024-01-10 06:26:47,221 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34323', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868007.221859')
2024-01-10 06:26:47,222 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:47,222 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:47,222 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38926; closing.
2024-01-10 06:26:47,223 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34535', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868007.2230554')
2024-01-10 06:26:47,223 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:47,223 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38960; closing.
2024-01-10 06:26:47,223 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38946; closing.
2024-01-10 06:26:47,224 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:38966; closing.
2024-01-10 06:26:47,224 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868007.2243161')
2024-01-10 06:26:47,224 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37197', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868007.2247555')
2024-01-10 06:26:47,225 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34601', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868007.2251043')
2024-01-10 06:26:47,225 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:26:48,079 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:26:48,079 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:26:48,080 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:26:48,081 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:26:48,082 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-10 06:26:50,233 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:50,237 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:26:50,240 - distributed.scheduler - INFO - State start
2024-01-10 06:26:50,263 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:50,264 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:26:50,265 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:26:50,265 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:26:50,428 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33961'
2024-01-10 06:26:50,446 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44667'
2024-01-10 06:26:50,455 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35295'
2024-01-10 06:26:50,470 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37299'
2024-01-10 06:26:50,473 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33271'
2024-01-10 06:26:50,481 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33621'
2024-01-10 06:26:50,490 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44427'
2024-01-10 06:26:50,501 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39769'
2024-01-10 06:26:50,735 - distributed.scheduler - INFO - Receive client connection: Client-3bd44eff-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:50,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42162
2024-01-10 06:26:52,317 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:52,317 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:52,321 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:52,322 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41537
2024-01-10 06:26:52,322 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41537
2024-01-10 06:26:52,322 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45803
2024-01-10 06:26:52,322 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:52,322 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:52,322 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:52,322 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:52,322 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tq6kdeu9
2024-01-10 06:26:52,323 - distributed.worker - INFO - Starting Worker plugin PreImport-86b9cbcc-42a9-4bb1-913e-535dd3cd50be
2024-01-10 06:26:52,323 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2a80c58c-1991-48a1-89d4-f361487c5bf8
2024-01-10 06:26:52,323 - distributed.worker - INFO - Starting Worker plugin RMMSetup-260629c7-b9ee-450a-a6ce-d412a5874c89
2024-01-10 06:26:52,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:52,335 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:52,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:52,335 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:52,337 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:52,337 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:52,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:52,339 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:52,340 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41413
2024-01-10 06:26:52,340 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45959
2024-01-10 06:26:52,340 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45959
2024-01-10 06:26:52,340 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41413
2024-01-10 06:26:52,340 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37891
2024-01-10 06:26:52,340 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35547
2024-01-10 06:26:52,340 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:52,340 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:52,340 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:52,340 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:52,340 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:52,340 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:52,340 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:52,340 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:52,340 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bh8acsow
2024-01-10 06:26:52,340 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3u10qxu_
2024-01-10 06:26:52,340 - distributed.worker - INFO - Starting Worker plugin PreImport-5c704f82-004b-42ec-b27f-f9ecd7c29593
2024-01-10 06:26:52,340 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2ffb2d04-c734-4320-99c4-d51378fd401d
2024-01-10 06:26:52,340 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-571cc5b6-711c-4b57-8e02-39b22d038762
2024-01-10 06:26:52,340 - distributed.worker - INFO - Starting Worker plugin RMMSetup-afe98530-3843-4866-8839-f45e84029f08
2024-01-10 06:26:52,341 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:52,342 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44183
2024-01-10 06:26:52,342 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44183
2024-01-10 06:26:52,342 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45253
2024-01-10 06:26:52,342 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:52,342 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:52,342 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:52,342 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:52,342 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pjcc4av2
2024-01-10 06:26:52,342 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f6b5ad1f-92c1-4591-9806-5f18ac984914
2024-01-10 06:26:52,343 - distributed.worker - INFO - Starting Worker plugin PreImport-1586e697-95ea-47b1-94c7-18e7f11d0d69
2024-01-10 06:26:52,344 - distributed.worker - INFO - Starting Worker plugin RMMSetup-af2a93e2-3738-41d4-852c-6330a22098f4
2024-01-10 06:26:52,400 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:52,401 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:52,405 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:52,406 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36271
2024-01-10 06:26:52,406 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36271
2024-01-10 06:26:52,406 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39705
2024-01-10 06:26:52,406 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:52,406 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:52,406 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:52,406 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:52,406 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jhkzprh2
2024-01-10 06:26:52,406 - distributed.worker - INFO - Starting Worker plugin RMMSetup-daa86182-ce9b-4cb9-8468-9f7bb0280d61
2024-01-10 06:26:52,420 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:52,420 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:52,425 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:52,426 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32993
2024-01-10 06:26:52,426 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32993
2024-01-10 06:26:52,426 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33653
2024-01-10 06:26:52,426 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:52,426 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:52,426 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:52,426 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:52,426 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pxco43d9
2024-01-10 06:26:52,426 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0e09991d-b6c4-4362-a54b-8669a0d95f58
2024-01-10 06:26:52,426 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:52,426 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:52,431 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:52,432 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36583
2024-01-10 06:26:52,432 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36583
2024-01-10 06:26:52,432 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37353
2024-01-10 06:26:52,432 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:52,432 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:52,432 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:52,432 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:52,432 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n4v0paxs
2024-01-10 06:26:52,432 - distributed.worker - INFO - Starting Worker plugin RMMSetup-df8b7d0d-a62b-428e-90aa-015f000488cb
2024-01-10 06:26:52,433 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:26:52,433 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:26:52,437 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:26:52,438 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37123
2024-01-10 06:26:52,438 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37123
2024-01-10 06:26:52,438 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45079
2024-01-10 06:26:52,438 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:26:52,438 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:52,438 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:26:52,438 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:26:52,439 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_sn9bqv6
2024-01-10 06:26:52,439 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6d9e0f79-f59a-47ea-bbcd-8c47d4d2071e
2024-01-10 06:26:54,590 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,625 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41537', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,627 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41537
2024-01-10 06:26:54,627 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42178
2024-01-10 06:26:54,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,629 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:54,629 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:54,658 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,659 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,660 - distributed.worker - INFO - Starting Worker plugin PreImport-99e4a832-9500-4b70-a056-c7688345fe30
2024-01-10 06:26:54,661 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-23c7a7c3-20ba-462f-bfdf-5dc14b37fa8e
2024-01-10 06:26:54,662 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,668 - distributed.worker - INFO - Starting Worker plugin PreImport-186505d3-c222-4ab3-b79d-d5e8d4d30c82
2024-01-10 06:26:54,669 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-16c605f8-d97b-44ce-9e11-235587006794
2024-01-10 06:26:54,669 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,685 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41413', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,686 - distributed.worker - INFO - Starting Worker plugin PreImport-ef2a2adb-861e-470e-b8d6-fec6e60d9be9
2024-01-10 06:26:54,686 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41413
2024-01-10 06:26:54,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42186
2024-01-10 06:26:54,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f2df2e6-b88b-4ffd-bf7d-8f5a17bb3d52
2024-01-10 06:26:54,687 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,688 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:54,688 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,689 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:54,693 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36271', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,694 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36271
2024-01-10 06:26:54,694 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42214
2024-01-10 06:26:54,694 - distributed.worker - INFO - Starting Worker plugin PreImport-8a721e05-9240-4543-9ae3-746e027d936b
2024-01-10 06:26:54,695 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2266740-4836-48bf-aff9-51ee9aabeedf
2024-01-10 06:26:54,695 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,695 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45959', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,695 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45959
2024-01-10 06:26:54,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42198
2024-01-10 06:26:54,696 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:54,696 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,697 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44183', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,697 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44183
2024-01-10 06:26:54,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:54,697 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42200
2024-01-10 06:26:54,698 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:54,698 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,699 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,700 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:54,700 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:54,702 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:54,702 - distributed.worker - INFO - Starting Worker plugin PreImport-a176364e-962b-4f4e-8aaa-e8a52c7b583d
2024-01-10 06:26:54,702 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1e7d02d-5128-46af-bcb9-95bd5571676e
2024-01-10 06:26:54,703 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,725 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36583', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,725 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36583
2024-01-10 06:26:54,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42234
2024-01-10 06:26:54,726 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32993', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,726 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,727 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32993
2024-01-10 06:26:54,727 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42230
2024-01-10 06:26:54,727 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:54,727 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,727 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37123', status: init, memory: 0, processing: 0>
2024-01-10 06:26:54,728 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37123
2024-01-10 06:26:54,728 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42244
2024-01-10 06:26:54,728 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,729 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:54,729 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:26:54,730 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:54,730 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:26:54,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:26:54,731 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:54,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:26:54,820 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:54,820 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:54,820 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:54,820 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:54,820 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:54,820 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:54,821 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:54,821 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:26:54,833 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:26:54,833 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:26:54,833 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:26:54,833 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:26:54,833 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:26:54,833 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:26:54,833 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:26:54,834 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:26:54,842 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:26:54,843 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:26:54,846 - distributed.scheduler - INFO - Remove client Client-3bd44eff-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:54,846 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42162; closing.
2024-01-10 06:26:54,846 - distributed.scheduler - INFO - Remove client Client-3bd44eff-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:54,846 - distributed.scheduler - INFO - Close client connection: Client-3bd44eff-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:26:54,847 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33961'. Reason: nanny-close
2024-01-10 06:26:54,848 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,848 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44667'. Reason: nanny-close
2024-01-10 06:26:54,849 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,849 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35295'. Reason: nanny-close
2024-01-10 06:26:54,849 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45959. Reason: nanny-close
2024-01-10 06:26:54,849 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,849 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37299'. Reason: nanny-close
2024-01-10 06:26:54,850 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,850 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41537. Reason: nanny-close
2024-01-10 06:26:54,850 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33271'. Reason: nanny-close
2024-01-10 06:26:54,850 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41413. Reason: nanny-close
2024-01-10 06:26:54,850 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,850 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33621'. Reason: nanny-close
2024-01-10 06:26:54,850 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37123. Reason: nanny-close
2024-01-10 06:26:54,851 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,851 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44427'. Reason: nanny-close
2024-01-10 06:26:54,851 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,851 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44183. Reason: nanny-close
2024-01-10 06:26:54,851 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39769'. Reason: nanny-close
2024-01-10 06:26:54,851 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42198; closing.
2024-01-10 06:26:54,851 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:54,852 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:26:54,852 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32993. Reason: nanny-close
2024-01-10 06:26:54,852 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36583. Reason: nanny-close
2024-01-10 06:26:54,852 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:54,852 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45959', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.8522422')
2024-01-10 06:26:54,852 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:54,852 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:54,852 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36271. Reason: nanny-close
2024-01-10 06:26:54,853 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42244; closing.
2024-01-10 06:26:54,853 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42186; closing.
2024-01-10 06:26:54,853 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,853 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,854 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,854 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:54,854 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:54,854 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,854 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37123', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.8545713')
2024-01-10 06:26:54,855 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41413', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.8549452')
2024-01-10 06:26:54,855 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:54,855 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:26:54,855 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42178; closing.
2024-01-10 06:26:54,855 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,855 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,856 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,857 - distributed.nanny - INFO - Worker closed
2024-01-10 06:26:54,855 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42244>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:42244>: Stream is closed
2024-01-10 06:26:54,857 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41537', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.8577778')
2024-01-10 06:26:54,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42200; closing.
2024-01-10 06:26:54,858 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44183', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.8588953')
2024-01-10 06:26:54,859 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42234; closing.
2024-01-10 06:26:54,859 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42230; closing.
2024-01-10 06:26:54,859 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42214; closing.
2024-01-10 06:26:54,860 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36583', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.8601384')
2024-01-10 06:26:54,860 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32993', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.8605068')
2024-01-10 06:26:54,860 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36271', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868014.8608606')
2024-01-10 06:26:54,861 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:26:55,813 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:26:55,814 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:26:55,814 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:26:55,815 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:26:55,816 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-10 06:26:58,034 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:58,039 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41617 instead
  warnings.warn(
2024-01-10 06:26:58,043 - distributed.scheduler - INFO - State start
2024-01-10 06:26:58,066 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:26:58,067 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-10 06:26:58,068 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:26:58,069 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-10 06:26:58,262 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41311'
2024-01-10 06:26:58,285 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34549'
2024-01-10 06:26:58,306 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37283'
2024-01-10 06:26:58,308 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33685'
2024-01-10 06:26:58,319 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46235'
2024-01-10 06:26:58,329 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34295'
2024-01-10 06:26:58,344 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41787'
2024-01-10 06:26:58,356 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37935'
2024-01-10 06:27:00,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:00,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:00,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:00,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:00,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:00,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:00,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:00,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:00,252 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:00,253 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:00,253 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37265
2024-01-10 06:27:00,253 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37265
2024-01-10 06:27:00,253 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41545
2024-01-10 06:27:00,253 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:00,253 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:00,253 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:00,253 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:00,253 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oelqy82n
2024-01-10 06:27:00,253 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40261
2024-01-10 06:27:00,254 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40261
2024-01-10 06:27:00,254 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-40b5de56-ac79-41b6-bbc1-9421ce4cc12d
2024-01-10 06:27:00,254 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37959
2024-01-10 06:27:00,254 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:00,254 - distributed.worker - INFO - Starting Worker plugin PreImport-3e8812cf-1f2b-424e-b247-41515efb113f
2024-01-10 06:27:00,254 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:00,254 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:00,254 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fb861161-00b7-4d20-a82a-2c6fac6007ea
2024-01-10 06:27:00,254 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:00,254 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4ygmp048
2024-01-10 06:27:00,254 - distributed.worker - INFO - Starting Worker plugin PreImport-39ca832a-abff-4e6a-bbc8-0467189cb263
2024-01-10 06:27:00,254 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-43603adf-1953-4ea9-8207-6c63609f6f4c
2024-01-10 06:27:00,254 - distributed.worker - INFO - Starting Worker plugin RMMSetup-50009ce9-7f5e-4e31-ad26-ae0ee679bb8e
2024-01-10 06:27:00,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:00,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44121
2024-01-10 06:27:00,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44121
2024-01-10 06:27:00,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41969
2024-01-10 06:27:00,256 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:00,256 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:00,256 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:00,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:00,256 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:00,256 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7lni44ca
2024-01-10 06:27:00,256 - distributed.worker - INFO - Starting Worker plugin PreImport-18fc40e3-7336-4f41-851c-9b6d010165ad
2024-01-10 06:27:00,256 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7067219-a9cc-40ea-be3c-5132362c7dc9
2024-01-10 06:27:00,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-09aa181b-4d5e-4d4a-b844-0e0af91c9376
2024-01-10 06:27:00,257 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33105
2024-01-10 06:27:00,257 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33105
2024-01-10 06:27:00,257 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37511
2024-01-10 06:27:00,257 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:00,257 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:00,257 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:00,257 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:00,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vqgekvgn
2024-01-10 06:27:00,258 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8a5adca5-0ad0-4ede-a242-624d31c1497d
2024-01-10 06:27:00,273 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:00,273 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:00,275 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:00,275 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:00,277 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:00,278 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46091
2024-01-10 06:27:00,278 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46091
2024-01-10 06:27:00,278 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37091
2024-01-10 06:27:00,278 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:00,278 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:00,278 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:00,278 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:00,278 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-404s2bav
2024-01-10 06:27:00,279 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38d42fa9-ed29-46a8-a8ba-fe3876ac2d50
2024-01-10 06:27:00,279 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:00,279 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:00,279 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:00,280 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41025
2024-01-10 06:27:00,280 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41025
2024-01-10 06:27:00,280 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41269
2024-01-10 06:27:00,280 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:00,280 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:00,280 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:00,280 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:00,280 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0e4psra_
2024-01-10 06:27:00,280 - distributed.worker - INFO - Starting Worker plugin PreImport-f6f27f65-7e27-4c16-a4dd-573c1fe00e0f
2024-01-10 06:27:00,280 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e5e8e26e-f642-4fb9-bfee-7f61e01685df
2024-01-10 06:27:00,281 - distributed.worker - INFO - Starting Worker plugin RMMSetup-41603357-bdf7-4b55-ba3e-2cf36b15542d
2024-01-10 06:27:00,283 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:00,283 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:00,283 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:00,284 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34683
2024-01-10 06:27:00,284 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34683
2024-01-10 06:27:00,284 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38607
2024-01-10 06:27:00,284 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:00,285 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:00,285 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:00,285 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:00,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ytjvyf6l
2024-01-10 06:27:00,285 - distributed.worker - INFO - Starting Worker plugin RMMSetup-55612f03-e178-452d-bbfd-2efb4818aba0
2024-01-10 06:27:00,287 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:00,288 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38703
2024-01-10 06:27:00,288 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38703
2024-01-10 06:27:00,288 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36251
2024-01-10 06:27:00,288 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:00,288 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:00,289 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:00,289 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:00,289 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-63xddu9t
2024-01-10 06:27:00,289 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8927d384-1e6b-4c23-a99c-dd4f79aa6114
2024-01-10 06:27:04,172 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41311'. Reason: nanny-close
2024-01-10 06:27:04,173 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34549'. Reason: nanny-close
2024-01-10 06:27:04,173 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37283'. Reason: nanny-close
2024-01-10 06:27:04,173 - distributed.worker - INFO - Starting Worker plugin PreImport-1644b8c7-f0f9-4f0b-8bf3-3a650cfc3e07
2024-01-10 06:27:04,173 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33685'. Reason: nanny-close
2024-01-10 06:27:04,173 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46235'. Reason: nanny-close
2024-01-10 06:27:04,174 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34295'. Reason: nanny-close
2024-01-10 06:27:04,174 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41787'. Reason: nanny-close
2024-01-10 06:27:04,174 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f9ee76fe-b6d5-461b-adfb-29ab8c667c7a
2024-01-10 06:27:04,174 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37935'. Reason: nanny-close
2024-01-10 06:27:04,174 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,199 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,201 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:04,202 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:04,203 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,204 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:04,224 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,225 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38703. Reason: nanny-close
2024-01-10 06:27:04,227 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,227 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,229 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,234 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,242 - distributed.worker - INFO - Starting Worker plugin PreImport-334831a2-6e57-47bd-ae4c-050484385dea
2024-01-10 06:27:04,242 - distributed.worker - INFO - Starting Worker plugin PreImport-746aece7-d4d8-4aa3-8d85-7e22357514c4
2024-01-10 06:27:04,242 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-30744305-2626-46c8-9ae8-b4fc3b8250a9
2024-01-10 06:27:04,242 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0fda9841-5bbc-4b86-a667-7d01f141c173
2024-01-10 06:27:04,243 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,243 - distributed.worker - INFO - Starting Worker plugin PreImport-50140ca9-8744-4463-9f7c-2c2dbde0c227
2024-01-10 06:27:04,243 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,243 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61a3e808-b865-47e5-9521-4562bf5ea59c
2024-01-10 06:27:04,244 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,250 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,252 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:04,253 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:04,254 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,256 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:04,263 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:04,264 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:04,264 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,266 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:04,270 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:04,272 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:04,272 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,272 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:04,272 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:04,273 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:04,274 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:04,278 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,279 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,279 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37265. Reason: nanny-close
2024-01-10 06:27:04,280 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41025. Reason: nanny-close
2024-01-10 06:27:04,282 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,283 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,283 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,285 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,288 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:04,290 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:04,290 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,290 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:04,291 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:04,291 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,292 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:04,293 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:04,294 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:04,296 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:04,296 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:04,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:04,325 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,325 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,325 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,325 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,326 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40261. Reason: nanny-close
2024-01-10 06:27:04,326 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46091. Reason: nanny-close
2024-01-10 06:27:04,327 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44121. Reason: nanny-close
2024-01-10 06:27:04,327 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33105. Reason: nanny-close
2024-01-10 06:27:04,328 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:04,328 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,329 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,329 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,330 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34683. Reason: nanny-close
2024-01-10 06:27:04,330 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,330 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,331 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,331 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,332 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:04,332 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:04,334 - distributed.nanny - INFO - Worker closed
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-10 06:27:07,662 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:07,666 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33725 instead
  warnings.warn(
2024-01-10 06:27:07,670 - distributed.scheduler - INFO - State start
2024-01-10 06:27:07,692 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:07,693 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-01-10 06:27:07,693 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:07,694 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4027, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 630, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-01-10 06:27:07,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39959'
2024-01-10 06:27:07,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46087'
2024-01-10 06:27:07,846 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46163'
2024-01-10 06:27:07,862 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44667'
2024-01-10 06:27:07,865 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39645'
2024-01-10 06:27:07,876 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32903'
2024-01-10 06:27:07,887 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35795'
2024-01-10 06:27:07,897 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43601'
2024-01-10 06:27:10,019 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,019 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,026 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,028 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42865
2024-01-10 06:27:10,028 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42865
2024-01-10 06:27:10,028 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35249
2024-01-10 06:27:10,028 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,028 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,028 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,028 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,028 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-igfedwse
2024-01-10 06:27:10,028 - distributed.worker - INFO - Starting Worker plugin PreImport-94367500-3da4-4f24-8e41-d85544bee26e
2024-01-10 06:27:10,029 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e011d376-31dd-4e9a-adf7-bfd53585632b
2024-01-10 06:27:10,029 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ee1f0ce8-6d55-4b61-bd50-60c53067d869
2024-01-10 06:27:10,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,031 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,033 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,035 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,036 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35605
2024-01-10 06:27:10,036 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35605
2024-01-10 06:27:10,037 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38605
2024-01-10 06:27:10,037 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,037 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,037 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,037 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,037 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v58uaa_x
2024-01-10 06:27:10,037 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42504c14-6a3f-4bc3-8d45-47ac406dfb2b
2024-01-10 06:27:10,038 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,039 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,039 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,040 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35677
2024-01-10 06:27:10,040 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35677
2024-01-10 06:27:10,040 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34341
2024-01-10 06:27:10,040 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,040 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,040 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,040 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,040 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h0ur4xel
2024-01-10 06:27:10,040 - distributed.worker - INFO - Starting Worker plugin PreImport-e55a5137-6827-4b6b-a489-22418e223adc
2024-01-10 06:27:10,041 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e1bdd19c-d339-46b4-9a13-aaba45a37de2
2024-01-10 06:27:10,041 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a49cfecd-c0f2-49ff-8262-e136095f98fd
2024-01-10 06:27:10,041 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,043 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:10,043 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:10,044 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,044 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,044 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40143
2024-01-10 06:27:10,045 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40143
2024-01-10 06:27:10,045 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33199
2024-01-10 06:27:10,045 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,045 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,045 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,045 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,045 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-su80gr44
2024-01-10 06:27:10,045 - distributed.worker - INFO - Starting Worker plugin PreImport-ca1f3417-ba35-427e-8c0b-75ee28f113c2
2024-01-10 06:27:10,045 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32807
2024-01-10 06:27:10,045 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f6d2be9b-7e9c-4684-9ce6-a9c72006e324
2024-01-10 06:27:10,045 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,045 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32807
2024-01-10 06:27:10,045 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0aed59f-e0eb-420b-8f23-14bcde610b37
2024-01-10 06:27:10,045 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42449
2024-01-10 06:27:10,045 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,045 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,045 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,045 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,046 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-07a0brki
2024-01-10 06:27:10,046 - distributed.worker - INFO - Starting Worker plugin PreImport-39145eb7-c89f-46c8-bd32-26dec5964a8b
2024-01-10 06:27:10,046 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bbc5abf0-6bef-4a1e-ad9b-be6d6caa5645
2024-01-10 06:27:10,046 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37281
2024-01-10 06:27:10,046 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37281
2024-01-10 06:27:10,046 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35551
2024-01-10 06:27:10,046 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,046 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,046 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,046 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,047 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ir9689ia
2024-01-10 06:27:10,047 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35131
2024-01-10 06:27:10,047 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97a5aac5-9c9c-470a-860a-e26bdce07345
2024-01-10 06:27:10,047 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35131
2024-01-10 06:27:10,047 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65546e11-236f-46f4-b0b1-cdd212f25851
2024-01-10 06:27:10,047 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43923
2024-01-10 06:27:10,047 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,047 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,047 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,047 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,047 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ghs_4qs
2024-01-10 06:27:10,048 - distributed.worker - INFO - Starting Worker plugin RMMSetup-198201e9-fb28-4be1-a818-244a4074cc58
2024-01-10 06:27:10,049 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:10,051 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38185
2024-01-10 06:27:10,051 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38185
2024-01-10 06:27:10,051 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32855
2024-01-10 06:27:10,051 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:10,051 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:10,051 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:10,051 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:27:10,051 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3lpzcr2y
2024-01-10 06:27:10,052 - distributed.worker - INFO - Starting Worker plugin RMMSetup-06dfb7fd-e64c-46a2-b578-b3779229b716
2024-01-10 06:27:13,544 - distributed.worker - INFO - Starting Worker plugin PreImport-1ccf7605-332e-4048-8769-b76b7b3fb13b
2024-01-10 06:27:13,545 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1321ef3b-b5d8-47b1-804a-d00be7e687e2
2024-01-10 06:27:13,546 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,579 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,580 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,580 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,582 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,617 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,636 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,640 - distributed.worker - INFO - Starting Worker plugin PreImport-7c13d534-c92c-48fe-b2f6-ae4d82d4797b
2024-01-10 06:27:13,641 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-922d8129-d45d-437e-be62-30c33222f72a
2024-01-10 06:27:13,641 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,642 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,642 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,642 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,644 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,668 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,670 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,670 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,672 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,672 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,673 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,673 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,675 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,675 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,680 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,686 - distributed.worker - INFO - Starting Worker plugin PreImport-d85a8ffa-fb85-4022-b406-9114c377228e
2024-01-10 06:27:13,686 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27f1cd2d-a6b0-4079-8bd8-19e5f444f5ce
2024-01-10 06:27:13,688 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,689 - distributed.worker - INFO - Starting Worker plugin PreImport-55bb91a8-a1af-46a5-a204-ba1ad751bce5
2024-01-10 06:27:13,690 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6bf2e124-016a-449f-90a6-833f839b657d
2024-01-10 06:27:13,690 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,700 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,701 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,701 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,703 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,705 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,707 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,707 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,708 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,722 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,723 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,723 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:13,729 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:13,730 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:13,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:13,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:25,125 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:25,125 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:25,125 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:25,126 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:25,125 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:25,126 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:25,126 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:25,126 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-10 06:27:25,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39959'. Reason: nanny-close
2024-01-10 06:27:25,135 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,135 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46087'. Reason: nanny-close
2024-01-10 06:27:25,136 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,136 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46163'. Reason: nanny-close
2024-01-10 06:27:25,136 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35677. Reason: nanny-close
2024-01-10 06:27:25,137 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44667'. Reason: nanny-close
2024-01-10 06:27:25,137 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,137 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39645'. Reason: nanny-close
2024-01-10 06:27:25,137 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42865. Reason: nanny-close
2024-01-10 06:27:25,138 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,138 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32903'. Reason: nanny-close
2024-01-10 06:27:25,138 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32807. Reason: nanny-close
2024-01-10 06:27:25,138 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35605. Reason: nanny-close
2024-01-10 06:27:25,138 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,138 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35795'. Reason: nanny-close
2024-01-10 06:27:25,139 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40143. Reason: nanny-close
2024-01-10 06:27:25,139 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,139 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,139 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43601'. Reason: nanny-close
2024-01-10 06:27:25,139 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:25,139 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38185. Reason: nanny-close
2024-01-10 06:27:25,140 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35131. Reason: nanny-close
2024-01-10 06:27:25,140 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,140 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,140 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,141 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,141 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37281. Reason: nanny-close
2024-01-10 06:27:25,141 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,141 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,142 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,142 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,142 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,142 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,143 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,143 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,143 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:25,144 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:25,145 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-10 06:27:28,355 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:28,359 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:27:28,362 - distributed.scheduler - INFO - State start
2024-01-10 06:27:28,385 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:28,385 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:27:28,386 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:27:28,386 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:27:28,412 - distributed.scheduler - INFO - Receive client connection: Client-528c787e-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:28,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60824
2024-01-10 06:27:28,503 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35925'
2024-01-10 06:27:28,553 - distributed.scheduler - INFO - Receive client connection: Client-52ad4d99-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:28,553 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60852
2024-01-10 06:27:30,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:30,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:30,989 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:30,989 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36209
2024-01-10 06:27:30,990 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36209
2024-01-10 06:27:30,990 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-10 06:27:30,990 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:30,990 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:30,990 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:30,990 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:27:30,990 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b_o9gthg
2024-01-10 06:27:30,990 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e5f56ed-613e-4940-bedb-4537d71fb6f2
2024-01-10 06:27:30,990 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8b50b24c-db08-4bd6-a99a-d13eae6610b3
2024-01-10 06:27:30,991 - distributed.worker - INFO - Starting Worker plugin PreImport-c6870c7c-1ca0-4788-9e3d-44b9ec0c4a29
2024-01-10 06:27:30,991 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:32,179 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36209', status: init, memory: 0, processing: 0>
2024-01-10 06:27:32,180 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36209
2024-01-10 06:27:32,180 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59446
2024-01-10 06:27:32,181 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:32,182 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:32,183 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:32,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:32,249 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:32,252 - distributed.scheduler - INFO - Remove client Client-528c787e-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:32,252 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60824; closing.
2024-01-10 06:27:32,252 - distributed.scheduler - INFO - Remove client Client-528c787e-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:32,253 - distributed.scheduler - INFO - Close client connection: Client-528c787e-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:32,254 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35925'. Reason: nanny-close
2024-01-10 06:27:32,254 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:32,255 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36209. Reason: nanny-close
2024-01-10 06:27:32,257 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:32,257 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59446; closing.
2024-01-10 06:27:32,257 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36209', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868052.257534')
2024-01-10 06:27:32,257 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:32,258 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:32,969 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:27:32,970 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:27:32,970 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:32,973 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:27:32,973 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-10 06:27:37,039 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:37,043 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:27:37,047 - distributed.scheduler - INFO - State start
2024-01-10 06:27:37,068 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:37,069 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:27:37,069 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:27:37,070 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:27:37,138 - distributed.scheduler - INFO - Receive client connection: Client-57bf92db-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:37,153 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60554
2024-01-10 06:27:37,206 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36701'
2024-01-10 06:27:37,537 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38651', status: init, memory: 0, processing: 0>
2024-01-10 06:27:37,538 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38651
2024-01-10 06:27:37,538 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60580
2024-01-10 06:27:37,568 - distributed.scheduler - INFO - Remove client Client-57bf92db-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:37,568 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60554; closing.
2024-01-10 06:27:37,568 - distributed.scheduler - INFO - Remove client Client-57bf92db-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:37,569 - distributed.scheduler - INFO - Close client connection: Client-57bf92db-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:37,570 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36701'. Reason: nanny-close
2024-01-10 06:27:37,594 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60580; closing.
2024-01-10 06:27:37,595 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38651', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868057.5952725')
2024-01-10 06:27:37,595 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:38,418 - distributed.scheduler - INFO - Receive client connection: Client-52ad4d99-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:38,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60586
2024-01-10 06:27:38,897 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:38,897 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:39,400 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:39,401 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44699
2024-01-10 06:27:39,401 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44699
2024-01-10 06:27:39,401 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33263
2024-01-10 06:27:39,401 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:39,401 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:39,401 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:39,401 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:27:39,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ua0mz0pr
2024-01-10 06:27:39,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b4e1e779-2aa5-4688-a47f-011af13b18d8
2024-01-10 06:27:39,402 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85bdd87c-7fd8-45f6-8443-57dcaf310490
2024-01-10 06:27:39,402 - distributed.worker - INFO - Starting Worker plugin PreImport-9f45a57f-bd07-47f9-ab0e-5f72c1ea7477
2024-01-10 06:27:39,404 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:39,461 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44699', status: init, memory: 0, processing: 0>
2024-01-10 06:27:39,462 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44699
2024-01-10 06:27:39,462 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:60612
2024-01-10 06:27:39,463 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:39,464 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:39,464 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:39,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:39,488 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:39,489 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44699. Reason: nanny-close
2024-01-10 06:27:39,491 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:60612; closing.
2024-01-10 06:27:39,491 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:39,491 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44699', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868059.4913747')
2024-01-10 06:27:39,491 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:39,492 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:40,240 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:27:40,240 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:27:40,241 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:40,242 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:27:40,243 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-10 06:27:42,164 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:42,168 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:27:42,171 - distributed.scheduler - INFO - State start
2024-01-10 06:27:42,192 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:42,193 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:27:42,194 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:27:42,194 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:27:42,408 - distributed.scheduler - INFO - Receive client connection: Client-52ad4d99-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:42,422 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40348
2024-01-10 06:27:44,510 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:40332'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4428, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:40332>: Stream is closed
2024-01-10 06:27:44,640 - distributed.scheduler - INFO - Remove client Client-52ad4d99-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:44,640 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40348; closing.
2024-01-10 06:27:44,641 - distributed.scheduler - INFO - Remove client Client-52ad4d99-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:44,641 - distributed.scheduler - INFO - Close client connection: Client-52ad4d99-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:44,729 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:27:44,730 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:27:44,730 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:44,731 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:27:44,731 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-10 06:27:46,820 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:46,824 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33303 instead
  warnings.warn(
2024-01-10 06:27:46,828 - distributed.scheduler - INFO - State start
2024-01-10 06:27:46,850 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:46,851 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-10 06:27:46,851 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:33303/status
2024-01-10 06:27:46,852 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:27:46,993 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37805'
2024-01-10 06:27:47,329 - distributed.scheduler - INFO - Receive client connection: Client-5d92edb2-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:47,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34494
2024-01-10 06:27:48,841 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:48,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:48,845 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:48,846 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36851
2024-01-10 06:27:48,846 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36851
2024-01-10 06:27:48,846 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39111
2024-01-10 06:27:48,846 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-10 06:27:48,846 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:48,846 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:48,847 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:27:48,847 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-pu2wpe92
2024-01-10 06:27:48,847 - distributed.worker - INFO - Starting Worker plugin RMMSetup-014aaa90-087f-46ff-b480-b00980f58ab4
2024-01-10 06:27:48,847 - distributed.worker - INFO - Starting Worker plugin PreImport-4b56e94b-3f70-431c-8212-bc606199b1d2
2024-01-10 06:27:48,847 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e95810cc-5541-482b-944a-32d458dee742
2024-01-10 06:27:48,847 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:49,600 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36851', status: init, memory: 0, processing: 0>
2024-01-10 06:27:49,601 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36851
2024-01-10 06:27:49,601 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:34506
2024-01-10 06:27:49,602 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:49,603 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-10 06:27:49,603 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:49,605 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-10 06:27:49,606 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:49,609 - distributed.scheduler - INFO - Remove client Client-5d92edb2-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:49,609 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34494; closing.
2024-01-10 06:27:49,609 - distributed.scheduler - INFO - Remove client Client-5d92edb2-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:49,610 - distributed.scheduler - INFO - Close client connection: Client-5d92edb2-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:49,611 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37805'. Reason: nanny-close
2024-01-10 06:27:49,631 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:49,633 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36851. Reason: nanny-close
2024-01-10 06:27:49,634 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-10 06:27:49,634 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:34506; closing.
2024-01-10 06:27:49,634 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36851', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868069.6349165')
2024-01-10 06:27:49,635 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:49,635 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:50,376 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:27:50,376 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:27:50,377 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:50,378 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-10 06:27:50,378 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 127, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 98, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 184, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/__init__.py", line 22, in <module>
    from distributed._version import get_versions
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 982, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 925, in _find_spec
  File "<frozen importlib._bootstrap_external>", line 1423, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1395, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1555, in find_spec
  File "<frozen importlib._bootstrap_external>", line 156, in _path_isfile
  File "<frozen importlib._bootstrap_external>", line 148, in _path_is_mode_type
  File "<frozen importlib._bootstrap_external>", line 142, in _path_stat
KeyboardInterrupt
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 127, in run_cli
    _register_command_ep(cli, ep)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 98, in _register_command_ep
    command = entry_point.load()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/importlib_metadata/__init__.py", line 184, in load
    module = import_module(match.group('module'))
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 972, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 850, in exec_module
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/__init__.py", line 23, in <module>
    from distributed.actor import Actor, ActorFuture, BaseActorFuture
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/actor.py", line 13, in <module>
    from distributed.client import Future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/client.py", line 50, in <module>
    from distributed.core import ErrorMessage, OKMessage
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 36, in <module>
    from distributed import profile, protocol
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/__init__.py", line 6, in <module>
    from distributed.protocol.core import decompress, dumps, loads, maybe_compress, msgpack
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 7, in <module>
    from distributed.protocol import pickle
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 11, in <module>
    from distributed.protocol.serialize import dask_deserialize, dask_serialize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/serialize.py", line 22, in <module>
    from distributed.protocol.utils import (
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/utils.py", line 28, in <module>
    import numpy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/__init__.py", line 151, in <module>
    from . import polynomial
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numpy/polynomial/__init__.py", line 121, in <module>
    from .laguerre import Laguerre
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 978, in get_code
  File "<frozen importlib._bootstrap_external>", line 647, in _compile_bytecode
KeyboardInterrupt
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-10 06:27:53,210 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:53,219 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:27:53,226 - distributed.scheduler - INFO - State start
2024-01-10 06:27:53,258 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:53,260 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:27:53,261 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:27:53,261 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:27:53,325 - distributed.scheduler - INFO - Receive client connection: Client-62325021-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:27:53,349 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46264
2024-01-10 06:27:53,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38753'
2024-01-10 06:27:53,584 - distributed.scheduler - INFO - Receive client connection: Client-61697a4a-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:53,584 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46290
2024-01-10 06:27:55,314 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:27:55,314 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:27:55,319 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:27:55,320 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34309
2024-01-10 06:27:55,320 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34309
2024-01-10 06:27:55,320 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42755
2024-01-10 06:27:55,320 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:27:55,320 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:55,320 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:27:55,320 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:27:55,320 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-driwdzjp
2024-01-10 06:27:55,320 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1eeba2b9-d723-4acb-8048-3186bcc42387
2024-01-10 06:27:55,611 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0a9eb9d9-3c17-47a8-abd2-d6444305bf41
2024-01-10 06:27:55,611 - distributed.worker - INFO - Starting Worker plugin PreImport-40be6277-8d6f-4f0f-a348-0a8780d08d40
2024-01-10 06:27:55,612 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:55,674 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34309', status: init, memory: 0, processing: 0>
2024-01-10 06:27:55,675 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34309
2024-01-10 06:27:55,675 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46364
2024-01-10 06:27:55,676 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:27:55,677 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:27:55,677 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:27:55,679 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:27:55,736 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:27:55,740 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:55,742 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:27:55,744 - distributed.scheduler - INFO - Remove client Client-61697a4a-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:55,744 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46290; closing.
2024-01-10 06:27:55,744 - distributed.scheduler - INFO - Remove client Client-61697a4a-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:55,745 - distributed.scheduler - INFO - Close client connection: Client-61697a4a-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:55,745 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38753'. Reason: nanny-close
2024-01-10 06:27:55,746 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:27:55,747 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34309. Reason: nanny-close
2024-01-10 06:27:55,749 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46364; closing.
2024-01-10 06:27:55,749 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:27:55,749 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34309', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868075.749529')
2024-01-10 06:27:55,749 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:55,750 - distributed.nanny - INFO - Worker closed
2024-01-10 06:27:56,461 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:27:56,462 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:27:56,463 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:27:56,465 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:27:56,466 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-10 06:27:58,823 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:58,828 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-10 06:27:58,831 - distributed.scheduler - INFO - State start
2024-01-10 06:27:58,887 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-10 06:27:58,889 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-10 06:27:58,890 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-10 06:27:58,890 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-10 06:27:58,917 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44435', status: init, memory: 0, processing: 0>
2024-01-10 06:27:58,936 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44435
2024-01-10 06:27:58,936 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46804
2024-01-10 06:27:58,937 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43255', status: init, memory: 0, processing: 0>
2024-01-10 06:27:58,938 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43255
2024-01-10 06:27:58,938 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46796
2024-01-10 06:27:58,940 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38119', status: init, memory: 0, processing: 0>
2024-01-10 06:27:58,940 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38119
2024-01-10 06:27:58,940 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46810
2024-01-10 06:27:58,942 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38757', status: init, memory: 0, processing: 0>
2024-01-10 06:27:58,943 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38757
2024-01-10 06:27:58,943 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46826
2024-01-10 06:27:58,950 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40247', status: init, memory: 0, processing: 0>
2024-01-10 06:27:58,950 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40247
2024-01-10 06:27:58,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46854
2024-01-10 06:27:58,951 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46810; closing.
2024-01-10 06:27:58,951 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38119', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868078.951303')
2024-01-10 06:27:58,952 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35099', status: init, memory: 0, processing: 0>
2024-01-10 06:27:58,953 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35099
2024-01-10 06:27:58,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46862
2024-01-10 06:27:58,969 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44609', status: init, memory: 0, processing: 0>
2024-01-10 06:27:58,970 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44609
2024-01-10 06:27:58,970 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46878
2024-01-10 06:27:58,993 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46796; closing.
2024-01-10 06:27:58,993 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43255', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868078.99386')
2024-01-10 06:27:58,994 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46862; closing.
2024-01-10 06:27:58,995 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35099', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868078.9958336')
2024-01-10 06:27:58,996 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46878; closing.
2024-01-10 06:27:58,997 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46804; closing.
2024-01-10 06:27:58,997 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46826; closing.
2024-01-10 06:27:58,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44609', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868078.9976082')
2024-01-10 06:27:58,997 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44435', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868078.9978766')
2024-01-10 06:27:58,998 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38757', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868078.9981925')
2024-01-10 06:27:59,000 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46854; closing.
2024-01-10 06:27:59,001 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40247', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868079.0009542')
2024-01-10 06:27:59,001 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:59,001 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:46854>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-10 06:27:59,023 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38411'
2024-01-10 06:27:59,033 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42587', status: init, memory: 0, processing: 0>
2024-01-10 06:27:59,034 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42587
2024-01-10 06:27:59,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46886
2024-01-10 06:27:59,044 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46886; closing.
2024-01-10 06:27:59,044 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42587', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868079.0444353')
2024-01-10 06:27:59,044 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:27:59,398 - distributed.scheduler - INFO - Receive client connection: Client-64ab2b02-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:27:59,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46924
2024-01-10 06:28:00,729 - distributed.scheduler - INFO - Receive client connection: Client-62325021-af81-11ee-bce9-d8c49764f6bb
2024-01-10 06:28:00,729 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39234
2024-01-10 06:28:00,844 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:28:00,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:28:00,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:28:00,849 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39565
2024-01-10 06:28:00,849 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39565
2024-01-10 06:28:00,849 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44763
2024-01-10 06:28:00,849 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-10 06:28:00,849 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:00,849 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:28:00,849 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:28:00,849 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-40ajvc4c
2024-01-10 06:28:00,850 - distributed.worker - INFO - Starting Worker plugin RMMSetup-37b1db0e-0846-48b0-b84b-9a143c5bc793
2024-01-10 06:28:01,148 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e80c6d18-1cad-4b1d-afab-f8a956157a2f
2024-01-10 06:28:01,148 - distributed.worker - INFO - Starting Worker plugin PreImport-1b966275-5674-4b21-811f-6518ee24efc7
2024-01-10 06:28:01,148 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:01,196 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39565', status: init, memory: 0, processing: 0>
2024-01-10 06:28:01,196 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39565
2024-01-10 06:28:01,196 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39236
2024-01-10 06:28:01,197 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:28:01,198 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-10 06:28:01,198 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:28:01,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-10 06:28:01,263 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-10 06:28:01,267 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-10 06:28:01,271 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:28:01,273 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:28:01,275 - distributed.scheduler - INFO - Remove client Client-64ab2b02-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:28:01,276 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46924; closing.
2024-01-10 06:28:01,276 - distributed.scheduler - INFO - Remove client Client-64ab2b02-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:28:01,276 - distributed.scheduler - INFO - Close client connection: Client-64ab2b02-af81-11ee-ba59-d8c49764f6bb
2024-01-10 06:28:01,277 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38411'. Reason: nanny-close
2024-01-10 06:28:01,278 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-10 06:28:01,279 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39565. Reason: nanny-close
2024-01-10 06:28:01,280 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-10 06:28:01,280 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39236; closing.
2024-01-10 06:28:01,281 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39565', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704868081.2810385')
2024-01-10 06:28:01,281 - distributed.scheduler - INFO - Lost all workers
2024-01-10 06:28:01,282 - distributed.nanny - INFO - Worker closed
2024-01-10 06:28:01,894 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-10 06:28:01,894 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-10 06:28:01,895 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-10 06:28:01,897 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-10 06:28:01,897 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45853 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39003 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37587 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37627 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42479 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37709 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44061 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46563 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38595 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37177 instead
  warnings.warn(
[1704868454.322632] [dgx13:74121:0]            sock.c:481  UCX  ERROR bind(fd=128 addr=0.0.0.0:33369) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46509 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] [1704868535.043320] [dgx13:75345:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:37624) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46657 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39285 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42819 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40895 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40673 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44555 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36447 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38681 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41011 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37645 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33767 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42941 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] [1704868887.761801] [dgx13:80396:0]            sock.c:481  UCX  ERROR bind(fd=130 addr=0.0.0.0:47001) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33965 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34141 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43747 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34379 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40929 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41753 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41191 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43117 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34365 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40993 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32945 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33595 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41253 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42047 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43335 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35063 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40613 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36509 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42819 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44965 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36493 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38231 instead
  warnings.warn(
2024-01-10 06:48:48,470 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-10 06:48:48,472 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
[1704869329.407764] [dgx13:87059] UCXPY  WARNING Listener object is being destroyed, but 2 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
[1704869329.407949] [dgx13:87059] UCXPY  WARNING Listener object is being destroyed, but 2 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46485 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46171 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41633 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39041 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41911 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39259 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38847 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] [1704869459.127189] [dgx13:64089:0]            sock.c:481  UCX  ERROR bind(fd=248 addr=0.0.0.0:39937) failed: Address already in use
[1704869464.248079] [dgx13:89196:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:43724) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33389 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39459 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39123 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39357 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42111 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35841 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33835 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37535 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1704869539.199572] [dgx13:89844:0]            sock.c:481  UCX  ERROR bind(fd=162 addr=0.0.0.0:59635) failed: Address already in use
2024-01-10 06:52:20,868 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #012] ep: 0x7f125c0840c0, tag: 0x6a3d5751f8da6c6a, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #012] ep: 0x7f125c0840c0, tag: 0x6a3d5751f8da6c6a, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] [1704869542.432133] [dgx13:64089:1]            sock.c:481  UCX  ERROR bind(fd=247 addr=0.0.0.0:43882) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] 2024-01-10 06:52:41,290 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-10 06:52:54,793 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:52:54,794 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:52:54,856 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:52:54,856 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:52:54,893 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:52:54,894 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:52:54,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:52:54,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:52:54,984 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:52:54,984 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:52:55,031 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:52:55,031 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:52:55,067 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:52:55,067 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:52:55,103 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:52:55,103 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:52:55,401 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:52:55,402 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43437
2024-01-10 06:52:55,402 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43437
2024-01-10 06:52:55,402 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40057
2024-01-10 06:52:55,402 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,402 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,402 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:52:55,402 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ctny_q_6
2024-01-10 06:52:55,402 - distributed.worker - INFO - Starting Worker plugin PreImport-f4207719-d740-4d4b-b7d2-7e80f0166657
2024-01-10 06:52:55,402 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0be55874-1756-4c09-bf20-489c33476160
2024-01-10 06:52:55,403 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6648f5fc-e539-4902-8d0e-f02f70acbee5
2024-01-10 06:52:55,403 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:52:55,470 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,470 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,471 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45579
2024-01-10 06:52:55,483 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:52:55,483 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45543
2024-01-10 06:52:55,483 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45543
2024-01-10 06:52:55,484 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41423
2024-01-10 06:52:55,484 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,484 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,484 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:52:55,484 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h_p1p7tv
2024-01-10 06:52:55,484 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-300221f3-f8bc-4000-9a8f-18bd87abe708
2024-01-10 06:52:55,484 - distributed.worker - INFO - Starting Worker plugin PreImport-da7fd0cc-cfc0-4874-8d69-e67762cea225
2024-01-10 06:52:55,485 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b468cf2d-5ff7-40fe-96ab-442cae38be6b
2024-01-10 06:52:55,485 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,516 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:52:55,517 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45769
2024-01-10 06:52:55,517 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45769
2024-01-10 06:52:55,517 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46693
2024-01-10 06:52:55,517 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,517 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,517 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:52:55,517 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_jl8mzd9
2024-01-10 06:52:55,518 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a000b973-d9a5-43c3-b03f-5d8b2eea606e
2024-01-10 06:52:55,518 - distributed.worker - INFO - Starting Worker plugin PreImport-80a42695-e242-44cc-bc2a-4b96d30d5711
2024-01-10 06:52:55,518 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6678d806-6d85-4885-b321-7c179cdaf614
2024-01-10 06:52:55,518 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,519 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:52:55,520 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39949
2024-01-10 06:52:55,520 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39949
2024-01-10 06:52:55,520 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45065
2024-01-10 06:52:55,520 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,520 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,520 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:52:55,520 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9ooql3d2
2024-01-10 06:52:55,521 - distributed.worker - INFO - Starting Worker plugin PreImport-6d0302df-e504-4ddf-af9d-04a708d46e2f
2024-01-10 06:52:55,521 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4608d2b-90f8-4803-a375-6a0e19af056e
2024-01-10 06:52:55,521 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0cf92bf0-1960-4a94-ab8b-2fa92a03bb95
2024-01-10 06:52:55,521 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,556 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:52:55,556 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,557 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,558 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45579
2024-01-10 06:52:55,605 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:52:55,606 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44771
2024-01-10 06:52:55,606 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44771
2024-01-10 06:52:55,607 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42949
2024-01-10 06:52:55,607 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,607 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,607 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:52:55,607 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-48nib2ub
2024-01-10 06:52:55,607 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-198b8336-ecee-4192-aec4-deade3c5923f
2024-01-10 06:52:55,607 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea91424f-e9dd-4b83-8a21-108947b91d8d
2024-01-10 06:52:55,607 - distributed.worker - INFO - Starting Worker plugin PreImport-4ceb5a8d-aa42-4a5a-ae0c-3448bd4eb49d
2024-01-10 06:52:55,608 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,615 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:52:55,616 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,616 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,617 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:52:55,617 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45579
2024-01-10 06:52:55,618 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,618 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,619 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45579
2024-01-10 06:52:55,652 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:52:55,653 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44601
2024-01-10 06:52:55,653 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44601
2024-01-10 06:52:55,653 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35435
2024-01-10 06:52:55,653 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,653 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,653 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:52:55,653 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ofi7t53n
2024-01-10 06:52:55,654 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-02d06798-ce2b-4e38-9b91-25cc40abbcf2
2024-01-10 06:52:55,654 - distributed.worker - INFO - Starting Worker plugin PreImport-c13167d7-011c-4dbe-844f-d49d9b46241b
2024-01-10 06:52:55,654 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8ed631a4-e5e9-4edd-8c0c-ad35d891c8e1
2024-01-10 06:52:55,654 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,674 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:52:55,675 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37551
2024-01-10 06:52:55,675 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37551
2024-01-10 06:52:55,675 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36309
2024-01-10 06:52:55,675 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,675 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,675 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:52:55,675 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zq6b9lct
2024-01-10 06:52:55,675 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-decb9e4e-91f7-4e57-848b-00a78bac8d4b
2024-01-10 06:52:55,676 - distributed.worker - INFO - Starting Worker plugin PreImport-a6a72ac3-1aec-4a67-a377-f2a8e23bd8c0
2024-01-10 06:52:55,676 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6411a016-c51e-46f4-a575-cb7dc1d45503
2024-01-10 06:52:55,676 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,676 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:52:55,677 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,677 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,678 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45579
2024-01-10 06:52:55,739 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:52:55,740 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,740 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,741 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45579
2024-01-10 06:52:55,749 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:52:55,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39621
2024-01-10 06:52:55,750 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39621
2024-01-10 06:52:55,750 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42207
2024-01-10 06:52:55,751 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,751 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,751 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:52:55,751 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m5vxh39j
2024-01-10 06:52:55,751 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a314bb03-0c9c-4ab8-abb2-a3c18eeda8af
2024-01-10 06:52:55,751 - distributed.worker - INFO - Starting Worker plugin PreImport-5d9bbc93-a712-4a30-86a2-1b9918855c08
2024-01-10 06:52:55,751 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b008c629-8d49-4b25-8fca-0b57f03ee627
2024-01-10 06:52:55,751 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,767 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:52:55,767 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,767 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,768 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45579
2024-01-10 06:52:55,810 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:52:55,811 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:45579
2024-01-10 06:52:55,811 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:52:55,812 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45579
2024-01-10 06:52:55,836 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:52:55,837 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:52:55,837 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:52:55,837 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:52:55,837 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:52:55,837 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:52:55,837 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:52:55,837 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-10 06:52:55,843 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43437. Reason: nanny-close
2024-01-10 06:52:55,843 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45543. Reason: nanny-close
2024-01-10 06:52:55,844 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45769. Reason: nanny-close
2024-01-10 06:52:55,845 - distributed.core - INFO - Connection to tcp://127.0.0.1:45579 has been closed.
2024-01-10 06:52:55,845 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39949. Reason: nanny-close
2024-01-10 06:52:55,845 - distributed.core - INFO - Connection to tcp://127.0.0.1:45579 has been closed.
2024-01-10 06:52:55,846 - distributed.nanny - INFO - Worker closed
2024-01-10 06:52:55,846 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44771. Reason: nanny-close
2024-01-10 06:52:55,846 - distributed.nanny - INFO - Worker closed
2024-01-10 06:52:55,847 - distributed.core - INFO - Connection to tcp://127.0.0.1:45579 has been closed.
2024-01-10 06:52:55,847 - distributed.core - INFO - Connection to tcp://127.0.0.1:45579 has been closed.
2024-01-10 06:52:55,848 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39621. Reason: nanny-close
2024-01-10 06:52:55,848 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44601. Reason: nanny-close
2024-01-10 06:52:55,848 - distributed.nanny - INFO - Worker closed
2024-01-10 06:52:55,848 - distributed.core - INFO - Connection to tcp://127.0.0.1:45579 has been closed.
2024-01-10 06:52:55,848 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37551. Reason: nanny-close
2024-01-10 06:52:55,848 - distributed.nanny - INFO - Worker closed
2024-01-10 06:52:55,849 - distributed.nanny - INFO - Worker closed
2024-01-10 06:52:55,849 - distributed.core - INFO - Connection to tcp://127.0.0.1:45579 has been closed.
2024-01-10 06:52:55,849 - distributed.core - INFO - Connection to tcp://127.0.0.1:45579 has been closed.
2024-01-10 06:52:55,850 - distributed.core - INFO - Connection to tcp://127.0.0.1:45579 has been closed.
2024-01-10 06:52:55,851 - distributed.nanny - INFO - Worker closed
2024-01-10 06:52:55,851 - distributed.nanny - INFO - Worker closed
2024-01-10 06:52:55,851 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-10 06:53:31,448 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:53:31,448 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:53:31,452 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:53:31,454 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37129
2024-01-10 06:53:31,454 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37129
2024-01-10 06:53:31,454 - distributed.worker - INFO -           Worker name:                          0
2024-01-10 06:53:31,454 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37789
2024-01-10 06:53:31,454 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:32849
2024-01-10 06:53:31,454 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:31,454 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:53:31,454 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-10 06:53:31,454 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y_as3d5w
2024-01-10 06:53:31,454 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-27071c0b-d060-417a-8375-c073dee1fda8
2024-01-10 06:53:31,455 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1926a1d-a006-42a2-a9fa-640c6df65824
2024-01-10 06:53:31,455 - distributed.worker - INFO - Starting Worker plugin PreImport-1296ff9d-23ef-45c5-b4c0-00af8a799e7b
2024-01-10 06:53:31,468 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-10 06:53:31,468 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37129. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-10 06:53:31,469 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-10 06:53:31,471 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-10 06:53:36,170 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:53:36,170 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:53:36,226 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:53:36,226 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:53:36,266 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:53:36,266 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:53:36,306 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:53:36,306 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:53:36,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:53:36,325 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:53:36,422 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:53:36,422 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:53:36,425 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:53:36,425 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:53:36,439 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-10 06:53:36,439 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-10 06:53:36,826 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:53:36,827 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43959
2024-01-10 06:53:36,827 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43959
2024-01-10 06:53:36,827 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36371
2024-01-10 06:53:36,827 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42183
2024-01-10 06:53:36,827 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:36,827 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:53:36,827 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:53:36,827 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n1ptrz94
2024-01-10 06:53:36,827 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0071d8d6-c2d6-4a47-81b1-6d58f484b83c
2024-01-10 06:53:36,827 - distributed.worker - INFO - Starting Worker plugin PreImport-65adf7b0-0bf3-41bd-969a-f328f3219c24
2024-01-10 06:53:36,827 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2087fda6-d4a0-448d-85cb-3ce6ae4dc742
2024-01-10 06:53:36,828 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:36,907 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:53:36,908 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40057
2024-01-10 06:53:36,908 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40057
2024-01-10 06:53:36,908 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46057
2024-01-10 06:53:36,908 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42183
2024-01-10 06:53:36,908 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:36,908 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:53:36,909 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:53:36,909 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kz5cx2f1
2024-01-10 06:53:36,909 - distributed.worker - INFO - Starting Worker plugin PreImport-7483bd10-42b8-4f08-9f5c-23ed254b543e
2024-01-10 06:53:36,909 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-60dfaa20-7bc4-45c7-b9c8-44f5a8621060
2024-01-10 06:53:36,909 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bb5581f1-c245-4408-874c-31bf94666e79
2024-01-10 06:53:36,909 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:36,949 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:53:36,950 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42419
2024-01-10 06:53:36,950 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42419
2024-01-10 06:53:36,950 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44353
2024-01-10 06:53:36,950 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42183
2024-01-10 06:53:36,950 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:36,950 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:53:36,950 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:53:36,950 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-29r2v8d6
2024-01-10 06:53:36,951 - distributed.worker - INFO - Starting Worker plugin PreImport-e512009e-794a-4afc-b655-adc7e3a98942
2024-01-10 06:53:36,951 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ddd6625-2db6-4d74-827a-3d28c9209106
2024-01-10 06:53:36,951 - distributed.worker - INFO - Starting Worker plugin RMMSetup-511d9adf-d13d-4787-adaf-0dff8218d260
2024-01-10 06:53:36,951 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:36,984 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:53:36,985 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38599
2024-01-10 06:53:36,986 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38599
2024-01-10 06:53:36,986 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40665
2024-01-10 06:53:36,986 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42183
2024-01-10 06:53:36,986 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:36,986 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:53:36,986 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:53:36,986 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6bsbpi5c
2024-01-10 06:53:36,986 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7476d8bc-8cf3-4392-8411-6b01013dbe5c
2024-01-10 06:53:36,986 - distributed.worker - INFO - Starting Worker plugin PreImport-87111e15-a119-4056-b202-e4dd424e5dd1
2024-01-10 06:53:36,986 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dea1f829-c07a-4fe2-b2da-a3c37d66b183
2024-01-10 06:53:36,986 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:36,987 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:53:36,988 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38859
2024-01-10 06:53:36,988 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38859
2024-01-10 06:53:36,988 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41461
2024-01-10 06:53:36,989 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42183
2024-01-10 06:53:36,989 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:36,989 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:53:36,989 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:53:36,989 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5uteajzg
2024-01-10 06:53:36,989 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80280090-375a-406c-a5ec-3d2f6d62060f
2024-01-10 06:53:36,989 - distributed.worker - INFO - Starting Worker plugin PreImport-c5246b59-1b99-4bfd-ab7b-79c028b9c468
2024-01-10 06:53:36,990 - distributed.worker - INFO - Starting Worker plugin RMMSetup-153072c7-e0e6-40cd-ba77-17ba6ec14506
2024-01-10 06:53:36,990 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:37,097 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:53:37,098 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46715
2024-01-10 06:53:37,098 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46715
2024-01-10 06:53:37,098 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40703
2024-01-10 06:53:37,098 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42183
2024-01-10 06:53:37,098 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:37,098 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:53:37,098 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:53:37,098 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vb3xo256
2024-01-10 06:53:37,098 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74ec1a78-11d5-4954-b710-884cdbd07e4f
2024-01-10 06:53:37,099 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d9f04e6-224e-457f-8d78-38a30271faa7
2024-01-10 06:53:37,099 - distributed.worker - INFO - Starting Worker plugin PreImport-1b6e2ca2-ccde-412a-bc79-c4cb6a2d099c
2024-01-10 06:53:37,099 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:37,103 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:53:37,103 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37283
2024-01-10 06:53:37,104 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37283
2024-01-10 06:53:37,104 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45357
2024-01-10 06:53:37,104 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42183
2024-01-10 06:53:37,104 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:37,104 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:53:37,104 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:53:37,104 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wfalz48j
2024-01-10 06:53:37,104 - distributed.worker - INFO - Starting Worker plugin PreImport-83a3ff3e-28b4-4298-9665-1499ac94a724
2024-01-10 06:53:37,104 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3bc2d328-b4b6-4854-81b0-7bf4e28ed5c9
2024-01-10 06:53:37,104 - distributed.worker - INFO - Starting Worker plugin RMMSetup-918cd8e3-d9bd-43b9-8b08-c2e76fcee14c
2024-01-10 06:53:37,105 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:37,111 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-10 06:53:37,112 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34681
2024-01-10 06:53:37,113 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34681
2024-01-10 06:53:37,113 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38319
2024-01-10 06:53:37,113 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42183
2024-01-10 06:53:37,113 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:37,113 - distributed.worker - INFO -               Threads:                          1
2024-01-10 06:53:37,113 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-10 06:53:37,113 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4q7i0oh2
2024-01-10 06:53:37,113 - distributed.worker - INFO - Starting Worker plugin PreImport-393889a2-cdf0-4b0a-b624-0c63acc5f2e8
2024-01-10 06:53:37,113 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ad891e6c-1376-4b2b-83e2-bc44cc7e5597
2024-01-10 06:53:37,114 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29871a2f-e2ee-4d47-988f-6b235079628f
2024-01-10 06:53:37,114 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:37,567 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:53:37,568 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42183
2024-01-10 06:53:37,568 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:37,569 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42183
2024-01-10 06:53:37,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:53:37,873 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42183
2024-01-10 06:53:37,873 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:37,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42183
2024-01-10 06:53:37,919 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:53:37,919 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42183
2024-01-10 06:53:37,920 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:37,921 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42183
2024-01-10 06:53:38,159 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:53:38,160 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42183
2024-01-10 06:53:38,160 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:38,161 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42183
2024-01-10 06:53:38,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:53:38,169 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42183
2024-01-10 06:53:38,169 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:38,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42183
2024-01-10 06:53:38,320 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:53:38,321 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42183
2024-01-10 06:53:38,321 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:38,322 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42183
2024-01-10 06:53:38,687 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:53:38,688 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42183
2024-01-10 06:53:38,688 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:38,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42183
2024-01-10 06:53:38,723 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-10 06:53:38,724 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42183
2024-01-10 06:53:38,724 - distributed.worker - INFO - -------------------------------------------------
2024-01-10 06:53:38,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42183
2024-01-10 06:53:38,746 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43959. Reason: nanny-close
2024-01-10 06:53:38,747 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40057. Reason: nanny-close
2024-01-10 06:53:38,748 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42419. Reason: nanny-close
2024-01-10 06:53:38,748 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38599. Reason: nanny-close
2024-01-10 06:53:38,748 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38859. Reason: nanny-close
2024-01-10 06:53:38,748 - distributed.core - INFO - Connection to tcp://127.0.0.1:42183 has been closed.
2024-01-10 06:53:38,749 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34681. Reason: nanny-close
2024-01-10 06:53:38,749 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46715. Reason: nanny-close
2024-01-10 06:53:38,749 - distributed.core - INFO - Connection to tcp://127.0.0.1:42183 has been closed.
2024-01-10 06:53:38,750 - distributed.core - INFO - Connection to tcp://127.0.0.1:42183 has been closed.
2024-01-10 06:53:38,750 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37283. Reason: nanny-close
2024-01-10 06:53:38,750 - distributed.core - INFO - Connection to tcp://127.0.0.1:42183 has been closed.
2024-01-10 06:53:38,750 - distributed.nanny - INFO - Worker closed
2024-01-10 06:53:38,751 - distributed.core - INFO - Connection to tcp://127.0.0.1:42183 has been closed.
2024-01-10 06:53:38,751 - distributed.nanny - INFO - Worker closed
2024-01-10 06:53:38,751 - distributed.nanny - INFO - Worker closed
2024-01-10 06:53:38,751 - distributed.nanny - INFO - Worker closed
2024-01-10 06:53:38,751 - distributed.core - INFO - Connection to tcp://127.0.0.1:42183 has been closed.
2024-01-10 06:53:38,752 - distributed.core - INFO - Connection to tcp://127.0.0.1:42183 has been closed.
2024-01-10 06:53:38,752 - distributed.core - INFO - Connection to tcp://127.0.0.1:42183 has been closed.
2024-01-10 06:53:38,752 - distributed.nanny - INFO - Worker closed
2024-01-10 06:53:38,753 - distributed.nanny - INFO - Worker closed
2024-01-10 06:53:38,753 - distributed.nanny - INFO - Worker closed
2024-01-10 06:53:38,754 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk PASSED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-10 06:55:21,926 - distributed.worker - WARNING - RMM allocation of 1.00 MiB failed, spill-on-demand couldn't find any device memory to spill.
RMM allocs: 1.00 MiB, <ProxyManager dev_limit=25.60 GiB host_limit=0.98 TiB disk=0 B(0) host=0 B(0) dev=0 B(0)>, traceback:
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 937, in _bootstrap
    self._bootstrap_inner()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/threadpoolexecutor.py", line 57, in _worker
    task.run()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/_concurrent_futures_thread.py", line 65, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1541, in <lambda>
    executor, lambda: context.run(func, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2954, in apply_function
    msg = apply_function_simple(function, args, kwargs, time_delay)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2990, in apply_function_simple
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_proxify_host_file.py", line 467, in task
    rmm.DeviceBuffer(size=rmm_pool_size),  # Trigger OOM
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/proxify_host_file.py", line 617, in oom
    traceback.print_stack(file=f)


2024-01-10 06:55:22,134 - distributed.worker - WARNING - Compute Failed
Key:       task-5c302f54392a5ca98adb3c55c8357b0e
Function:  task
args:      ()
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_serializer PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[numpy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[cupy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_name PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj0] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj1] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[dask] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[pickle] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[disk] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-send_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[tcp-send_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucx-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucx-send_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucx-send_serializers2] [1704869767.549130] [dgx13:64089:1]            sock.c:481  UCX  ERROR bind(fd=247 addr=0.0.0.0:33594) failed: Address already in use
PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucxx-None] PASSED
dask_cuda/tests/test_proxy.py::test_communicating_proxy_objects[ucxx-send_serializers1] /opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
