[dgx13:78507:0:78507] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  78507) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f795c403c0d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2de04) [0x7f795c403e04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2dfca) [0x7f795c403fca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f79870e7420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f795c486987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x65d) [0x7f795c4b083d]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2193f) [0x7f795c3ba93f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x24b18) [0x7f795c3bdb18]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f795c40d509]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f795c3bcc2d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f795c48385a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f795c54306a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x564130c763f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564130c70fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564130c82469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564130c724e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x564130d256d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f79716bd1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x564130c7a6ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x564130c353ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x564130c79723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x564130c77929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564130c82712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564130c724e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564130c82712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564130c724e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564130c82712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564130c724e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564130c82712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564130c724e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564130c70fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564130c82469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x564130c73042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564130c70fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x564130c8f8cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x564130c9004c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x564130d5380e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x564130c7a6ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x564130c763f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564130c82712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x564130c8f9ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x564130c763f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564130c82712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564130c724e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564130c70fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564130c82469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x564130c724e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x564130c82712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x564130c72232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564130c70fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x564130c82469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x564130c73042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x564130c70fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x564130c70c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x564130c70c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x564130d1e2cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x564130d4b6ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x564130d47a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x564130d3f87a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x564130d3f76c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x564130d3e9a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x564130d12107]
=================================
[dgx13:78517:0:78517] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
[dgx13:78526:0:78526] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  78517) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7ff6ac544c0d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2de04) [0x7ff6ac544e04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2dfca) [0x7ff6ac544fca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7ff6e524e420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7ff6ac5c7987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x65d) [0x7ff6ac5f183d]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2193f) [0x7ff6ac4fb93f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x24b18) [0x7ff6ac4feb18]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7ff6ac54e509]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7ff6ac4fdc2d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7ff6ac5c485a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7ff6ac68406a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x562bc0db23f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562bc0dacfb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x562bc0dbe469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562bc0dae4e6]
16  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x562bc0dbe712]
17  /opt/conda/envs/gdf/bin/python(+0x14ca83) [0x562bc0dcba83]
18  /opt/conda/envs/gdf/bin/python(+0x25819c) [0x562bc0ed719c]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x562bc0d713ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x562bc0db5723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x562bc0db3929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x562bc0dbe712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562bc0dae4e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x562bc0dbe712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562bc0dae4e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x562bc0dbe712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562bc0dae4e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x562bc0dbe712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562bc0dae4e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562bc0dacfb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x562bc0dbe469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x562bc0daf042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562bc0dacfb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x562bc0dcb8cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x562bc0dcc04c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x562bc0e8f80e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x562bc0db66ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x562bc0db23f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x562bc0dbe712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x562bc0dcb9ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x562bc0db23f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x562bc0dbe712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562bc0dae4e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562bc0dacfb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x562bc0dbe469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x562bc0dae4e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x562bc0dbe712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x562bc0dae232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562bc0dacfb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x562bc0dbe469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x562bc0daf042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x562bc0dacfb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x562bc0dacc88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x562bc0dacc39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x562bc0e5a2cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x562bc0e876ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x562bc0e83a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x562bc0e7b87a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x562bc0e7b76c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x562bc0e7a9a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x562bc0e4e107]
=================================
==== backtrace (tid:  78526) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f1ae0e3cc0d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2de04) [0x7f1ae0e3ce04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2dfca) [0x7f1ae0e3cfca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f1b0bb21420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7f1ae0ebf987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x65d) [0x7f1ae0ee983d]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2193f) [0x7f1ae0df393f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x24b18) [0x7f1ae0df6b18]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f1ae0e46509]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f1ae0df5c2d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f1ae0ebc85a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7f1ae0f7c06a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x556bb85b23f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bb85acfb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556bb85be469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bb85ae4e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x556bb86616d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7f1af60fc1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x556bb85b66ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x556bb85713ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x556bb85b5723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x556bb85b3929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bb85be712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bb85ae4e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bb85be712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bb85ae4e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bb85be712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bb85ae4e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bb85be712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bb85ae4e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bb85acfb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556bb85be469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x556bb85af042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bb85acfb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x556bb85cb8cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x556bb85cc04c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x556bb868f80e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x556bb85b66ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x556bb85b23f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bb85be712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x556bb85cb9ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x556bb85b23f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bb85be712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bb85ae4e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bb85acfb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556bb85be469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x556bb85ae4e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x556bb85be712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x556bb85ae232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bb85acfb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x556bb85be469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x556bb85af042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x556bb85acfb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x556bb85acc88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x556bb85acc39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x556bb865a2cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x556bb86876ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x556bb8683a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x556bb867b87a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x556bb867b76c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x556bb867a9a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x556bb864e107]
=================================
[dgx13:78534:0:78534] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  78534) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fe2acd76c0d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2de04) [0x7fe2acd76e04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2dfca) [0x7fe2acd76fca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fe2e5a5e420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7fe2acdf9987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x65d) [0x7fe2ace2383d]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2193f) [0x7fe2acd2d93f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x24b18) [0x7fe2acd30b18]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fe2acd80509]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fe2acd2fc2d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fe2acdf685a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7fe2aceb606a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x561c80f6e3f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x561c80f68fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561c80f7a469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561c80f6a4e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x561c8101d6d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7fe2d907f1e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x561c80f726ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x561c80f2d3ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x561c80f71723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x561c80f6f929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x561c80f7a712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561c80f6a4e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x561c80f7a712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561c80f6a4e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x561c80f7a712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561c80f6a4e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x561c80f7a712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561c80f6a4e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x561c80f68fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561c80f7a469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x561c80f6b042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x561c80f68fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x561c80f878cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x561c80f8804c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x561c8104b80e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x561c80f726ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x561c80f6e3f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x561c80f7a712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x561c80f879ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x561c80f6e3f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x561c80f7a712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561c80f6a4e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x561c80f68fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561c80f7a469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x561c80f6a4e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x561c80f7a712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x561c80f6a232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x561c80f68fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x561c80f7a469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x561c80f6b042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x561c80f68fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x561c80f68c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x561c80f68c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x561c810162cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x561c810436ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x561c8103fa63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x561c8103787a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x561c8103776c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x561c810369a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x561c8100a107]
=================================
[dgx13:78538:0:78538] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x450)
==== backtrace (tid:  78538) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7ff86de46c0d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2de04) [0x7ff86de46e04]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x2dfca) [0x7ff86de46fca]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7ff8aab5c420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x17) [0x7ff86dec9987]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x65d) [0x7ff86def383d]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2193f) [0x7ff86ddfd93f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x24b18) [0x7ff86de00b18]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7ff86de50509]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7ff86ddffc2d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7ff86dec685a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x3206a) [0x7ff86df8606a]
12  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55f834af83f6]
13  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f834af2fb4]
14  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f834b04469]
15  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f834af44e6]
16  /opt/conda/envs/gdf/bin/python(+0x1e26d2) [0x55f834ba76d2]
17  /opt/conda/envs/gdf/lib/python3.9/lib-dynload/_asyncio.cpython-39-x86_64-linux-gnu.so(+0x81e9) [0x7ff8951351e9]
18  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55f834afc6ac]
19  /opt/conda/envs/gdf/bin/python(+0xf23ff) [0x55f834ab73ff]
20  /opt/conda/envs/gdf/bin/python(+0x136723) [0x55f834afb723]
21  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ad9) [0x55f834af9929]
22  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f834b04712]
23  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f834af44e6]
24  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f834b04712]
25  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f834af44e6]
26  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f834b04712]
27  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f834af44e6]
28  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f834b04712]
29  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f834af44e6]
30  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f834af2fb4]
31  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f834b04469]
32  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55f834af5042]
33  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f834af2fb4]
34  /opt/conda/envs/gdf/bin/python(+0x14c8cb) [0x55f834b118cb]
35  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xbc) [0x55f834b1204c]
36  /opt/conda/envs/gdf/bin/python(+0x21080e) [0x55f834bd580e]
37  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2ec) [0x55f834afc6ac]
38  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55f834af83f6]
39  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f834b04712]
40  /opt/conda/envs/gdf/bin/python(+0x14c9ac) [0x55f834b119ac]
41  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x45a6) [0x55f834af83f6]
42  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f834b04712]
43  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f834af44e6]
44  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f834af2fb4]
45  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f834b04469]
46  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x696) [0x55f834af44e6]
47  /opt/conda/envs/gdf/bin/python(+0x13f712) [0x55f834b04712]
48  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3e2) [0x55f834af4232]
49  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f834af2fb4]
50  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd9) [0x55f834b04469]
51  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x11f2) [0x55f834af5042]
52  /opt/conda/envs/gdf/bin/python(+0x12dfb4) [0x55f834af2fb4]
53  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x48) [0x55f834af2c88]
54  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55f834af2c39]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55f834ba02cb]
56  /opt/conda/envs/gdf/bin/python(+0x2086ca) [0x55f834bcd6ca]
57  /opt/conda/envs/gdf/bin/python(+0x204a63) [0x55f834bc9a63]
58  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9a) [0x55f834bc187a]
59  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3c) [0x55f834bc176c]
60  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x267) [0x55f834bc09a7]
61  /opt/conda/envs/gdf/bin/python(Py_BytesMain+0x37) [0x55f834b94107]
=================================
2024-02-10 07:49:13,630 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:38093
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #008] ep: 0x7f3a08334280, tag: 0x98a34f56221ee83d, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #008] ep: 0x7f3a08334280, tag: 0x98a34f56221ee83d, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-02-10 07:49:13,630 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:38093
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f8f887661c0, tag: 0x9177c8eb0d35b48, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f8f887661c0, tag: 0x9177c8eb0d35b48, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-10 07:49:13,630 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:38093
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fd1609292c0, tag: 0x98290e2571656140, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fd1609292c0, tag: 0x98290e2571656140, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-10 07:49:13,710 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51769
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fd160929200, tag: 0xd1d2414ec4ddc55b, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fd160929200, tag: 0xd1d2414ec4ddc55b, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-10 07:49:13,710 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51769
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f3a083341c0, tag: 0x8fba583b854d94b1, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f3a083341c0, tag: 0x8fba583b854d94b1, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-02-10 07:49:13,711 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:60647 -> ucx://127.0.0.1:51769
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fd160929380, tag: 0xd91c11b6ecc92380, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,751 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:58877
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fd1609291c0, tag: 0xbd2a26cbe2c9d14, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fd1609291c0, tag: 0xbd2a26cbe2c9d14, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-10 07:49:13,751 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47411
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7fd160929240, tag: 0xe9aab6f3ab90399e, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7fd160929240, tag: 0xe9aab6f3ab90399e, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-10 07:49:13,751 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:49667 -> ucx://127.0.0.1:58877
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f3a08334300, tag: 0xd41d822aa0ffbeee, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,752 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:49667 -> ucx://127.0.0.1:38093
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f3a08334400, tag: 0x4b14ce1abedf3ba4, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,753 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:60647 -> ucx://127.0.0.1:47411
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fd160929100, tag: 0xb9615fbd263f7b41, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,753 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:49667 -> ucx://127.0.0.1:51769
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #009] ep: 0x7f3a08334380, tag: 0x16b725f8c5c91fbf, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,753 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:58877
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f3a08334200, tag: 0x361dab5bc8efdeaa, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f3a08334200, tag: 0x361dab5bc8efdeaa, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-10 07:49:13,754 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47411
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f3a08334100, tag: 0xeb6f0e2ef7cbcf21, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f3a08334100, tag: 0xeb6f0e2ef7cbcf21, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-10 07:49:13,738 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34319 -> ucx://127.0.0.1:51769
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f8f88766380, tag: 0xa91ff40b0f10bfad, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,761 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:51769
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f8f88766100, tag: 0x7006cc93277c54ee, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f8f88766100, tag: 0x7006cc93277c54ee, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-10 07:49:13,762 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34319 -> ucx://127.0.0.1:58877
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f8f88766300, tag: 0x2c33c32ed28fce60, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,763 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34319 -> ucx://127.0.0.1:47411
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f8f88766340, tag: 0x57173abf5d45fa16, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,763 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34319 -> ucx://127.0.0.1:38093
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #010] ep: 0x7f8f887662c0, tag: 0xf26acee772b9fef6, nbytes: 50000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,764 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47411
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7f8f88766240, tag: 0xf8da1021427bf4e4, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7f8f88766240, tag: 0xf8da1021427bf4e4, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2024-02-10 07:49:13,764 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:58877
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f8f88766140, tag: 0x1a2da8ffa4d3fd3, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f8f88766140, tag: 0x1a2da8ffa4d3fd3, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-10 07:49:13,885 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:60647 -> ucx://127.0.0.1:33595
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fd160929440, tag: 0xf918afbb9a238e61, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,891 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:60647 -> ucx://127.0.0.1:38093
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fd160929340, tag: 0x833c5e88d5c8d9ef, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,896 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:60647 -> ucx://127.0.0.1:58877
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fd160929300, tag: 0x9e2a622ff5aac2d9, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:13,903 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33595
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fd160929140, tag: 0xe554809f2bedf908, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fd160929140, tag: 0xe554809f2bedf908, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
Task exception was never retrieved
future: <Task finished name='Task-1289' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:140> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 155, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 55, in exchange_peer_info
    await asyncio.wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
2024-02-10 07:49:13,906 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33595
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 392, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #009] ep: 0x7f8f88766180, tag: 0x4540b8149b462e43, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 398, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #009] ep: 0x7f8f88766180, tag: 0x4540b8149b462e43, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: ")
2024-02-10 07:49:13,933 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:33595
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 350, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f3a08334140, tag: 0x90ed1279de1c8ffc, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f3a08334140, tag: 0x90ed1279de1c8ffc, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2024-02-10 07:49:15,910 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-b9e75a656c00804c86f03e519074f660', 6)
Function:  shuffle_group
args:      (                 key   payload  _partitions
0         1418863339  99051695            1
1         1452208751  78953234            3
2          677278035  81293845            1
3         1453618300  74974697            1
4         1402090160  62919800            2
...              ...       ...          ...
99999995    75213713  72068972            2
99999996  1449206659  33160153            7
99999997  1459655348  40095643            5
99999998  1452540937  33956372            3
99999999  1412812172  90324512            2

[100000000 rows x 3 columns], '_partitions', 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-10 07:49:18,137 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 7)
Function:  _concat
args:      ([                key   payload  _partitions
11851     709246232   6990347            7
11856     839370800   3242602            7
61984     862518746  21737795            7
11859     112006312  76858154            7
61989     815888422  94222409            7
...             ...       ...          ...
99984191  709692868  64578795            7
99971041  402220402  52666746            7
99971054    6076276  46373632            7
99973056    5831250  38109725            7
99973066  833833375  81484676            7

[12503560 rows x 3 columns],                 key   payload  _partitions
43015     951032910  59566571            7
61994     921353739  46410377            7
43017     914235314  60932352            7
62002     955195260  92763240            7
62003     949349406  18106440            7
...             ...       ...          ...
99962365  903859468  45742274            7
99990956  412693611  87685011            7
99990957  113606780  79341479            7
99990958  921863797  9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-10 07:49:18,188 - distributed.comm.ucx - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-10 07:49:18,189 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 374, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 167, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded
2024-02-10 07:49:18,241 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-54efe4e1c1ff02b405ca9dd1c57e206a', 2)
Function:  shuffle_group
args:      (               key   payload  _partitions
shuffle                                  
2             2255  94174013            1
2             2259   6394530            0
2             2262  18237792            5
2             2265  61558644            0
2             2267  55546397            4
...            ...       ...          ...
2        799998475  93696931            5
2        799998476   6978385            0
2        799998486  26797377            7
2        799998492  12373318            2
2        799998494  32317948            6

[100000000 rows x 3 columns], '_partitions', 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-10 07:49:18,281 - distributed.worker - WARNING - Compute Failed
Key:       ('group-simple-shuffle-54efe4e1c1ff02b405ca9dd1c57e206a', 0)
Function:  shuffle_group
args:      (               key   payload  _partitions
shuffle                                  
0             2240  16693209            7
0             2245   3792679            3
0             2257   7743954            3
0             2263  21444228            6
0             2270  53004324            2
...            ...       ...          ...
0        799998472  47845648            1
0        799980232  75071969            6
0        799980243  81989300            7
0        799980252  38952005            5
0        799998473  91776524            2

[100000000 rows x 3 columns], '_partitions', 0, 8, 8, False, 8)
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-10 07:49:18,340 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 6)
Function:  _concat
args:      ([                key   payload  _partitions
11844     310168199   4527719            6
11845     837037707  52351233            6
61986     866441335   6255663            6
11847     806324339  81785900            6
61997     859184817  55472250            6
...             ...       ...          ...
99973082  804897345  57538883            6
99973084  602370664  58315727            6
99971063  839344791  33139838            6
99984185  609479084  18468796            6
99984189  804662602  62919218            6

[12495842 rows x 3 columns],                 key   payload  _partitions
43023     421202498  55994095            6
61993     927572658  26344055            6
43028     324971577  76813775            6
62006     933845950  19770827            6
43034     523605165  79554223            6
...             ...       ...          ...
99962357  945931845  52711577            6
99990953  932714964  26302781            6
99990954  929788808  80827757            6
99990974  124488436  6
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-10 07:49:18,471 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-b9e75a656c00804c86f03e519074f660', 1)
Function:  _concat
args:      ([                key   payload  _partitions
11846     864271229  23286807            1
11850     811355806   5433961            1
61992     804131220  55209701            1
11855     861147394   6572786            1
62011     824431928   7844978            1
...             ...       ...          ...
99984186  866194166  63315136            1
99971060  862212081  28678847            1
99971068  816285932  21526002            1
99971070  815978832   5391039            1
99971071    9638240  19751107            1

[12497240 rows x 3 columns],                 key   payload  _partitions
43011      21199772  33562109            1
61990     945593778  60051373            1
43014     904890323  44482374            1
61991     514771484  27532155            1
43021     901204952   3916140            1
...             ...       ...          ...
99962360  614878927  73814367            1
99962366  920968357  34904401            1
99990944  947441263  82011541            1
99990968  945774392  1
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-10 07:49:19,334 - distributed.worker - ERROR - 'data'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2075, in gather_dep
    data=response["data"],
KeyError: 'data'
2024-02-10 07:49:19,340 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:34319
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 360, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 737, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #167] ep: 0x7f3a08334180, tag: 0x76190a361bd3ba, nbytes: 480, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2056, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2860, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 368, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #167] ep: 0x7f3a08334180, tag: 0x76190a361bd3ba, nbytes: 480, type: <class 'numpy.ndarray'>>: Message truncated")
2024-02-10 07:49:19,343 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:34319 -> ucx://127.0.0.1:49667
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 334, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 662, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #168] ep: 0x7f8f887663c0, tag: 0x76190a361bd3ba, nbytes: 49983628, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1779, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 338, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2024-02-10 07:49:19,475 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 1)
Function:  subgraph_callable-98f04ccc-7696-465e-99de-9a6b24b2
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0             31809        1  50949392            1
1             31820        1  92867663            1
2             31829        1  76326682            1
3             31839        1  41440715            1
4             31941        1  44889211            1
...             ...      ...       ...          ...
99999995  799989379        1  66230539            1
99999996  799989385        1  72835883            1
99999997  799989388        1  27321524            1
99999998  799989392        1  50953155            1
99999999  799989393        1  25178389            1

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-10 07:49:19,698 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e', 5)
Function:  _concat
args:      ([               key  shuffle   payload  _partitions
0            31811        5  85287566            5
1            31817        5  54989760            5
2            31826        5  52862617            5
3            31827        5  49201082            5
4            31831        5  66142001            5
...            ...      ...       ...          ...
12499995  99997670        5  78792824            5
12499996  99997482        5   4675274            5
12499997  99997489        5  12805286            5
12499998  99997490        5  27736983            5
12499999  99997491        5  77734185            5

[12500000 rows x 4 columns],                 key  shuffle   payload  _partitions
0         100103040        5   2878498            5
1         100103042        5  99081216            5
2         100002183        5  42954834            5
3         100103044        5   5393107            5
4         100002191        5  20334108            5
...             ...      ...       ...      
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-10 07:49:20,642 - distributed.worker - WARNING - Compute Failed
Key:       ('sort_index-86563d3c54ed11bf71e0c240596313fc', 0)
Function:  subgraph_callable-98f04ccc-7696-465e-99de-9a6b24b2
args:      ('set_index_post_scalar-ceb9308523ab39bbcf59f6dccd678d89',                 key  shuffle   payload  _partitions
0             31812        0  95545958            0
1             31822        0  22650092            0
2             31837        0  34942018            0
3             31939        0  35748287            0
4             31943        0  91949071            0
...             ...      ...       ...          ...
99999995  799989381        0   8370365            0
99999996  799989384        0  58475343            0
99999997  799989389        0  84243025            0
99999998  799989400        0  71926235            0
99999999  799989401        0  29585608            0

[100000000 rows x 4 columns], 'simple-shuffle-2f7c39ce5e6399e29bac23110d347e6e')
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:313: Maximum pool size exceeded')"

2024-02-10 07:49:28,157 - distributed.nanny - WARNING - Restarting worker
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 18 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
