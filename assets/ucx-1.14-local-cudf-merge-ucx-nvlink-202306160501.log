2023-06-16 05:53:34,698 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:34,698 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:34,761 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:34,761 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:34,792 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:34,792 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:34,819 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:34,819 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:34,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:34,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:34,832 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:34,832 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:34,848 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:34,848 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:34,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:34,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:74749:0:74749] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  74749) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f8299c18f1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7f8299c19114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7f8299c192da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f833e409420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f8299c925d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f8299cb7859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7f8299bd442f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7f8299bd7798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f8299c21989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f8299bd662d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f8299c8fc4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7f8299d3a17a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55a2e992fb08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55a2e9920112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a2e991927a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55a2e992ac05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a2e991a81b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a2e992aef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x55a2e9938a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x55a2e9a489b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55a2e98d6817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55a2e9921f83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55a2e991fd36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a2e992aef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a2e991a81b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a2e992aef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a2e991a81b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a2e992aef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a2e991a81b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a2e992aef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a2e991a81b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a2e991927a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55a2e992ac05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55a2e991efa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a2e991927a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55a2e9938935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55a2e9939104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55a2e99fffc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55a2e99232bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55a2e991e1bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a2e992aef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55a2e9938c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55a2e991e1bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a2e992aef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a2e991a81b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a2e991927a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55a2e992ac05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55a2e991a81b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55a2e992aef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55a2e991a568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a2e991927a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55a2e992ac05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55a2e991b3cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55a2e991927a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55a2e9918f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55a2e9918eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55a2e99c98bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55a2e99f7adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55a2e99f3c24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55a2e99eb7ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55a2e99eb6bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x55a2e99ea8a2]
=================================
2023-06-16 05:53:43,611 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47813 -> ucx://127.0.0.1:33599
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa9dcc1c100, tag: 0x9a8351bca6cb9e0e, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-16 05:53:43,696 - distributed.nanny - WARNING - Restarting worker
[dgx13:74750:0:74750] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  74750) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fdb6c2aaf1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7fdb6c2ab114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7fdb6c2ab2da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fdbfeaad420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fdb6c3245d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fdb6c349859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7fdb6c26642f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7fdb6c269798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fdb6c2b3989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fdb6c26862d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fdb6c321c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7fdb6c3cc17a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5580dce1bb08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x5580dce0c112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5580dce0527a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5580dce16c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5580dce0681b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5580dce16ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x5580dce24a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x5580dcf349b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x5580dcdc2817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5580dce0df83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5580dce0bd36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5580dce16ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5580dce0681b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5580dce16ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5580dce0681b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5580dce16ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5580dce0681b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5580dce16ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5580dce0681b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5580dce0527a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5580dce16c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5580dce0afa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5580dce0527a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5580dce24935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5580dce25104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5580dceebfc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5580dce0f2bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5580dce0a1bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5580dce16ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5580dce24c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5580dce0a1bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5580dce16ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5580dce0681b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5580dce0527a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5580dce16c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5580dce0681b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5580dce16ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5580dce06568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5580dce0527a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5580dce16c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5580dce073cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5580dce0527a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x5580dce04f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5580dce04eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5580dceb58bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5580dcee3adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5580dcedfc24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5580dced77ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5580dced76bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x5580dced68a2]
=================================
2023-06-16 05:53:44,050 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47813 -> ucx://127.0.0.1:52923
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa9dcc1c100, tag: 0x9a75616992caf464, nbytes: 800000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-16 05:53:44,130 - distributed.nanny - WARNING - Restarting worker
[dgx13:74745:0:74745] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  74745) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fe95cad2f1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7fe95cad3114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7fe95cad32da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fe9fd2dd420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fe95cb4c5d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fe95cb71859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7fe95ca8e42f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7fe95ca91798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fe95cadb989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fe95ca9062d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fe95cb49c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7fe95cbf417a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55acc5fbab08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55acc5fab112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55acc5fa427a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55acc5fb5c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55acc5fa581b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55acc5fb5ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x55acc5fc3a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x55acc60d39b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55acc5f61817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55acc5facf83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55acc5faad36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55acc5fb5ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55acc5fa581b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55acc5fb5ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55acc5fa581b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55acc5fb5ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55acc5fa581b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55acc5fb5ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55acc5fa581b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55acc5fa427a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55acc5fb5c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55acc5fa9fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55acc5fa427a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55acc5fc3935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55acc5fc4104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55acc608afc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55acc5fae2bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55acc5fa91bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55acc5fb5ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55acc5fc3c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55acc5fa91bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55acc5fb5ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55acc5fa581b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55acc5fa427a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55acc5fb5c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55acc5fa581b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55acc5fb5ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55acc5fa5568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55acc5fa427a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55acc5fb5c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55acc5fa63cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55acc5fa427a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55acc5fa3f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55acc5fa3eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55acc60548bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55acc6082adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55acc607ec24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55acc60767ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55acc60766bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x55acc60758a2]
=================================
[dgx13:74740:0:74740] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  74740) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fcef36edf1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7fcef36ee114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7fcef36ee2da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fcf93f08420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fcef37675d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fcef378c859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7fcef36a942f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7fcef36ac798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fcef36f6989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fcef36ab62d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fcef3764c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7fcef380f17a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5603b90bab08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x5603b90ab112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5603b90a427a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5603b90b5c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5603b90a581b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5603b90b5ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x5603b90c3a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x5603b91d39b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x5603b9061817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5603b90acf83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5603b90aad36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5603b90b5ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5603b90a581b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5603b90b5ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5603b90a581b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5603b90b5ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5603b90a581b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5603b90b5ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5603b90a581b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5603b90a427a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5603b90b5c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5603b90a9fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5603b90a427a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5603b90c3935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5603b90c4104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5603b918afc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5603b90ae2bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5603b90a91bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5603b90b5ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5603b90c3c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5603b90a91bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5603b90b5ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5603b90a581b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5603b90a427a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5603b90b5c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5603b90a581b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5603b90b5ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5603b90a5568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5603b90a427a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5603b90b5c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5603b90a63cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5603b90a427a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x5603b90a3f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5603b90a3eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5603b91548bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5603b9182adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5603b917ec24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5603b91767ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5603b91766bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x5603b91758a2]
=================================
Task exception was never retrieved
future: <Task finished name='Task-996' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
Task exception was never retrieved
future: <Task finished name='Task-981' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_recv>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 52, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_recv>: Connection reset by remote peer
2023-06-16 05:53:44,659 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:60177 -> ucx://127.0.0.1:47505
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa5e4fe4240, tag: 0xb029f21d4021fee3, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-16 05:53:44,660 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47505
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fa5e4fe4100, tag: 0xf991de5325f7b93e, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fa5e4fe4100, tag: 0xf991de5325f7b93e, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-06-16 05:53:44,669 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47813 -> ucx://127.0.0.1:42613
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa9dcc1c300, tag: 0x6a24d843e6e0521e, nbytes: 100000000, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-16 05:53:44,670 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42613
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 391, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #007] ep: 0x7fa9dcc1c1c0, tag: 0xc44f704859b8c6f7, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 397, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #007] ep: 0x7fa9dcc1c1c0, tag: 0xc44f704859b8c6f7, nbytes: 100000000, type: <class 'rmm._lib.device_buffer.DeviceBuffer'>>: Connection reset by remote peer")
2023-06-16 05:53:44,669 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42613
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f8714033140, tag: 0x2d46aa77273b8588, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f8714033140, tag: 0x2d46aa77273b8588, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-16 05:53:44,671 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47505
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-06-16 05:53:44,671 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47505
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXNotConnected: <stream_recv>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 468, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-06-16 05:53:44,739 - distributed.nanny - WARNING - Restarting worker
2023-06-16 05:53:44,670 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42613
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #000] ep: 0x7fa5e4fe41c0, tag: 0x26c6466cc6f18db5, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 329, in connect
    handshake = await wait_for(comm.read(), time_left())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #000] ep: 0x7fa5e4fe41c0, tag: 0x26c6466cc6f18db5, nbytes: 16, type: <class 'numpy.ndarray'>>: ")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 334, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to ucx://127.0.0.1:42613 after 30 s
2023-06-16 05:53:44,669 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:42613
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 349, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXCanceled: <[Recv #003] ep: 0x7f3ea592f300, tag: 0x6a2dfd317fae9008, nbytes: 16, type: <class 'numpy.ndarray'>>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXCanceled("<[Recv #003] ep: 0x7f3ea592f300, tag: 0x6a2dfd317fae9008, nbytes: 16, type: <class 'numpy.ndarray'>>: ")
2023-06-16 05:53:44,791 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47505
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 466, in connect
    ep = await ucp.create_endpoint(ip, port)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 1004, in create_endpoint
    return await _get_ctx().create_endpoint(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 316, in create_endpoint
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 54, in exchange_peer_info
    await comm.stream_recv(endpoint, peer_info_arr, peer_info_arr.nbytes)
ucp._libs.exceptions.UCXNotConnected: <stream_recv>: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 292, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1849, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 479, in wait_for
    return fut.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 468, in connect
    raise CommClosedError("Connection closed before handshake completed")
distributed.comm.core.CommClosedError: Connection closed before handshake completed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1427, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 316, in connect
    await asyncio.sleep(backoff)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 652, in sleep
    return await future
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2889, in get_data_from_worker
    comm = await rpc.connect(worker)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1506, in connect
    return await connect_attempt
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1450, in _connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: Address removed.
2023-06-16 05:53:44,796 - distributed.nanny - WARNING - Restarting worker
2023-06-16 05:53:45,354 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:45,354 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:45,792 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:45,792 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:46,248 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:46,248 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:46,454 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:46,454 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
[dgx13:75262:0:75262] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  75262) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7fa5d8265f1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7fa5d8266114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7fa5d82662da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7fa678933420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7fa5d82df5d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7fa5d8304859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7fa5d822142f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7fa5d8224798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7fa5d826e989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7fa5d822362d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7fa5d82dcc4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7fa5d838717a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x55ee865b9b08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x55ee865aa112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55ee865a327a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55ee865b4c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ee865a481b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55ee865b4ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x55ee865c2a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x55ee866d29b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x55ee86560817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x55ee865abf83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x55ee865a9d36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55ee865b4ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ee865a481b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55ee865b4ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ee865a481b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55ee865b4ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ee865a481b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55ee865b4ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ee865a481b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55ee865a327a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55ee865b4c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x55ee865a8fa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55ee865a327a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x55ee865c2935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x55ee865c3104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x55ee86689fc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x55ee865ad2bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55ee865a81bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55ee865b4ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x55ee865c2c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x55ee865a81bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55ee865b4ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ee865a481b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55ee865a327a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55ee865b4c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x55ee865a481b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x55ee865b4ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x55ee865a4568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55ee865a327a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x55ee865b4c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x55ee865a53cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x55ee865a327a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x55ee865a2f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x55ee865a2eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x55ee866538bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x55ee86681adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x55ee8667dc24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x55ee866757ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x55ee866756bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x55ee866748a2]
=================================
2023-06-16 05:53:46,626 - distributed.worker - WARNING - Compute Failed
Key:       ('merge_chunk-feee96511e642af8981667d04dc1183b', 0)
Function:  subgraph_callable-dd1737c7-222d-46a9-86dd-90a6bfec
args:      (               key   payload
shuffle                     
0           174912   6183863
0           176881  49984394
0           245736  50524900
0            37928  12108692
0            49297  24877150
...            ...       ...
7        799716326  63842486
7        799621826  67899390
7        799551064   2894699
7        799907572  33710924
7        799959411  24102895

[99999977 rows x 2 columns],                  key   payload
52352      300143644  20321415
52371      607309620   4421508
11981      826312204  88216588
52375      411986224  19017095
11992      823206506  28989660
...              ...       ...
99999205  1564522028  47547408
99999206   789754154  21277106
99999208  1512795401  36752781
99999229  1508036490  54594408
99999231  1543492327  88919516

[100005446 rows x 2 columns])
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-16 05:53:46,705 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:36475 -> ucx://127.0.0.1:57203
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f8714033100, tag: 0x766156766357d639, nbytes: 100050144, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-16 05:53:46,711 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-5cc2917f232a20f59eb05c2596aa8a60', 2)
Function:  <dask.layers.CallableLazyImport object at 0x7fa488
args:      ([               key   payload
shuffle                     
0           234853  60153597
0            59445  95882614
0           200900  60425211
0            94887  52152731
0            18685  59392269
...            ...       ...
0        799948571  35573131
0        799711483  21341214
0        799810234   8687407
0        799912261  26223733
0        799966045  46658186

[12497244 rows x 2 columns],                key   payload
shuffle                     
1           189393  61089423
1           305540  71651472
1           221640  73070602
1           138026  81006227
1           171990  16615442
...            ...       ...
1        799917192  69512521
1        799973841  35745400
1        799891534  29568309
1        799954300  68036624
1        799881728  70685953

[12500558 rows x 2 columns],                key   payload
shuffle                     
2           151554  40050502
2           147542  34261618
2           187188  77213387
2            82750  66085106
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-16 05:53:46,714 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47813 -> ucx://127.0.0.1:57203
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa9dcc1c1c0, tag: 0x44443894c7bd9146, nbytes: 99985208, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-16 05:53:46,769 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-16 05:53:46,770 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-16 05:53:46,774 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:59901 -> ucx://127.0.0.1:57203
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7f3ea592f240, tag: 0x3de2857f2aa1e2f6, nbytes: 100020640, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Endpoint timeout

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-16 05:53:46,786 - distributed.nanny - WARNING - Restarting worker
[dgx13:75266:0:75266] Caught signal 11 (Segmentation fault: address not mapped to object at address 0x448)
==== backtrace (tid:  75266) ====
 0  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_handle_error+0x2fd) [0x7f51300fcf1d]
 1  /opt/conda/envs/gdf/lib/libucs.so.0(+0x29114) [0x7f51300fd114]
 2  /opt/conda/envs/gdf/lib/libucs.so.0(+0x292da) [0x7f51300fd2da]
 3  /usr/lib/x86_64-linux-gnu/libpthread.so.0(+0x14420) [0x7f51c27b2420]
 4  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_mem_type_unpack+0x14) [0x7f51301765d4]
 5  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_rndv_data_handler+0x4e9) [0x7f513019b859]
 6  /opt/conda/envs/gdf/lib/libuct.so.0(+0x2042f) [0x7f51300b842f]
 7  /opt/conda/envs/gdf/lib/libuct.so.0(+0x23798) [0x7f51300bb798]
 8  /opt/conda/envs/gdf/lib/libucs.so.0(ucs_event_set_wait+0xb9) [0x7f5130105989]
 9  /opt/conda/envs/gdf/lib/libuct.so.0(uct_tcp_iface_progress+0x7d) [0x7f51300ba62d]
10  /opt/conda/envs/gdf/lib/libucp.so.0(ucp_worker_progress+0x5a) [0x7f5130173c4a]
11  /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/_libs/ucx_api.cpython-39-x86_64-linux-gnu.so(+0x2b17a) [0x7f513021e17a]
12  /opt/conda/envs/gdf/bin/python(+0x13eb08) [0x5575ac40cb08]
13  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5f82) [0x5575ac3fd112]
14  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5575ac3f627a]
15  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5575ac407c05]
16  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5575ac3f781b]
17  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5575ac407ef3]
18  /opt/conda/envs/gdf/bin/python(+0x147a16) [0x5575ac415a16]
19  /opt/conda/envs/gdf/bin/python(+0x2579b1) [0x5575ac5259b1]
20  /opt/conda/envs/gdf/bin/python(+0xe5817) [0x5575ac3b3817]
21  /opt/conda/envs/gdf/bin/python(+0x130f83) [0x5575ac3fef83]
22  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x5ba6) [0x5575ac3fcd36]
23  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5575ac407ef3]
24  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5575ac3f781b]
25  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5575ac407ef3]
26  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5575ac3f781b]
27  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5575ac407ef3]
28  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5575ac3f781b]
29  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5575ac407ef3]
30  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5575ac3f781b]
31  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5575ac3f627a]
32  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5575ac407c05]
33  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x4e17) [0x5575ac3fbfa7]
34  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5575ac3f627a]
35  /opt/conda/envs/gdf/bin/python(+0x147935) [0x5575ac415935]
36  /opt/conda/envs/gdf/bin/python(PyObject_Call+0xb4) [0x5575ac416104]
37  /opt/conda/envs/gdf/bin/python(+0x20efc8) [0x5575ac4dcfc8]
38  /opt/conda/envs/gdf/bin/python(_PyObject_MakeTpCall+0x2fc) [0x5575ac4002bc]
39  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5575ac3fb1bb]
40  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5575ac407ef3]
41  /opt/conda/envs/gdf/bin/python(+0x147c72) [0x5575ac415c72]
42  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x402b) [0x5575ac3fb1bb]
43  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5575ac407ef3]
44  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5575ac3f781b]
45  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5575ac3f627a]
46  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5575ac407c05]
47  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x68b) [0x5575ac3f781b]
48  /opt/conda/envs/gdf/bin/python(+0x139ef3) [0x5575ac407ef3]
49  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x3d8) [0x5575ac3f7568]
50  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5575ac3f627a]
51  /opt/conda/envs/gdf/bin/python(_PyFunction_Vectorcall+0xd5) [0x5575ac407c05]
52  /opt/conda/envs/gdf/bin/python(_PyEval_EvalFrameDefault+0x123b) [0x5575ac3f83cb]
53  /opt/conda/envs/gdf/bin/python(+0x12827a) [0x5575ac3f627a]
54  /opt/conda/envs/gdf/bin/python(_PyEval_EvalCodeWithName+0x47) [0x5575ac3f5f07]
55  /opt/conda/envs/gdf/bin/python(PyEval_EvalCodeEx+0x39) [0x5575ac3f5eb9]
56  /opt/conda/envs/gdf/bin/python(PyEval_EvalCode+0x1b) [0x5575ac4a68bb]
57  /opt/conda/envs/gdf/bin/python(+0x206adc) [0x5575ac4d4adc]
58  /opt/conda/envs/gdf/bin/python(+0x202c24) [0x5575ac4d0c24]
59  /opt/conda/envs/gdf/bin/python(PyRun_StringFlags+0x9d) [0x5575ac4c87ed]
60  /opt/conda/envs/gdf/bin/python(PyRun_SimpleStringFlags+0x3d) [0x5575ac4c86bd]
61  /opt/conda/envs/gdf/bin/python(Py_RunMain+0x262) [0x5575ac4c78a2]
=================================
2023-06-16 05:53:46,839 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 3)
Function:  <dask.layers.CallableLazyImport object at 0x7fa488
args:      ([                key   payload
52361     845874532  49972219
11969     809639735  27499924
11971     862223315  86004209
11974     404055237  86876214
11978     864210713  78639098
...             ...       ...
99983250  869560834  44036448
99988964  503551619  88502134
99988968  823642240  63789121
99988978  818457209  64192785
99988982  833779610  95447319

[12498632 rows x 2 columns],                 key   payload
93314     922258017  25893666
93339     969999243  61321754
14565     924739189  61334848
93341     224711439  52606668
14585     923617105  79859781
...             ...       ...
99984779  903080960  61521118
99984790  952405317  98783213
99979537  968139646  41858583
99979538  903504646  65859528
99979544  513922785  51755715

[12502237 rows x 2 columns],                  key   payload
11494     1006868113  47937112
11500     1020182072  72309059
11503     1022460200  16275859
11518      527681614  78094831
61986     1066441335   6255663
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-16 05:53:46,882 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-5cc2917f232a20f59eb05c2596aa8a60', 3)
Function:  <dask.layers.CallableLazyImport object at 0x7f396b
args:      ([               key   payload
shuffle                     
0           176446   5526106
0            66801  27795434
0           210321  33401056
0            27192  56567218
0            58928  36720553
...            ...       ...
0        799860417  72860648
0        799810255  15731093
0        799907816  51304735
0        799986982  59123984
0        799803925  98987672

[12497076 rows x 2 columns],                key   payload
shuffle                     
1           195575  48804980
1           192888   7568150
1          1115761  30296026
1          1092347  11518528
1          1156164  63797978
...            ...       ...
1        799940029  26540213
1        799882791  64843025
1        799872731  74431281
1        799976873  56756607
1        799993614   2714786

[12501362 rows x 2 columns],                key   payload
shuffle                     
2            82619   8601398
2           254150  80704158
2           173557  42101805
2           185777  87288931
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-16 05:53:47,036 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:60177 -> ucx://127.0.0.1:51279
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 333, in write
    await self.ep.send(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 650, in send
    return await comm.tag_send(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Send #007] ep: 0x7fa5e4fe4100, tag: 0x98c11363751ea48b, nbytes: 99986296, type: <class 'cudf.core.buffer.buffer.Buffer'>>: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-16 05:53:47,047 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-16 05:53:47,047 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Task exception was never retrieved
future: <Task finished name='Task-2007' coro=<_listener_handler_coroutine() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py:128> exception=UCXError('<stream_send>: Connection reset by remote peer')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 143, in _listener_handler_coroutine
    peer_info = await exchange_peer_info(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 51, in exchange_peer_info
    await comm.stream_send(endpoint, my_info_arr, my_info_arr.nbytes)
ucp._libs.exceptions.UCXError: <stream_send>: Connection reset by remote peer
2023-06-16 05:53:47,054 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
2023-06-16 05:53:47,061 - distributed.core - ERROR - unpack(b) received extra data.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
2023-06-16 05:53:47,062 - distributed.worker - ERROR - unpack(b) received extra data.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 402, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 158, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
2023-06-16 05:53:47,070 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-16 05:53:47,071 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-16 05:53:47,076 - distributed.core - ERROR - Exception while handling op get_data
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 830, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 987, in wrapper
    return await func(self, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1794, in get_data
    assert response == "OK", response
AssertionError: {'op': 'get_data', 'keys': {"('split-simple-shuffle-5cc2917f232a20f59eb05c2596aa8a60', 6, 4)"}, 'who': 'ucx://127.0.0.1:59901', 'max_connections': None, 'reply': True}
2023-06-16 05:53:47,079 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:47813
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 359, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #083] ep: 0x7f3ea592f280, tag: 0x39a7de092ae8347c, nbytes: 1368, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #083] ep: 0x7f3ea592f280, tag: 0x39a7de092ae8347c, nbytes: 1368, type: <class 'numpy.ndarray'>>: Message truncated")
2023-06-16 05:53:47,080 - distributed.worker - ERROR - failed during get data with ucx://127.0.0.1:47813 -> ucx://127.0.0.1:59901
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 315, in write
    await self.ep.send(struct.pack("?Q", False, nframes))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 629, in send
    self._ep.raise_on_error()
  File "ucp/_libs/ucx_endpoint.pyx", line 353, in ucp._libs.ucx_api.UCXEndpoint.raise_on_error
ucp._libs.exceptions.UCXConnectionReset: Endpoint 0x7fa9dcc1c340 error: Connection reset by remote peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1792, in get_data
    compressed = await comm.write(msg, serializers=serializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 337, in write
    raise CommClosedError("While writing, the connection was closed")
distributed.comm.core.CommClosedError: While writing, the connection was closed
2023-06-16 05:53:47,080 - distributed.worker - ERROR - Worker stream died during communication: ucx://127.0.0.1:60177
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 359, in read
    await self.ep.recv(header)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/core.py", line 725, in recv
    ret = await comm.tag_recv(self._ep, buffer, nbytes, tag, name=log)
ucp._libs.exceptions.UCXError: <[Recv #080] ep: 0x7f3ea592f340, tag: 0xd5fb5d47e6bbca7f, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 367, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXError("<[Recv #080] ep: 0x7f3ea592f340, tag: 0xd5fb5d47e6bbca7f, nbytes: 16, type: <class 'numpy.ndarray'>>: Message truncated")
2023-06-16 05:53:47,118 - distributed.nanny - WARNING - Restarting worker
2023-06-16 05:53:47,132 - distributed.core - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-16 05:53:47,132 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2066, in gather_dep
    response = await get_data_from_worker(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2892, in get_data_from_worker
    response = await send_recv(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 752, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 372, in read
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 373, in <listcomp>
    device_array(each_size) if is_cuda else host_array(each_size)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 171, in device_array
    return rmm.DeviceBuffer(size=n)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded
2023-06-16 05:53:47,286 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-84a0276e6a854cf0d6c4324869a61d39', 6)
Function:  <dask.layers.CallableLazyImport object at 0x7fa488
args:      ([                key   payload
52356     107015767  65816641
52363     860285758  91329192
11977     108472361  94223463
52364     847487724  64866791
11979     606322617  41005339
...             ...       ...
99983259  858130947  98743454
99988963  813770375  53271316
99988969  820789229  29925606
99988974  822885650  93160534
99988987  855857119  63667796

[12497796 rows x 2 columns],                 key   payload
93315     953363793  32105320
93316     968678558  18922311
14563     917281600  29321174
93322     925079527  39318298
14570     930152530   8447666
...             ...       ...
99984771  907835094  78838950
99984773  913245931  15457828
99984787  953452903  14163861
99984799  617579875  93240527
99979523  964012979  15519510

[12497151 rows x 2 columns],                  key   payload
11492       32905415  22123917
11501       36867759  76980431
61998     1033817756  90655125
63687     1050139201  38849758
62012     1064342578   5693645
...              ...       ...
9
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-16 05:53:47,447 - distributed.worker - WARNING - Compute Failed
Key:       ('simple-shuffle-5cc2917f232a20f59eb05c2596aa8a60', 7)
Function:  <dask.layers.CallableLazyImport object at 0x7fa488
args:      ([               key   payload
shuffle                     
0           173922  88069786
0           229935  15251244
0           183227  71192648
0            54973  29191818
0            10857  78481497
...            ...       ...
0        799785647   1667167
0        799977396  45718063
0        799971606  74943828
0        799852573  31202353
0        799825944  70269550

[12498151 rows x 2 columns],                key   payload
shuffle                     
1           243088  64795186
1           144172  11491729
1           192876   3899066
1           275304  40148898
1          1113199  14071429
...            ...       ...
1        799867577  20435932
1        799907496  20392982
1        799896753  27928698
1        799899733  59418851
1        799908449  71125128

[12498591 rows x 2 columns],                key   payload
shuffle                     
2           170684  71845220
2           128142  83100427
2           162541  66281042
2           221101  39464387
2         
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/envs/gdf/include/rmm/mr/device/pool_memory_resource.hpp:196: Maximum pool size exceeded')"

2023-06-16 05:53:48,314 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:48,314 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-06-16 05:53:48,604 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-06-16 05:53:48,604 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
