============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-07 06:31:17,052 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:17,056 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42377 instead
  warnings.warn(
2024-01-07 06:31:17,060 - distributed.scheduler - INFO - State start
2024-01-07 06:31:17,122 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:17,123 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-07 06:31:17,124 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42377/status
2024-01-07 06:31:17,124 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:31:17,221 - distributed.scheduler - INFO - Receive client connection: Client-5ba8f27e-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:17,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47610
2024-01-07 06:31:17,381 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39555'
2024-01-07 06:31:17,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44439'
2024-01-07 06:31:17,403 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43129'
2024-01-07 06:31:17,411 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39461'
2024-01-07 06:31:19,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:19,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:19,222 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:19,222 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:19,226 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:19,226 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:19,227 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35607
2024-01-07 06:31:19,227 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35607
2024-01-07 06:31:19,227 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45823
2024-01-07 06:31:19,227 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34465
2024-01-07 06:31:19,227 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45823
2024-01-07 06:31:19,227 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-07 06:31:19,227 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,227 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34865
2024-01-07 06:31:19,227 - distributed.worker - INFO -               Threads:                          4
2024-01-07 06:31:19,227 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-07 06:31:19,227 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,227 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-07 06:31:19,227 - distributed.worker - INFO -               Threads:                          4
2024-01-07 06:31:19,227 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-4jhyb2oc
2024-01-07 06:31:19,227 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-07 06:31:19,227 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-v96q8c0j
2024-01-07 06:31:19,228 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f5411f6-f334-4a8a-84f7-6cf5f6b91f28
2024-01-07 06:31:19,228 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c68855d6-17a2-4592-b95b-42ba805006fe
2024-01-07 06:31:19,228 - distributed.worker - INFO - Starting Worker plugin PreImport-92af08d7-7739-4068-930e-197767d62ef6
2024-01-07 06:31:19,228 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5c6e8509-2f6c-4a85-99f8-cd8a3919a378
2024-01-07 06:31:19,228 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,228 - distributed.worker - INFO - Starting Worker plugin PreImport-ceb4b3f7-b44e-4c38-a62f-b32ff62770d4
2024-01-07 06:31:19,228 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e7fe0f9f-92d8-4345-a339-eb0eaa9a9c60
2024-01-07 06:31:19,228 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,255 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:19,255 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:19,255 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:19,255 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:19,259 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:19,259 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:19,260 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46387
2024-01-07 06:31:19,260 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46387
2024-01-07 06:31:19,260 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42047
2024-01-07 06:31:19,260 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46531
2024-01-07 06:31:19,260 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42047
2024-01-07 06:31:19,260 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-07 06:31:19,260 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41905
2024-01-07 06:31:19,260 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,261 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-07 06:31:19,261 - distributed.worker - INFO -               Threads:                          4
2024-01-07 06:31:19,261 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,261 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-07 06:31:19,261 - distributed.worker - INFO -               Threads:                          4
2024-01-07 06:31:19,261 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-6wn1di6y
2024-01-07 06:31:19,261 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-07 06:31:19,261 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-f0h8a5kt
2024-01-07 06:31:19,261 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6cd6626f-ac2d-446b-888d-bbfed823a327
2024-01-07 06:31:19,261 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6b9e4fa-67b5-45a2-9dfa-acf0b93e6bc7
2024-01-07 06:31:19,261 - distributed.worker - INFO - Starting Worker plugin PreImport-ce4fc11c-f137-49eb-81e2-bbf2167e3981
2024-01-07 06:31:19,261 - distributed.worker - INFO - Starting Worker plugin RMMSetup-71f1c329-c5ec-4d46-8d42-711a3fad68fd
2024-01-07 06:31:19,261 - distributed.worker - INFO - Starting Worker plugin PreImport-76db8a93-a53a-4d70-8672-dba23261a1ab
2024-01-07 06:31:19,261 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,261 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b786a0c-31d5-4ec1-9ea6-9cf3e67e042f
2024-01-07 06:31:19,262 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,330 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35607', status: init, memory: 0, processing: 0>
2024-01-07 06:31:19,332 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35607
2024-01-07 06:31:19,332 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47678
2024-01-07 06:31:19,333 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:19,333 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-07 06:31:19,333 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,335 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-07 06:31:19,345 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45823', status: init, memory: 0, processing: 0>
2024-01-07 06:31:19,346 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45823
2024-01-07 06:31:19,346 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47688
2024-01-07 06:31:19,347 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:19,348 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-07 06:31:19,348 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,349 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-07 06:31:19,379 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42047', status: init, memory: 0, processing: 0>
2024-01-07 06:31:19,380 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42047
2024-01-07 06:31:19,380 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47696
2024-01-07 06:31:19,381 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:19,382 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-07 06:31:19,382 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,382 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46387', status: init, memory: 0, processing: 0>
2024-01-07 06:31:19,382 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46387
2024-01-07 06:31:19,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47710
2024-01-07 06:31:19,383 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-07 06:31:19,383 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:19,384 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-07 06:31:19,384 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:19,386 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-07 06:31:19,460 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-07 06:31:19,460 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-07 06:31:19,460 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-07 06:31:19,460 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-07 06:31:19,465 - distributed.scheduler - INFO - Remove client Client-5ba8f27e-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:19,465 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47610; closing.
2024-01-07 06:31:19,466 - distributed.scheduler - INFO - Remove client Client-5ba8f27e-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:19,466 - distributed.scheduler - INFO - Close client connection: Client-5ba8f27e-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:19,467 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39555'. Reason: nanny-close
2024-01-07 06:31:19,467 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:19,468 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44439'. Reason: nanny-close
2024-01-07 06:31:19,468 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:19,468 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43129'. Reason: nanny-close
2024-01-07 06:31:19,469 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:19,468 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45823. Reason: nanny-close
2024-01-07 06:31:19,469 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39461'. Reason: nanny-close
2024-01-07 06:31:19,469 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35607. Reason: nanny-close
2024-01-07 06:31:19,469 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:19,469 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46387. Reason: nanny-close
2024-01-07 06:31:19,470 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42047. Reason: nanny-close
2024-01-07 06:31:19,471 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-07 06:31:19,471 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-07 06:31:19,471 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47688; closing.
2024-01-07 06:31:19,471 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45823', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609079.471528')
2024-01-07 06:31:19,471 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-07 06:31:19,472 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-07 06:31:19,472 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47678; closing.
2024-01-07 06:31:19,472 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:19,472 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:19,472 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35607', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609079.4727771')
2024-01-07 06:31:19,473 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47696; closing.
2024-01-07 06:31:19,473 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:19,473 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:19,473 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42047', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609079.4736474')
2024-01-07 06:31:19,474 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47710; closing.
2024-01-07 06:31:19,474 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:47678>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-07 06:31:19,475 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:47696>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:47696>: Stream is closed
2024-01-07 06:31:19,476 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46387', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609079.4761856')
2024-01-07 06:31:19,476 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:31:20,132 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:31:20,133 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:31:20,133 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:31:20,135 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-07 06:31:20,136 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-07 06:31:22,340 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:22,346 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-07 06:31:22,349 - distributed.scheduler - INFO - State start
2024-01-07 06:31:22,373 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:22,375 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:31:22,376 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-07 06:31:22,376 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:31:22,489 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32959'
2024-01-07 06:31:22,512 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37575'
2024-01-07 06:31:22,531 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45821'
2024-01-07 06:31:22,535 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36245'
2024-01-07 06:31:22,545 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35425'
2024-01-07 06:31:22,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36165'
2024-01-07 06:31:22,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42875'
2024-01-07 06:31:22,576 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43539'
2024-01-07 06:31:23,791 - distributed.scheduler - INFO - Receive client connection: Client-5ecadaf8-ad26-11ee-b255-d8c49764f6bb
2024-01-07 06:31:23,808 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58810
2024-01-07 06:31:23,809 - distributed.scheduler - INFO - Receive client connection: Client-5ec08ffe-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:23,810 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58820
2024-01-07 06:31:24,547 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:24,547 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:24,551 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:24,552 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45771
2024-01-07 06:31:24,552 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45771
2024-01-07 06:31:24,553 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34795
2024-01-07 06:31:24,553 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:24,553 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:24,553 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:24,553 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:24,553 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j_fv98r8
2024-01-07 06:31:24,553 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0e9f00be-165b-4370-800a-568cc904136d
2024-01-07 06:31:24,553 - distributed.worker - INFO - Starting Worker plugin PreImport-b5ad8157-f243-48b2-bbe0-505fcc28ddb4
2024-01-07 06:31:24,553 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9618a2b5-f477-4dff-9d26-1f56908097ba
2024-01-07 06:31:24,560 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:24,560 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:24,561 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:24,561 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:24,561 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:24,561 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:24,563 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:24,563 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:24,563 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:24,563 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:24,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:24,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:24,564 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:24,564 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:24,565 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:24,565 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40021
2024-01-07 06:31:24,566 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40021
2024-01-07 06:31:24,566 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37133
2024-01-07 06:31:24,566 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:24,566 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:24,566 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:24,566 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:24,566 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3aghl1xd
2024-01-07 06:31:24,566 - distributed.worker - INFO - Starting Worker plugin PreImport-fe6ccd71-880f-47f8-8194-7740f987f6e8
2024-01-07 06:31:24,566 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e00e976-f9d5-43f6-ae03-17adbc3d52d8
2024-01-07 06:31:24,566 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:24,566 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ad4a503-631e-48b3-8cf3-48a87d84f6ec
2024-01-07 06:31:24,567 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40327
2024-01-07 06:31:24,567 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40327
2024-01-07 06:31:24,567 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:24,567 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45445
2024-01-07 06:31:24,567 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:24,568 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:24,568 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:24,568 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:24,568 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7pxa6s1h
2024-01-07 06:31:24,568 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11bc6e87-250d-44f7-a355-5c0a959109d0
2024-01-07 06:31:24,568 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34167
2024-01-07 06:31:24,568 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34167
2024-01-07 06:31:24,568 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42277
2024-01-07 06:31:24,568 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:24,568 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:24,568 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:24,568 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:24,569 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:24,569 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:24,569 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-69g4apou
2024-01-07 06:31:24,569 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fb00ec7-3bf0-447d-8511-4f8f92e499cb
2024-01-07 06:31:24,569 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:24,569 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45295
2024-01-07 06:31:24,569 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45295
2024-01-07 06:31:24,569 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37913
2024-01-07 06:31:24,569 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:24,570 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:24,570 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:24,570 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:24,570 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39335
2024-01-07 06:31:24,570 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9otjdjtw
2024-01-07 06:31:24,570 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39335
2024-01-07 06:31:24,570 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42997
2024-01-07 06:31:24,570 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:24,570 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:24,570 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b98c294-5554-42a8-a787-c113b758c1e2
2024-01-07 06:31:24,570 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:24,570 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:24,570 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:24,570 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d31s6sa2
2024-01-07 06:31:24,570 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46037
2024-01-07 06:31:24,570 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46037
2024-01-07 06:31:24,570 - distributed.worker - INFO - Starting Worker plugin PreImport-77eedc19-99c9-43a5-ace9-4be9235311a4
2024-01-07 06:31:24,570 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45067
2024-01-07 06:31:24,570 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:24,570 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-531b3842-25c2-4c84-9b35-660e22ba9e15
2024-01-07 06:31:24,570 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:24,570 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:24,571 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:24,571 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-93nw2fbh
2024-01-07 06:31:24,571 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85eb336d-f813-4f22-86e1-353cfff5e819
2024-01-07 06:31:24,571 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f1473fb1-dcc7-4835-a353-5cef1cb53a44
2024-01-07 06:31:24,571 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37387
2024-01-07 06:31:24,572 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37387
2024-01-07 06:31:24,572 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39225
2024-01-07 06:31:24,572 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:24,572 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:24,572 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:24,572 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:24,572 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3_w54ju_
2024-01-07 06:31:24,572 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d245d440-effb-4cbf-accd-afce1296a2cf
2024-01-07 06:31:25,024 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41705', status: init, memory: 0, processing: 0>
2024-01-07 06:31:25,026 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41705
2024-01-07 06:31:25,026 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58844
2024-01-07 06:31:25,036 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:25,038 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:25,040 - distributed.scheduler - INFO - Remove client Client-5ecadaf8-ad26-11ee-b255-d8c49764f6bb
2024-01-07 06:31:25,041 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58810; closing.
2024-01-07 06:31:25,041 - distributed.scheduler - INFO - Remove client Client-5ecadaf8-ad26-11ee-b255-d8c49764f6bb
2024-01-07 06:31:25,042 - distributed.scheduler - INFO - Close client connection: Client-5ecadaf8-ad26-11ee-b255-d8c49764f6bb
2024-01-07 06:31:25,045 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58844; closing.
2024-01-07 06:31:25,045 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41705', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609085.0457127')
2024-01-07 06:31:25,046 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:31:26,491 - distributed.scheduler - INFO - Receive client connection: Client-6250204b-ad26-11ee-b255-d8c49764f6bb
2024-01-07 06:31:26,491 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58860
2024-01-07 06:31:26,764 - distributed.worker - INFO - Starting Worker plugin PreImport-c09f52c9-3406-4bdf-8891-b4ed3149bfeb
2024-01-07 06:31:26,766 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b7ca3e25-1432-49eb-ac2f-19c594f3cdce
2024-01-07 06:31:26,766 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,774 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,796 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45295', status: init, memory: 0, processing: 0>
2024-01-07 06:31:26,797 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45295
2024-01-07 06:31:26,797 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58872
2024-01-07 06:31:26,798 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:26,799 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45771', status: init, memory: 0, processing: 0>
2024-01-07 06:31:26,799 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:26,799 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,800 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45771
2024-01-07 06:31:26,800 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58878
2024-01-07 06:31:26,801 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:26,801 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:26,802 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:26,802 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,804 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-07 06:31:26,804 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:26,804 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb19b6db-13ea-4a0e-afc2-656d61b74d41
2024-01-07 06:31:26,805 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-07 06:31:26,806 - distributed.worker - INFO - Starting Worker plugin PreImport-c862b8d8-8df5-48b3-b3e0-3c324a03aa2b
2024-01-07 06:31:26,806 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,806 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,823 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:26,823 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b79c58de-57c2-4d8a-9a99-62e5c292eabc
2024-01-07 06:31:26,823 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:26,823 - distributed.worker - INFO - Starting Worker plugin PreImport-920c87aa-bb9d-46d6-8d33-0740044de050
2024-01-07 06:31:26,825 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,826 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,829 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6d5c067-5859-4d39-b9f3-5c0cac51c3e6
2024-01-07 06:31:26,830 - distributed.worker - INFO - Starting Worker plugin PreImport-76f47262-b9fb-499e-aefb-e56824cc7581
2024-01-07 06:31:26,831 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,831 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:26,833 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:26,837 - distributed.scheduler - INFO - Remove client Client-6250204b-ad26-11ee-b255-d8c49764f6bb
2024-01-07 06:31:26,838 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58860; closing.
2024-01-07 06:31:26,838 - distributed.scheduler - INFO - Remove client Client-6250204b-ad26-11ee-b255-d8c49764f6bb
2024-01-07 06:31:26,838 - distributed.scheduler - INFO - Close client connection: Client-6250204b-ad26-11ee-b255-d8c49764f6bb
2024-01-07 06:31:26,841 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a774a60e-deda-4166-84ab-c90b071797c6
2024-01-07 06:31:26,842 - distributed.worker - INFO - Starting Worker plugin PreImport-8c274b5c-b95f-4fcd-a408-87a9379cfad6
2024-01-07 06:31:26,843 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,845 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40021', status: init, memory: 0, processing: 0>
2024-01-07 06:31:26,846 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40021
2024-01-07 06:31:26,846 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58886
2024-01-07 06:31:26,848 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:26,849 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:26,849 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,851 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:26,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46037', status: init, memory: 0, processing: 0>
2024-01-07 06:31:26,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46037
2024-01-07 06:31:26,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58880
2024-01-07 06:31:26,860 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:26,862 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:26,862 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:26,865 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40327', status: init, memory: 0, processing: 0>
2024-01-07 06:31:26,866 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40327
2024-01-07 06:31:26,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58904
2024-01-07 06:31:26,867 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:26,868 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:26,869 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:26,871 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39335', status: init, memory: 0, processing: 0>
2024-01-07 06:31:26,872 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39335
2024-01-07 06:31:26,872 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58902
2024-01-07 06:31:26,873 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:26,874 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:26,874 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,876 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:26,879 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34167', status: init, memory: 0, processing: 0>
2024-01-07 06:31:26,880 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34167
2024-01-07 06:31:26,880 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58888
2024-01-07 06:31:26,883 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:26,885 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:26,885 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:26,890 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37387', status: init, memory: 0, processing: 0>
2024-01-07 06:31:26,890 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37387
2024-01-07 06:31:26,890 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:58920
2024-01-07 06:31:26,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:26,894 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:26,894 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:26,897 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:26,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:26,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:26,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:26,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:26,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:26,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:26,971 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:26,972 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:26,977 - distributed.scheduler - INFO - Remove client Client-5ec08ffe-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:26,978 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58820; closing.
2024-01-07 06:31:26,978 - distributed.scheduler - INFO - Remove client Client-5ec08ffe-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:26,978 - distributed.scheduler - INFO - Close client connection: Client-5ec08ffe-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:26,979 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32959'. Reason: nanny-close
2024-01-07 06:31:26,980 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:26,980 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37575'. Reason: nanny-close
2024-01-07 06:31:26,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:26,981 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45821'. Reason: nanny-close
2024-01-07 06:31:26,981 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45771. Reason: nanny-close
2024-01-07 06:31:26,981 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:26,982 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36245'. Reason: nanny-close
2024-01-07 06:31:26,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40021. Reason: nanny-close
2024-01-07 06:31:26,982 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:26,982 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35425'. Reason: nanny-close
2024-01-07 06:31:26,982 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:26,982 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39335. Reason: nanny-close
2024-01-07 06:31:26,983 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36165'. Reason: nanny-close
2024-01-07 06:31:26,983 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:26,983 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42875'. Reason: nanny-close
2024-01-07 06:31:26,983 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:26,983 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37387. Reason: nanny-close
2024-01-07 06:31:26,983 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:26,983 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58878; closing.
2024-01-07 06:31:26,983 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45295. Reason: nanny-close
2024-01-07 06:31:26,984 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43539'. Reason: nanny-close
2024-01-07 06:31:26,984 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45771', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609086.9841127')
2024-01-07 06:31:26,984 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:26,984 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46037. Reason: nanny-close
2024-01-07 06:31:26,984 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:26,985 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40327. Reason: nanny-close
2024-01-07 06:31:26,985 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:26,985 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:26,986 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:26,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58902; closing.
2024-01-07 06:31:26,986 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58886; closing.
2024-01-07 06:31:26,986 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:26,987 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:26,986 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34167. Reason: nanny-close
2024-01-07 06:31:26,987 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:26,987 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:26,987 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39335', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609086.9876125')
2024-01-07 06:31:26,987 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:26,988 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40021', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609086.9880452')
2024-01-07 06:31:26,988 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:26,988 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58920; closing.
2024-01-07 06:31:26,988 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58872; closing.
2024-01-07 06:31:26,988 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:26,989 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:26,989 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37387', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609086.9893143')
2024-01-07 06:31:26,989 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:26,989 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45295', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609086.9897194')
2024-01-07 06:31:26,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58880; closing.
2024-01-07 06:31:26,990 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58904; closing.
2024-01-07 06:31:26,990 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46037', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609086.9907436')
2024-01-07 06:31:26,991 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609086.9911168')
2024-01-07 06:31:26,991 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:26,992 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:58888; closing.
2024-01-07 06:31:26,992 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34167', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609086.992417')
2024-01-07 06:31:26,992 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:31:26,993 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:26,992 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:58888>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-07 06:31:28,046 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:31:28,047 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:31:28,048 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:31:28,050 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:31:28,050 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-07 06:31:30,397 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:30,402 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-07 06:31:30,405 - distributed.scheduler - INFO - State start
2024-01-07 06:31:30,427 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:30,428 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:31:30,428 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-07 06:31:30,429 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:31:30,547 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35191'
2024-01-07 06:31:30,562 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46109'
2024-01-07 06:31:30,570 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37251'
2024-01-07 06:31:30,587 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45751'
2024-01-07 06:31:30,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46207'
2024-01-07 06:31:30,602 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41155'
2024-01-07 06:31:30,615 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36905'
2024-01-07 06:31:30,625 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39153'
2024-01-07 06:31:30,721 - distributed.scheduler - INFO - Receive client connection: Client-638d3d96-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:30,736 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44454
2024-01-07 06:31:32,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:32,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:32,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:32,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:32,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:32,524 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:32,524 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:32,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:32,525 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:32,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:32,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:32,528 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:32,528 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:32,528 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:32,528 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:32,528 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:32,528 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:32,529 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35873
2024-01-07 06:31:32,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:32,529 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35873
2024-01-07 06:31:32,529 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44029
2024-01-07 06:31:32,529 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44029
2024-01-07 06:31:32,529 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43231
2024-01-07 06:31:32,529 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42387
2024-01-07 06:31:32,529 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:32,529 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43499
2024-01-07 06:31:32,529 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42387
2024-01-07 06:31:32,529 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:32,529 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:32,529 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35431
2024-01-07 06:31:32,529 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:32,529 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:32,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:32,530 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:32,530 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:32,530 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:32,530 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:32,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1iht3uxn
2024-01-07 06:31:32,530 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:32,530 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:32,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0v_8w82p
2024-01-07 06:31:32,530 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:32,530 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dhceep7v
2024-01-07 06:31:32,530 - distributed.worker - INFO - Starting Worker plugin RMMSetup-94440d9a-cb25-4675-ba30-b7ab3f1cee5c
2024-01-07 06:31:32,530 - distributed.worker - INFO - Starting Worker plugin PreImport-7677d134-8dc3-4b3f-81b7-4fac38dca178
2024-01-07 06:31:32,530 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9071ebc6-add9-449b-8cc7-7dab9a1fe022
2024-01-07 06:31:32,530 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8ae639f-1e3d-4b75-8bf1-a42d2d963812
2024-01-07 06:31:32,530 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ff30b89-293a-4ea5-94f7-2938ded16f20
2024-01-07 06:31:32,530 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37323
2024-01-07 06:31:32,530 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37323
2024-01-07 06:31:32,530 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35881
2024-01-07 06:31:32,530 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38515
2024-01-07 06:31:32,530 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35881
2024-01-07 06:31:32,530 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:32,530 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43139
2024-01-07 06:31:32,530 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:32,530 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:32,531 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:32,530 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:32,531 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:32,531 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:32,531 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:32,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v1m0xzow
2024-01-07 06:31:32,531 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lyqhr74m
2024-01-07 06:31:32,531 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73e241dc-1ec0-4c96-8347-f146ebf6c6c6
2024-01-07 06:31:32,531 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-03d23dcf-e178-495d-97b8-926d2b959d40
2024-01-07 06:31:32,531 - distributed.worker - INFO - Starting Worker plugin PreImport-ce294631-a64e-4b89-a4ca-12fd0e5d61a9
2024-01-07 06:31:32,531 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c862516f-cc0f-446e-aa1e-dbc00a6dec27
2024-01-07 06:31:32,532 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:32,532 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:32,533 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39747
2024-01-07 06:31:32,533 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39747
2024-01-07 06:31:32,533 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45311
2024-01-07 06:31:32,533 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:32,533 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:32,533 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:32,533 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:32,533 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46389
2024-01-07 06:31:32,533 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-29lf05l0
2024-01-07 06:31:32,533 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46389
2024-01-07 06:31:32,533 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36503
2024-01-07 06:31:32,533 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:32,533 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:32,533 - distributed.worker - INFO - Starting Worker plugin RMMSetup-616a88e3-9196-4139-be20-0fcec9908b03
2024-01-07 06:31:32,533 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:32,533 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:32,533 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uwd0u7r7
2024-01-07 06:31:32,534 - distributed.worker - INFO - Starting Worker plugin PreImport-7397a16c-3911-4592-9769-c8e24cd6ef3f
2024-01-07 06:31:32,534 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e289bc39-9660-4543-ac3b-0b61308e8084
2024-01-07 06:31:32,534 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21ba7cf7-8d92-4bba-87ed-a786fcccfdd7
2024-01-07 06:31:32,607 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:32,607 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:32,612 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:32,612 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45357
2024-01-07 06:31:32,613 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45357
2024-01-07 06:31:32,613 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44523
2024-01-07 06:31:32,613 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:32,613 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:32,613 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:32,613 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:32,613 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z7wasg7t
2024-01-07 06:31:32,613 - distributed.worker - INFO - Starting Worker plugin PreImport-f732742b-e866-423b-a148-add20b3c4c6f
2024-01-07 06:31:32,613 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6272179c-cda4-4444-a11b-868a4737b81e
2024-01-07 06:31:32,613 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72df0c0b-c1bd-49e7-bd48-ed440b4acec1
2024-01-07 06:31:34,574 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-63366c9f-df5c-4190-a9a1-c8ce06dc4bf9
2024-01-07 06:31:34,575 - distributed.worker - INFO - Starting Worker plugin PreImport-bc96159c-e389-446a-9232-0310ca9babfd
2024-01-07 06:31:34,576 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,614 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35881', status: init, memory: 0, processing: 0>
2024-01-07 06:31:34,616 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35881
2024-01-07 06:31:34,616 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44472
2024-01-07 06:31:34,617 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:34,618 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:34,618 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,621 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:34,641 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-97358ffd-864f-4f26-a04c-3f43f29046cc
2024-01-07 06:31:34,641 - distributed.worker - INFO - Starting Worker plugin PreImport-61d55c8d-a871-4580-8fca-5a08c883cfb5
2024-01-07 06:31:34,642 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,655 - distributed.worker - INFO - Starting Worker plugin PreImport-8b8ec2b7-bd76-4cf8-83be-e8c8c3db55c1
2024-01-07 06:31:34,656 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fa76c011-1ee3-416d-9557-49a8e333476d
2024-01-07 06:31:34,658 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,661 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,667 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42387', status: init, memory: 0, processing: 0>
2024-01-07 06:31:34,668 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42387
2024-01-07 06:31:34,668 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44488
2024-01-07 06:31:34,669 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:34,670 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:34,670 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:34,685 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44029', status: init, memory: 0, processing: 0>
2024-01-07 06:31:34,686 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44029
2024-01-07 06:31:34,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44504
2024-01-07 06:31:34,687 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:34,688 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:34,688 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:34,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8369ec90-5fd2-451b-87b2-04bb69e72194
2024-01-07 06:31:34,695 - distributed.worker - INFO - Starting Worker plugin PreImport-df522220-46f2-4df0-a0e9-0c279536fc89
2024-01-07 06:31:34,695 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39747', status: init, memory: 0, processing: 0>
2024-01-07 06:31:34,696 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39747
2024-01-07 06:31:34,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44500
2024-01-07 06:31:34,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:34,699 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:34,699 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:34,709 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,723 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,733 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35873', status: init, memory: 0, processing: 0>
2024-01-07 06:31:34,734 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35873
2024-01-07 06:31:34,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44510
2024-01-07 06:31:34,735 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45357', status: init, memory: 0, processing: 0>
2024-01-07 06:31:34,735 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45357
2024-01-07 06:31:34,735 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44516
2024-01-07 06:31:34,735 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:34,736 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:34,736 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:34,736 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,737 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:34,737 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:34,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:34,746 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46389', status: init, memory: 0, processing: 0>
2024-01-07 06:31:34,747 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46389
2024-01-07 06:31:34,747 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44520
2024-01-07 06:31:34,748 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:34,749 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:34,749 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,750 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:34,771 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,806 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37323', status: init, memory: 0, processing: 0>
2024-01-07 06:31:34,807 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37323
2024-01-07 06:31:34,807 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44526
2024-01-07 06:31:34,808 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:34,809 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:34,809 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:34,811 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:34,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:34,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:34,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:34,834 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:34,834 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:34,834 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:34,834 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:34,834 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:34,838 - distributed.scheduler - INFO - Remove client Client-638d3d96-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:34,839 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44454; closing.
2024-01-07 06:31:34,839 - distributed.scheduler - INFO - Remove client Client-638d3d96-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:34,839 - distributed.scheduler - INFO - Close client connection: Client-638d3d96-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:34,840 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35191'. Reason: nanny-close
2024-01-07 06:31:34,841 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:34,841 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46109'. Reason: nanny-close
2024-01-07 06:31:34,841 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:34,842 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37251'. Reason: nanny-close
2024-01-07 06:31:34,842 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37323. Reason: nanny-close
2024-01-07 06:31:34,842 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:34,842 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45751'. Reason: nanny-close
2024-01-07 06:31:34,842 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:34,842 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35873. Reason: nanny-close
2024-01-07 06:31:34,843 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46207'. Reason: nanny-close
2024-01-07 06:31:34,843 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42387. Reason: nanny-close
2024-01-07 06:31:34,843 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:34,843 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41155'. Reason: nanny-close
2024-01-07 06:31:34,843 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44029. Reason: nanny-close
2024-01-07 06:31:34,843 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:34,843 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36905'. Reason: nanny-close
2024-01-07 06:31:34,844 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35881. Reason: nanny-close
2024-01-07 06:31:34,844 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:34,844 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39153'. Reason: nanny-close
2024-01-07 06:31:34,844 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:34,844 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39747. Reason: nanny-close
2024-01-07 06:31:34,844 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:34,844 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45357. Reason: nanny-close
2024-01-07 06:31:34,844 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44526; closing.
2024-01-07 06:31:34,844 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:34,845 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37323', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609094.845103')
2024-01-07 06:31:34,845 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:34,845 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46389. Reason: nanny-close
2024-01-07 06:31:34,845 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:34,845 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44488; closing.
2024-01-07 06:31:34,846 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42387', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609094.8461673')
2024-01-07 06:31:34,846 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:34,846 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:34,846 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:34,846 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44510; closing.
2024-01-07 06:31:34,846 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:34,846 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:34,846 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:34,847 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:34,847 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:34,847 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35873', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609094.8475032')
2024-01-07 06:31:34,847 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:34,848 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44504; closing.
2024-01-07 06:31:34,848 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:34,848 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:34,848 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:34,848 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:44488>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-07 06:31:34,850 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44472; closing.
2024-01-07 06:31:34,850 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44029', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609094.8504977')
2024-01-07 06:31:34,850 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44516; closing.
2024-01-07 06:31:34,851 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44500; closing.
2024-01-07 06:31:34,851 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35881', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609094.851514')
2024-01-07 06:31:34,852 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45357', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609094.8519633')
2024-01-07 06:31:34,852 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39747', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609094.8523436')
2024-01-07 06:31:34,852 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44520; closing.
2024-01-07 06:31:34,853 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46389', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609094.8531365')
2024-01-07 06:31:34,853 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:31:35,806 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:31:35,807 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:31:35,807 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:31:35,808 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:31:35,809 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-07 06:31:38,064 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:38,069 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-07 06:31:38,073 - distributed.scheduler - INFO - State start
2024-01-07 06:31:38,115 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:38,116 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:31:38,116 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-07 06:31:38,116 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:31:38,283 - distributed.scheduler - INFO - Receive client connection: Client-68184a04-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:38,299 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44606
2024-01-07 06:31:38,314 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43419'
2024-01-07 06:31:38,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46051'
2024-01-07 06:31:38,342 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43177'
2024-01-07 06:31:38,357 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35767'
2024-01-07 06:31:38,360 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35539'
2024-01-07 06:31:38,370 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35707'
2024-01-07 06:31:38,380 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44491'
2024-01-07 06:31:38,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39605'
2024-01-07 06:31:40,282 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:40,282 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:40,287 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:40,287 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42729
2024-01-07 06:31:40,288 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42729
2024-01-07 06:31:40,288 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37799
2024-01-07 06:31:40,288 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:40,288 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:40,288 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:40,288 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:40,288 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nk5srgf5
2024-01-07 06:31:40,288 - distributed.worker - INFO - Starting Worker plugin RMMSetup-716cdb77-f974-40c3-8b2a-69a87630b779
2024-01-07 06:31:40,296 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:40,296 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:40,301 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:40,301 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43567
2024-01-07 06:31:40,301 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43567
2024-01-07 06:31:40,301 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41133
2024-01-07 06:31:40,302 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:40,302 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:40,302 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:40,302 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:40,302 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6d3yo6vt
2024-01-07 06:31:40,302 - distributed.worker - INFO - Starting Worker plugin PreImport-014709e9-e04e-4d34-a727-a22a8f0910d7
2024-01-07 06:31:40,302 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-04910e84-f229-4b4f-9cef-8e2c536f7d24
2024-01-07 06:31:40,302 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9eb8ed68-156c-4ff3-8983-ed73d2434f2b
2024-01-07 06:31:40,323 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:40,323 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:40,328 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:40,328 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34731
2024-01-07 06:31:40,328 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34731
2024-01-07 06:31:40,328 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37819
2024-01-07 06:31:40,329 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:40,329 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:40,329 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:40,329 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:40,329 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qoavsrr2
2024-01-07 06:31:40,329 - distributed.worker - INFO - Starting Worker plugin PreImport-1da55c89-789d-4865-ab0c-27341f315e8e
2024-01-07 06:31:40,329 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c224c8c0-15e1-421b-af39-069421889459
2024-01-07 06:31:40,330 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4dc4e07b-4398-4e69-bfd7-f55633ca4d75
2024-01-07 06:31:40,347 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:40,348 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:40,352 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:40,353 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35741
2024-01-07 06:31:40,353 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35741
2024-01-07 06:31:40,353 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43823
2024-01-07 06:31:40,353 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:40,353 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:40,353 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:40,353 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:40,353 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bwa_jt0u
2024-01-07 06:31:40,353 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ac8a1dd5-fd5e-4c03-a977-4fecb94af466
2024-01-07 06:31:40,358 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:40,358 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:40,363 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:40,364 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34347
2024-01-07 06:31:40,364 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34347
2024-01-07 06:31:40,364 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44639
2024-01-07 06:31:40,364 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:40,364 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:40,364 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:40,364 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:40,364 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dsczeokj
2024-01-07 06:31:40,364 - distributed.worker - INFO - Starting Worker plugin RMMSetup-250ae238-4825-4564-9ba5-a314fbae2d80
2024-01-07 06:31:40,378 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:40,378 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:40,379 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:40,379 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:40,384 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:40,385 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43761
2024-01-07 06:31:40,385 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43761
2024-01-07 06:31:40,385 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45475
2024-01-07 06:31:40,385 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:40,385 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:40,385 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:40,385 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:40,385 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:40,385 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mijx7pr0
2024-01-07 06:31:40,385 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f2625a65-ce63-4663-8485-9c17da4df9ba
2024-01-07 06:31:40,386 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38189
2024-01-07 06:31:40,386 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38189
2024-01-07 06:31:40,386 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45935
2024-01-07 06:31:40,386 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:40,386 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:40,386 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:40,386 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:40,386 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ck3buqa1
2024-01-07 06:31:40,387 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c29b3c22-e0be-4b38-8dc7-76d39bb244c2
2024-01-07 06:31:40,412 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:40,412 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:40,418 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:40,419 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45121
2024-01-07 06:31:40,419 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45121
2024-01-07 06:31:40,419 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45879
2024-01-07 06:31:40,419 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:40,419 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:40,419 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:40,419 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:40,419 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zpa00coh
2024-01-07 06:31:40,420 - distributed.worker - INFO - Starting Worker plugin RMMSetup-afc9d6a3-53a2-4776-91e3-11cc16d0bf66
2024-01-07 06:31:42,579 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,580 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-77ae1ea9-d10b-47cf-b5cf-51745c149cc3
2024-01-07 06:31:42,581 - distributed.worker - INFO - Starting Worker plugin PreImport-94f48f66-e76c-49e0-ae60-a55c322d0458
2024-01-07 06:31:42,582 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,594 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5a174074-c6ee-4179-ada9-5e26844cd08a
2024-01-07 06:31:42,595 - distributed.worker - INFO - Starting Worker plugin PreImport-c61cf14f-35bf-401f-9df0-03a61e2fee1d
2024-01-07 06:31:42,595 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,606 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43567', status: init, memory: 0, processing: 0>
2024-01-07 06:31:42,608 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43567
2024-01-07 06:31:42,608 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43706
2024-01-07 06:31:42,609 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:42,609 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:42,609 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:42,623 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35741', status: init, memory: 0, processing: 0>
2024-01-07 06:31:42,623 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35741
2024-01-07 06:31:42,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43720
2024-01-07 06:31:42,624 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:42,625 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:42,625 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:42,627 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42729', status: init, memory: 0, processing: 0>
2024-01-07 06:31:42,628 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42729
2024-01-07 06:31:42,628 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43716
2024-01-07 06:31:42,629 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:42,631 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:42,631 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,631 - distributed.worker - INFO - Starting Worker plugin PreImport-673f0098-f1bb-4d6f-8e19-656c98579578
2024-01-07 06:31:42,632 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d3514df-13fa-4384-8a35-511dd8253642
2024-01-07 06:31:42,633 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,633 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:42,675 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34347', status: init, memory: 0, processing: 0>
2024-01-07 06:31:42,676 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34347
2024-01-07 06:31:42,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43734
2024-01-07 06:31:42,677 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:42,679 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:42,679 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,681 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:42,685 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98703053-5051-4bb2-b7f1-92da22d06f3a
2024-01-07 06:31:42,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-555cb350-af7a-4c30-8601-adc2885ea8cb
2024-01-07 06:31:42,690 - distributed.worker - INFO - Starting Worker plugin PreImport-8f41247d-a83a-4ff3-b4b0-368b36a59ce2
2024-01-07 06:31:42,690 - distributed.worker - INFO - Starting Worker plugin PreImport-9e4640ec-9d89-4366-959c-bfbac2602dab
2024-01-07 06:31:42,690 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,690 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,690 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e0188719-b0bc-4081-a0c9-8f27b5b6c06e
2024-01-07 06:31:42,691 - distributed.worker - INFO - Starting Worker plugin PreImport-cc74cbff-6be9-4c7f-9937-d2b6f69dc17f
2024-01-07 06:31:42,692 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,715 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38189', status: init, memory: 0, processing: 0>
2024-01-07 06:31:42,715 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38189
2024-01-07 06:31:42,715 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43750
2024-01-07 06:31:42,716 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45121', status: init, memory: 0, processing: 0>
2024-01-07 06:31:42,716 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:42,717 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45121
2024-01-07 06:31:42,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43762
2024-01-07 06:31:42,717 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:42,717 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:42,719 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:42,719 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,719 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:42,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:42,727 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34731', status: init, memory: 0, processing: 0>
2024-01-07 06:31:42,728 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34731
2024-01-07 06:31:42,728 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43740
2024-01-07 06:31:42,729 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:42,730 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:42,730 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,733 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:42,733 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43761', status: init, memory: 0, processing: 0>
2024-01-07 06:31:42,734 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43761
2024-01-07 06:31:42,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43774
2024-01-07 06:31:42,736 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:42,737 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:42,737 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:42,739 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:42,832 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:42,832 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:42,832 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:42,832 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:42,832 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:42,832 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:42,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:42,833 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:31:42,844 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:42,844 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:42,844 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:42,844 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:42,845 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:42,845 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:42,845 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:42,845 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:42,853 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:42,855 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:42,858 - distributed.scheduler - INFO - Remove client Client-68184a04-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:42,858 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:44606; closing.
2024-01-07 06:31:42,858 - distributed.scheduler - INFO - Remove client Client-68184a04-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:42,859 - distributed.scheduler - INFO - Close client connection: Client-68184a04-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:42,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43419'. Reason: nanny-close
2024-01-07 06:31:42,860 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:42,860 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46051'. Reason: nanny-close
2024-01-07 06:31:42,861 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:42,862 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43177'. Reason: nanny-close
2024-01-07 06:31:42,862 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42729. Reason: nanny-close
2024-01-07 06:31:42,862 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:42,862 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35767'. Reason: nanny-close
2024-01-07 06:31:42,862 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34731. Reason: nanny-close
2024-01-07 06:31:42,863 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:42,863 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35539'. Reason: nanny-close
2024-01-07 06:31:42,863 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43567. Reason: nanny-close
2024-01-07 06:31:42,863 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:42,863 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35707'. Reason: nanny-close
2024-01-07 06:31:42,863 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38189. Reason: nanny-close
2024-01-07 06:31:42,864 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:42,864 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44491'. Reason: nanny-close
2024-01-07 06:31:42,864 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:42,864 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34347. Reason: nanny-close
2024-01-07 06:31:42,864 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39605'. Reason: nanny-close
2024-01-07 06:31:42,864 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:42,864 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43761. Reason: nanny-close
2024-01-07 06:31:42,865 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:42,865 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43716; closing.
2024-01-07 06:31:42,865 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:42,865 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35741. Reason: nanny-close
2024-01-07 06:31:42,865 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609102.8653553')
2024-01-07 06:31:42,865 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:42,866 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45121. Reason: nanny-close
2024-01-07 06:31:42,866 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:42,866 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43706; closing.
2024-01-07 06:31:42,866 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:42,866 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:42,866 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:42,867 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:42,867 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:42,867 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43567', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609102.8676686')
2024-01-07 06:31:42,867 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:42,868 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43750; closing.
2024-01-07 06:31:42,868 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43740; closing.
2024-01-07 06:31:42,868 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:42,868 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:42,868 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:42,869 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:42,869 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:42,869 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38189', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609102.869239')
2024-01-07 06:31:42,869 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34731', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609102.869596')
2024-01-07 06:31:42,869 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43720; closing.
2024-01-07 06:31:42,870 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43734; closing.
2024-01-07 06:31:42,870 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35741', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609102.8707507')
2024-01-07 06:31:42,870 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:42,871 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34347', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609102.8710449')
2024-01-07 06:31:42,871 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43774; closing.
2024-01-07 06:31:42,871 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43762; closing.
2024-01-07 06:31:42,872 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43761', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609102.8720348')
2024-01-07 06:31:42,872 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45121', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609102.872436')
2024-01-07 06:31:42,872 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:31:44,180 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:31:44,180 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:31:44,181 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:31:44,182 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:31:44,182 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-07 06:31:46,679 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:46,684 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-07 06:31:46,688 - distributed.scheduler - INFO - State start
2024-01-07 06:31:46,937 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:46,938 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:31:46,940 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-07 06:31:46,940 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:31:47,192 - distributed.scheduler - INFO - Receive client connection: Client-6d2a9413-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:47,208 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43882
2024-01-07 06:31:47,264 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46837'
2024-01-07 06:31:47,280 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43695'
2024-01-07 06:31:47,293 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41265'
2024-01-07 06:31:47,303 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37371'
2024-01-07 06:31:47,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39553'
2024-01-07 06:31:47,316 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44603'
2024-01-07 06:31:47,324 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38305'
2024-01-07 06:31:47,334 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33385'
2024-01-07 06:31:49,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:49,166 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:49,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:49,166 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:49,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:49,171 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:49,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42477
2024-01-07 06:31:49,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42811
2024-01-07 06:31:49,171 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42477
2024-01-07 06:31:49,171 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42811
2024-01-07 06:31:49,171 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35027
2024-01-07 06:31:49,171 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37493
2024-01-07 06:31:49,172 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:49,172 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:49,172 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:49,172 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:49,172 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:49,172 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:49,172 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:49,172 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:49,172 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ve8wlyfj
2024-01-07 06:31:49,172 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4u6pgp38
2024-01-07 06:31:49,172 - distributed.worker - INFO - Starting Worker plugin RMMSetup-07761bce-44c0-44e4-bd9d-4a04baa2e9b9
2024-01-07 06:31:49,172 - distributed.worker - INFO - Starting Worker plugin PreImport-55b9314a-1170-48d7-892d-3e96b0658aa9
2024-01-07 06:31:49,172 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e8a80b7-f482-4184-b130-1f4a71ec8a4a
2024-01-07 06:31:49,172 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d783cc2-43e1-45f8-8a30-f5bb449fbea9
2024-01-07 06:31:49,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:49,197 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:49,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:49,197 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:49,202 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:49,202 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:49,202 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38119
2024-01-07 06:31:49,202 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36115
2024-01-07 06:31:49,202 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38119
2024-01-07 06:31:49,202 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36115
2024-01-07 06:31:49,203 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40191
2024-01-07 06:31:49,203 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36805
2024-01-07 06:31:49,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:49,203 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:49,203 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:49,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:49,203 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:49,203 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:49,203 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:49,203 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:49,203 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:49,203 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:49,203 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-abl7zns2
2024-01-07 06:31:49,203 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s08ydor4
2024-01-07 06:31:49,203 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c810a27-551b-455c-bbb6-ba9c2e0c86a2
2024-01-07 06:31:49,203 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a02ff611-188b-4d80-a3e4-7aa73a121f51
2024-01-07 06:31:49,205 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:49,205 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:49,207 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:49,208 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46303
2024-01-07 06:31:49,208 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46303
2024-01-07 06:31:49,208 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36225
2024-01-07 06:31:49,208 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:49,208 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:49,208 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:49,208 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:49,208 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-8yjffgrs
2024-01-07 06:31:49,209 - distributed.worker - INFO - Starting Worker plugin PreImport-04216706-d4d6-4b73-be61-19cd3f32c4f3
2024-01-07 06:31:49,209 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-434664a8-11ad-4664-a1dd-6fc3ba132c9d
2024-01-07 06:31:49,209 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b7db214-7c56-4cb4-9bfa-2d450dbd4ae7
2024-01-07 06:31:49,209 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:49,210 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35211
2024-01-07 06:31:49,210 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35211
2024-01-07 06:31:49,210 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44469
2024-01-07 06:31:49,210 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:49,210 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:49,210 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:49,210 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:49,210 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pfi6p0aj
2024-01-07 06:31:49,211 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bf18bf25-2df1-4acc-a5e1-18336a3653c0
2024-01-07 06:31:49,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:49,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:49,297 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:49,298 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42533
2024-01-07 06:31:49,298 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42533
2024-01-07 06:31:49,298 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40161
2024-01-07 06:31:49,298 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:49,298 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:49,298 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:49,299 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:49,299 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0v4k_hfo
2024-01-07 06:31:49,299 - distributed.worker - INFO - Starting Worker plugin RMMSetup-93c73f8f-6160-4617-a738-d43c66f3ea5c
2024-01-07 06:31:49,300 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:49,300 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:49,304 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:49,305 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35775
2024-01-07 06:31:49,305 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35775
2024-01-07 06:31:49,305 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41675
2024-01-07 06:31:49,305 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:49,306 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:49,306 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:49,306 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:49,306 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kdamcg2w
2024-01-07 06:31:49,306 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3b645585-295b-4461-8c6b-b67bfb60b6cf
2024-01-07 06:31:51,823 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5af93bf8-aa6e-421e-8bfe-acbb155c846c
2024-01-07 06:31:51,823 - distributed.worker - INFO - Starting Worker plugin PreImport-0e467e6f-eb3b-48d1-8936-0beca091f2a1
2024-01-07 06:31:51,824 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,839 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1216a4da-aef7-4bd2-ae3e-4144e46bee40
2024-01-07 06:31:51,839 - distributed.worker - INFO - Starting Worker plugin PreImport-5d7d118e-e95e-4c2a-a4a0-65211b26370c
2024-01-07 06:31:51,840 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,843 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5e664885-a152-4ca1-ae16-6e349888639c
2024-01-07 06:31:51,844 - distributed.worker - INFO - Starting Worker plugin PreImport-947380c4-c7bc-46bf-ba40-c01b0695988c
2024-01-07 06:31:51,845 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,849 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36115', status: init, memory: 0, processing: 0>
2024-01-07 06:31:51,851 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36115
2024-01-07 06:31:51,852 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43300
2024-01-07 06:31:51,852 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:51,853 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:51,853 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,854 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,855 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:51,855 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f71cb49-e411-4938-a9b1-fec4a3884fdc
2024-01-07 06:31:51,856 - distributed.worker - INFO - Starting Worker plugin PreImport-ae988300-6ec2-4b0f-80c7-34f00eabc429
2024-01-07 06:31:51,856 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,879 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38119', status: init, memory: 0, processing: 0>
2024-01-07 06:31:51,879 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38119
2024-01-07 06:31:51,879 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43302
2024-01-07 06:31:51,881 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,881 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35211', status: init, memory: 0, processing: 0>
2024-01-07 06:31:51,881 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:51,881 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35211
2024-01-07 06:31:51,881 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43324
2024-01-07 06:31:51,882 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:51,882 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,882 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:51,883 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:51,883 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,884 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:51,885 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:51,886 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42811', status: init, memory: 0, processing: 0>
2024-01-07 06:31:51,886 - distributed.worker - INFO - Starting Worker plugin PreImport-169c8ba1-e3f6-4189-9c85-d0a159db62cf
2024-01-07 06:31:51,886 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42811
2024-01-07 06:31:51,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43308
2024-01-07 06:31:51,887 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e4a374f7-7829-4857-9d88-579fe1cf88cd
2024-01-07 06:31:51,888 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,888 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:51,889 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:51,889 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,891 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42477', status: init, memory: 0, processing: 0>
2024-01-07 06:31:51,892 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42477
2024-01-07 06:31:51,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43328
2024-01-07 06:31:51,892 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:51,893 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:51,895 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:51,895 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,897 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f75a662-4b89-45ea-bf2d-1fab848f87ed
2024-01-07 06:31:51,898 - distributed.worker - INFO - Starting Worker plugin PreImport-b1bdaa67-d4d0-4c7f-99f1-e81c442d84cd
2024-01-07 06:31:51,898 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:51,898 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,910 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46303', status: init, memory: 0, processing: 0>
2024-01-07 06:31:51,911 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46303
2024-01-07 06:31:51,911 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43344
2024-01-07 06:31:51,912 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:51,913 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:51,913 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,915 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:51,930 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35775', status: init, memory: 0, processing: 0>
2024-01-07 06:31:51,931 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35775
2024-01-07 06:31:51,931 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43358
2024-01-07 06:31:51,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:51,932 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:51,932 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:51,946 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42533', status: init, memory: 0, processing: 0>
2024-01-07 06:31:51,947 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42533
2024-01-07 06:31:51,947 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43354
2024-01-07 06:31:51,950 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:31:51,951 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:31:51,952 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:51,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:31:51,986 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:51,987 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:51,987 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:51,988 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:51,988 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:51,988 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:51,991 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:51,992 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:52,009 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:52,010 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:52,010 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:52,011 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:52,011 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:52,013 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:52,013 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:52,016 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:31:52,027 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:52,029 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:31:52,032 - distributed.scheduler - INFO - Remove client Client-6d2a9413-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:52,032 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43882; closing.
2024-01-07 06:31:52,032 - distributed.scheduler - INFO - Remove client Client-6d2a9413-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:52,033 - distributed.scheduler - INFO - Close client connection: Client-6d2a9413-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:52,034 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46837'. Reason: nanny-close
2024-01-07 06:31:52,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:52,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43695'. Reason: nanny-close
2024-01-07 06:31:52,035 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:52,035 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41265'. Reason: nanny-close
2024-01-07 06:31:52,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:52,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42811. Reason: nanny-close
2024-01-07 06:31:52,036 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37371'. Reason: nanny-close
2024-01-07 06:31:52,036 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42477. Reason: nanny-close
2024-01-07 06:31:52,036 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:52,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39553'. Reason: nanny-close
2024-01-07 06:31:52,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46303. Reason: nanny-close
2024-01-07 06:31:52,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:52,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44603'. Reason: nanny-close
2024-01-07 06:31:52,037 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35211. Reason: nanny-close
2024-01-07 06:31:52,037 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:52,037 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38305'. Reason: nanny-close
2024-01-07 06:31:52,038 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42533. Reason: nanny-close
2024-01-07 06:31:52,038 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:52,038 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33385'. Reason: nanny-close
2024-01-07 06:31:52,038 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38119. Reason: nanny-close
2024-01-07 06:31:52,038 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43308; closing.
2024-01-07 06:31:52,038 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:31:52,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:52,038 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42811', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609112.0387423')
2024-01-07 06:31:52,038 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36115. Reason: nanny-close
2024-01-07 06:31:52,038 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:52,039 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:52,039 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35775. Reason: nanny-close
2024-01-07 06:31:52,039 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:52,040 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:52,040 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:52,040 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:52,040 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:52,040 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:52,040 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43324; closing.
2024-01-07 06:31:52,041 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43328; closing.
2024-01-07 06:31:52,041 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43344; closing.
2024-01-07 06:31:52,041 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:52,041 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:52,041 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:31:52,042 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35211', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609112.0421546')
2024-01-07 06:31:52,042 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:52,042 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:52,042 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42477', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609112.0425313')
2024-01-07 06:31:52,042 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:52,042 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46303', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609112.04292')
2024-01-07 06:31:52,043 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43354; closing.
2024-01-07 06:31:52,043 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43302; closing.
2024-01-07 06:31:52,043 - distributed.nanny - INFO - Worker closed
2024-01-07 06:31:52,044 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42533', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609112.0441222')
2024-01-07 06:31:52,044 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38119', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609112.0444074')
2024-01-07 06:31:52,044 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43300; closing.
2024-01-07 06:31:52,045 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609112.0454183')
2024-01-07 06:31:52,045 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43358; closing.
2024-01-07 06:31:52,046 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35775', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609112.046167')
2024-01-07 06:31:52,046 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:31:53,100 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:31:53,101 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:31:53,102 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:31:53,103 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:31:53,103 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-07 06:31:55,532 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:55,536 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40965 instead
  warnings.warn(
2024-01-07 06:31:55,541 - distributed.scheduler - INFO - State start
2024-01-07 06:31:55,565 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:31:55,566 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:31:55,566 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40965/status
2024-01-07 06:31:55,567 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:31:55,661 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42035'
2024-01-07 06:31:55,667 - distributed.scheduler - INFO - Receive client connection: Client-727fcdbf-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:31:55,677 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46077'
2024-01-07 06:31:55,683 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43518
2024-01-07 06:31:55,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43619'
2024-01-07 06:31:55,700 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35863'
2024-01-07 06:31:55,704 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36467'
2024-01-07 06:31:55,712 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38291'
2024-01-07 06:31:55,723 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45157'
2024-01-07 06:31:55,735 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39897'
2024-01-07 06:31:57,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:57,735 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:57,739 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:57,740 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:57,740 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:57,740 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35589
2024-01-07 06:31:57,740 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35589
2024-01-07 06:31:57,740 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40749
2024-01-07 06:31:57,740 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:57,740 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:57,740 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:57,740 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:57,740 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-39m88r10
2024-01-07 06:31:57,740 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c480a796-7fc8-4760-89e1-06f3d8b9cb60
2024-01-07 06:31:57,743 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:57,743 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:57,743 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:57,743 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:57,743 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:57,744 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:57,744 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:57,744 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33721
2024-01-07 06:31:57,745 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33721
2024-01-07 06:31:57,745 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37633
2024-01-07 06:31:57,745 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:57,745 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:57,745 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:57,745 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:57,745 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-no4urov2
2024-01-07 06:31:57,745 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cb4b0d95-e266-4e14-81e7-aa65ce4abd31
2024-01-07 06:31:57,747 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:57,747 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:57,748 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:57,748 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44077
2024-01-07 06:31:57,748 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44077
2024-01-07 06:31:57,748 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35115
2024-01-07 06:31:57,748 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34595
2024-01-07 06:31:57,748 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35115
2024-01-07 06:31:57,748 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:57,748 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:57,748 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44213
2024-01-07 06:31:57,748 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:57,748 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:57,748 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:57,748 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:57,748 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:57,748 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zd20ez_p
2024-01-07 06:31:57,748 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:57,749 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t9rjb49i
2024-01-07 06:31:57,749 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42295
2024-01-07 06:31:57,749 - distributed.worker - INFO - Starting Worker plugin PreImport-ee7c39f4-eb19-41b4-9295-e63a543f045a
2024-01-07 06:31:57,749 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42295
2024-01-07 06:31:57,749 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33813
2024-01-07 06:31:57,749 - distributed.worker - INFO - Starting Worker plugin RMMSetup-14afa4fc-39fb-4dd5-ac90-5fdc21845914
2024-01-07 06:31:57,749 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ae41ad0-6d19-4cfc-a4df-29b2e3924ced
2024-01-07 06:31:57,749 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:57,749 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:57,749 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:57,749 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:57,749 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:57,749 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-drfyor5q
2024-01-07 06:31:57,749 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:57,749 - distributed.worker - INFO - Starting Worker plugin RMMSetup-21917a48-b983-4bd9-a0c7-3ef9bbccc5a4
2024-01-07 06:31:57,749 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b52c0deb-81fc-4511-b2c7-30f03ae5f1ca
2024-01-07 06:31:57,752 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:57,752 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:57,753 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:31:57,753 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:31:57,753 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:57,754 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36423
2024-01-07 06:31:57,754 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36423
2024-01-07 06:31:57,754 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44875
2024-01-07 06:31:57,754 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:57,754 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:57,754 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:57,754 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:57,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9fi3kx8k
2024-01-07 06:31:57,755 - distributed.worker - INFO - Starting Worker plugin PreImport-1a537419-c986-46bd-bebd-ec66d1ea7958
2024-01-07 06:31:57,755 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e8bd6561-25d5-48d9-822e-18c98a0ca6c4
2024-01-07 06:31:57,755 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b2a093cc-1555-47aa-a7af-1210a24b9b08
2024-01-07 06:31:57,757 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:57,757 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:31:57,758 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36705
2024-01-07 06:31:57,758 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36705
2024-01-07 06:31:57,758 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34147
2024-01-07 06:31:57,758 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36377
2024-01-07 06:31:57,758 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:57,758 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36377
2024-01-07 06:31:57,758 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:57,758 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34027
2024-01-07 06:31:57,758 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:57,758 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:31:57,758 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:57,758 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:31:57,758 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_8bj6ei3
2024-01-07 06:31:57,758 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:31:57,758 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:31:57,758 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lwj1yph0
2024-01-07 06:31:57,758 - distributed.worker - INFO - Starting Worker plugin RMMSetup-88de8415-e026-417d-b241-6bceb48fcec0
2024-01-07 06:31:57,758 - distributed.worker - INFO - Starting Worker plugin RMMSetup-46ea5e43-d979-4cd8-87dd-ab2d8528d2ef
2024-01-07 06:32:01,628 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:01,652 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36423', status: init, memory: 0, processing: 0>
2024-01-07 06:32:01,653 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36423
2024-01-07 06:32:01,653 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37780
2024-01-07 06:32:01,654 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:01,655 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:01,655 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:01,656 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:01,914 - distributed.worker - INFO - Starting Worker plugin PreImport-cf370d5c-bc5d-440c-bba8-80e407a93e5b
2024-01-07 06:32:01,915 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f09c977d-2ad8-4222-9849-8e88c11f4866
2024-01-07 06:32:01,916 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:01,924 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37b0278c-6e56-481b-b20e-8f30bd8e1b51
2024-01-07 06:32:01,924 - distributed.worker - INFO - Starting Worker plugin PreImport-a2683dcd-9fdb-4d0f-86f3-a0c5093f3f14
2024-01-07 06:32:01,925 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:01,947 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33721', status: init, memory: 0, processing: 0>
2024-01-07 06:32:01,947 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33721
2024-01-07 06:32:01,947 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37790
2024-01-07 06:32:01,948 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36377', status: init, memory: 0, processing: 0>
2024-01-07 06:32:01,948 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:01,949 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36377
2024-01-07 06:32:01,949 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37782
2024-01-07 06:32:01,949 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:01,949 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:01,950 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:01,950 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:01,951 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:01,951 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:01,953 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:01,974 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c5ec7b3-f141-4186-8bb3-85b3f0beb526
2024-01-07 06:32:01,974 - distributed.worker - INFO - Starting Worker plugin PreImport-8362944a-d773-4136-b0dc-796d0d8896ba
2024-01-07 06:32:01,975 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:01,984 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:01,996 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42295', status: init, memory: 0, processing: 0>
2024-01-07 06:32:01,997 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42295
2024-01-07 06:32:01,997 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37792
2024-01-07 06:32:01,997 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:01,998 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:01,998 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:01,999 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d6f2c6aa-bbad-4bc2-9cdc-4d7eb2f64997
2024-01-07 06:32:01,999 - distributed.worker - INFO - Starting Worker plugin PreImport-08aa0e28-1b4f-4e48-8f68-bbf0b3d34572
2024-01-07 06:32:02,000 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:02,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:02,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a01f4ec5-12cd-4b9a-a525-799052dcdf50
2024-01-07 06:32:02,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6afbd5c4-9552-4fe0-bf19-3b7b3a1024f2
2024-01-07 06:32:02,009 - distributed.worker - INFO - Starting Worker plugin PreImport-5ffaa0b6-9187-4eec-a648-cacd8e20b5e5
2024-01-07 06:32:02,009 - distributed.worker - INFO - Starting Worker plugin PreImport-71582f62-4674-4849-9bb7-49398b332384
2024-01-07 06:32:02,010 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:02,010 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:02,017 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44077', status: init, memory: 0, processing: 0>
2024-01-07 06:32:02,017 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44077
2024-01-07 06:32:02,017 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37798
2024-01-07 06:32:02,019 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:02,020 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:02,020 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:02,022 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35115', status: init, memory: 0, processing: 0>
2024-01-07 06:32:02,023 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:02,023 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35115
2024-01-07 06:32:02,023 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37810
2024-01-07 06:32:02,024 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:02,025 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:02,025 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:02,027 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:02,038 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36705', status: init, memory: 0, processing: 0>
2024-01-07 06:32:02,038 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36705
2024-01-07 06:32:02,038 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37824
2024-01-07 06:32:02,040 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:02,040 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:02,040 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:02,041 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35589', status: init, memory: 0, processing: 0>
2024-01-07 06:32:02,041 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35589
2024-01-07 06:32:02,041 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37836
2024-01-07 06:32:02,043 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:02,043 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:02,044 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:02,044 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:02,046 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:02,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:02,142 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:02,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:02,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:02,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:02,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:02,143 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:02,144 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:02,150 - distributed.scheduler - INFO - Remove client Client-727fcdbf-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:02,150 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:43518; closing.
2024-01-07 06:32:02,151 - distributed.scheduler - INFO - Remove client Client-727fcdbf-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:02,151 - distributed.scheduler - INFO - Close client connection: Client-727fcdbf-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:02,152 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42035'. Reason: nanny-close
2024-01-07 06:32:02,152 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:02,153 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46077'. Reason: nanny-close
2024-01-07 06:32:02,153 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:02,153 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43619'. Reason: nanny-close
2024-01-07 06:32:02,153 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35589. Reason: nanny-close
2024-01-07 06:32:02,154 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:02,154 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35863'. Reason: nanny-close
2024-01-07 06:32:02,154 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44077. Reason: nanny-close
2024-01-07 06:32:02,154 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:02,154 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36467'. Reason: nanny-close
2024-01-07 06:32:02,154 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36423. Reason: nanny-close
2024-01-07 06:32:02,155 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:02,155 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38291'. Reason: nanny-close
2024-01-07 06:32:02,155 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33721. Reason: nanny-close
2024-01-07 06:32:02,155 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:02,155 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45157'. Reason: nanny-close
2024-01-07 06:32:02,156 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36377. Reason: nanny-close
2024-01-07 06:32:02,156 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:02,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:02,156 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39897'. Reason: nanny-close
2024-01-07 06:32:02,156 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36705. Reason: nanny-close
2024-01-07 06:32:02,156 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37836; closing.
2024-01-07 06:32:02,156 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:02,156 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:02,156 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42295. Reason: nanny-close
2024-01-07 06:32:02,157 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35589', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609122.1569452')
2024-01-07 06:32:02,157 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:02,157 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:02,157 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35115. Reason: nanny-close
2024-01-07 06:32:02,158 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:02,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:02,158 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:02,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:02,158 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:02,158 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37790; closing.
2024-01-07 06:32:02,158 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:02,158 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37798; closing.
2024-01-07 06:32:02,159 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:02,159 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:02,159 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:02,160 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:02,160 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:02,160 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33721', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609122.160216')
2024-01-07 06:32:02,160 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:02,160 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44077', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609122.160735')
2024-01-07 06:32:02,161 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37780; closing.
2024-01-07 06:32:02,162 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37790>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:37790>: Stream is closed
2024-01-07 06:32:02,165 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36423', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609122.165034')
2024-01-07 06:32:02,165 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37782; closing.
2024-01-07 06:32:02,165 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37824; closing.
2024-01-07 06:32:02,166 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37792; closing.
2024-01-07 06:32:02,166 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36377', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609122.1667256')
2024-01-07 06:32:02,167 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36705', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609122.1672301')
2024-01-07 06:32:02,167 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42295', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609122.1677413')
2024-01-07 06:32:02,168 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37810; closing.
2024-01-07 06:32:02,168 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609122.1687925')
2024-01-07 06:32:02,169 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:32:03,068 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:32:03,071 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:32:03,071 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:32:03,073 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:32:03,073 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-07 06:32:05,504 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:05,509 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39259 instead
  warnings.warn(
2024-01-07 06:32:05,513 - distributed.scheduler - INFO - State start
2024-01-07 06:32:05,536 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:05,537 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:32:05,537 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39259/status
2024-01-07 06:32:05,537 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:32:05,588 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43403'
2024-01-07 06:32:06,644 - distributed.scheduler - INFO - Receive client connection: Client-786e7abd-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:06,660 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37950
2024-01-07 06:32:07,469 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:07,469 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:08,102 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:08,103 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43517
2024-01-07 06:32:08,103 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43517
2024-01-07 06:32:08,103 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-07 06:32:08,103 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:08,103 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:08,103 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:08,103 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-07 06:32:08,103 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zw_lk7pr
2024-01-07 06:32:08,104 - distributed.worker - INFO - Starting Worker plugin RMMSetup-782ff26e-c850-4f01-8369-365c09ff58de
2024-01-07 06:32:08,104 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86b89dd8-169d-4269-9d62-bdc6307dc2f3
2024-01-07 06:32:08,104 - distributed.worker - INFO - Starting Worker plugin PreImport-273f0a0d-5a67-4720-85db-365779782f9f
2024-01-07 06:32:08,104 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:08,170 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43517', status: init, memory: 0, processing: 0>
2024-01-07 06:32:08,172 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43517
2024-01-07 06:32:08,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:37966
2024-01-07 06:32:08,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:08,174 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:08,174 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:08,175 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:08,188 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:08,191 - distributed.scheduler - INFO - Remove client Client-786e7abd-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:08,192 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37950; closing.
2024-01-07 06:32:08,192 - distributed.scheduler - INFO - Remove client Client-786e7abd-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:08,193 - distributed.scheduler - INFO - Close client connection: Client-786e7abd-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:08,193 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43403'. Reason: nanny-close
2024-01-07 06:32:08,219 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:08,220 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43517. Reason: nanny-close
2024-01-07 06:32:08,222 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:08,222 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:37966; closing.
2024-01-07 06:32:08,222 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43517', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609128.2225254')
2024-01-07 06:32:08,222 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:32:08,223 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:08,908 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:32:08,909 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:32:08,909 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:32:08,911 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:32:08,911 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-07 06:32:13,646 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:13,650 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34475 instead
  warnings.warn(
2024-01-07 06:32:13,654 - distributed.scheduler - INFO - State start
2024-01-07 06:32:13,677 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:13,678 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:32:13,679 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34475/status
2024-01-07 06:32:13,679 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:32:13,819 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38871'
2024-01-07 06:32:14,328 - distributed.scheduler - INFO - Receive client connection: Client-7d3a84a9-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:14,343 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33238
2024-01-07 06:32:15,703 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:15,703 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:16,332 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:16,333 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34349
2024-01-07 06:32:16,333 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34349
2024-01-07 06:32:16,333 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32959
2024-01-07 06:32:16,333 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:16,333 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:16,333 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:16,333 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-07 06:32:16,333 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gwpglr9j
2024-01-07 06:32:16,333 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9ace0a75-6c93-4e1c-b88f-ef1fddcbe958
2024-01-07 06:32:16,333 - distributed.worker - INFO - Starting Worker plugin PreImport-35883401-e2ba-43ad-be84-14628324993a
2024-01-07 06:32:16,334 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-872e2f96-4f52-4131-a2a2-da91793346ac
2024-01-07 06:32:16,335 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:16,388 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34349', status: init, memory: 0, processing: 0>
2024-01-07 06:32:16,390 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34349
2024-01-07 06:32:16,390 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33266
2024-01-07 06:32:16,391 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:16,391 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:16,391 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:16,392 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:16,487 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:16,490 - distributed.scheduler - INFO - Remove client Client-7d3a84a9-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:16,490 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33238; closing.
2024-01-07 06:32:16,490 - distributed.scheduler - INFO - Remove client Client-7d3a84a9-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:16,490 - distributed.scheduler - INFO - Close client connection: Client-7d3a84a9-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:16,491 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38871'. Reason: nanny-close
2024-01-07 06:32:16,492 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:16,493 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34349. Reason: nanny-close
2024-01-07 06:32:16,494 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:16,494 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:33266; closing.
2024-01-07 06:32:16,495 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34349', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609136.4952002')
2024-01-07 06:32:16,495 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:32:16,496 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:17,257 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:32:17,258 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:32:17,258 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:32:17,259 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:32:17,260 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-07 06:32:19,735 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:19,740 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36341 instead
  warnings.warn(
2024-01-07 06:32:19,744 - distributed.scheduler - INFO - State start
2024-01-07 06:32:19,768 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:19,769 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:32:19,770 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36341/status
2024-01-07 06:32:19,770 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:32:22,466 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:41754'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 969, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4428, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:41754>: Stream is closed
2024-01-07 06:32:22,760 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:32:22,760 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:32:22,760 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:32:22,761 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:32:22,761 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-07 06:32:25,127 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:25,132 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37879 instead
  warnings.warn(
2024-01-07 06:32:25,136 - distributed.scheduler - INFO - State start
2024-01-07 06:32:25,160 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:25,161 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-07 06:32:25,161 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37879/status
2024-01-07 06:32:25,161 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:32:25,345 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33319'
2024-01-07 06:32:25,973 - distributed.scheduler - INFO - Receive client connection: Client-8419d236-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:25,988 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42064
2024-01-07 06:32:27,338 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:27,338 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:27,343 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:27,344 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35781
2024-01-07 06:32:27,344 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35781
2024-01-07 06:32:27,344 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42129
2024-01-07 06:32:27,344 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-07 06:32:27,344 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:27,344 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:27,344 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-07 06:32:27,344 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ohgogi3n
2024-01-07 06:32:27,344 - distributed.worker - INFO - Starting Worker plugin PreImport-976b64d7-f7a4-48d7-8811-f1487e01a9a1
2024-01-07 06:32:27,344 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4e4ef784-57e3-4695-a38d-8e78d5eceda9
2024-01-07 06:32:27,344 - distributed.worker - INFO - Starting Worker plugin RMMSetup-04c2ba52-e2c6-4322-92b6-522e02d9ee25
2024-01-07 06:32:27,344 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:27,402 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35781', status: init, memory: 0, processing: 0>
2024-01-07 06:32:27,403 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35781
2024-01-07 06:32:27,403 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42086
2024-01-07 06:32:27,404 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:27,405 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-07 06:32:27,405 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:27,406 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-07 06:32:27,418 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:27,421 - distributed.scheduler - INFO - Remove client Client-8419d236-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:27,421 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42064; closing.
2024-01-07 06:32:27,421 - distributed.scheduler - INFO - Remove client Client-8419d236-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:27,422 - distributed.scheduler - INFO - Close client connection: Client-8419d236-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:27,422 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33319'. Reason: nanny-close
2024-01-07 06:32:27,425 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:27,426 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35781. Reason: nanny-close
2024-01-07 06:32:27,428 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42086; closing.
2024-01-07 06:32:27,428 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-07 06:32:27,428 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35781', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609147.4284246')
2024-01-07 06:32:27,428 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:32:27,429 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:27,987 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:32:27,988 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:32:27,988 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:32:27,989 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-07 06:32:27,989 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-07 06:32:30,438 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:30,443 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41047 instead
  warnings.warn(
2024-01-07 06:32:30,448 - distributed.scheduler - INFO - State start
2024-01-07 06:32:30,472 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:30,473 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:32:30,474 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41047/status
2024-01-07 06:32:30,474 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:32:30,595 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40651'
2024-01-07 06:32:30,618 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40819'
2024-01-07 06:32:30,633 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42933'
2024-01-07 06:32:30,643 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32947'
2024-01-07 06:32:30,646 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41693'
2024-01-07 06:32:30,654 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43387'
2024-01-07 06:32:30,663 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44915'
2024-01-07 06:32:30,672 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46095'
2024-01-07 06:32:31,811 - distributed.scheduler - INFO - Receive client connection: Client-87441ea9-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:31,828 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49348
2024-01-07 06:32:32,475 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:32,476 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:32,480 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:32,480 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46637
2024-01-07 06:32:32,480 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46637
2024-01-07 06:32:32,480 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36551
2024-01-07 06:32:32,481 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:32,481 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:32,481 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:32,481 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:32:32,481 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_b1_9awz
2024-01-07 06:32:32,481 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60dedf66-3ff3-4cec-a50f-66660b41a68b
2024-01-07 06:32:32,522 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:32,522 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:32,526 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:32,527 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35619
2024-01-07 06:32:32,527 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35619
2024-01-07 06:32:32,527 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33047
2024-01-07 06:32:32,527 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:32,527 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:32,527 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:32,527 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:32:32,527 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ctbojyn1
2024-01-07 06:32:32,528 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7b92e186-f63c-4397-a8ee-7f3845d35ece
2024-01-07 06:32:32,689 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:32,689 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:32,696 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:32,697 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45567
2024-01-07 06:32:32,697 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45567
2024-01-07 06:32:32,697 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33485
2024-01-07 06:32:32,697 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:32,697 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:32,697 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:32,697 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:32:32,697 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rp05somk
2024-01-07 06:32:32,698 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5eabd8c-938d-4d57-82da-704b4b2e6640
2024-01-07 06:32:32,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:32,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:32,702 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:32,702 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:32,706 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:32,706 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:32,707 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44191
2024-01-07 06:32:32,707 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44191
2024-01-07 06:32:32,707 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41241
2024-01-07 06:32:32,707 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:32,707 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:32,707 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45639
2024-01-07 06:32:32,707 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:32,707 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45639
2024-01-07 06:32:32,707 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:32:32,707 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-st1i32lf
2024-01-07 06:32:32,707 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35673
2024-01-07 06:32:32,707 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:32,708 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:32,708 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:32,708 - distributed.worker - INFO - Starting Worker plugin RMMSetup-befd9dbf-39b1-4741-bfd6-c9a91ed2dbf9
2024-01-07 06:32:32,708 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:32:32,708 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cfl3gx3n
2024-01-07 06:32:32,708 - distributed.worker - INFO - Starting Worker plugin PreImport-fd3563a4-00a8-4c98-9d0b-69f65926dd4a
2024-01-07 06:32:32,708 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ffcf0473-a826-48fe-9904-0f3974bbff15
2024-01-07 06:32:32,708 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e165c82-7423-4741-8d74-107f522de8e4
2024-01-07 06:32:32,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:32,717 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:32,720 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:32,720 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:32,721 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:32,722 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40171
2024-01-07 06:32:32,722 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40171
2024-01-07 06:32:32,722 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41741
2024-01-07 06:32:32,722 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:32,722 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:32,722 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:32,722 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:32:32,722 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i_yttwum
2024-01-07 06:32:32,722 - distributed.worker - INFO - Starting Worker plugin PreImport-41a990e6-526f-4412-8087-2ee73afe8e99
2024-01-07 06:32:32,722 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fdfc5ce9-254f-46d7-bfe3-f09792a1fded
2024-01-07 06:32:32,722 - distributed.worker - INFO - Starting Worker plugin RMMSetup-33fa8cd2-1457-43b8-9088-fb7b382518a8
2024-01-07 06:32:32,724 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:32,724 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:32,726 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:32,727 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36967
2024-01-07 06:32:32,727 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36967
2024-01-07 06:32:32,727 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41531
2024-01-07 06:32:32,727 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:32,727 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:32,727 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:32,727 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:32:32,727 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e2knc03r
2024-01-07 06:32:32,728 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84a2e8ab-2669-4470-b813-9ca14b02e59c
2024-01-07 06:32:32,729 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:32,731 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35065
2024-01-07 06:32:32,731 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35065
2024-01-07 06:32:32,731 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36487
2024-01-07 06:32:32,731 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:32,731 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:32,731 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:32,731 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:32:32,731 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wtm78enn
2024-01-07 06:32:32,731 - distributed.worker - INFO - Starting Worker plugin PreImport-658ac2ad-0fe0-4625-b25c-a7b88dda570a
2024-01-07 06:32:32,732 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-945a8bad-bd89-4222-b02e-e211489c7b70
2024-01-07 06:32:32,732 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4f237d1-ef95-4e0e-b7e1-78585f5e4fa4
2024-01-07 06:32:33,103 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5427fcbe-b09a-40a6-84d5-e38f19e514f4
2024-01-07 06:32:33,103 - distributed.worker - INFO - Starting Worker plugin PreImport-aa5e8a45-766c-4b36-b080-630bcd562d71
2024-01-07 06:32:33,104 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:33,129 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46637', status: init, memory: 0, processing: 0>
2024-01-07 06:32:33,131 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46637
2024-01-07 06:32:33,131 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49366
2024-01-07 06:32:33,132 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:33,132 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:33,132 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:33,134 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:34,522 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4fa77ba8-56a5-48a0-a5a7-a7efda7ec8de
2024-01-07 06:32:34,523 - distributed.worker - INFO - Starting Worker plugin PreImport-f1c7f98a-8257-474d-8566-4501642e114e
2024-01-07 06:32:34,524 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,550 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35619', status: init, memory: 0, processing: 0>
2024-01-07 06:32:34,550 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35619
2024-01-07 06:32:34,550 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49388
2024-01-07 06:32:34,551 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:34,552 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:34,552 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,554 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:34,602 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,608 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,624 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de2ee242-7378-423b-93b1-7bd44aad8d1c
2024-01-07 06:32:34,625 - distributed.worker - INFO - Starting Worker plugin PreImport-191762d6-f937-4d72-9cc2-ff836a074341
2024-01-07 06:32:34,625 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,625 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45639', status: init, memory: 0, processing: 0>
2024-01-07 06:32:34,626 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45639
2024-01-07 06:32:34,626 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49404
2024-01-07 06:32:34,627 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:34,628 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:34,628 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,629 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:34,630 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40171', status: init, memory: 0, processing: 0>
2024-01-07 06:32:34,631 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40171
2024-01-07 06:32:34,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49414
2024-01-07 06:32:34,632 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:34,633 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:34,633 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,634 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:34,642 - distributed.worker - INFO - Starting Worker plugin PreImport-a0893648-2b41-4eb4-acb7-ea86726b06fb
2024-01-07 06:32:34,643 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ce53b39-e406-4c92-a13e-bf3265d84eab
2024-01-07 06:32:34,643 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,647 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44191', status: init, memory: 0, processing: 0>
2024-01-07 06:32:34,647 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44191
2024-01-07 06:32:34,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49430
2024-01-07 06:32:34,648 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:34,649 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:34,649 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,650 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:34,655 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,656 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8d72705b-7373-46c5-b6c1-e0cd0852b044
2024-01-07 06:32:34,657 - distributed.worker - INFO - Starting Worker plugin PreImport-0adc8fa4-26ce-4776-841b-608b1cb13268
2024-01-07 06:32:34,658 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,668 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36967', status: init, memory: 0, processing: 0>
2024-01-07 06:32:34,668 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36967
2024-01-07 06:32:34,669 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49442
2024-01-07 06:32:34,669 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:34,670 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:34,670 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,672 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:34,683 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35065', status: init, memory: 0, processing: 0>
2024-01-07 06:32:34,683 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35065
2024-01-07 06:32:34,683 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49444
2024-01-07 06:32:34,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:34,686 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:34,686 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,687 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:34,689 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45567', status: init, memory: 0, processing: 0>
2024-01-07 06:32:34,689 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45567
2024-01-07 06:32:34,689 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49458
2024-01-07 06:32:34,691 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:34,692 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:34,692 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:34,693 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:34,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:34,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:34,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:34,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:34,768 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:34,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:34,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:34,769 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-07 06:32:34,787 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:34,787 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:34,787 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:34,787 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:34,787 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:34,788 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:34,788 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:34,788 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:34,793 - distributed.scheduler - INFO - Remove client Client-87441ea9-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:34,794 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49348; closing.
2024-01-07 06:32:34,794 - distributed.scheduler - INFO - Remove client Client-87441ea9-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:34,794 - distributed.scheduler - INFO - Close client connection: Client-87441ea9-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:34,795 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40651'. Reason: nanny-close
2024-01-07 06:32:34,796 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:34,796 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40819'. Reason: nanny-close
2024-01-07 06:32:34,796 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:34,796 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42933'. Reason: nanny-close
2024-01-07 06:32:34,797 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46637. Reason: nanny-close
2024-01-07 06:32:34,797 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:34,797 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32947'. Reason: nanny-close
2024-01-07 06:32:34,797 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:34,797 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45567. Reason: nanny-close
2024-01-07 06:32:34,797 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41693'. Reason: nanny-close
2024-01-07 06:32:34,797 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44191. Reason: nanny-close
2024-01-07 06:32:34,797 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:34,798 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43387'. Reason: nanny-close
2024-01-07 06:32:34,798 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45639. Reason: nanny-close
2024-01-07 06:32:34,798 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:34,798 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44915'. Reason: nanny-close
2024-01-07 06:32:34,798 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35619. Reason: nanny-close
2024-01-07 06:32:34,798 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:34,798 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46095'. Reason: nanny-close
2024-01-07 06:32:34,799 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36967. Reason: nanny-close
2024-01-07 06:32:34,799 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:34,799 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:34,799 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35065. Reason: nanny-close
2024-01-07 06:32:34,799 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:34,799 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49404; closing.
2024-01-07 06:32:34,799 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:34,799 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40171. Reason: nanny-close
2024-01-07 06:32:34,800 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:34,800 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45639', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609154.800339')
2024-01-07 06:32:34,800 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:34,800 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:34,800 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:34,801 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49430; closing.
2024-01-07 06:32:34,801 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:34,801 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49366; closing.
2024-01-07 06:32:34,801 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:34,801 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:34,801 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:34,801 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:34,802 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:34,802 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44191', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609154.8022132')
2024-01-07 06:32:34,802 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:34,802 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46637', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609154.8027031')
2024-01-07 06:32:34,802 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:34,803 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49388; closing.
2024-01-07 06:32:34,803 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:34,804 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35619', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609154.804499')
2024-01-07 06:32:34,805 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49442; closing.
2024-01-07 06:32:34,805 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49458; closing.
2024-01-07 06:32:34,805 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49388>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 298, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 309, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49388>: Stream is closed
2024-01-07 06:32:34,807 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49430>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-07 06:32:34,808 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49366>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 263, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-07 06:32:34,808 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36967', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609154.808736')
2024-01-07 06:32:34,809 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45567', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609154.8092')
2024-01-07 06:32:34,809 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49444; closing.
2024-01-07 06:32:34,809 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49414; closing.
2024-01-07 06:32:34,810 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35065', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609154.8103018')
2024-01-07 06:32:34,810 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40171', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609154.8107395')
2024-01-07 06:32:34,810 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:32:35,861 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:32:35,861 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:32:35,862 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:32:35,863 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:32:35,863 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-07 06:32:38,206 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:38,211 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41277 instead
  warnings.warn(
2024-01-07 06:32:38,215 - distributed.scheduler - INFO - State start
2024-01-07 06:32:38,239 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:38,240 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:32:38,240 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41277/status
2024-01-07 06:32:38,241 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:32:38,406 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35029'
2024-01-07 06:32:39,145 - distributed.scheduler - INFO - Receive client connection: Client-8bf68b65-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:39,161 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49594
2024-01-07 06:32:40,280 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:40,280 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:40,284 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:40,285 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38401
2024-01-07 06:32:40,285 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38401
2024-01-07 06:32:40,285 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40075
2024-01-07 06:32:40,285 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:40,285 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:40,285 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:40,285 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-07 06:32:40,285 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kgej6twn
2024-01-07 06:32:40,286 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4b6771b-911a-4a96-93c8-8fff5d620ec8
2024-01-07 06:32:40,587 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d1432678-3c92-4a9c-8ab7-7e02014aeb57
2024-01-07 06:32:40,588 - distributed.worker - INFO - Starting Worker plugin PreImport-3325179c-3b85-4b16-9922-dee0636d3509
2024-01-07 06:32:40,588 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:40,654 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38401', status: init, memory: 0, processing: 0>
2024-01-07 06:32:40,655 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38401
2024-01-07 06:32:40,655 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48988
2024-01-07 06:32:40,656 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:40,657 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:40,657 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:40,659 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:40,694 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:32:40,698 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:40,700 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:40,702 - distributed.scheduler - INFO - Remove client Client-8bf68b65-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:40,702 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49594; closing.
2024-01-07 06:32:40,703 - distributed.scheduler - INFO - Remove client Client-8bf68b65-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:40,703 - distributed.scheduler - INFO - Close client connection: Client-8bf68b65-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:40,704 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35029'. Reason: nanny-close
2024-01-07 06:32:40,704 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:40,705 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38401. Reason: nanny-close
2024-01-07 06:32:40,707 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48988; closing.
2024-01-07 06:32:40,707 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:40,707 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38401', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609160.707694')
2024-01-07 06:32:40,707 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:32:40,708 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:41,419 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:32:41,420 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:32:41,420 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:32:41,421 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:32:41,421 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-07 06:32:43,867 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:43,872 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39717 instead
  warnings.warn(
2024-01-07 06:32:43,876 - distributed.scheduler - INFO - State start
2024-01-07 06:32:43,900 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-07 06:32:43,901 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-07 06:32:43,901 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39717/status
2024-01-07 06:32:43,902 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-07 06:32:44,046 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42901'
2024-01-07 06:32:44,342 - distributed.scheduler - INFO - Receive client connection: Client-8f456cb3-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:44,358 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49080
2024-01-07 06:32:45,940 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:32:45,940 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:32:45,945 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:32:45,945 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41287
2024-01-07 06:32:45,946 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41287
2024-01-07 06:32:45,946 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33187
2024-01-07 06:32:45,946 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-07 06:32:45,946 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:45,946 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:32:45,946 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-07 06:32:45,946 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ij6f71uu
2024-01-07 06:32:45,946 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76bd84c3-4819-4f95-ac34-66cddcb32859
2024-01-07 06:32:46,410 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac008046-6d3d-40b1-aa28-34be1bf7ed30
2024-01-07 06:32:46,411 - distributed.worker - INFO - Starting Worker plugin PreImport-68491162-1d8e-4508-aac0-8786459e39a3
2024-01-07 06:32:46,411 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:46,483 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41287', status: init, memory: 0, processing: 0>
2024-01-07 06:32:46,484 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41287
2024-01-07 06:32:46,484 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49094
2024-01-07 06:32:46,485 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:32:46,485 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-07 06:32:46,486 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:32:46,487 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-07 06:32:46,499 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-07 06:32:46,504 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-07 06:32:46,508 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:46,510 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:32:46,512 - distributed.scheduler - INFO - Remove client Client-8f456cb3-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:46,512 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49080; closing.
2024-01-07 06:32:46,513 - distributed.scheduler - INFO - Remove client Client-8f456cb3-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:46,513 - distributed.scheduler - INFO - Close client connection: Client-8f456cb3-ad26-11ee-b55b-d8c49764f6bb
2024-01-07 06:32:46,514 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42901'. Reason: nanny-close
2024-01-07 06:32:46,532 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-07 06:32:46,533 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41287. Reason: nanny-close
2024-01-07 06:32:46,534 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49094; closing.
2024-01-07 06:32:46,534 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-07 06:32:46,535 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41287', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1704609166.534984')
2024-01-07 06:32:46,535 - distributed.scheduler - INFO - Lost all workers
2024-01-07 06:32:46,536 - distributed.nanny - INFO - Worker closed
2024-01-07 06:32:47,229 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-07 06:32:47,229 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-07 06:32:47,230 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-07 06:32:47,231 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-07 06:32:47,231 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_d/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 48 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41417 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33733 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35649 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40653 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41427 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35503 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37611 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38585 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37097 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43359 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33851 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36919 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45747 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46313 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36083 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42057 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34955 instead
  warnings.warn(
2024-01-07 06:36:04,155 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 439, in read
    await self.ep.recv(each_frame)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 445, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-07 06:36:04,158 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
2024-01-07 06:36:04,184 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/ucxx/_lib_async/endpoint.py", line 383, in recv
    ret = await req.wait()
  File "libucxx.pyx", line 737, in wait
  File "libucxx.pyx", line 722, in wait_yield
  File "libucxx.pyx", line 717, in ucxx._lib.libucxx.UCXRequest.check_error
ucxx.UCXXCanceledError: Request canceled

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: UCXXCanceledError('Request canceled')
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45329 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35715 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45745 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43151 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41095 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38735 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39973 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42015 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42777 instead
  warnings.warn(
[1704609519.043979] [dgx13:72728:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:41911) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39429 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36667 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35397 instead
  warnings.warn(
[1704609581.554589] [dgx13:73699:0]            sock.c:481  UCX  ERROR bind(fd=159 addr=0.0.0.0:45181) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33075 instead
  warnings.warn(
[1704609613.702235] [dgx13:74107:0]            sock.c:481  UCX  ERROR bind(fd=151 addr=0.0.0.0:34086) failed: Address already in use
[1704609613.976751] [dgx13:74107:0]            sock.c:481  UCX  ERROR bind(fd=161 addr=0.0.0.0:36571) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41881 instead
  warnings.warn(
[1704609651.720867] [dgx13:74722:0]            sock.c:481  UCX  ERROR bind(fd=153 addr=0.0.0.0:51125) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42639 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46625 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32829 instead
  warnings.warn(
[1704609726.143661] [dgx13:75620:0]            sock.c:481  UCX  ERROR bind(fd=150 addr=0.0.0.0:39718) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43671 instead
  warnings.warn(
[1704609762.952034] [dgx13:76086:0]            sock.c:481  UCX  ERROR bind(fd=155 addr=0.0.0.0:51861) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45121 instead
  warnings.warn(
[1704609793.812296] [dgx13:76322:0]            sock.c:481  UCX  ERROR bind(fd=157 addr=0.0.0.0:37984) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43501 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36489 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38997 instead
  warnings.warn(
2024-01-07 06:44:28,310 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1394, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1153, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 237, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:57346 remote=tcp://127.0.0.1:41387>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38807 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39189 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45245 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44925 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35499 instead
  warnings.warn(
[1704609958.690880] [dgx13:79328:0]            sock.c:481  UCX  ERROR bind(fd=122 addr=0.0.0.0:58755) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42869 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33707 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41979 instead
  warnings.warn(
[1704610020.582166] [dgx13:80300:0]            sock.c:481  UCX  ERROR bind(fd=158 addr=0.0.0.0:37454) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37299 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41511 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45449 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42435 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35949 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33417 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33315 instead
  warnings.warn(
[1704610222.634846] [dgx13:82931:0]            sock.c:481  UCX  ERROR bind(fd=150 addr=0.0.0.0:38054) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37951 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38949 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42361 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43213 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42405 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40883 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46695 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36391 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38571 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44211 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43569 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35177 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45555 instead
  warnings.warn(
[1704610421.799415] [dgx13:85887:0]            sock.c:481  UCX  ERROR bind(fd=124 addr=0.0.0.0:36026) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37941 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43187 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44413 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33237 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36265 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42671 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44583 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35515 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] 2024-01-07 06:56:13,356 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
2024-01-07 06:56:13,361 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] [1704610575.909297] [dgx13:62811:0]            sock.c:481  UCX  ERROR bind(fd=253 addr=0.0.0.0:33120) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43309 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42617 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37089 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41635 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40589 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40901 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41169 instead
  warnings.warn(
[1704610636.422603] [dgx13:88564:0]            sock.c:481  UCX  ERROR bind(fd=169 addr=0.0.0.0:49078) failed: Address already in use
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37853 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1704610656.135559] [dgx13:62811:1]            sock.c:481  UCX  ERROR bind(fd=245 addr=0.0.0.0:36698) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] [1704610663.613562] [dgx13:62811:1]            sock.c:481  UCX  ERROR bind(fd=253 addr=0.0.0.0:37652) failed: Address already in use
[1704610663.613617] [dgx13:62811:1]            sock.c:481  UCX  ERROR bind(fd=253 addr=0.0.0.0:33522) failed: Address already in use
2024-01-07 06:57:49,558 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1563, in _connect
    comm = await connect(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 342, in connect
    comm = await wait_for(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 466, in wait_for
    await waiter
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1673, in connect
    return connect_attempt.result()
asyncio.exceptions.CancelledError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1391, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1675, in connect
    raise CommClosedError(reason)
distributed.comm.core.CommClosedError: ConnectionPool closing.
[1704610670.744460] [dgx13:62811] UCXPY  WARNING Listener object is being destroyed, but 1 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
[1704610670.744536] [dgx13:62811] UCXPY  WARNING Listener object is being destroyed, but 1 client handler(s) is(are) still alive. This usually indicates the Listener was prematurely destroyed.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] [1704610677.231439] [dgx13:62811:1]            sock.c:481  UCX  ERROR bind(fd=253 addr=0.0.0.0:48892) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-07 06:58:16,844 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:16,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:16,921 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:16,921 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:16,968 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:16,968 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:16,998 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:16,998 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:17,107 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:17,108 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:17,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:17,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:17,151 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:17,151 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:17,408 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:17,409 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:17,466 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:17,467 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43649
2024-01-07 06:58:17,467 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43649
2024-01-07 06:58:17,467 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38639
2024-01-07 06:58:17,467 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,467 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,467 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:17,467 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ipq0n826
2024-01-07 06:58:17,468 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e9a77d0-f316-4797-986a-a324a5ab4224
2024-01-07 06:58:17,468 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6284926-c359-499e-9dda-3adbbaca44c4
2024-01-07 06:58:17,469 - distributed.worker - INFO - Starting Worker plugin PreImport-bd03114c-ff17-48ee-9175-457336580999
2024-01-07 06:58:17,469 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,537 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:17,538 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,538 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,539 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42475
2024-01-07 06:58:17,598 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:17,599 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35337
2024-01-07 06:58:17,599 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35337
2024-01-07 06:58:17,599 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45835
2024-01-07 06:58:17,599 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,599 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,599 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:17,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2zprrijm
2024-01-07 06:58:17,600 - distributed.worker - INFO - Starting Worker plugin PreImport-a9f095d1-08f4-4152-bc16-ceb156bcfc87
2024-01-07 06:58:17,600 - distributed.worker - INFO - Starting Worker plugin RMMSetup-29cbff79-9a52-4fb9-879c-9fba7c875717
2024-01-07 06:58:17,600 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6e2a3f24-9ea7-4887-98c0-d1353a39b3a9
2024-01-07 06:58:17,601 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,631 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:17,632 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35287
2024-01-07 06:58:17,632 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35287
2024-01-07 06:58:17,632 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33225
2024-01-07 06:58:17,632 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,632 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,632 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:17,632 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9xcax92v
2024-01-07 06:58:17,632 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6fa61047-18e7-4e19-8cea-77a9a59cd9ba
2024-01-07 06:58:17,632 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-372d6a3b-c715-4ebf-b60e-79d0a0b3237f
2024-01-07 06:58:17,632 - distributed.worker - INFO - Starting Worker plugin PreImport-aa8b31ef-8ed0-435d-8367-eb28cf9055c3
2024-01-07 06:58:17,633 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,638 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:17,639 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40679
2024-01-07 06:58:17,639 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40679
2024-01-07 06:58:17,639 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38467
2024-01-07 06:58:17,639 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,639 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,639 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:17,639 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o6saae2t
2024-01-07 06:58:17,640 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f299ea08-4442-4d5b-8b43-2cb2eb20c507
2024-01-07 06:58:17,640 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c94a7422-a93d-4331-8539-77ffdf0474a7
2024-01-07 06:58:17,640 - distributed.worker - INFO - Starting Worker plugin PreImport-8cfb3bfd-b53e-4ac6-99d5-17a1500d6c4f
2024-01-07 06:58:17,640 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,684 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:17,684 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,684 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,686 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42475
2024-01-07 06:58:17,738 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:17,739 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,739 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42475
2024-01-07 06:58:17,740 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:17,741 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,741 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,742 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42475
2024-01-07 06:58:17,778 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:17,779 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34951
2024-01-07 06:58:17,779 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34951
2024-01-07 06:58:17,779 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33523
2024-01-07 06:58:17,779 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,779 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,779 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:17,779 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_5bi0ig8
2024-01-07 06:58:17,780 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b8c10c1a-16b8-482d-9830-01317920bed5
2024-01-07 06:58:17,780 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f5c4b9f-7b4e-494b-9eaa-1fc6de870766
2024-01-07 06:58:17,781 - distributed.worker - INFO - Starting Worker plugin PreImport-86e84bf2-8990-438c-b7f2-8030c00eb272
2024-01-07 06:58:17,781 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,794 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:17,796 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41919
2024-01-07 06:58:17,796 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41919
2024-01-07 06:58:17,796 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42835
2024-01-07 06:58:17,796 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,796 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,796 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:17,796 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uwhp41he
2024-01-07 06:58:17,796 - distributed.worker - INFO - Starting Worker plugin RMMSetup-84b69fea-f951-4d26-bd40-ef43ffe7dcac
2024-01-07 06:58:17,796 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-75bb6e2f-745a-4d88-a838-493e47984d6c
2024-01-07 06:58:17,797 - distributed.worker - INFO - Starting Worker plugin PreImport-a9885c59-354c-467a-a7f3-31c03eb946ce
2024-01-07 06:58:17,797 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,885 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:17,886 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,886 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,887 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42475
2024-01-07 06:58:17,889 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:17,890 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,890 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42475
2024-01-07 06:58:17,977 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:17,978 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44267
2024-01-07 06:58:17,978 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44267
2024-01-07 06:58:17,978 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46865
2024-01-07 06:58:17,978 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42475
2024-01-07 06:58:17,978 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:17,978 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:17,978 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s61ewz_7
2024-01-07 06:58:17,979 - distributed.worker - INFO - Starting Worker plugin RMMSetup-74e9571d-0f62-4492-a016-dfd8cbe4904e
2024-01-07 06:58:17,979 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-efc254b0-6d89-4f22-8980-4081d7f4dd14
2024-01-07 06:58:17,979 - distributed.worker - INFO - Starting Worker plugin PreImport-d2560a98-f2db-4f43-98fa-a7f1919031b4
2024-01-07 06:58:17,979 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:18,058 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:18,058 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42475
2024-01-07 06:58:18,059 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:18,060 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42475
2024-01-07 06:58:18,131 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:18,133 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34953
2024-01-07 06:58:18,133 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34953
2024-01-07 06:58:18,133 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44841
2024-01-07 06:58:18,133 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:42475
2024-01-07 06:58:18,133 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:18,133 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:18,133 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7snkknu5
2024-01-07 06:58:18,134 - distributed.worker - INFO - Starting Worker plugin PreImport-30e709d8-049a-4f59-951c-506699b11ed3
2024-01-07 06:58:18,134 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-142393b3-fa45-4fcf-a0c4-2e31c38f511b
2024-01-07 06:58:18,134 - distributed.worker - INFO - Starting Worker plugin RMMSetup-721ab886-18bd-4465-927b-1d9f28a2c50f
2024-01-07 06:58:18,135 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:18,215 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:18,216 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:42475
2024-01-07 06:58:18,216 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:18,218 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42475
2024-01-07 06:58:18,243 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:58:18,243 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:58:18,243 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:58:18,244 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:58:18,244 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:58:18,244 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:58:18,246 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:58:18,246 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-07 06:58:18,251 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43649. Reason: nanny-close
2024-01-07 06:58:18,252 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35337. Reason: nanny-close
2024-01-07 06:58:18,252 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40679. Reason: nanny-close
2024-01-07 06:58:18,253 - distributed.core - INFO - Connection to tcp://127.0.0.1:42475 has been closed.
2024-01-07 06:58:18,254 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35287. Reason: nanny-close
2024-01-07 06:58:18,254 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34953. Reason: nanny-close
2024-01-07 06:58:18,254 - distributed.core - INFO - Connection to tcp://127.0.0.1:42475 has been closed.
2024-01-07 06:58:18,255 - distributed.core - INFO - Connection to tcp://127.0.0.1:42475 has been closed.
2024-01-07 06:58:18,255 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:18,255 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34951. Reason: nanny-close
2024-01-07 06:58:18,255 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44267. Reason: nanny-close
2024-01-07 06:58:18,255 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:18,255 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41919. Reason: nanny-close
2024-01-07 06:58:18,256 - distributed.core - INFO - Connection to tcp://127.0.0.1:42475 has been closed.
2024-01-07 06:58:18,256 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:18,257 - distributed.core - INFO - Connection to tcp://127.0.0.1:42475 has been closed.
2024-01-07 06:58:18,257 - distributed.core - INFO - Connection to tcp://127.0.0.1:42475 has been closed.
2024-01-07 06:58:18,257 - distributed.core - INFO - Connection to tcp://127.0.0.1:42475 has been closed.
2024-01-07 06:58:18,257 - distributed.core - INFO - Connection to tcp://127.0.0.1:42475 has been closed.
2024-01-07 06:58:18,258 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:18,258 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:18,258 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:18,259 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:18,259 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool 2024-01-07 06:58:24,448 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-07 06:58:24,451 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async 2024-01-07 06:58:34,473 - distributed.worker - ERROR - CUDA error at: /opt/conda/conda-bld/work/include/rmm/detail/cuda_util.hpp:27: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 342, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/detail/cuda_util.hpp:27: cudaErrorMemoryAllocation out of memory
2024-01-07 06:58:34,475 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 342, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/detail/cuda_util.hpp:27: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size 2024-01-07 06:58:40,802 - distributed.worker - ERROR - CUDA error at: /opt/conda/conda-bld/work/include/rmm/detail/cuda_util.hpp:27: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 342, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/detail/cuda_util.hpp:27: cudaErrorMemoryAllocation out of memory
2024-01-07 06:58:40,806 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 342, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/detail/cuda_util.hpp:27: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging 2024-01-07 06:58:44,582 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1001, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-07 06:58:44,587 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1001, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-07 06:58:51,538 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:51,538 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:51,542 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:51,542 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45079
2024-01-07 06:58:51,542 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45079
2024-01-07 06:58:51,543 - distributed.worker - INFO -           Worker name:                          0
2024-01-07 06:58:51,543 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40271
2024-01-07 06:58:51,543 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:36879
2024-01-07 06:58:51,543 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:51,543 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:51,543 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-07 06:58:51,543 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t7u9tzxz
2024-01-07 06:58:51,543 - distributed.worker - INFO - Starting Worker plugin PreImport-276f1d06-bd5e-4d53-a47a-42ff8e184686
2024-01-07 06:58:51,547 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-07 06:58:51,547 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54214f5e-14d5-4005-8ddf-a7f0dc12a2dc
2024-01-07 06:58:51,547 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3be2e4df-98e8-4ac8-89d0-b593754691d4
2024-01-07 06:58:51,547 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45079. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-07 06:58:51,547 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-07 06:58:51,549 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-07 06:58:56,380 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:56,380 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:56,391 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:56,391 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:56,436 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:56,437 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:56,459 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:56,459 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:56,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:56,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:56,609 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:56,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:56,714 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:56,714 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:56,735 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-07 06:58:56,735 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-07 06:58:56,999 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:57,000 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42571
2024-01-07 06:58:57,000 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42571
2024-01-07 06:58:57,000 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36815
2024-01-07 06:58:57,000 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,000 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,001 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:57,001 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:58:57,001 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zy7hnmkw
2024-01-07 06:58:57,001 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-528319a3-c785-4507-996c-a10c94acf738
2024-01-07 06:58:57,002 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eec1128e-b516-44c5-8f2c-6dc11303f48f
2024-01-07 06:58:57,002 - distributed.worker - INFO - Starting Worker plugin PreImport-c863d252-cf7d-488c-841c-28e23ea91bb3
2024-01-07 06:58:57,002 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,004 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:57,005 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34919
2024-01-07 06:58:57,005 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34919
2024-01-07 06:58:57,005 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41121
2024-01-07 06:58:57,005 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,005 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,005 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:57,005 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:58:57,005 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f_vx1ys3
2024-01-07 06:58:57,006 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fd4bd840-a861-4aea-87e9-ab9ddfc9503d
2024-01-07 06:58:57,007 - distributed.worker - INFO - Starting Worker plugin PreImport-77427643-42d2-485c-9f52-049c4cbbe832
2024-01-07 06:58:57,007 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d75a6a57-ccb7-4023-b3cc-5ec0cd326250
2024-01-07 06:58:57,007 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,062 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:57,063 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41155
2024-01-07 06:58:57,063 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41155
2024-01-07 06:58:57,063 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45117
2024-01-07 06:58:57,063 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,063 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,063 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:57,063 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:58:57,063 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-la9p2btr
2024-01-07 06:58:57,063 - distributed.worker - INFO - Starting Worker plugin PreImport-7a45d94c-df5d-40fb-b227-fa9e03c6585a
2024-01-07 06:58:57,063 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-dfd2dbb6-32bc-4eff-8c0f-4dc2d6354761
2024-01-07 06:58:57,063 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a87c12b4-efb2-4304-84e9-87eb93b3e606
2024-01-07 06:58:57,064 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,085 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:57,085 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42623
2024-01-07 06:58:57,086 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42623
2024-01-07 06:58:57,086 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42975
2024-01-07 06:58:57,086 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,086 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,086 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:57,086 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:58:57,086 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2r9klwcr
2024-01-07 06:58:57,086 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5fd4011-b96c-4533-91bf-c0f009502329
2024-01-07 06:58:57,086 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c2676265-f540-40ba-b0c1-c8f90aefcbf2
2024-01-07 06:58:57,087 - distributed.worker - INFO - Starting Worker plugin PreImport-bbb83e11-feaa-42f1-b4eb-0662f679f3de
2024-01-07 06:58:57,087 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,104 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:57,105 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,105 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,106 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43275
2024-01-07 06:58:57,108 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:57,109 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,109 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43275
2024-01-07 06:58:57,153 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:57,154 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,154 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43275
2024-01-07 06:58:57,162 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:57,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43539
2024-01-07 06:58:57,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43539
2024-01-07 06:58:57,163 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45575
2024-01-07 06:58:57,163 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,163 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,163 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:57,163 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:58:57,163 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ppy3o33l
2024-01-07 06:58:57,164 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-61e95166-c792-4c5b-b60d-917eec5328a6
2024-01-07 06:58:57,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3667f778-ec64-4e19-b284-ce637d37b5b2
2024-01-07 06:58:57,164 - distributed.worker - INFO - Starting Worker plugin PreImport-eff11592-41ee-4212-b3ca-5bd9febce07b
2024-01-07 06:58:57,164 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,168 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:57,169 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,169 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,170 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43275
2024-01-07 06:58:57,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:57,227 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,227 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,228 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43275
2024-01-07 06:58:57,349 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:57,350 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35113
2024-01-07 06:58:57,350 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35113
2024-01-07 06:58:57,350 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38301
2024-01-07 06:58:57,351 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,351 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,351 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:57,351 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:58:57,351 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-klria91q
2024-01-07 06:58:57,351 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aa70c503-2557-4c1d-b4d0-10ba1c54ae63
2024-01-07 06:58:57,352 - distributed.worker - INFO - Starting Worker plugin PreImport-042de1cc-1b6f-4fd4-a323-a52326833f13
2024-01-07 06:58:57,352 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9fcc6122-14df-4adb-8f80-786e1eceaeea
2024-01-07 06:58:57,352 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,370 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:57,371 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40643
2024-01-07 06:58:57,371 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40643
2024-01-07 06:58:57,371 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46807
2024-01-07 06:58:57,371 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,371 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,371 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:57,371 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:58:57,371 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nqdp6ypi
2024-01-07 06:58:57,372 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f74df6d8-dcf8-406e-8762-5454c1d2fffb
2024-01-07 06:58:57,372 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6eadd46a-2c3f-4ff5-a4d7-779989ed86e3
2024-01-07 06:58:57,372 - distributed.worker - INFO - Starting Worker plugin PreImport-67e91a88-3f32-4185-b6be-c82b1dd297d2
2024-01-07 06:58:57,372 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,390 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-07 06:58:57,391 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41563
2024-01-07 06:58:57,391 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41563
2024-01-07 06:58:57,391 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41331
2024-01-07 06:58:57,391 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,391 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,392 - distributed.worker - INFO -               Threads:                          1
2024-01-07 06:58:57,392 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-07 06:58:57,392 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jh_sbwaq
2024-01-07 06:58:57,392 - distributed.worker - INFO - Starting Worker plugin PreImport-4a0cd7ec-df80-4858-a9bc-5aed84a5e462
2024-01-07 06:58:57,392 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c68e00ae-e99e-47a3-9dcb-8d3c51d35eee
2024-01-07 06:58:57,392 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d261c37-a764-4e54-be05-00a944861de5
2024-01-07 06:58:57,395 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,436 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:57,436 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,436 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,437 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43275
2024-01-07 06:58:57,471 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:57,472 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,472 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,473 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43275
2024-01-07 06:58:57,482 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-07 06:58:57,483 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43275
2024-01-07 06:58:57,483 - distributed.worker - INFO - -------------------------------------------------
2024-01-07 06:58:57,484 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43275
2024-01-07 06:58:57,497 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42571. Reason: nanny-close
2024-01-07 06:58:57,497 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34919. Reason: nanny-close
2024-01-07 06:58:57,498 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42623. Reason: nanny-close
2024-01-07 06:58:57,499 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41155. Reason: nanny-close
2024-01-07 06:58:57,499 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43539. Reason: nanny-close
2024-01-07 06:58:57,499 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35113. Reason: nanny-close
2024-01-07 06:58:57,500 - distributed.core - INFO - Connection to tcp://127.0.0.1:43275 has been closed.
2024-01-07 06:58:57,500 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41563. Reason: nanny-close
2024-01-07 06:58:57,500 - distributed.core - INFO - Connection to tcp://127.0.0.1:43275 has been closed.
2024-01-07 06:58:57,500 - distributed.core - INFO - Connection to tcp://127.0.0.1:43275 has been closed.
2024-01-07 06:58:57,500 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40643. Reason: nanny-close
2024-01-07 06:58:57,500 - distributed.core - INFO - Connection to tcp://127.0.0.1:43275 has been closed.
2024-01-07 06:58:57,501 - distributed.core - INFO - Connection to tcp://127.0.0.1:43275 has been closed.
2024-01-07 06:58:57,501 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:57,501 - distributed.core - INFO - Connection to tcp://127.0.0.1:43275 has been closed.
2024-01-07 06:58:57,501 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:57,501 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:57,502 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:57,502 - distributed.core - INFO - Connection to tcp://127.0.0.1:43275 has been closed.
2024-01-07 06:58:57,502 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:57,502 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:57,503 - distributed.core - INFO - Connection to tcp://127.0.0.1:43275 has been closed.
2024-01-07 06:58:57,503 - distributed.nanny - INFO - Worker closed
2024-01-07 06:58:57,504 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations 2024-01-07 06:59:05,030 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-07 06:59:05,034 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration 2024-01-07 06:59:08,200 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-07 06:59:08,204 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-01-07 06:59:10,410 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded
2024-01-07 06:59:10,417 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 945, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1005, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 385, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] 2024-01-07 06:59:14,194 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 770, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-01-07 06:59:14,199 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] 2024-01-07 06:59:17,909 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 585, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 139, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 770, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-01-07 06:59:18,369 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 663, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 858, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 585, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 139, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 677, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 671, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] 2024-01-07 06:59:23,361 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 147, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-07 06:59:23,369 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe58b32fd60>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 147, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 147, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-07 06:59:25,373 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] 2024-01-07 06:59:43,777 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 147, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-07 06:59:43,785 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:40513'. Shutting down.
2024-01-07 06:59:43,787 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fd91cd53d30>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 147, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1024, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 248, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 150, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 147, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-07 06:59:45,791 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-07 07:00:48,198 - distributed.worker - WARNING - RMM allocation of 1.00 MiB failed, spill-on-demand couldn't find any device memory to spill.
RMM allocs: 1.00 MiB, <ProxyManager dev_limit=25.60 GiB host_limit=0.98 TiB disk=0 B(0) host=0 B(0) dev=0 B(0)>, traceback:
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 937, in _bootstrap
    self._bootstrap_inner()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/threadpoolexecutor.py", line 57, in _worker
    task.run()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/_concurrent_futures_thread.py", line 65, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1541, in <lambda>
    executor, lambda: context.run(func, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2954, in apply_function
    msg = apply_function_simple(function, args, kwargs, time_delay)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 2990, in apply_function_simple
    result = function(*args, **kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_proxify_host_file.py", line 467, in task
    rmm.DeviceBuffer(size=rmm_pool_size),  # Trigger OOM
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/proxify_host_file.py", line 617, in oom
    traceback.print_stack(file=f)


2024-01-07 07:00:48,429 - distributed.worker - WARNING - Compute Failed
Key:       task-5c302f54392a5ca98adb3c55c8357b0e
Function:  task
args:      ()
kwargs:    {}
Exception: "MemoryError('std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:273: Maximum pool size exceeded')"

PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_serializer PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[None-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second1-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-None] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first1] PASSED
dask_cuda/tests/test_proxy.py::test_double_proxy_object[serializers_second2-serializers_first2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[numpy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_array[cupy-serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[None] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_proxy_object_of_cudf[serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers0-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-None] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers1] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers2] PASSED
dask_cuda/tests/test_proxy.py::test_serialize_of_proxied_cudf[dask_serializers1-proxy_serializers3] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[numpy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_length[cupy] PASSED
dask_cuda/tests/test_proxy.py::test_fixed_attribute_name PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxy.py::test_spilling_local_cuda_cluster[False] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj0] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_to_disk[obj1] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[dask] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[pickle] PASSED
dask_cuda/tests/test_proxy.py::test_multiple_deserializations[disk] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers2-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers3-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[numpy-serializers4-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-None-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers1-10000] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10] PASSED
dask_cuda/tests/test_proxy.py::test_serializing_array_to_disk[cupy-serializers2-10000] 