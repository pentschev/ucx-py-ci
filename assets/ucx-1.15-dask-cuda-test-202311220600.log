============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.3, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.12.0
collecting ... collected 1197 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2023-11-22 06:37:52,038 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:37:52,042 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36015 instead
  warnings.warn(
2023-11-22 06:37:52,046 - distributed.scheduler - INFO - State start
2023-11-22 06:37:52,395 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:37:52,396 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-22 06:37:52,397 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36015/status
2023-11-22 06:37:52,398 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:37:52,527 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44677'
2023-11-22 06:37:52,552 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37467'
2023-11-22 06:37:52,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36103'
2023-11-22 06:37:52,563 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41599'
2023-11-22 06:37:53,047 - distributed.scheduler - INFO - Receive client connection: Client-a8039b68-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:37:53,062 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52848
2023-11-22 06:37:54,234 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:37:54,234 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:37:54,237 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:37:54,252 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:37:54,252 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:37:54,256 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:37:54,395 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:37:54,395 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:37:54,399 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:37:54,446 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:37:54,446 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:37:54,450 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
Unable to start CUDA Context
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/initialize.py", line 31, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 133, in init_once
    cuda_visible_device = get_device_index_and_uuid(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/diagnostics/nvml.py", line 256, in get_device_index_and_uuid
    device_handle = pynvml.nvmlDeviceGetHandleByIndex(device_index)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 1655, in nvmlDeviceGetHandleByIndex
    _nvmlCheckReturn(ret)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/pynvml/nvml.py", line 765, in _nvmlCheckReturn
    raise NVMLError(ret)
pynvml.nvml.NVMLError_InvalidArgument: Invalid Argument
2023-11-22 06:37:54,544 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39689
2023-11-22 06:37:54,544 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39689
2023-11-22 06:37:54,544 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33501
2023-11-22 06:37:54,544 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-22 06:37:54,544 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:54,544 - distributed.worker - INFO -               Threads:                          4
2023-11-22 06:37:54,544 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-22 06:37:54,544 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-cuqvftgt
2023-11-22 06:37:54,545 - distributed.worker - INFO - Starting Worker plugin PreImport-98e2ec12-a4bb-43e1-a98e-652480668217
2023-11-22 06:37:54,545 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12460ee3-3b47-48a8-9306-6a0e5514fc03
2023-11-22 06:37:54,545 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ad31de0-745d-4b3c-a9a0-117a8826638a
2023-11-22 06:37:54,545 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:55,464 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39689', status: init, memory: 0, processing: 0>
2023-11-22 06:37:55,465 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39689
2023-11-22 06:37:55,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52866
2023-11-22 06:37:55,466 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:37:55,467 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-22 06:37:55,467 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:55,469 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-22 06:37:56,078 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33913
2023-11-22 06:37:56,079 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33913
2023-11-22 06:37:56,078 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35135
2023-11-22 06:37:56,079 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35799
2023-11-22 06:37:56,079 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35135
2023-11-22 06:37:56,079 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-22 06:37:56,079 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:56,079 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45401
2023-11-22 06:37:56,079 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-22 06:37:56,079 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:56,079 - distributed.worker - INFO -               Threads:                          4
2023-11-22 06:37:56,079 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-22 06:37:56,079 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-r9q97tb7
2023-11-22 06:37:56,079 - distributed.worker - INFO -               Threads:                          4
2023-11-22 06:37:56,079 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-22 06:37:56,079 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-gezoaoo0
2023-11-22 06:37:56,080 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d3fbd145-84ac-4edc-9f74-c2ee697b240e
2023-11-22 06:37:56,080 - distributed.worker - INFO - Starting Worker plugin PreImport-bda7ac6d-83d7-47d5-a4cc-f8bf9e853576
2023-11-22 06:37:56,080 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-97ecc3a9-2c86-4f11-bef4-21c71984e817
2023-11-22 06:37:56,080 - distributed.worker - INFO - Starting Worker plugin RMMSetup-62aef38e-86f0-4910-b1ef-2ac80e558346
2023-11-22 06:37:56,080 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:56,080 - distributed.worker - INFO - Starting Worker plugin RMMSetup-348ee948-e884-460d-b571-0445f859d34f
2023-11-22 06:37:56,080 - distributed.worker - INFO - Starting Worker plugin PreImport-e4f19b37-fcab-415d-b447-3004f6da0a05
2023-11-22 06:37:56,080 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:56,104 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33913', status: init, memory: 0, processing: 0>
2023-11-22 06:37:56,104 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33913
2023-11-22 06:37:56,105 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52874
2023-11-22 06:37:56,105 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:37:56,106 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-22 06:37:56,106 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:56,110 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-22 06:37:56,111 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35135', status: init, memory: 0, processing: 0>
2023-11-22 06:37:56,111 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35135
2023-11-22 06:37:56,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52886
2023-11-22 06:37:56,113 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:37:56,114 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-22 06:37:56,114 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:56,121 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-22 06:37:56,203 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33507
2023-11-22 06:37:56,204 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33507
2023-11-22 06:37:56,204 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36681
2023-11-22 06:37:56,204 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-22 06:37:56,204 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:56,204 - distributed.worker - INFO -               Threads:                          4
2023-11-22 06:37:56,205 - distributed.worker - INFO -                Memory:                 251.94 GiB
2023-11-22 06:37:56,205 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-g64_pcf4
2023-11-22 06:37:56,205 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-91b51ec2-fc9b-477d-a78c-9ef2ceb4fa3c
2023-11-22 06:37:56,205 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f11fac6a-700c-4ac8-bc0c-17ba8579553e
2023-11-22 06:37:56,206 - distributed.worker - INFO - Starting Worker plugin PreImport-1fb7744e-4a84-4e7e-88a2-1c0db3deb588
2023-11-22 06:37:56,206 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:56,238 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33507', status: init, memory: 0, processing: 0>
2023-11-22 06:37:56,238 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33507
2023-11-22 06:37:56,239 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52898
2023-11-22 06:37:56,240 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:37:56,242 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-22 06:37:56,242 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:37:56,249 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-22 06:37:56,293 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-22 06:37:56,294 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-22 06:37:56,294 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-22 06:37:56,294 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2023-11-22 06:37:56,299 - distributed.scheduler - INFO - Remove client Client-a8039b68-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:37:56,299 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52848; closing.
2023-11-22 06:37:56,300 - distributed.scheduler - INFO - Remove client Client-a8039b68-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:37:56,300 - distributed.scheduler - INFO - Close client connection: Client-a8039b68-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:37:56,301 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44677'. Reason: nanny-close
2023-11-22 06:37:56,301 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:37:56,302 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37467'. Reason: nanny-close
2023-11-22 06:37:56,302 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:37:56,303 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33913. Reason: nanny-close
2023-11-22 06:37:56,303 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36103'. Reason: nanny-close
2023-11-22 06:37:56,303 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:37:56,303 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35135. Reason: nanny-close
2023-11-22 06:37:56,304 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-22 06:37:56,304 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52874; closing.
2023-11-22 06:37:56,305 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33913', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635076.305359')
2023-11-22 06:37:56,306 - distributed.nanny - INFO - Worker closed
2023-11-22 06:37:56,307 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-22 06:37:56,307 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52886; closing.
2023-11-22 06:37:56,308 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35135', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635076.3080716')
2023-11-22 06:37:56,308 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41599'. Reason: nanny-close
2023-11-22 06:37:56,308 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:37:56,309 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33507. Reason: nanny-close
2023-11-22 06:37:56,309 - distributed.nanny - INFO - Worker closed
2023-11-22 06:37:56,310 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39689. Reason: nanny-close
2023-11-22 06:37:56,310 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-22 06:37:56,311 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52898; closing.
2023-11-22 06:37:56,311 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33507', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635076.311386')
2023-11-22 06:37:56,312 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-22 06:37:56,312 - distributed.nanny - INFO - Worker closed
2023-11-22 06:37:56,312 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52866; closing.
2023-11-22 06:37:56,313 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39689', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635076.3131795')
2023-11-22 06:37:56,313 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:37:56,314 - distributed.nanny - INFO - Worker closed
2023-11-22 06:37:56,313 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:52866>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-22 06:37:57,518 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:37:57,518 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:37:57,519 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:37:57,521 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-22 06:37:57,521 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2023-11-22 06:37:59,733 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:37:59,738 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39967 instead
  warnings.warn(
2023-11-22 06:37:59,742 - distributed.scheduler - INFO - State start
2023-11-22 06:37:59,764 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:37:59,765 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:37:59,766 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39967/status
2023-11-22 06:37:59,766 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:37:59,886 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36535'
2023-11-22 06:37:59,899 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39725'
2023-11-22 06:37:59,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45591'
2023-11-22 06:37:59,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44191'
2023-11-22 06:37:59,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34903'
2023-11-22 06:37:59,934 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38329'
2023-11-22 06:37:59,943 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46017'
2023-11-22 06:37:59,951 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38457'
2023-11-22 06:38:00,199 - distributed.scheduler - INFO - Receive client connection: Client-aca38a98-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:00,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56276
2023-11-22 06:38:01,801 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:01,801 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:01,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:01,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:01,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:01,804 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:01,804 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:01,804 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:01,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:01,806 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:01,808 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:01,809 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:01,833 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:01,833 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:01,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:01,834 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:01,834 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:01,835 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:01,837 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:01,838 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:01,839 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:01,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:01,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:01,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:04,747 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44401
2023-11-22 06:38:04,748 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44401
2023-11-22 06:38:04,748 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41793
2023-11-22 06:38:04,748 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:04,748 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:04,748 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:04,749 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:04,749 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7f1plff4
2023-11-22 06:38:04,749 - distributed.worker - INFO - Starting Worker plugin PreImport-fb12ab63-cc82-4de5-952e-5cc4f4fbe377
2023-11-22 06:38:04,749 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d848b657-8690-4a8b-bae8-dbe33830c51f
2023-11-22 06:38:04,750 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6b03fed7-f848-4b42-a7ab-0e19d6d6c0fa
2023-11-22 06:38:04,750 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32947
2023-11-22 06:38:04,751 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32947
2023-11-22 06:38:04,751 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41329
2023-11-22 06:38:04,752 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:04,752 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:04,752 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:04,752 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:04,751 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41743
2023-11-22 06:38:04,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ytig2hgx
2023-11-22 06:38:04,752 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41743
2023-11-22 06:38:04,751 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34965
2023-11-22 06:38:04,752 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45503
2023-11-22 06:38:04,752 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34965
2023-11-22 06:38:04,752 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:04,752 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:04,752 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41981
2023-11-22 06:38:04,752 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:04,752 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:04,751 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43401
2023-11-22 06:38:04,752 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:04,752 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43401
2023-11-22 06:38:04,752 - distributed.worker - INFO - Starting Worker plugin RMMSetup-aacdbc1b-8e23-4c7d-814d-293f4aa42001
2023-11-22 06:38:04,752 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:04,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zzqutjpf
2023-11-22 06:38:04,752 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:04,752 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38747
2023-11-22 06:38:04,753 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:04,753 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:04,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ibf3csjq
2023-11-22 06:38:04,753 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:04,753 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:04,753 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6830cba1-58b3-4340-976e-860bee31c505
2023-11-22 06:38:04,753 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:04,753 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vuum2d1u
2023-11-22 06:38:04,753 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a09a1177-9545-459e-ab04-c9c48381f2e3
2023-11-22 06:38:04,753 - distributed.worker - INFO - Starting Worker plugin RMMSetup-efbc9f86-7086-45b3-9cc6-d68fdf446768
2023-11-22 06:38:04,753 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0ab6d6c-d567-4c2d-9602-d4d9e0be3d0e
2023-11-22 06:38:04,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34599
2023-11-22 06:38:04,754 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4f4c1502-0b92-4a69-a31c-d11a1a38b037
2023-11-22 06:38:04,754 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34599
2023-11-22 06:38:04,754 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38615
2023-11-22 06:38:04,754 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:04,754 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:04,754 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:04,754 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:04,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-83keegi7
2023-11-22 06:38:04,755 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-59936752-0b29-44e3-8d0c-208ca4c5d1f3
2023-11-22 06:38:04,755 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eccb35e3-e856-415b-9fab-06ab3016a0cc
2023-11-22 06:38:04,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39451
2023-11-22 06:38:04,809 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39451
2023-11-22 06:38:04,809 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40007
2023-11-22 06:38:04,809 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:04,809 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:04,809 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:04,809 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:04,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-92amussd
2023-11-22 06:38:04,810 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9bc32181-f9c3-47ae-8cdb-0eeb3ce91ac1
2023-11-22 06:38:04,818 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37695
2023-11-22 06:38:04,819 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37695
2023-11-22 06:38:04,819 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44645
2023-11-22 06:38:04,819 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:04,819 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:04,819 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:04,820 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:04,820 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tupg3b40
2023-11-22 06:38:04,820 - distributed.worker - INFO - Starting Worker plugin RMMSetup-986036c2-3cb0-4fee-a2cc-30d111202039
2023-11-22 06:38:05,196 - distributed.worker - INFO - Starting Worker plugin PreImport-b77b678b-63a7-4f07-885f-32911336c000
2023-11-22 06:38:05,196 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,196 - distributed.worker - INFO - Starting Worker plugin PreImport-ae69429f-de1e-4747-947d-afc96b931066
2023-11-22 06:38:05,196 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,196 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,361 - distributed.worker - INFO - Starting Worker plugin PreImport-363dcd0f-f57d-494b-ab67-1ca509715fae
2023-11-22 06:38:05,361 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e723b175-1622-4136-86ce-1516ed6da418
2023-11-22 06:38:05,361 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,364 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41743', status: init, memory: 0, processing: 0>
2023-11-22 06:38:05,365 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41743
2023-11-22 06:38:05,365 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56318
2023-11-22 06:38:05,366 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34965', status: init, memory: 0, processing: 0>
2023-11-22 06:38:05,366 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34965
2023-11-22 06:38:05,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56296
2023-11-22 06:38:05,367 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:05,367 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44401', status: init, memory: 0, processing: 0>
2023-11-22 06:38:05,368 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:05,368 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44401
2023-11-22 06:38:05,368 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56304
2023-11-22 06:38:05,370 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:05,372 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:05,373 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,374 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:05,374 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:05,376 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:05,376 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:05,376 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,378 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:05,389 - distributed.worker - INFO - Starting Worker plugin PreImport-c4387ab5-4daa-4761-afe0-83a0e3543848
2023-11-22 06:38:05,389 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,394 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4ef43416-9da3-4f0c-b784-7dc3df1ec05c
2023-11-22 06:38:05,395 - distributed.worker - INFO - Starting Worker plugin PreImport-3d1300c6-27e6-4cde-8a3a-7c683a9ca861
2023-11-22 06:38:05,395 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,396 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39451', status: init, memory: 0, processing: 0>
2023-11-22 06:38:05,397 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39451
2023-11-22 06:38:05,397 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56334
2023-11-22 06:38:05,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:05,401 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:05,401 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,403 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:05,541 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e9eb6dee-bbc6-4aed-b7d1-73876e447086
2023-11-22 06:38:05,542 - distributed.worker - INFO - Starting Worker plugin PreImport-6e62df87-41d3-41d6-8be8-4579dd712499
2023-11-22 06:38:05,542 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,542 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-00f17dcc-05ad-4919-993a-61a0b565564d
2023-11-22 06:38:05,542 - distributed.worker - INFO - Starting Worker plugin PreImport-9fe82e4d-3315-4d70-a92a-684cc97d21a7
2023-11-22 06:38:05,543 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,694 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34599', status: init, memory: 0, processing: 0>
2023-11-22 06:38:05,695 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34599
2023-11-22 06:38:05,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56342
2023-11-22 06:38:05,696 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43401', status: init, memory: 0, processing: 0>
2023-11-22 06:38:05,696 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43401
2023-11-22 06:38:05,696 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56346
2023-11-22 06:38:05,697 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:05,697 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37695', status: init, memory: 0, processing: 0>
2023-11-22 06:38:05,698 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37695
2023-11-22 06:38:05,698 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56362
2023-11-22 06:38:05,698 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:05,698 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32947', status: init, memory: 0, processing: 0>
2023-11-22 06:38:05,699 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:05,699 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32947
2023-11-22 06:38:05,699 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56374
2023-11-22 06:38:05,700 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:05,702 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:05,702 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,703 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:05,703 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,703 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:05,704 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:05,704 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,705 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:05,705 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:05,705 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:05,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:05,707 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:05,869 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:05,869 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:05,870 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:05,870 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:05,870 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:05,870 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:05,870 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:05,870 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:05,874 - distributed.scheduler - INFO - Remove client Client-aca38a98-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:05,875 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56276; closing.
2023-11-22 06:38:05,875 - distributed.scheduler - INFO - Remove client Client-aca38a98-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:05,875 - distributed.scheduler - INFO - Close client connection: Client-aca38a98-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:05,876 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36535'. Reason: nanny-close
2023-11-22 06:38:05,877 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:05,877 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39725'. Reason: nanny-close
2023-11-22 06:38:05,878 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:05,878 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39451. Reason: nanny-close
2023-11-22 06:38:05,878 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45591'. Reason: nanny-close
2023-11-22 06:38:05,878 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:05,878 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37695. Reason: nanny-close
2023-11-22 06:38:05,879 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44191'. Reason: nanny-close
2023-11-22 06:38:05,879 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:05,879 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34903'. Reason: nanny-close
2023-11-22 06:38:05,880 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:05,880 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:05,880 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56334; closing.
2023-11-22 06:38:05,880 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34599. Reason: nanny-close
2023-11-22 06:38:05,880 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39451', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635085.8804238')
2023-11-22 06:38:05,880 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38329'. Reason: nanny-close
2023-11-22 06:38:05,880 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:05,880 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:05,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46017'. Reason: nanny-close
2023-11-22 06:38:05,881 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:05,881 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:05,881 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56362; closing.
2023-11-22 06:38:05,881 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32947. Reason: nanny-close
2023-11-22 06:38:05,881 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38457'. Reason: nanny-close
2023-11-22 06:38:05,882 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:05,882 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:05,882 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37695', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635085.8825734')
2023-11-22 06:38:05,882 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:05,883 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43401. Reason: nanny-close
2023-11-22 06:38:05,883 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56342; closing.
2023-11-22 06:38:05,883 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34599', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635085.883432')
2023-11-22 06:38:05,884 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56374; closing.
2023-11-22 06:38:05,884 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:05,884 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:05,884 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:05,885 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32947', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635085.8849425')
2023-11-22 06:38:05,885 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56346; closing.
2023-11-22 06:38:05,885 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43401', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635085.8856664')
2023-11-22 06:38:05,886 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:05,886 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:06,009 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34965. Reason: nanny-close
2023-11-22 06:38:06,011 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44401. Reason: nanny-close
2023-11-22 06:38:06,012 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:06,012 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56296; closing.
2023-11-22 06:38:06,012 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34965', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635086.0125234')
2023-11-22 06:38:06,013 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:06,015 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56304; closing.
2023-11-22 06:38:06,015 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:06,015 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44401', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635086.015232')
2023-11-22 06:38:06,016 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41743. Reason: nanny-close
2023-11-22 06:38:06,017 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:06,020 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56318; closing.
2023-11-22 06:38:06,020 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:06,020 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41743', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635086.0202756')
2023-11-22 06:38:06,020 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:38:06,022 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:07,594 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:38:07,594 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:38:07,595 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:38:07,596 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:38:07,596 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2023-11-22 06:38:09,738 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:38:09,742 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34809 instead
  warnings.warn(
2023-11-22 06:38:09,746 - distributed.scheduler - INFO - State start
2023-11-22 06:38:09,768 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:38:09,769 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:38:09,769 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34809/status
2023-11-22 06:38:09,770 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:38:09,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33485'
2023-11-22 06:38:09,918 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42175'
2023-11-22 06:38:09,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35059'
2023-11-22 06:38:09,939 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41755'
2023-11-22 06:38:09,948 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36723'
2023-11-22 06:38:09,956 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46025'
2023-11-22 06:38:09,966 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33449'
2023-11-22 06:38:09,978 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46369'
2023-11-22 06:38:11,284 - distributed.scheduler - INFO - Receive client connection: Client-b29ae7bc-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:11,300 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40594
2023-11-22 06:38:11,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:11,806 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:11,807 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:11,807 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:11,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:11,809 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:11,809 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:11,809 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:11,811 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:11,811 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:11,813 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:11,813 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:11,866 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:11,866 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:11,870 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:11,880 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:11,880 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:11,881 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:11,881 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:11,882 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:11,882 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:11,884 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:11,885 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:11,886 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:14,760 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34289
2023-11-22 06:38:14,761 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34289
2023-11-22 06:38:14,761 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33591
2023-11-22 06:38:14,761 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,761 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,761 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:14,761 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:14,761 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m0bdlw0r
2023-11-22 06:38:14,762 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bd94a855-7d91-4594-86da-359d18f04d51
2023-11-22 06:38:14,771 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41895
2023-11-22 06:38:14,772 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41895
2023-11-22 06:38:14,772 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46421
2023-11-22 06:38:14,772 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,772 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,772 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:14,773 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:14,773 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d2n5ajxf
2023-11-22 06:38:14,773 - distributed.worker - INFO - Starting Worker plugin RMMSetup-272b6e1a-574e-47a5-925f-9f173ac845f2
2023-11-22 06:38:14,773 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41419
2023-11-22 06:38:14,775 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41419
2023-11-22 06:38:14,775 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41369
2023-11-22 06:38:14,775 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,775 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,775 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34787
2023-11-22 06:38:14,776 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:14,776 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34787
2023-11-22 06:38:14,776 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:14,776 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44781
2023-11-22 06:38:14,776 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-klcaub7q
2023-11-22 06:38:14,776 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,776 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,776 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:14,776 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:14,776 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1r80ed11
2023-11-22 06:38:14,777 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae09dc7c-eeb2-4aa3-af01-54e8eb9e539b
2023-11-22 06:38:14,777 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87ea7cf8-a237-4d3f-96dc-897862c3203b
2023-11-22 06:38:14,777 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36729
2023-11-22 06:38:14,778 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36729
2023-11-22 06:38:14,778 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44433
2023-11-22 06:38:14,778 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,778 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,778 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:14,778 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:14,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7hg_7ulw
2023-11-22 06:38:14,779 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-891b38d1-634c-4ab8-9ec1-ef841602f676
2023-11-22 06:38:14,779 - distributed.worker - INFO - Starting Worker plugin RMMSetup-66963732-88ed-402d-a1a0-208c4221bad0
2023-11-22 06:38:14,782 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43097
2023-11-22 06:38:14,783 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43097
2023-11-22 06:38:14,783 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45785
2023-11-22 06:38:14,783 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,783 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,783 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:14,783 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:14,783 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fi2goux5
2023-11-22 06:38:14,784 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f8cdabd9-3f3f-4191-b14b-cb6fe845edef
2023-11-22 06:38:14,783 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45305
2023-11-22 06:38:14,784 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45305
2023-11-22 06:38:14,784 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38317
2023-11-22 06:38:14,784 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,784 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,784 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:14,784 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:14,784 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-glg4p6yz
2023-11-22 06:38:14,785 - distributed.worker - INFO - Starting Worker plugin RMMSetup-73e0b9da-437e-4b64-8380-c8343c5a9c26
2023-11-22 06:38:14,792 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36003
2023-11-22 06:38:14,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36003
2023-11-22 06:38:14,793 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45369
2023-11-22 06:38:14,793 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,793 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,793 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:14,793 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:14,793 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6g2b6o0j
2023-11-22 06:38:14,794 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0f5a154e-6055-4f7a-8c0f-771fcdddee64
2023-11-22 06:38:14,816 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-547467a0-62b5-4e3a-9f0c-3bf25f94f7a5
2023-11-22 06:38:14,816 - distributed.worker - INFO - Starting Worker plugin PreImport-7be21dc5-e08c-45ed-a2ff-e7281b9c1db8
2023-11-22 06:38:14,817 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,817 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c93cac95-7747-43ce-b1e0-5e84bfc4c37b
2023-11-22 06:38:14,817 - distributed.worker - INFO - Starting Worker plugin PreImport-8a60438b-eebe-4ba7-abb5-3804cc8b7507
2023-11-22 06:38:14,817 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,818 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e638d0f7-4458-450c-90dd-076643d46691
2023-11-22 06:38:14,819 - distributed.worker - INFO - Starting Worker plugin PreImport-a5e7f0db-8a19-4679-b430-6511b6503b75
2023-11-22 06:38:14,819 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,826 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9fb736ba-5c69-4620-a1e9-9281bfbafd7f
2023-11-22 06:38:14,826 - distributed.worker - INFO - Starting Worker plugin PreImport-ee6e426f-b490-42b3-a1f8-17e03153ecdd
2023-11-22 06:38:14,826 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,828 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-86bfacea-5003-4aab-a5a9-976576842736
2023-11-22 06:38:14,829 - distributed.worker - INFO - Starting Worker plugin PreImport-36e7dd08-a773-4478-a223-76487be45d8b
2023-11-22 06:38:14,829 - distributed.worker - INFO - Starting Worker plugin PreImport-33ac65b7-abed-412e-8c87-c1c68bdef248
2023-11-22 06:38:14,829 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,829 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,832 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ca123e9-9b0b-49ad-bdd0-7562a3fde2fd
2023-11-22 06:38:14,833 - distributed.worker - INFO - Starting Worker plugin PreImport-55144db9-eb87-4f4e-907f-77ada6445f24
2023-11-22 06:38:14,833 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,835 - distributed.worker - INFO - Starting Worker plugin PreImport-cb4a7ba9-a849-47be-8d59-9e4dd37d8597
2023-11-22 06:38:14,836 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3749b0f4-903b-4fc2-aefe-bcd7e40faf1c
2023-11-22 06:38:14,837 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,842 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34289', status: init, memory: 0, processing: 0>
2023-11-22 06:38:14,843 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34289
2023-11-22 06:38:14,843 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40624
2023-11-22 06:38:14,847 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:14,848 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,848 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,849 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:14,851 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41895', status: init, memory: 0, processing: 0>
2023-11-22 06:38:14,852 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41895
2023-11-22 06:38:14,852 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40654
2023-11-22 06:38:14,853 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:14,853 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34787', status: init, memory: 0, processing: 0>
2023-11-22 06:38:14,854 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,854 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,854 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34787
2023-11-22 06:38:14,854 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40634
2023-11-22 06:38:14,855 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:14,856 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,856 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,857 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43097', status: init, memory: 0, processing: 0>
2023-11-22 06:38:14,858 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43097
2023-11-22 06:38:14,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40690
2023-11-22 06:38:14,858 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:14,859 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45305', status: init, memory: 0, processing: 0>
2023-11-22 06:38:14,859 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:14,859 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45305
2023-11-22 06:38:14,859 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40638
2023-11-22 06:38:14,859 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,860 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,860 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41419', status: init, memory: 0, processing: 0>
2023-11-22 06:38:14,860 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41419
2023-11-22 06:38:14,860 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40670
2023-11-22 06:38:14,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:14,861 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:14,863 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:14,864 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36729', status: init, memory: 0, processing: 0>
2023-11-22 06:38:14,864 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:14,865 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36729
2023-11-22 06:38:14,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40676
2023-11-22 06:38:14,866 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:14,867 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,867 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,867 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,867 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,868 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:14,868 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,868 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,871 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:14,873 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36003', status: init, memory: 0, processing: 0>
2023-11-22 06:38:14,873 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36003
2023-11-22 06:38:14,873 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40696
2023-11-22 06:38:14,875 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:14,875 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:14,881 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:14,882 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:14,883 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:14,947 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:14,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:14,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:14,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:14,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:14,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:14,948 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:14,949 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:14,953 - distributed.scheduler - INFO - Remove client Client-b29ae7bc-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:14,954 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40594; closing.
2023-11-22 06:38:14,954 - distributed.scheduler - INFO - Remove client Client-b29ae7bc-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:14,954 - distributed.scheduler - INFO - Close client connection: Client-b29ae7bc-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:14,955 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33485'. Reason: nanny-close
2023-11-22 06:38:14,956 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:14,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42175'. Reason: nanny-close
2023-11-22 06:38:14,957 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:14,957 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36003. Reason: nanny-close
2023-11-22 06:38:14,957 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35059'. Reason: nanny-close
2023-11-22 06:38:14,957 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:14,958 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41755'. Reason: nanny-close
2023-11-22 06:38:14,958 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45305. Reason: nanny-close
2023-11-22 06:38:14,958 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:14,958 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34289. Reason: nanny-close
2023-11-22 06:38:14,958 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36723'. Reason: nanny-close
2023-11-22 06:38:14,959 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:14,959 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41419. Reason: nanny-close
2023-11-22 06:38:14,959 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46025'. Reason: nanny-close
2023-11-22 06:38:14,959 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:14,960 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34787. Reason: nanny-close
2023-11-22 06:38:14,960 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33449'. Reason: nanny-close
2023-11-22 06:38:14,960 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:14,960 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:14,960 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40696; closing.
2023-11-22 06:38:14,960 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:14,960 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40624; closing.
2023-11-22 06:38:14,960 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36729. Reason: nanny-close
2023-11-22 06:38:14,960 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46369'. Reason: nanny-close
2023-11-22 06:38:14,960 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:14,961 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36003', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635094.9609404')
2023-11-22 06:38:14,961 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41895. Reason: nanny-close
2023-11-22 06:38:14,961 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:14,961 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:14,961 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34289', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635094.961515')
2023-11-22 06:38:14,961 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43097. Reason: nanny-close
2023-11-22 06:38:14,961 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:14,962 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:14,962 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:14,962 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:14,962 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:14,962 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:14,963 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40670; closing.
2023-11-22 06:38:14,963 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40638; closing.
2023-11-22 06:38:14,963 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:14,963 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:14,964 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:14,964 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:14,964 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41419', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635094.9643912')
2023-11-22 06:38:14,964 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40634; closing.
2023-11-22 06:38:14,964 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:14,965 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45305', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635094.9650354')
2023-11-22 06:38:14,965 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40676; closing.
2023-11-22 06:38:14,965 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:14,965 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40654; closing.
2023-11-22 06:38:14,966 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34787', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635094.96606')
2023-11-22 06:38:14,966 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635094.966395')
2023-11-22 06:38:14,966 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41895', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635094.9666665')
2023-11-22 06:38:14,967 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40690; closing.
2023-11-22 06:38:14,967 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43097', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635094.967414')
2023-11-22 06:38:14,967 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:38:16,624 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:38:16,624 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:38:16,625 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:38:16,626 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:38:16,626 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2023-11-22 06:38:18,778 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:38:18,783 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39075 instead
  warnings.warn(
2023-11-22 06:38:18,789 - distributed.scheduler - INFO - State start
2023-11-22 06:38:18,813 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:38:18,815 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:38:18,816 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:39075/status
2023-11-22 06:38:18,816 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:38:18,930 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33459'
2023-11-22 06:38:18,943 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35861'
2023-11-22 06:38:18,955 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32891'
2023-11-22 06:38:18,970 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40619'
2023-11-22 06:38:18,972 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46339'
2023-11-22 06:38:18,981 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42093'
2023-11-22 06:38:18,988 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40801'
2023-11-22 06:38:18,997 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38359'
2023-11-22 06:38:19,073 - distributed.scheduler - INFO - Receive client connection: Client-b803e082-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:19,086 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:40882
2023-11-22 06:38:20,806 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:20,806 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:20,807 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:20,807 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:20,810 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:20,811 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:20,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:20,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:20,845 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:20,845 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:20,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:20,849 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:20,868 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:20,868 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:20,869 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:20,869 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:20,872 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:20,873 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:20,873 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:20,873 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:20,877 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:20,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:20,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:20,915 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:24,305 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35483
2023-11-22 06:38:24,306 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35483
2023-11-22 06:38:24,306 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41127
2023-11-22 06:38:24,306 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:24,306 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:24,307 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:24,307 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:24,307 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4bfrhht1
2023-11-22 06:38:24,307 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6ab1e6bf-9cd1-4365-ae09-89db87b186b5
2023-11-22 06:38:24,325 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41811
2023-11-22 06:38:24,326 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41811
2023-11-22 06:38:24,326 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40363
2023-11-22 06:38:24,326 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:24,326 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:24,326 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:24,327 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:24,327 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-078mimpq
2023-11-22 06:38:24,327 - distributed.worker - INFO - Starting Worker plugin PreImport-f3f04a2d-02cf-48aa-b6d8-8fe74116442c
2023-11-22 06:38:24,328 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ace3df96-45ae-4e62-907e-406cf8335e1c
2023-11-22 06:38:24,399 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33367
2023-11-22 06:38:24,400 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33367
2023-11-22 06:38:24,400 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42177
2023-11-22 06:38:24,400 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:24,400 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:24,401 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:24,401 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:24,401 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5xqt12_x
2023-11-22 06:38:24,401 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ab88dd32-e43c-4f30-90b7-2f79ab7eb53f
2023-11-22 06:38:24,402 - distributed.worker - INFO - Starting Worker plugin RMMSetup-471cf935-2fb8-4be3-b1ab-9f9f52829b83
2023-11-22 06:38:24,408 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35089
2023-11-22 06:38:24,409 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35089
2023-11-22 06:38:24,409 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45173
2023-11-22 06:38:24,410 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:24,410 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:24,410 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:24,410 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:24,410 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yihebymu
2023-11-22 06:38:24,411 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f9abe2e8-2c1c-43d5-be84-8f63723e6ea2
2023-11-22 06:38:24,414 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38323
2023-11-22 06:38:24,416 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38323
2023-11-22 06:38:24,414 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44439
2023-11-22 06:38:24,416 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39125
2023-11-22 06:38:24,416 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44439
2023-11-22 06:38:24,416 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:24,416 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:24,416 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38773
2023-11-22 06:38:24,416 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:24,416 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:24,416 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:24,416 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:24,416 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cmu2x_g6
2023-11-22 06:38:24,416 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:24,416 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:24,416 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-riu_oj8b
2023-11-22 06:38:24,416 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44793
2023-11-22 06:38:24,417 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44793
2023-11-22 06:38:24,417 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32807
2023-11-22 06:38:24,417 - distributed.worker - INFO - Starting Worker plugin PreImport-7f829e45-8fb1-4554-89e5-e8a0bf444347
2023-11-22 06:38:24,417 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:24,417 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:24,417 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9da8139d-2e7f-4bcf-9c26-1d7d6efa2699
2023-11-22 06:38:24,417 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d0ab2546-885a-47f8-a975-a5a1d71868b6
2023-11-22 06:38:24,417 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:24,418 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:24,418 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-63b0fzhk
2023-11-22 06:38:24,418 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f5fae84-fab9-4c33-99e8-8bfc2947b1e5
2023-11-22 06:38:24,418 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3a051b02-c557-4ea2-aad7-807ca7067ef8
2023-11-22 06:38:24,419 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cbb50dd3-3ce6-441d-ba12-25253a80a4ea
2023-11-22 06:38:24,418 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33315
2023-11-22 06:38:24,419 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33315
2023-11-22 06:38:24,419 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45663
2023-11-22 06:38:24,419 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:24,419 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:24,419 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:24,419 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:24,419 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-naugm355
2023-11-22 06:38:24,420 - distributed.worker - INFO - Starting Worker plugin PreImport-04880fbc-336b-45af-b0df-f2a83fde4093
2023-11-22 06:38:24,420 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d361dde8-540b-4da2-b25f-31546f992fae
2023-11-22 06:38:24,420 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6eb167b1-1595-4804-ad51-5e994e402361
2023-11-22 06:38:25,600 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0322b9f0-ec33-4c64-8c54-ecb995e676c2
2023-11-22 06:38:25,603 - distributed.worker - INFO - Starting Worker plugin PreImport-16a1eb42-f56f-4d89-b3ac-4158719e3fd0
2023-11-22 06:38:25,604 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,615 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c0bbfa4b-a569-4efb-8c53-a8a5d541c8e4
2023-11-22 06:38:25,620 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,646 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35483', status: init, memory: 0, processing: 0>
2023-11-22 06:38:25,647 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35483
2023-11-22 06:38:25,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51076
2023-11-22 06:38:25,649 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:25,655 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:25,656 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,658 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:25,665 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41811', status: init, memory: 0, processing: 0>
2023-11-22 06:38:25,665 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41811
2023-11-22 06:38:25,666 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51078
2023-11-22 06:38:25,667 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:25,674 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:25,674 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,676 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:25,681 - distributed.worker - INFO - Starting Worker plugin PreImport-719d79c3-e125-4271-bce2-c8a9ffb1717e
2023-11-22 06:38:25,681 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,686 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-52adef9a-eb87-4c39-be68-b4f73f87cdab
2023-11-22 06:38:25,686 - distributed.worker - INFO - Starting Worker plugin PreImport-22164350-43e5-4030-8c09-d477288d9476
2023-11-22 06:38:25,686 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-08eff0fb-c656-4c1f-aef9-edbe11178209
2023-11-22 06:38:25,687 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,687 - distributed.worker - INFO - Starting Worker plugin PreImport-2364f832-ec59-42bb-b105-3f84384ee14d
2023-11-22 06:38:25,687 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,694 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,701 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,705 - distributed.worker - INFO - Starting Worker plugin PreImport-e7aa5dea-8bb7-4f6a-8f28-736a1f4ba554
2023-11-22 06:38:25,705 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,714 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44793', status: init, memory: 0, processing: 0>
2023-11-22 06:38:25,715 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44793
2023-11-22 06:38:25,715 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51080
2023-11-22 06:38:25,716 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:25,720 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:25,720 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,721 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:25,723 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35089', status: init, memory: 0, processing: 0>
2023-11-22 06:38:25,724 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35089
2023-11-22 06:38:25,724 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51110
2023-11-22 06:38:25,725 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:25,730 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:25,730 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,732 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:25,734 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44439', status: init, memory: 0, processing: 0>
2023-11-22 06:38:25,734 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44439
2023-11-22 06:38:25,734 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51094
2023-11-22 06:38:25,735 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:25,738 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33315', status: init, memory: 0, processing: 0>
2023-11-22 06:38:25,738 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33315
2023-11-22 06:38:25,738 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51130
2023-11-22 06:38:25,739 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:25,740 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38323', status: init, memory: 0, processing: 0>
2023-11-22 06:38:25,740 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38323
2023-11-22 06:38:25,740 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51116
2023-11-22 06:38:25,741 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:25,741 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,741 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:25,742 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:25,743 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:25,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:25,749 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33367', status: init, memory: 0, processing: 0>
2023-11-22 06:38:25,749 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33367
2023-11-22 06:38:25,749 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51136
2023-11-22 06:38:25,750 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:25,750 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,751 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:25,752 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:25,758 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:25,758 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:25,760 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:25,760 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:25,761 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:25,761 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:25,761 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:25,761 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:25,761 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:25,761 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:25,762 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:25,771 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:25,771 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:25,771 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:25,772 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:25,772 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:25,772 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:25,772 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:25,772 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:25,778 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:25,780 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:25,783 - distributed.scheduler - INFO - Remove client Client-b803e082-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:25,783 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:40882; closing.
2023-11-22 06:38:25,783 - distributed.scheduler - INFO - Remove client Client-b803e082-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:25,784 - distributed.scheduler - INFO - Close client connection: Client-b803e082-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:25,784 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33459'. Reason: nanny-close
2023-11-22 06:38:25,785 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:25,786 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35861'. Reason: nanny-close
2023-11-22 06:38:25,786 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:25,786 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41811. Reason: nanny-close
2023-11-22 06:38:25,786 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32891'. Reason: nanny-close
2023-11-22 06:38:25,786 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:25,787 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35483. Reason: nanny-close
2023-11-22 06:38:25,787 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40619'. Reason: nanny-close
2023-11-22 06:38:25,787 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:25,787 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38323. Reason: nanny-close
2023-11-22 06:38:25,788 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46339'. Reason: nanny-close
2023-11-22 06:38:25,788 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:25,788 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33315. Reason: nanny-close
2023-11-22 06:38:25,788 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:25,788 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42093'. Reason: nanny-close
2023-11-22 06:38:25,789 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51078; closing.
2023-11-22 06:38:25,789 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:25,789 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33367. Reason: nanny-close
2023-11-22 06:38:25,789 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:25,789 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41811', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635105.7896721')
2023-11-22 06:38:25,789 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:25,789 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40801'. Reason: nanny-close
2023-11-22 06:38:25,790 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:25,790 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:25,790 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35089. Reason: nanny-close
2023-11-22 06:38:25,790 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51116; closing.
2023-11-22 06:38:25,790 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:25,790 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38359'. Reason: nanny-close
2023-11-22 06:38:25,791 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44793. Reason: nanny-close
2023-11-22 06:38:25,791 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:25,791 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38323', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635105.791428')
2023-11-22 06:38:25,791 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:25,791 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:25,791 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:25,792 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:25,792 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51076; closing.
2023-11-22 06:38:25,792 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:25,792 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44439. Reason: nanny-close
2023-11-22 06:38:25,792 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:25,793 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51130; closing.
2023-11-22 06:38:25,793 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:25,793 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35483', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635105.793665')
2023-11-22 06:38:25,794 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:25,794 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:25,795 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:25,794 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51116>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-22 06:38:25,796 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33315', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635105.7961729')
2023-11-22 06:38:25,796 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51136; closing.
2023-11-22 06:38:25,797 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:25,797 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33367', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635105.7976806')
2023-11-22 06:38:25,798 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51110; closing.
2023-11-22 06:38:25,798 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51080; closing.
2023-11-22 06:38:25,798 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35089', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635105.7988398')
2023-11-22 06:38:25,799 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44793', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635105.799169')
2023-11-22 06:38:25,799 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51094; closing.
2023-11-22 06:38:25,800 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44439', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635105.7999346')
2023-11-22 06:38:25,800 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:38:25,800 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51094>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-22 06:38:27,653 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:38:27,653 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:38:27,654 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:38:27,655 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:38:27,655 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2023-11-22 06:38:29,828 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:38:29,832 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40677 instead
  warnings.warn(
2023-11-22 06:38:29,836 - distributed.scheduler - INFO - State start
2023-11-22 06:38:29,879 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:38:29,880 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:38:29,881 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40677/status
2023-11-22 06:38:29,881 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:38:30,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38553'
2023-11-22 06:38:30,049 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41473'
2023-11-22 06:38:30,061 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40567'
2023-11-22 06:38:30,074 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46607'
2023-11-22 06:38:30,076 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36959'
2023-11-22 06:38:30,085 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46657'
2023-11-22 06:38:30,093 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46323'
2023-11-22 06:38:30,102 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40517'
2023-11-22 06:38:30,733 - distributed.scheduler - INFO - Receive client connection: Client-be9f79eb-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:30,751 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47768
2023-11-22 06:38:31,824 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:31,824 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:31,828 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:31,907 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:31,908 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:31,911 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:31,911 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:31,912 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:31,915 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:31,977 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:31,977 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:31,982 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:31,988 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:31,988 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:31,996 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:32,011 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:32,012 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:32,019 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:32,032 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:32,033 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:32,040 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:32,042 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:32,042 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:32,047 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:33,465 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46065
2023-11-22 06:38:33,466 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46065
2023-11-22 06:38:33,466 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37703
2023-11-22 06:38:33,466 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:33,466 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:33,466 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:33,466 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:33,466 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-g30wftff
2023-11-22 06:38:33,466 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c3149173-b96b-49d8-a63c-a987d6a19010
2023-11-22 06:38:34,130 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-648eb8d9-2cb6-4dde-af85-301571bdc767
2023-11-22 06:38:34,131 - distributed.worker - INFO - Starting Worker plugin PreImport-507663e6-d29d-4201-a106-2a89688b7144
2023-11-22 06:38:34,132 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:34,169 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46065', status: init, memory: 0, processing: 0>
2023-11-22 06:38:34,172 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46065
2023-11-22 06:38:34,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47796
2023-11-22 06:38:34,173 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:34,178 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:34,178 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:34,179 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:34,500 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36139
2023-11-22 06:38:34,501 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36139
2023-11-22 06:38:34,502 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37439
2023-11-22 06:38:34,502 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:34,502 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:34,502 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:34,502 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:34,502 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-evp5m9w2
2023-11-22 06:38:34,503 - distributed.worker - INFO - Starting Worker plugin PreImport-d954869c-ed3a-4540-8bcd-0ba32a01c305
2023-11-22 06:38:34,503 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-937e37dd-0aa9-46f9-ba65-f94a1cbbab44
2023-11-22 06:38:34,504 - distributed.worker - INFO - Starting Worker plugin RMMSetup-505a705a-e85d-4818-976f-0614b05a01cd
2023-11-22 06:38:34,518 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38377
2023-11-22 06:38:34,520 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38377
2023-11-22 06:38:34,520 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42423
2023-11-22 06:38:34,520 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:34,520 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:34,520 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:34,520 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:34,520 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2w3hvv45
2023-11-22 06:38:34,521 - distributed.worker - INFO - Starting Worker plugin PreImport-1159e21b-546f-4bb5-9673-b9db0edf98be
2023-11-22 06:38:34,521 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9db6ddcf-b8bc-478f-8154-58a59d897ecf
2023-11-22 06:38:34,753 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46407
2023-11-22 06:38:34,754 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46407
2023-11-22 06:38:34,754 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46153
2023-11-22 06:38:34,754 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:34,754 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:34,754 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:34,754 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:34,754 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f8mn1mrw
2023-11-22 06:38:34,755 - distributed.worker - INFO - Starting Worker plugin PreImport-a91eadc8-194b-42e8-a28d-84f06f468e96
2023-11-22 06:38:34,755 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c333d505-7f25-43a5-b60a-1acbb43c7c2f
2023-11-22 06:38:34,755 - distributed.worker - INFO - Starting Worker plugin RMMSetup-23f0b963-75f6-405f-a964-f1b6870857a1
2023-11-22 06:38:35,341 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34271
2023-11-22 06:38:35,342 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34271
2023-11-22 06:38:35,342 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34835
2023-11-22 06:38:35,342 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:35,342 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:35,342 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:35,342 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:35,342 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wyqh1zgz
2023-11-22 06:38:35,343 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f03b79ab-d9dc-41bb-bafb-99199421328e
2023-11-22 06:38:35,410 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37777
2023-11-22 06:38:35,412 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37777
2023-11-22 06:38:35,412 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38013
2023-11-22 06:38:35,412 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:35,412 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:35,412 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:35,412 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:35,412 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-76zx1jqg
2023-11-22 06:38:35,413 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-45ebdf99-9d9c-4ef7-b223-40e71216f0e8
2023-11-22 06:38:35,415 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e60dc4f1-8be3-443b-ab16-6929fcf181d1
2023-11-22 06:38:35,567 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44871
2023-11-22 06:38:35,570 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44871
2023-11-22 06:38:35,570 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34597
2023-11-22 06:38:35,570 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:35,570 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:35,570 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:35,570 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:35,570 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-xt8mp74q
2023-11-22 06:38:35,571 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-44580b03-17cd-45c5-9875-1cdec18bb816
2023-11-22 06:38:35,572 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9f761643-9f47-41b5-968f-432947b3b02b
2023-11-22 06:38:35,571 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35697
2023-11-22 06:38:35,573 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35697
2023-11-22 06:38:35,573 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40297
2023-11-22 06:38:35,573 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:35,573 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:35,573 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:35,574 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:35,574 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wkfqp5m5
2023-11-22 06:38:35,575 - distributed.worker - INFO - Starting Worker plugin RMMSetup-448ce7e6-e594-4f94-adec-7887d200d733
2023-11-22 06:38:35,895 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,060 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36139', status: init, memory: 0, processing: 0>
2023-11-22 06:38:36,060 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36139
2023-11-22 06:38:36,061 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47802
2023-11-22 06:38:36,061 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:36,067 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:36,067 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:36,121 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0e48ec1f-79c6-4b62-a412-7fd7b31c13b6
2023-11-22 06:38:36,122 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,159 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38377', status: init, memory: 0, processing: 0>
2023-11-22 06:38:36,160 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38377
2023-11-22 06:38:36,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47814
2023-11-22 06:38:36,161 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:36,165 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:36,165 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,167 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:36,259 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,289 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46407', status: init, memory: 0, processing: 0>
2023-11-22 06:38:36,290 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46407
2023-11-22 06:38:36,290 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47826
2023-11-22 06:38:36,291 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:36,294 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:36,294 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,295 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:36,314 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-580db983-2276-4c95-8f49-bc9e73461601
2023-11-22 06:38:36,314 - distributed.worker - INFO - Starting Worker plugin PreImport-4d4e8a09-7569-4f2c-841f-02ce81e4e1b7
2023-11-22 06:38:36,315 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,349 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34271', status: init, memory: 0, processing: 0>
2023-11-22 06:38:36,350 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34271
2023-11-22 06:38:36,350 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47836
2023-11-22 06:38:36,351 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:36,354 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:36,354 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,355 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:36,366 - distributed.worker - INFO - Starting Worker plugin PreImport-b83457fc-02ba-4d43-b78e-a6720d5243c2
2023-11-22 06:38:36,366 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,396 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37777', status: init, memory: 0, processing: 0>
2023-11-22 06:38:36,397 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37777
2023-11-22 06:38:36,397 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47852
2023-11-22 06:38:36,398 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:36,402 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:36,402 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,403 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:36,428 - distributed.worker - INFO - Starting Worker plugin PreImport-2d625833-e1cd-4d4a-be76-03437d5a9896
2023-11-22 06:38:36,428 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,446 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f17f156-886b-46be-8194-2a001f9a5c0a
2023-11-22 06:38:36,446 - distributed.worker - INFO - Starting Worker plugin PreImport-1d96f442-6cb6-4ab1-a68c-ddf42f917204
2023-11-22 06:38:36,447 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,463 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44871', status: init, memory: 0, processing: 0>
2023-11-22 06:38:36,463 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44871
2023-11-22 06:38:36,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47868
2023-11-22 06:38:36,465 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:36,471 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:36,471 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,473 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:36,485 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35697', status: init, memory: 0, processing: 0>
2023-11-22 06:38:36,485 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35697
2023-11-22 06:38:36,485 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47880
2023-11-22 06:38:36,487 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:36,493 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:36,493 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:36,495 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:36,508 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:36,509 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:36,509 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:36,509 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:36,509 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:36,509 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:36,509 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:36,509 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:36,521 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:36,521 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:36,521 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:36,521 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:36,521 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:36,522 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:36,522 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:36,522 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:38:36,528 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:36,530 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:36,532 - distributed.scheduler - INFO - Remove client Client-be9f79eb-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:36,532 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47768; closing.
2023-11-22 06:38:36,532 - distributed.scheduler - INFO - Remove client Client-be9f79eb-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:36,533 - distributed.scheduler - INFO - Close client connection: Client-be9f79eb-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:36,534 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38553'. Reason: nanny-close
2023-11-22 06:38:36,534 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:36,535 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41473'. Reason: nanny-close
2023-11-22 06:38:36,535 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:36,536 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38377. Reason: nanny-close
2023-11-22 06:38:36,536 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40567'. Reason: nanny-close
2023-11-22 06:38:36,536 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:36,536 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46065. Reason: nanny-close
2023-11-22 06:38:36,536 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46607'. Reason: nanny-close
2023-11-22 06:38:36,537 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:36,537 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36139. Reason: nanny-close
2023-11-22 06:38:36,537 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36959'. Reason: nanny-close
2023-11-22 06:38:36,537 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:36,537 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46407. Reason: nanny-close
2023-11-22 06:38:36,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:36,538 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46657'. Reason: nanny-close
2023-11-22 06:38:36,538 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44871. Reason: nanny-close
2023-11-22 06:38:36,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:36,538 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47814; closing.
2023-11-22 06:38:36,538 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46323'. Reason: nanny-close
2023-11-22 06:38:36,538 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:36,539 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:36,539 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38377', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635116.5391223')
2023-11-22 06:38:36,539 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40517'. Reason: nanny-close
2023-11-22 06:38:36,539 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:36,539 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:36,539 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:36,539 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37777. Reason: nanny-close
2023-11-22 06:38:36,539 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47796; closing.
2023-11-22 06:38:36,540 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:36,540 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:36,540 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34271. Reason: nanny-close
2023-11-22 06:38:36,540 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:36,541 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46065', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635116.5409427')
2023-11-22 06:38:36,541 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:36,541 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47826; closing.
2023-11-22 06:38:36,541 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47802; closing.
2023-11-22 06:38:36,541 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:36,542 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:36,542 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46407', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635116.542609')
2023-11-22 06:38:36,542 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:36,542 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:36,543 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36139', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635116.543097')
2023-11-22 06:38:36,543 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:36,543 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35697. Reason: nanny-close
2023-11-22 06:38:36,544 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:36,545 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:36,544 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47796>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-22 06:38:36,545 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47826>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 316, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 327, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47826>: Stream is closed
2023-11-22 06:38:36,546 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47868; closing.
2023-11-22 06:38:36,546 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44871', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635116.5469365')
2023-11-22 06:38:36,547 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:36,547 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47852; closing.
2023-11-22 06:38:36,547 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47836; closing.
2023-11-22 06:38:36,548 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37777', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635116.5481544')
2023-11-22 06:38:36,548 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34271', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635116.5485284')
2023-11-22 06:38:36,549 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47880; closing.
2023-11-22 06:38:36,549 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35697', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635116.549611')
2023-11-22 06:38:36,549 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:38:38,402 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:38:38,403 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:38:38,404 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:38:38,405 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:38:38,405 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2023-11-22 06:38:40,664 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:38:40,668 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41727 instead
  warnings.warn(
2023-11-22 06:38:40,672 - distributed.scheduler - INFO - State start
2023-11-22 06:38:40,694 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:38:40,695 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:38:40,696 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41727/status
2023-11-22 06:38:40,696 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:38:40,979 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38891'
2023-11-22 06:38:40,997 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37639'
2023-11-22 06:38:41,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36661'
2023-11-22 06:38:41,025 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36535'
2023-11-22 06:38:41,027 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42351'
2023-11-22 06:38:41,036 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45503'
2023-11-22 06:38:41,045 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46473'
2023-11-22 06:38:41,058 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38325'
2023-11-22 06:38:41,961 - distributed.scheduler - INFO - Receive client connection: Client-c512779a-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:41,980 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50166
2023-11-22 06:38:42,937 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:42,938 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:42,942 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:42,942 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:42,942 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:42,947 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:42,966 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:42,966 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:42,967 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:42,967 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:42,969 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:42,969 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:42,971 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:42,971 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:42,973 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:42,973 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:42,974 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:42,976 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:42,976 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:42,978 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:42,980 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:42,985 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:42,985 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:42,990 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:46,147 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45403
2023-11-22 06:38:46,148 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45403
2023-11-22 06:38:46,148 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42693
2023-11-22 06:38:46,148 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,148 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,148 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:46,148 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:46,149 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lq60zx_9
2023-11-22 06:38:46,149 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f584fbb6-09ab-42f8-ad63-a0b21b713861
2023-11-22 06:38:46,168 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34171
2023-11-22 06:38:46,169 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34171
2023-11-22 06:38:46,169 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39659
2023-11-22 06:38:46,169 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,169 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,169 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:46,169 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:46,169 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-llzm45ee
2023-11-22 06:38:46,170 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c8572125-894d-45f3-a0bc-1c49b3b02588
2023-11-22 06:38:46,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39771
2023-11-22 06:38:46,249 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39771
2023-11-22 06:38:46,249 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37159
2023-11-22 06:38:46,249 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,249 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,249 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:46,249 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:46,249 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-b56x49f5
2023-11-22 06:38:46,250 - distributed.worker - INFO - Starting Worker plugin PreImport-644a99ac-1555-4377-88c2-b6a4186784b5
2023-11-22 06:38:46,250 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3b6fbaad-7f4c-4802-bda5-90465d8ada8e
2023-11-22 06:38:46,250 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b9a2baee-8fe5-461c-b7b6-b4f488ef226e
2023-11-22 06:38:46,254 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33093
2023-11-22 06:38:46,255 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33093
2023-11-22 06:38:46,255 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38015
2023-11-22 06:38:46,255 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,255 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,255 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:46,256 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:46,256 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3lg37wv9
2023-11-22 06:38:46,256 - distributed.worker - INFO - Starting Worker plugin PreImport-f53c26f6-35ba-4e49-8aa3-20525985a06e
2023-11-22 06:38:46,256 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-179f791a-ea8a-4bba-9dc3-80c3434ba1e4
2023-11-22 06:38:46,256 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d3507d35-b156-4a63-afa2-043770ffb3f0
2023-11-22 06:38:46,259 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33153
2023-11-22 06:38:46,259 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33153
2023-11-22 06:38:46,259 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35177
2023-11-22 06:38:46,259 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,260 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,260 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:46,260 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:46,260 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c8ui5vdl
2023-11-22 06:38:46,260 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c87939b1-af38-42d8-b9f3-a51079c9ec66
2023-11-22 06:38:46,260 - distributed.worker - INFO - Starting Worker plugin RMMSetup-496506c9-7358-474e-ae26-3bdce47a3bb3
2023-11-22 06:38:46,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38967
2023-11-22 06:38:46,267 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38967
2023-11-22 06:38:46,267 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41131
2023-11-22 06:38:46,267 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,267 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,267 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:46,267 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:46,267 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tuabsnok
2023-11-22 06:38:46,268 - distributed.worker - INFO - Starting Worker plugin RMMSetup-202892bb-b8bd-43a2-8b2f-7e74235fd67a
2023-11-22 06:38:46,266 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34327
2023-11-22 06:38:46,268 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34327
2023-11-22 06:38:46,269 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34923
2023-11-22 06:38:46,269 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,269 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,269 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:46,269 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:46,269 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x5gvd47j
2023-11-22 06:38:46,270 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95866a30-9abd-4320-83a7-0e6129fc2340
2023-11-22 06:38:46,271 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d69478fc-8a5e-4ca4-8aad-93575f0a651f
2023-11-22 06:38:46,324 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36785
2023-11-22 06:38:46,326 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36785
2023-11-22 06:38:46,326 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38951
2023-11-22 06:38:46,326 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,326 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,326 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:46,326 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:38:46,326 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h8ngwuii
2023-11-22 06:38:46,327 - distributed.worker - INFO - Starting Worker plugin PreImport-c4bfc51f-7fc0-4024-94dd-26bb3b398ebe
2023-11-22 06:38:46,327 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fdb5e90-379a-4841-8315-50a48c74c7bd
2023-11-22 06:38:46,387 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a8f77269-1c80-4d6e-b791-ba91de37f7fe
2023-11-22 06:38:46,388 - distributed.worker - INFO - Starting Worker plugin PreImport-f4c42fdc-441e-4507-a67a-2afb1e0f0d53
2023-11-22 06:38:46,388 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,412 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-80b11c39-09fe-49aa-9855-689b3d5937af
2023-11-22 06:38:46,412 - distributed.worker - INFO - Starting Worker plugin PreImport-9cb2d96a-d7c4-4403-b319-c38b7fbab6c7
2023-11-22 06:38:46,413 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,424 - distributed.worker - INFO - Starting Worker plugin PreImport-69979b6e-143d-45cf-bea7-020a8a008984
2023-11-22 06:38:46,424 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,424 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,432 - distributed.worker - INFO - Starting Worker plugin PreImport-67de94e2-793b-416f-8565-6d110dac67ce
2023-11-22 06:38:46,432 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,436 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e4ca3a1-2743-4d43-94cd-345a176a347f
2023-11-22 06:38:46,436 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,437 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,438 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45403', status: init, memory: 0, processing: 0>
2023-11-22 06:38:46,439 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45403
2023-11-22 06:38:46,439 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50180
2023-11-22 06:38:46,441 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:46,442 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,442 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,443 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-42b04d52-d609-4f4e-a79c-7da5a66c97f1
2023-11-22 06:38:46,443 - distributed.worker - INFO - Starting Worker plugin PreImport-ca08b936-2cfb-4993-b623-428253174366
2023-11-22 06:38:46,444 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,444 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34171', status: init, memory: 0, processing: 0>
2023-11-22 06:38:46,445 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34171
2023-11-22 06:38:46,445 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50190
2023-11-22 06:38:46,446 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:46,447 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,447 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:46,452 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:46,452 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33153', status: init, memory: 0, processing: 0>
2023-11-22 06:38:46,453 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33153
2023-11-22 06:38:46,453 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50204
2023-11-22 06:38:46,454 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39771', status: init, memory: 0, processing: 0>
2023-11-22 06:38:46,454 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:46,454 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39771
2023-11-22 06:38:46,454 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50198
2023-11-22 06:38:46,455 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,455 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,455 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:46,456 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,456 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,458 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:46,460 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:46,464 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33093', status: init, memory: 0, processing: 0>
2023-11-22 06:38:46,465 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33093
2023-11-22 06:38:46,465 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50210
2023-11-22 06:38:46,465 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34327', status: init, memory: 0, processing: 0>
2023-11-22 06:38:46,466 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:46,466 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34327
2023-11-22 06:38:46,466 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50206
2023-11-22 06:38:46,467 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,467 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,467 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36785', status: init, memory: 0, processing: 0>
2023-11-22 06:38:46,467 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36785
2023-11-22 06:38:46,467 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50212
2023-11-22 06:38:46,467 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:46,468 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,469 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,469 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:46,470 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,470 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,470 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:46,474 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:46,475 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:46,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38967', status: init, memory: 0, processing: 0>
2023-11-22 06:38:46,477 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38967
2023-11-22 06:38:46,477 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50214
2023-11-22 06:38:46,478 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:46,479 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:46,479 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:46,486 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:46,567 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:46,568 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:46,568 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:46,568 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:46,568 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:46,568 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:46,569 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:46,569 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:38:46,573 - distributed.scheduler - INFO - Remove client Client-c512779a-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:46,573 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50166; closing.
2023-11-22 06:38:46,573 - distributed.scheduler - INFO - Remove client Client-c512779a-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:46,573 - distributed.scheduler - INFO - Close client connection: Client-c512779a-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:46,574 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38891'. Reason: nanny-close
2023-11-22 06:38:46,575 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:46,576 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37639'. Reason: nanny-close
2023-11-22 06:38:46,576 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:46,576 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36785. Reason: nanny-close
2023-11-22 06:38:46,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36661'. Reason: nanny-close
2023-11-22 06:38:46,577 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:46,577 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38967. Reason: nanny-close
2023-11-22 06:38:46,577 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36535'. Reason: nanny-close
2023-11-22 06:38:46,578 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:46,578 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39771. Reason: nanny-close
2023-11-22 06:38:46,578 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42351'. Reason: nanny-close
2023-11-22 06:38:46,579 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:46,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:46,579 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33093. Reason: nanny-close
2023-11-22 06:38:46,579 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50212; closing.
2023-11-22 06:38:46,579 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36785', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635126.5797215')
2023-11-22 06:38:46,579 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:46,579 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45503'. Reason: nanny-close
2023-11-22 06:38:46,580 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:46,580 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34327. Reason: nanny-close
2023-11-22 06:38:46,580 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50214; closing.
2023-11-22 06:38:46,580 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46473'. Reason: nanny-close
2023-11-22 06:38:46,580 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:46,581 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:46,581 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:46,581 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:46,581 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45403. Reason: nanny-close
2023-11-22 06:38:46,581 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38325'. Reason: nanny-close
2023-11-22 06:38:46,581 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:46,581 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:46,581 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38967', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635126.5817099')
2023-11-22 06:38:46,582 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50198; closing.
2023-11-22 06:38:46,582 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33153. Reason: nanny-close
2023-11-22 06:38:46,582 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34171. Reason: nanny-close
2023-11-22 06:38:46,582 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:46,582 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39771', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635126.5828452')
2023-11-22 06:38:46,583 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:46,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:46,583 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50210; closing.
2023-11-22 06:38:46,583 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33093', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635126.5836558')
2023-11-22 06:38:46,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:46,583 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:46,584 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:46,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50206; closing.
2023-11-22 06:38:46,584 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50180; closing.
2023-11-22 06:38:46,584 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:46,585 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:46,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635126.5853603')
2023-11-22 06:38:46,585 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45403', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635126.5856555')
2023-11-22 06:38:46,585 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:46,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50204; closing.
2023-11-22 06:38:46,586 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50190; closing.
2023-11-22 06:38:46,586 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:46,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33153', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635126.5865242')
2023-11-22 06:38:46,586 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34171', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635126.5868905')
2023-11-22 06:38:46,587 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:38:48,042 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:38:48,042 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:38:48,043 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:38:48,044 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:38:48,044 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2023-11-22 06:38:50,303 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:38:50,307 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34209 instead
  warnings.warn(
2023-11-22 06:38:50,311 - distributed.scheduler - INFO - State start
2023-11-22 06:38:50,488 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:38:50,489 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:38:50,490 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34209/status
2023-11-22 06:38:50,490 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:38:50,693 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42163'
2023-11-22 06:38:51,731 - distributed.scheduler - INFO - Receive client connection: Client-cabeb3ed-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:51,744 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46016
2023-11-22 06:38:53,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:38:53,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:38:53,920 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:38:55,049 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39115
2023-11-22 06:38:55,051 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39115
2023-11-22 06:38:55,051 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2023-11-22 06:38:55,051 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:38:55,051 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:55,051 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:38:55,051 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-22 06:38:55,051 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t46zt5ws
2023-11-22 06:38:55,052 - distributed.worker - INFO - Starting Worker plugin PreImport-5ad4b1b7-b1d7-4aec-92bc-648a7d8633c1
2023-11-22 06:38:55,053 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2516c5d8-0c37-4ce0-a20a-1c28d48d1b49
2023-11-22 06:38:55,053 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f438b50b-539c-4521-b079-c89b742d64d9
2023-11-22 06:38:55,054 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:55,086 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39115', status: init, memory: 0, processing: 0>
2023-11-22 06:38:55,087 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39115
2023-11-22 06:38:55,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:46036
2023-11-22 06:38:55,089 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:38:55,090 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:38:55,090 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:38:55,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:38:55,115 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:38:55,117 - distributed.scheduler - INFO - Remove client Client-cabeb3ed-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:55,118 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46016; closing.
2023-11-22 06:38:55,118 - distributed.scheduler - INFO - Remove client Client-cabeb3ed-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:55,119 - distributed.scheduler - INFO - Close client connection: Client-cabeb3ed-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:38:55,120 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42163'. Reason: nanny-close
2023-11-22 06:38:55,135 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:38:55,137 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39115. Reason: nanny-close
2023-11-22 06:38:55,139 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:38:55,139 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:46036; closing.
2023-11-22 06:38:55,139 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39115', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635135.1397982')
2023-11-22 06:38:55,140 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:38:55,141 - distributed.nanny - INFO - Worker closed
2023-11-22 06:38:56,236 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:38:56,236 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:38:56,237 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:38:56,238 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:38:56,238 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2023-11-22 06:39:00,445 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:00,449 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46099 instead
  warnings.warn(
2023-11-22 06:39:00,453 - distributed.scheduler - INFO - State start
2023-11-22 06:39:00,476 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:00,477 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:39:00,477 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46099/status
2023-11-22 06:39:00,478 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:39:00,683 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46113'
2023-11-22 06:39:02,392 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:02,392 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:02,934 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:03,192 - distributed.scheduler - INFO - Receive client connection: Client-d0df1794-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:03,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59482
2023-11-22 06:39:05,526 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33563
2023-11-22 06:39:05,527 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33563
2023-11-22 06:39:05,527 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44059
2023-11-22 06:39:05,527 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:05,527 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:05,527 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:05,527 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-22 06:39:05,527 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y7uih_wh
2023-11-22 06:39:05,528 - distributed.worker - INFO - Starting Worker plugin PreImport-fed1a359-4e31-4e3b-8023-247d5fbc2524
2023-11-22 06:39:05,529 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f27beff7-959c-487d-a274-283a693f7bd0
2023-11-22 06:39:05,529 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bd392444-a6c6-469b-9ec0-63f6accd2da4
2023-11-22 06:39:05,530 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:05,578 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33563', status: init, memory: 0, processing: 0>
2023-11-22 06:39:05,579 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33563
2023-11-22 06:39:05,579 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59502
2023-11-22 06:39:05,580 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:05,582 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:05,582 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:05,585 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:05,628 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:05,631 - distributed.scheduler - INFO - Remove client Client-d0df1794-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:05,631 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59482; closing.
2023-11-22 06:39:05,632 - distributed.scheduler - INFO - Remove client Client-d0df1794-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:05,632 - distributed.scheduler - INFO - Close client connection: Client-d0df1794-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:05,633 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46113'. Reason: nanny-close
2023-11-22 06:39:05,634 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:05,635 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33563. Reason: nanny-close
2023-11-22 06:39:05,638 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59502; closing.
2023-11-22 06:39:05,638 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:05,639 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33563', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635145.639057')
2023-11-22 06:39:05,639 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:39:05,640 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:07,001 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:39:07,001 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:39:07,002 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:39:07,003 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:39:07,003 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2023-11-22 06:39:09,244 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:09,249 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36325 instead
  warnings.warn(
2023-11-22 06:39:09,253 - distributed.scheduler - INFO - State start
2023-11-22 06:39:09,293 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:09,294 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:39:09,294 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:36325/status
2023-11-22 06:39:09,295 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:39:13,323 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:59512'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 224, in read
    frames_nbytes = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 921, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4351, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 240, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 143, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:59512>: Stream is closed
2023-11-22 06:39:13,576 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:39:13,576 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:39:13,577 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:39:13,577 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:39:13,578 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2023-11-22 06:39:15,750 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:15,754 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41723 instead
  warnings.warn(
2023-11-22 06:39:15,758 - distributed.scheduler - INFO - State start
2023-11-22 06:39:16,016 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:16,017 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2023-11-22 06:39:16,018 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41723/status
2023-11-22 06:39:16,018 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:39:16,196 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38929'
2023-11-22 06:39:16,243 - distributed.scheduler - INFO - Receive client connection: Client-d9fb3ac4-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:16,254 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56438
2023-11-22 06:39:17,821 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:17,821 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:17,824 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:18,993 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37857
2023-11-22 06:39:18,994 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37857
2023-11-22 06:39:18,994 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40879
2023-11-22 06:39:18,994 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2023-11-22 06:39:18,994 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:18,994 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:18,994 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-22 06:39:18,994 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-cafsqs2_
2023-11-22 06:39:18,995 - distributed.worker - INFO - Starting Worker plugin PreImport-f598a39c-5c19-46e5-824a-9f356a127d04
2023-11-22 06:39:18,995 - distributed.worker - INFO - Starting Worker plugin RMMSetup-640b0b28-4d4f-4fc0-b0fb-2499e7ed1780
2023-11-22 06:39:18,995 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1a20e867-b019-461c-a860-99a5180c8c30
2023-11-22 06:39:18,996 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:19,026 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37857', status: init, memory: 0, processing: 0>
2023-11-22 06:39:19,027 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37857
2023-11-22 06:39:19,027 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56446
2023-11-22 06:39:19,028 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:19,029 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2023-11-22 06:39:19,029 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:19,032 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2023-11-22 06:39:19,129 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:19,132 - distributed.scheduler - INFO - Remove client Client-d9fb3ac4-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:19,132 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56438; closing.
2023-11-22 06:39:19,132 - distributed.scheduler - INFO - Remove client Client-d9fb3ac4-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:19,133 - distributed.scheduler - INFO - Close client connection: Client-d9fb3ac4-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:19,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38929'. Reason: nanny-close
2023-11-22 06:39:19,134 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:19,135 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37857. Reason: nanny-close
2023-11-22 06:39:19,137 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2023-11-22 06:39:19,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56446; closing.
2023-11-22 06:39:19,138 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37857', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635159.1380734')
2023-11-22 06:39:19,138 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:39:19,139 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:20,100 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:39:20,101 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:39:20,101 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:39:20,102 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2023-11-22 06:39:20,103 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2023-11-22 06:39:22,322 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:22,327 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37181 instead
  warnings.warn(
2023-11-22 06:39:22,331 - distributed.scheduler - INFO - State start
2023-11-22 06:39:22,488 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:22,489 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:39:22,490 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37181/status
2023-11-22 06:39:22,490 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:39:22,669 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44519'
2023-11-22 06:39:22,689 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36933'
2023-11-22 06:39:22,691 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44171'
2023-11-22 06:39:22,699 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42467'
2023-11-22 06:39:22,707 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46721'
2023-11-22 06:39:22,716 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36937'
2023-11-22 06:39:22,725 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41359'
2023-11-22 06:39:22,734 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42757'
2023-11-22 06:39:24,222 - distributed.scheduler - INFO - Receive client connection: Client-dde7dd9a-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:24,238 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39030
2023-11-22 06:39:24,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:24,556 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:24,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:24,556 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:24,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:24,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:24,560 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:24,560 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:24,560 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:24,560 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:24,562 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:24,564 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:24,722 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:24,722 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:24,722 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:24,722 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:24,727 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:24,727 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:24,750 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:24,750 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:24,755 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:24,802 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:24,802 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:24,807 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:27,925 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42859
2023-11-22 06:39:27,926 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42859
2023-11-22 06:39:27,926 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44727
2023-11-22 06:39:27,926 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:27,927 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:27,927 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:27,927 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:39:27,927 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-s6zsbhgf
2023-11-22 06:39:27,927 - distributed.worker - INFO - Starting Worker plugin PreImport-2d7af518-1234-4be7-9bbc-c56f47fd7822
2023-11-22 06:39:27,927 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7a9917a4-8dac-400d-943f-2e4345442a07
2023-11-22 06:39:27,927 - distributed.worker - INFO - Starting Worker plugin RMMSetup-88f925c3-669a-47f8-92fd-71b66acf7759
2023-11-22 06:39:28,030 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46233
2023-11-22 06:39:28,031 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46233
2023-11-22 06:39:28,031 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34531
2023-11-22 06:39:28,031 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,031 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,031 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:28,032 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:39:28,032 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tuo530ih
2023-11-22 06:39:28,032 - distributed.worker - INFO - Starting Worker plugin PreImport-4aa0406a-c95e-488a-bcf4-e96e7284aa96
2023-11-22 06:39:28,032 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7046e7fa-2822-45f8-9f40-5f1528a684c4
2023-11-22 06:39:28,032 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b28b4930-417b-443d-a980-e5180c6eeab6
2023-11-22 06:39:28,054 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41295
2023-11-22 06:39:28,055 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41295
2023-11-22 06:39:28,055 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44833
2023-11-22 06:39:28,055 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,055 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,055 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:28,055 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:39:28,055 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-my4wsvoa
2023-11-22 06:39:28,056 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b81fa910-0b97-4ef5-85fc-a0cc06e47a29
2023-11-22 06:39:28,077 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34729
2023-11-22 06:39:28,077 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34729
2023-11-22 06:39:28,078 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33477
2023-11-22 06:39:28,078 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,078 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,078 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:28,078 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:39:28,078 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fvv2rtkf
2023-11-22 06:39:28,078 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbb3bfff-a847-4837-ae31-40315d8c2877
2023-11-22 06:39:28,079 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d460cd36-cc68-4cd7-9c0c-cd09a0834d4c
2023-11-22 06:39:28,095 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,127 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43581
2023-11-22 06:39:28,128 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43581
2023-11-22 06:39:28,128 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40861
2023-11-22 06:39:28,128 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,126 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42859', status: init, memory: 0, processing: 0>
2023-11-22 06:39:28,128 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,128 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:28,128 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:39:28,128 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-c3d15qli
2023-11-22 06:39:28,129 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c0f47949-aab0-43bf-91db-7a6806555633
2023-11-22 06:39:28,129 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42859
2023-11-22 06:39:28,130 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39046
2023-11-22 06:39:28,130 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:28,131 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,131 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,131 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38591
2023-11-22 06:39:28,132 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38591
2023-11-22 06:39:28,132 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36315
2023-11-22 06:39:28,132 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,132 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,132 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:28,132 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:39:28,132 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nlj1953x
2023-11-22 06:39:28,132 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f119cb31-7427-4d01-a9af-e6cdf10221db
2023-11-22 06:39:28,134 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35969
2023-11-22 06:39:28,135 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35969
2023-11-22 06:39:28,135 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45277
2023-11-22 06:39:28,134 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39027
2023-11-22 06:39:28,135 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,135 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39027
2023-11-22 06:39:28,135 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,135 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:28,135 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44187
2023-11-22 06:39:28,135 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,135 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:28,135 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,135 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:39:28,135 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4yv9u6uc
2023-11-22 06:39:28,135 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:28,135 - distributed.worker - INFO -                Memory:                 125.97 GiB
2023-11-22 06:39:28,135 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bfdhyjtn
2023-11-22 06:39:28,136 - distributed.worker - INFO - Starting Worker plugin PreImport-ee9b89d0-549d-4547-9326-dd0447a043ec
2023-11-22 06:39:28,136 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eb8c3d0c-e3db-4098-a168-6c437b08dff9
2023-11-22 06:39:28,136 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b251bc8c-edb9-4cd1-9037-6da473427afa
2023-11-22 06:39:28,136 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7c750a2e-8cf8-4dfe-922e-202b428d0b4c
2023-11-22 06:39:28,433 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,433 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb319114-2407-40b6-9c60-a5ac4bb6ca9e
2023-11-22 06:39:28,435 - distributed.worker - INFO - Starting Worker plugin PreImport-718af291-ba67-434e-8c12-cb00074e824a
2023-11-22 06:39:28,436 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,451 - distributed.worker - INFO - Starting Worker plugin PreImport-377807cc-e9c4-442e-9e5d-fd537d8cbc70
2023-11-22 06:39:28,451 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,461 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9370933e-9bd5-477c-b2f3-26afa16431c8
2023-11-22 06:39:28,461 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-627a59d2-6ff5-48dc-9001-66e094b6947c
2023-11-22 06:39:28,461 - distributed.worker - INFO - Starting Worker plugin PreImport-57da1cab-ba7a-4f1c-98c6-062ab8301112
2023-11-22 06:39:28,461 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,461 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,463 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46233', status: init, memory: 0, processing: 0>
2023-11-22 06:39:28,464 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46233
2023-11-22 06:39:28,464 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39048
2023-11-22 06:39:28,465 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:28,466 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,466 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,469 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8fbafac0-345d-49d0-9a9b-6bdffa94c6ea
2023-11-22 06:39:28,469 - distributed.worker - INFO - Starting Worker plugin PreImport-1adff9ad-de7a-49af-8d0a-4704096bc63e
2023-11-22 06:39:28,470 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:28,470 - distributed.worker - INFO - Starting Worker plugin PreImport-0662406d-0f72-49a8-8721-82a878841afe
2023-11-22 06:39:28,470 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,470 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,473 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41295', status: init, memory: 0, processing: 0>
2023-11-22 06:39:28,474 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41295
2023-11-22 06:39:28,474 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39056
2023-11-22 06:39:28,475 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:28,476 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,476 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,477 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39027', status: init, memory: 0, processing: 0>
2023-11-22 06:39:28,478 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39027
2023-11-22 06:39:28,478 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39060
2023-11-22 06:39:28,479 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:28,480 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,480 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,484 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:28,484 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:28,487 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38591', status: init, memory: 0, processing: 0>
2023-11-22 06:39:28,488 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38591
2023-11-22 06:39:28,488 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39076
2023-11-22 06:39:28,489 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:28,489 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,489 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,493 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:28,501 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35969', status: init, memory: 0, processing: 0>
2023-11-22 06:39:28,502 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35969
2023-11-22 06:39:28,502 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39086
2023-11-22 06:39:28,503 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:28,504 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,504 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,506 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43581', status: init, memory: 0, processing: 0>
2023-11-22 06:39:28,506 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43581
2023-11-22 06:39:28,506 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39110
2023-11-22 06:39:28,508 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34729', status: init, memory: 0, processing: 0>
2023-11-22 06:39:28,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:28,508 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34729
2023-11-22 06:39:28,508 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39102
2023-11-22 06:39:28,509 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,509 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,510 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:28,511 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:28,511 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:28,514 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:28,518 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:28,520 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:28,524 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:39:28,524 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:39:28,524 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:39:28,525 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:39:28,525 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:39:28,525 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:39:28,525 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:39:28,526 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2023-11-22 06:39:28,545 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:28,545 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:28,545 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:28,545 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:28,545 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:28,546 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:28,546 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:28,546 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:28,553 - distributed.scheduler - INFO - Remove client Client-dde7dd9a-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:28,553 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39030; closing.
2023-11-22 06:39:28,554 - distributed.scheduler - INFO - Remove client Client-dde7dd9a-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:28,554 - distributed.scheduler - INFO - Close client connection: Client-dde7dd9a-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:28,555 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44519'. Reason: nanny-close
2023-11-22 06:39:28,556 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:28,556 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36933'. Reason: nanny-close
2023-11-22 06:39:28,557 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:28,557 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35969. Reason: nanny-close
2023-11-22 06:39:28,557 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44171'. Reason: nanny-close
2023-11-22 06:39:28,557 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:28,558 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42467'. Reason: nanny-close
2023-11-22 06:39:28,558 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43581. Reason: nanny-close
2023-11-22 06:39:28,558 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:28,558 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42859. Reason: nanny-close
2023-11-22 06:39:28,558 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46721'. Reason: nanny-close
2023-11-22 06:39:28,559 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:28,559 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46233. Reason: nanny-close
2023-11-22 06:39:28,559 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36937'. Reason: nanny-close
2023-11-22 06:39:28,559 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:28,560 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:28,560 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41359'. Reason: nanny-close
2023-11-22 06:39:28,560 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34729. Reason: nanny-close
2023-11-22 06:39:28,560 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:28,560 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:28,560 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39086; closing.
2023-11-22 06:39:28,560 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41295. Reason: nanny-close
2023-11-22 06:39:28,560 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:28,560 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42757'. Reason: nanny-close
2023-11-22 06:39:28,561 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:28,561 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39027. Reason: nanny-close
2023-11-22 06:39:28,561 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:28,561 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35969', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635168.5612185')
2023-11-22 06:39:28,561 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38591. Reason: nanny-close
2023-11-22 06:39:28,562 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:28,562 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39048; closing.
2023-11-22 06:39:28,562 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:28,562 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39046; closing.
2023-11-22 06:39:28,562 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:28,562 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:28,562 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:28,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:28,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:28,563 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46233', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635168.5632653')
2023-11-22 06:39:28,563 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:28,563 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42859', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635168.563804')
2023-11-22 06:39:28,564 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39110; closing.
2023-11-22 06:39:28,564 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:28,564 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:28,565 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:28,565 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:28,565 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43581', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635168.5657248')
2023-11-22 06:39:28,567 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:39046>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-22 06:39:28,569 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:39048>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 268, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2023-11-22 06:39:28,570 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39102; closing.
2023-11-22 06:39:28,570 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39056; closing.
2023-11-22 06:39:28,570 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39060; closing.
2023-11-22 06:39:28,571 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34729', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635168.5711768')
2023-11-22 06:39:28,571 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41295', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635168.5717735')
2023-11-22 06:39:28,572 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39027', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635168.5722623')
2023-11-22 06:39:28,572 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39076; closing.
2023-11-22 06:39:28,573 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38591', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635168.5733445')
2023-11-22 06:39:28,573 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:39:30,173 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:39:30,174 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:39:30,174 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:39:30,176 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:39:30,176 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2023-11-22 06:39:32,628 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:32,634 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38725 instead
  warnings.warn(
2023-11-22 06:39:32,638 - distributed.scheduler - INFO - State start
2023-11-22 06:39:32,667 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:32,668 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:39:32,669 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:38725/status
2023-11-22 06:39:32,669 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:39:32,728 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40387'
2023-11-22 06:39:34,148 - distributed.scheduler - INFO - Receive client connection: Client-e3f21465-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:34,160 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42616
2023-11-22 06:39:34,525 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:34,525 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:34,529 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:35,429 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34407
2023-11-22 06:39:35,430 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34407
2023-11-22 06:39:35,430 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42489
2023-11-22 06:39:35,430 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:35,430 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:35,430 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:35,430 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-22 06:39:35,430 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_t99jonw
2023-11-22 06:39:35,430 - distributed.worker - INFO - Starting Worker plugin RMMSetup-11e5168b-c513-4b3e-a99e-56a197811749
2023-11-22 06:39:35,529 - distributed.worker - INFO - Starting Worker plugin PreImport-46d7f915-383f-4880-b960-eb7acda37874
2023-11-22 06:39:35,529 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d4d470f2-83cd-410c-8dc7-df0d2fa8a0cc
2023-11-22 06:39:35,529 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:35,559 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34407', status: init, memory: 0, processing: 0>
2023-11-22 06:39:35,561 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34407
2023-11-22 06:39:35,561 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42646
2023-11-22 06:39:35,562 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:35,563 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:35,563 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:35,564 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:35,655 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:39:35,659 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:35,661 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:35,663 - distributed.scheduler - INFO - Remove client Client-e3f21465-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:35,664 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42616; closing.
2023-11-22 06:39:35,664 - distributed.scheduler - INFO - Remove client Client-e3f21465-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:35,664 - distributed.scheduler - INFO - Close client connection: Client-e3f21465-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:35,665 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40387'. Reason: nanny-close
2023-11-22 06:39:35,666 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:35,667 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34407. Reason: nanny-close
2023-11-22 06:39:35,668 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:35,668 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42646; closing.
2023-11-22 06:39:35,669 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34407', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635175.6691484')
2023-11-22 06:39:35,669 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:39:35,670 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:36,782 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:39:36,782 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:39:36,783 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:39:36,784 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:39:36,784 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2023-11-22 06:39:38,952 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:38,956 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35083 instead
  warnings.warn(
2023-11-22 06:39:38,960 - distributed.scheduler - INFO - State start
2023-11-22 06:39:38,981 - distributed.scheduler - INFO - -----------------------------------------------
2023-11-22 06:39:38,981 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2023-11-22 06:39:38,982 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35083/status
2023-11-22 06:39:38,982 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2023-11-22 06:39:39,117 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38121'
2023-11-22 06:39:40,749 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2023-11-22 06:39:40,749 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2023-11-22 06:39:40,752 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2023-11-22 06:39:41,757 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45219
2023-11-22 06:39:41,757 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45219
2023-11-22 06:39:41,758 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39697
2023-11-22 06:39:41,758 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2023-11-22 06:39:41,758 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:41,758 - distributed.worker - INFO -               Threads:                          1
2023-11-22 06:39:41,758 - distributed.worker - INFO -                Memory:                   0.98 TiB
2023-11-22 06:39:41,758 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-dujnhern
2023-11-22 06:39:41,759 - distributed.worker - INFO - Starting Worker plugin RMMSetup-428c3be8-7d59-4b15-8562-93680f7834ef
2023-11-22 06:39:41,883 - distributed.worker - INFO - Starting Worker plugin PreImport-d8b45a16-2707-4ef2-8be9-260d3b88afeb
2023-11-22 06:39:41,883 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e914d70a-8958-4ac7-8cea-9e0d23a0d306
2023-11-22 06:39:41,883 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:41,907 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45219', status: init, memory: 0, processing: 0>
2023-11-22 06:39:41,923 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45219
2023-11-22 06:39:41,923 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53854
2023-11-22 06:39:41,924 - distributed.worker - INFO - Starting Worker plugin shuffle
2023-11-22 06:39:41,925 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2023-11-22 06:39:41,925 - distributed.worker - INFO - -------------------------------------------------
2023-11-22 06:39:41,927 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2023-11-22 06:39:42,684 - distributed.scheduler - INFO - Receive client connection: Client-e7d495be-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:42,684 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53856
2023-11-22 06:39:42,690 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2023-11-22 06:39:42,694 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2023-11-22 06:39:42,697 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:42,698 - distributed.worker - INFO - Run out-of-band function 'lambda'
2023-11-22 06:39:42,700 - distributed.scheduler - INFO - Remove client Client-e7d495be-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:42,701 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53856; closing.
2023-11-22 06:39:42,701 - distributed.scheduler - INFO - Remove client Client-e7d495be-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:42,701 - distributed.scheduler - INFO - Close client connection: Client-e7d495be-8901-11ee-b6e1-d8c49764f6bb
2023-11-22 06:39:42,702 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38121'. Reason: nanny-close
2023-11-22 06:39:42,702 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2023-11-22 06:39:42,703 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45219. Reason: nanny-close
2023-11-22 06:39:42,705 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53854; closing.
2023-11-22 06:39:42,705 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2023-11-22 06:39:42,705 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45219', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1700635182.705632')
2023-11-22 06:39:42,705 - distributed.scheduler - INFO - Lost all workers
2023-11-22 06:39:42,707 - distributed.nanny - INFO - Worker closed
2023-11-22 06:39:43,718 - distributed._signals - INFO - Received signal SIGINT (2)
2023-11-22 06:39:43,719 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2023-11-22 06:39:43,719 - distributed.scheduler - INFO - Scheduler closing all comms
2023-11-22 06:39:43,720 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2023-11-22 06:39:43,720 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45201 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45797 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43761 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36507 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45397 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46239 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39591 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36323 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36483 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44853 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40687 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33245 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35187 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45179 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34191 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40901 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33981 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39113 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43931 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42763 instead
  warnings.warn(
[1700635453.259937] [dgx13:71875:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:34919) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40225 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38881 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43571 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41861 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46029 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42683 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40641 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44493 instead
  warnings.warn(
2023-11-22 06:47:20,976 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 47, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-11-22 06:47:20,987 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:40931'.
2023-11-22 06:47:20,987 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:40931'. Shutting down.
2023-11-22 06:47:20,990 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f2435908e50>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 47, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=CudaAPIError(2, 'Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/__init__.py", line 27, in <module>
    from cudf.core.algorithms import factorize
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/algorithms.py", line 10, in <module>
    from cudf.core.indexed_frame import IndexedFrame
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 59, in <module>
    from cudf.core.groupby.groupby import GroupBy
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/__init__.py", line 3, in <module>
    from cudf.core.groupby.groupby import GroupBy, Grouper
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/groupby/groupby.py", line 31, in <module>
    from cudf.core.udf.groupby_utils import _can_be_jitted, jit_groupby_apply
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/groupby_utils.py", line 11, in <module>
    import cudf.core.udf.utils
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/udf/utils.py", line 66, in <module>
    _PTX_FILE = _get_ptx_file(os.path.dirname(__file__), "shim_")
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/_numba.py", line 47, in _get_ptx_file
    dev = cuda.get_current_device()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/api.py", line 443, in get_current_device
    return current_context().device
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 675, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 331, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 399, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2023-11-22 06:47:22,772 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-22 06:47:22,779 - distributed.scheduler - WARNING - Received heartbeat from unregistered worker 'tcp://127.0.0.1:42207'.
2023-11-22 06:47:22,779 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:42207'. Shutting down.
2023-11-22 06:47:22,782 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7febf1d6ee50>>, <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-5' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:202> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 205, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1300, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 974, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 253, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 100, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 83, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 108, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/compat.py", line 97, in _decode_default
    return pickle.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 172, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 173, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 82, in as_buffer
    return Buffer._from_host_memory(data)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 167, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/envs/gdf/include/rmm/mr/device/cuda_memory_resource.hpp
2023-11-22 06:47:22,993 - distributed.nanny - ERROR - Worker process died unexpectedly
2023-11-22 06:47:24,784 - distributed.nanny - ERROR - Worker process died unexpectedly
