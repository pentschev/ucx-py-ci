============================= test session starts ==============================
platform linux -- Python 3.9.19, pytest-8.1.1, pluggy-1.4.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.6
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-03-21 05:47:31,546 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:47:31,550 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:47:31,554 - distributed.scheduler - INFO - State start
2024-03-21 05:47:31,576 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:47:31,577 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-21 05:47:31,578 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:47:31,578 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:47:31,640 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41599'
2024-03-21 05:47:31,657 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46669'
2024-03-21 05:47:31,660 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46719'
2024-03-21 05:47:31,667 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44289'
2024-03-21 05:47:32,846 - distributed.scheduler - INFO - Receive client connection: Client-814f8f39-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:32,857 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42524
2024-03-21 05:47:33,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:33,361 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:33,361 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:33,361 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:33,362 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:33,362 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:33,365 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:33,365 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:33,366 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35795
2024-03-21 05:47:33,366 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43879
2024-03-21 05:47:33,366 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35795
2024-03-21 05:47:33,366 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43879
2024-03-21 05:47:33,366 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38833
2024-03-21 05:47:33,366 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33625
2024-03-21 05:47:33,366 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:47:33,366 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:47:33,366 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,366 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,367 - distributed.worker - INFO -               Threads:                          4
2024-03-21 05:47:33,367 - distributed.worker - INFO -               Threads:                          4
2024-03-21 05:47:33,366 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:33,367 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:33,367 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-21 05:47:33,367 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-21 05:47:33,367 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-o4nd9d3z
2024-03-21 05:47:33,367 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-keg3a_d7
2024-03-21 05:47:33,367 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:33,367 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c32fd75e-7dcf-4de3-8799-36d744008fbe
2024-03-21 05:47:33,367 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b6b4fe5-0eae-4fb2-b9b9-0ed8435f461d
2024-03-21 05:47:33,367 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2f01bf6d-a499-4278-a886-2fa96cb25289
2024-03-21 05:47:33,367 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c4525a5-2590-4172-a405-642fd055ad8f
2024-03-21 05:47:33,367 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45519
2024-03-21 05:47:33,367 - distributed.worker - INFO - Starting Worker plugin PreImport-560c72a8-1a54-4b6f-9dfc-f696679cb499
2024-03-21 05:47:33,367 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45519
2024-03-21 05:47:33,368 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43627
2024-03-21 05:47:33,368 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,368 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:47:33,368 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,368 - distributed.worker - INFO -               Threads:                          4
2024-03-21 05:47:33,368 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-21 05:47:33,368 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-may4cn64
2024-03-21 05:47:33,368 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1b27f383-6969-4215-b990-56619f3bf4c3
2024-03-21 05:47:33,368 - distributed.worker - INFO - Starting Worker plugin PreImport-5070b8b2-4fb0-4960-8eb2-d82e7ba3a65e
2024-03-21 05:47:33,368 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fc0f1248-a815-4e68-928c-fb920f4ad52e
2024-03-21 05:47:33,369 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,369 - distributed.worker - INFO - Starting Worker plugin PreImport-9874cb35-5e5e-4723-924f-78f8bf677b5c
2024-03-21 05:47:33,370 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,371 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:33,372 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33239
2024-03-21 05:47:33,372 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33239
2024-03-21 05:47:33,372 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35929
2024-03-21 05:47:33,372 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:47:33,372 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,372 - distributed.worker - INFO -               Threads:                          4
2024-03-21 05:47:33,372 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-03-21 05:47:33,372 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-hhkh70yt
2024-03-21 05:47:33,372 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e320e756-b7d7-4578-89dc-c77814e485cd
2024-03-21 05:47:33,372 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fca5de19-6786-41ff-bd0e-69f84a71ba54
2024-03-21 05:47:33,373 - distributed.worker - INFO - Starting Worker plugin PreImport-9c672550-6f68-432c-89b9-e57c44912bf6
2024-03-21 05:47:33,373 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,500 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35795', status: init, memory: 0, processing: 0>
2024-03-21 05:47:33,501 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35795
2024-03-21 05:47:33,501 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42540
2024-03-21 05:47:33,502 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:33,502 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45519', status: init, memory: 0, processing: 0>
2024-03-21 05:47:33,503 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:47:33,503 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,503 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45519
2024-03-21 05:47:33,503 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42552
2024-03-21 05:47:33,504 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:33,504 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33239', status: init, memory: 0, processing: 0>
2024-03-21 05:47:33,504 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:47:33,505 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:47:33,505 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,505 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33239
2024-03-21 05:47:33,505 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42578
2024-03-21 05:47:33,506 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:47:33,506 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:33,506 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43879', status: init, memory: 0, processing: 0>
2024-03-21 05:47:33,507 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:47:33,507 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43879
2024-03-21 05:47:33,507 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,507 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:42568
2024-03-21 05:47:33,508 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:33,508 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:47:33,508 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:47:33,508 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:33,509 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:47:33,578 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-21 05:47:33,578 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-21 05:47:33,578 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-21 05:47:33,578 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-03-21 05:47:33,585 - distributed.scheduler - INFO - Remove client Client-814f8f39-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:33,585 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42524; closing.
2024-03-21 05:47:33,585 - distributed.scheduler - INFO - Remove client Client-814f8f39-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:33,585 - distributed.scheduler - INFO - Close client connection: Client-814f8f39-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:33,586 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41599'. Reason: nanny-close
2024-03-21 05:47:33,587 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:33,587 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46669'. Reason: nanny-close
2024-03-21 05:47:33,588 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:33,588 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46719'. Reason: nanny-close
2024-03-21 05:47:33,588 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45519. Reason: nanny-close
2024-03-21 05:47:33,589 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:33,589 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44289'. Reason: nanny-close
2024-03-21 05:47:33,589 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43879. Reason: nanny-close
2024-03-21 05:47:33,589 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:33,589 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33239. Reason: nanny-close
2024-03-21 05:47:33,590 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35795. Reason: nanny-close
2024-03-21 05:47:33,590 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:47:33,591 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42552; closing.
2024-03-21 05:47:33,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:47:33,591 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45519', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000053.5914738')
2024-03-21 05:47:33,591 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:47:33,592 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:33,592 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:33,592 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42578; closing.
2024-03-21 05:47:33,593 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42568; closing.
2024-03-21 05:47:33,593 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:47:33,593 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:33,593 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33239', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000053.5938768')
2024-03-21 05:47:33,594 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43879', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000053.5944648')
2024-03-21 05:47:33,595 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:33,595 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:42578>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:42578>: Stream is closed
2024-03-21 05:47:33,599 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42540; closing.
2024-03-21 05:47:33,599 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35795', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000053.5996773')
2024-03-21 05:47:33,600 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:47:34,202 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:47:34,202 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:47:34,202 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:47:34,203 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-21 05:47:34,204 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-03-21 05:47:36,318 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:47:36,323 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:47:36,326 - distributed.scheduler - INFO - State start
2024-03-21 05:47:36,347 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:47:36,348 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:47:36,348 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:47:36,349 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:47:36,410 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40165'
2024-03-21 05:47:36,421 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34361'
2024-03-21 05:47:36,429 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41917'
2024-03-21 05:47:36,443 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45397'
2024-03-21 05:47:36,447 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44429'
2024-03-21 05:47:36,456 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46571'
2024-03-21 05:47:36,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44321'
2024-03-21 05:47:36,476 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41557'
2024-03-21 05:47:36,653 - distributed.scheduler - INFO - Receive client connection: Client-844e0c17-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:47:36,667 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56748
2024-03-21 05:47:37,780 - distributed.scheduler - INFO - Receive client connection: Client-8419c616-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:37,780 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56842
2024-03-21 05:47:38,324 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,328 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,329 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37589
2024-03-21 05:47:38,329 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37589
2024-03-21 05:47:38,329 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36955
2024-03-21 05:47:38,329 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,330 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,330 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,330 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,330 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tvdvh6lt
2024-03-21 05:47:38,330 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c945b01d-4f48-4984-8995-90e5ad4fa2f9
2024-03-21 05:47:38,330 - distributed.worker - INFO - Starting Worker plugin RMMSetup-bdcf6f40-27b0-4af8-9744-b8424163577e
2024-03-21 05:47:38,402 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,403 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,407 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,408 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37967
2024-03-21 05:47:38,408 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37967
2024-03-21 05:47:38,408 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33569
2024-03-21 05:47:38,408 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,408 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,408 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,408 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,408 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f6h9at5a
2024-03-21 05:47:38,409 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb65192a-125b-445a-996c-b40591c2786e
2024-03-21 05:47:38,410 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3d202bbe-56ef-428b-9b83-88f4fecb1174
2024-03-21 05:47:38,573 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,573 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,578 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,579 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35235
2024-03-21 05:47:38,579 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35235
2024-03-21 05:47:38,579 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40047
2024-03-21 05:47:38,579 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,579 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,579 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,579 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,579 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-am89rp22
2024-03-21 05:47:38,580 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3507bd88-27c2-4a5e-8259-fc1964895aea
2024-03-21 05:47:38,580 - distributed.worker - INFO - Starting Worker plugin PreImport-030394e5-036d-43c4-a891-742716ee1520
2024-03-21 05:47:38,580 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d66bd528-7105-43bc-b36b-62a76cfdc679
2024-03-21 05:47:38,581 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,582 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,582 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,583 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44663
2024-03-21 05:47:38,583 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44663
2024-03-21 05:47:38,584 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38409
2024-03-21 05:47:38,584 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,584 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,584 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,584 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,584 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-26ckoqb3
2024-03-21 05:47:38,584 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1112b6a2-eaee-47d8-bb69-cb3958b89745
2024-03-21 05:47:38,584 - distributed.worker - INFO - Starting Worker plugin PreImport-d714cda8-b097-46b3-ba54-47169b5bcf53
2024-03-21 05:47:38,584 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1462a61e-855e-4666-9acb-8920a75dad0c
2024-03-21 05:47:38,588 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,589 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33651
2024-03-21 05:47:38,590 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33651
2024-03-21 05:47:38,590 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41551
2024-03-21 05:47:38,590 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,590 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,590 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,590 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,590 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e555v2jk
2024-03-21 05:47:38,590 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-20ef3f4d-88b2-4604-91b7-76dd28d85898
2024-03-21 05:47:38,590 - distributed.worker - INFO - Starting Worker plugin PreImport-61521596-21f5-49cc-a1d8-2827e9786904
2024-03-21 05:47:38,591 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ea6dbcdf-56ce-4648-be60-bd6f94b7d520
2024-03-21 05:47:38,608 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,609 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,609 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,615 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,616 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34979
2024-03-21 05:47:38,616 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34979
2024-03-21 05:47:38,617 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39857
2024-03-21 05:47:38,617 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,617 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,617 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,617 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,617 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yn39eza6
2024-03-21 05:47:38,617 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1538d647-011e-4cb1-a91a-737c99e6e583
2024-03-21 05:47:38,621 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,625 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33353
2024-03-21 05:47:38,625 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33353
2024-03-21 05:47:38,625 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38215
2024-03-21 05:47:38,625 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,625 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,625 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,625 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,625 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nu_hktjt
2024-03-21 05:47:38,626 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-49174068-b07d-43fa-8193-2f301beac038
2024-03-21 05:47:38,626 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ca25f2ae-1530-4b92-9fac-6783088c81c1
2024-03-21 05:47:38,629 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:38,630 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:38,636 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:38,637 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41439
2024-03-21 05:47:38,637 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41439
2024-03-21 05:47:38,637 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34681
2024-03-21 05:47:38,638 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:38,638 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,638 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:38,638 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:38,638 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mikim6nx
2024-03-21 05:47:38,638 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8377b763-336b-4bb3-860d-013c51af6832
2024-03-21 05:47:38,638 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4e989ed2-5808-40ba-96f7-cd06d7188197
2024-03-21 05:47:38,968 - distributed.worker - INFO - Starting Worker plugin PreImport-accef907-c987-4385-b64d-d4f3ef5ecc58
2024-03-21 05:47:38,970 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:38,999 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37589', status: init, memory: 0, processing: 0>
2024-03-21 05:47:39,000 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37589
2024-03-21 05:47:39,000 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56858
2024-03-21 05:47:39,001 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:39,002 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:39,002 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:39,004 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,157 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:47:42,160 - distributed.worker - INFO - Starting Worker plugin PreImport-d67bfc73-6a46-425a-b1b5-0a79f15e1aba
2024-03-21 05:47:42,160 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37967. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:47:42,161 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:47:42,164 - distributed.worker - INFO - Starting Worker plugin PreImport-13e06222-6d5d-4b13-a6c2-866ab2b5f72c
2024-03-21 05:47:42,165 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,167 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:47:42,187 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41439', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,188 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41439
2024-03-21 05:47:42,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45600
2024-03-21 05:47:42,189 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,190 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,190 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,191 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,195 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:47:42,198 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40165'. Reason: nanny-instantiate-failed
2024-03-21 05:47:42,199 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:47:42,207 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,213 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-37a94455-c46e-4f12-95c3-daf68a76bb00
2024-03-21 05:47:42,213 - distributed.worker - INFO - Starting Worker plugin PreImport-297fa5b6-13a5-4a64-8842-a9f4548ab8e1
2024-03-21 05:47:42,214 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,230 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44663', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,231 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44663
2024-03-21 05:47:42,231 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45610
2024-03-21 05:47:42,232 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,232 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,233 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,233 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,234 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34979', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,234 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34979
2024-03-21 05:47:42,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45620
2024-03-21 05:47:42,235 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,236 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,236 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,237 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,263 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35235', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,264 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35235
2024-03-21 05:47:42,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45630
2024-03-21 05:47:42,265 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:42,267 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:42,267 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:42,269 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:42,284 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45609', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,285 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45609
2024-03-21 05:47:42,285 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45646
2024-03-21 05:47:42,290 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39751', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,291 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39751
2024-03-21 05:47:42,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45650
2024-03-21 05:47:42,302 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45291', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,303 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45291
2024-03-21 05:47:42,303 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45658
2024-03-21 05:47:42,343 - distributed.nanny - INFO - Worker process 42666 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:47:42,347 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42697 parent=42498 started daemon>
2024-03-21 05:47:42,348 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42693 parent=42498 started daemon>
2024-03-21 05:47:42,348 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42689 parent=42498 started daemon>
2024-03-21 05:47:42,348 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42685 parent=42498 started daemon>
2024-03-21 05:47:42,348 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42680 parent=42498 started daemon>
2024-03-21 05:47:42,344 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:56674'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56674>: Stream is closed
2024-03-21 05:47:42,348 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42675 parent=42498 started daemon>
2024-03-21 05:47:42,348 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=42670 parent=42498 started daemon>
2024-03-21 05:47:42,376 - distributed.core - INFO - Connection to tcp://127.0.0.1:45620 has been closed.
2024-03-21 05:47:42,376 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34979', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.3764856')
2024-03-21 05:47:42,377 - distributed.core - INFO - Connection to tcp://127.0.0.1:45610 has been closed.
2024-03-21 05:47:42,378 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44663', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.3779745')
2024-03-21 05:47:42,378 - distributed.core - INFO - Connection to tcp://127.0.0.1:45600 has been closed.
2024-03-21 05:47:42,378 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41439', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.3783658')
2024-03-21 05:47:42,379 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:34979 failed: CommClosedError: Address removed.
2024-03-21 05:47:42,379 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:34979 failed: CommClosedError: Address removed.
2024-03-21 05:47:42,380 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45600>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-21 05:47:42,381 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45610>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-21 05:47:42,382 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:44663 failed: CommClosedError: Address removed.
2024-03-21 05:47:42,382 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:44663 failed: CommClosedError: Address removed.
2024-03-21 05:47:42,382 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:41439 failed: CommClosedError: Address removed.
2024-03-21 05:47:42,382 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:41439 failed: CommClosedError: Address removed.
2024-03-21 05:47:42,382 - distributed.core - INFO - Connection to tcp://127.0.0.1:45630 has been closed.
2024-03-21 05:47:42,382 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35235', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.3826413')
2024-03-21 05:47:42,383 - distributed.core - INFO - Connection to tcp://127.0.0.1:56858 has been closed.
2024-03-21 05:47:42,383 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37589', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.383606')
2024-03-21 05:47:42,384 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35483', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,385 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35483
2024-03-21 05:47:42,385 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45670
2024-03-21 05:47:42,386 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:35235 failed: CommClosedError: Address removed.
2024-03-21 05:47:42,386 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:35235 failed: CommClosedError: Address removed.
2024-03-21 05:47:42,386 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:37589 failed: CommClosedError: Address removed.
2024-03-21 05:47:42,387 - distributed.scheduler - ERROR - broadcast to tcp://127.0.0.1:37589 failed: CommClosedError: Address removed.
2024-03-21 05:47:42,390 - distributed.scheduler - INFO - Remove client Client-844e0c17-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:47:42,390 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56748; closing.
2024-03-21 05:47:42,390 - distributed.scheduler - INFO - Remove client Client-844e0c17-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:47:42,391 - distributed.scheduler - INFO - Remove client Client-8419c616-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:42,391 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56842; closing.
2024-03-21 05:47:42,391 - distributed.scheduler - INFO - Remove client Client-8419c616-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:42,391 - distributed.scheduler - INFO - Close client connection: Client-844e0c17-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:47:42,392 - distributed.scheduler - INFO - Close client connection: Client-8419c616-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:42,397 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45650; closing.
2024-03-21 05:47:42,397 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39751', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.3975317')
2024-03-21 05:47:42,398 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41311', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,399 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41311
2024-03-21 05:47:42,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45678
2024-03-21 05:47:42,399 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45658; closing.
2024-03-21 05:47:42,400 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45291', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.400057')
2024-03-21 05:47:42,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45646; closing.
2024-03-21 05:47:42,400 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45609', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.4009159')
2024-03-21 05:47:42,403 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41889', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,404 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41889
2024-03-21 05:47:42,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45682
2024-03-21 05:47:42,413 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45682; closing.
2024-03-21 05:47:42,413 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41889', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.4133158')
2024-03-21 05:47:42,414 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45670; closing.
2024-03-21 05:47:42,414 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35483', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.414647')
2024-03-21 05:47:42,415 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45678; closing.
2024-03-21 05:47:42,415 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41311', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.415321')
2024-03-21 05:47:42,415 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:47:42,592 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 42675 exit status was already read will report exitcode 255
2024-03-21 05:47:42,619 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 42693 exit status was already read will report exitcode 255
2024-03-21 05:47:42,654 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 42670 exit status was already read will report exitcode 255
2024-03-21 05:47:42,700 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45479', status: init, memory: 0, processing: 0>
2024-03-21 05:47:42,701 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45479
2024-03-21 05:47:42,701 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45690
2024-03-21 05:47:42,718 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45690; closing.
2024-03-21 05:47:42,718 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45479', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000062.7187214')
2024-03-21 05:47:42,719 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:47:42,939 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:56724'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56724>: Stream is closed
2024-03-21 05:47:42,939 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:56692'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56692>: Stream is closed
2024-03-21 05:47:42,957 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:47:42,958 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:47:42,958 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:47:42,960 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:47:42,960 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-03-21 05:47:45,431 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:47:45,435 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:47:45,438 - distributed.scheduler - INFO - State start
2024-03-21 05:47:45,440 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-am89rp22', purging
2024-03-21 05:47:45,440 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-yn39eza6', purging
2024-03-21 05:47:45,441 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-tvdvh6lt', purging
2024-03-21 05:47:45,441 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-e555v2jk', purging
2024-03-21 05:47:45,441 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-26ckoqb3', purging
2024-03-21 05:47:45,441 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-mikim6nx', purging
2024-03-21 05:47:45,442 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nu_hktjt', purging
2024-03-21 05:47:45,461 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:47:45,461 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:47:45,462 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:47:45,462 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:47:45,521 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36089'
2024-03-21 05:47:45,533 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33953'
2024-03-21 05:47:45,541 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34903'
2024-03-21 05:47:45,556 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44925'
2024-03-21 05:47:45,560 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35787'
2024-03-21 05:47:45,569 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45401'
2024-03-21 05:47:45,580 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34115'
2024-03-21 05:47:45,589 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40009'
2024-03-21 05:47:46,327 - distributed.scheduler - INFO - Receive client connection: Client-8994b9c3-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:46,340 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45868
2024-03-21 05:47:47,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:47,199 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:47,203 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:47,204 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32883
2024-03-21 05:47:47,204 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32883
2024-03-21 05:47:47,204 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44553
2024-03-21 05:47:47,204 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:47,204 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:47,204 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:47,204 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:47,204 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1_ggcw_w
2024-03-21 05:47:47,204 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6161ea9-3d5a-4898-a2a1-f01a4a3a2971
2024-03-21 05:47:47,205 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b44cfd76-b4f2-4d18-8247-31e11526d0b0
2024-03-21 05:47:47,440 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:47,440 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:47,440 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:47,441 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:47,445 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:47,445 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:47,446 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42363
2024-03-21 05:47:47,446 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42363
2024-03-21 05:47:47,446 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39811
2024-03-21 05:47:47,446 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:47,446 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:47,446 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:47,446 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46345
2024-03-21 05:47:47,446 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:47,446 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zxyttkvn
2024-03-21 05:47:47,446 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46345
2024-03-21 05:47:47,446 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37475
2024-03-21 05:47:47,446 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:47,446 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:47,446 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:47,446 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-78ef48d1-5961-4f7d-bfe5-d837bc660b45
2024-03-21 05:47:47,446 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:47,446 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-u7_jailo
2024-03-21 05:47:47,446 - distributed.worker - INFO - Starting Worker plugin PreImport-f81cb402-fd42-48d9-9caa-3374ac60fc2d
2024-03-21 05:47:47,447 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e2e761e1-27aa-4da2-b36e-286f0141fe88
2024-03-21 05:47:47,447 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0d366c99-ab6c-4f1c-8918-e9b6defce29f
2024-03-21 05:47:47,447 - distributed.worker - INFO - Starting Worker plugin PreImport-415f290e-fae0-4dfc-b4d2-36164f2deddb
2024-03-21 05:47:47,447 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b05234f-643b-475b-9d9a-d7ea8375e509
2024-03-21 05:47:47,447 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:47,447 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:47,452 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:47,452 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42343
2024-03-21 05:47:47,453 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42343
2024-03-21 05:47:47,453 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36699
2024-03-21 05:47:47,453 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:47,453 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:47,453 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:47,453 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:47,453 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rpny228p
2024-03-21 05:47:47,453 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9d1f33ad-7aba-4a18-9e6e-007dd64ca8f7
2024-03-21 05:47:47,453 - distributed.worker - INFO - Starting Worker plugin RMMSetup-97228ba8-d4be-4d99-bd0f-dd6d994b7f40
2024-03-21 05:47:47,498 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:47,498 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:47,501 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:47,501 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:47,502 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:47,503 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43857
2024-03-21 05:47:47,503 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43857
2024-03-21 05:47:47,503 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44439
2024-03-21 05:47:47,503 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:47,504 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:47,504 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:47,504 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:47,504 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-7px992no
2024-03-21 05:47:47,504 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4dbd6ced-9cdf-4bd2-861a-bfe239c8d023
2024-03-21 05:47:47,504 - distributed.worker - INFO - Starting Worker plugin PreImport-dfe8d204-a06e-4447-81c4-ae9b3d6a9590
2024-03-21 05:47:47,505 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8850c639-170b-46e0-9f1f-b2398ca793e6
2024-03-21 05:47:47,505 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:47,506 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42057
2024-03-21 05:47:47,506 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42057
2024-03-21 05:47:47,506 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45005
2024-03-21 05:47:47,506 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:47,507 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:47,507 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:47,507 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:47,507 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2k8vce56
2024-03-21 05:47:47,507 - distributed.worker - INFO - Starting Worker plugin PreImport-f20487fb-0ec6-4a97-9ec7-8189c124dab2
2024-03-21 05:47:47,507 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9af3d765-18e8-4eaa-a402-3a37d04872fe
2024-03-21 05:47:47,507 - distributed.worker - INFO - Starting Worker plugin RMMSetup-38946d70-b841-44d9-86e9-3e79086234f1
2024-03-21 05:47:47,515 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:47,515 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:47,519 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:47,520 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36535
2024-03-21 05:47:47,520 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36535
2024-03-21 05:47:47,520 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43705
2024-03-21 05:47:47,520 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:47,521 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:47,521 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:47,521 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:47,521 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tyvf971n
2024-03-21 05:47:47,521 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-35f123ad-ac36-49d8-8367-4b1365d88827
2024-03-21 05:47:47,521 - distributed.worker - INFO - Starting Worker plugin PreImport-96f65867-a06c-4160-9d8e-505d3ed9250f
2024-03-21 05:47:47,521 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:47,521 - distributed.worker - INFO - Starting Worker plugin RMMSetup-604d54a0-8271-4659-9e9a-0c80644affe1
2024-03-21 05:47:47,521 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:47,526 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:47,527 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39043
2024-03-21 05:47:47,527 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39043
2024-03-21 05:47:47,527 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41613
2024-03-21 05:47:47,527 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:47,527 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:47,527 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:47,527 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:47,527 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mhii9a8_
2024-03-21 05:47:47,527 - distributed.worker - INFO - Starting Worker plugin PreImport-27e4cf65-1325-42cc-9967-417dc64c5265
2024-03-21 05:47:47,527 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e7df1a48-b7c2-4240-a41c-f324e2c0178e
2024-03-21 05:47:47,528 - distributed.worker - INFO - Starting Worker plugin RMMSetup-93b44e7e-f3c5-4bab-9ea9-a1a6d9ad4611
2024-03-21 05:47:47,570 - distributed.worker - INFO - Starting Worker plugin PreImport-543a23fd-0294-4b5c-a18a-e402bdcffbff
2024-03-21 05:47:47,571 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:47,594 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32883', status: init, memory: 0, processing: 0>
2024-03-21 05:47:47,596 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32883
2024-03-21 05:47:47,596 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45874
2024-03-21 05:47:47,597 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:47,597 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:47,598 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:47,599 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:49,053 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,059 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,081 - distributed.worker - INFO - Starting Worker plugin PreImport-1f1eebbd-03d6-407d-8846-7347a448a3d8
2024-03-21 05:47:49,082 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,087 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42363', status: init, memory: 0, processing: 0>
2024-03-21 05:47:49,088 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42363
2024-03-21 05:47:49,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45900
2024-03-21 05:47:49,089 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:49,090 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46345', status: init, memory: 0, processing: 0>
2024-03-21 05:47:49,090 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:49,091 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,091 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46345
2024-03-21 05:47:49,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45906
2024-03-21 05:47:49,092 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:49,093 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:49,093 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:49,093 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:49,111 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42343', status: init, memory: 0, processing: 0>
2024-03-21 05:47:49,112 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42343
2024-03-21 05:47:49,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45920
2024-03-21 05:47:49,113 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:49,114 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:49,114 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,116 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:49,151 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,168 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,174 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42057', status: init, memory: 0, processing: 0>
2024-03-21 05:47:49,174 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42057
2024-03-21 05:47:49,174 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45924
2024-03-21 05:47:49,175 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:49,176 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:49,176 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,176 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,177 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:49,179 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,192 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39043', status: init, memory: 0, processing: 0>
2024-03-21 05:47:49,192 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39043
2024-03-21 05:47:49,192 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45934
2024-03-21 05:47:49,193 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:49,194 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:49,194 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,195 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:49,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36535', status: init, memory: 0, processing: 0>
2024-03-21 05:47:49,199 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36535
2024-03-21 05:47:49,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45948
2024-03-21 05:47:49,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:49,200 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:49,200 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,201 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:49,206 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43857', status: init, memory: 0, processing: 0>
2024-03-21 05:47:49,207 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43857
2024-03-21 05:47:49,207 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45946
2024-03-21 05:47:49,208 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:49,209 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:49,209 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:49,211 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:49,257 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:49,258 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:49,258 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:49,258 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:49,258 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:49,259 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:49,259 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:49,258 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-03-21 05:47:49,263 - distributed.scheduler - INFO - Remove client Client-8994b9c3-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:49,263 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45868; closing.
2024-03-21 05:47:49,263 - distributed.scheduler - INFO - Remove client Client-8994b9c3-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:49,264 - distributed.scheduler - INFO - Close client connection: Client-8994b9c3-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:49,265 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36089'. Reason: nanny-close
2024-03-21 05:47:49,265 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:49,265 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33953'. Reason: nanny-close
2024-03-21 05:47:49,266 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:49,266 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34903'. Reason: nanny-close
2024-03-21 05:47:49,266 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:49,267 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42343. Reason: nanny-close
2024-03-21 05:47:49,267 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44925'. Reason: nanny-close
2024-03-21 05:47:49,267 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43857. Reason: nanny-close
2024-03-21 05:47:49,267 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:49,267 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35787'. Reason: nanny-close
2024-03-21 05:47:49,267 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42057. Reason: nanny-close
2024-03-21 05:47:49,268 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:49,268 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45401'. Reason: nanny-close
2024-03-21 05:47:49,268 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39043. Reason: nanny-close
2024-03-21 05:47:49,268 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:49,268 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34115'. Reason: nanny-close
2024-03-21 05:47:49,269 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:49,269 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42363. Reason: nanny-close
2024-03-21 05:47:49,269 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40009'. Reason: nanny-close
2024-03-21 05:47:49,269 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45920; closing.
2024-03-21 05:47:49,269 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:49,269 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:47:49,269 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46345. Reason: nanny-close
2024-03-21 05:47:49,269 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:49,269 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42343', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000069.269716')
2024-03-21 05:47:49,269 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:49,270 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:32883. Reason: nanny-close
2024-03-21 05:47:49,270 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36535. Reason: nanny-close
2024-03-21 05:47:49,270 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:49,270 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45934; closing.
2024-03-21 05:47:49,271 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45924; closing.
2024-03-21 05:47:49,271 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45946; closing.
2024-03-21 05:47:49,271 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:49,271 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:49,271 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:49,271 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:49,271 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:49,272 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39043', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000069.2721672')
2024-03-21 05:47:49,272 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42057', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000069.2726257')
2024-03-21 05:47:49,272 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:49,272 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:49,273 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43857', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000069.2730541')
2024-03-21 05:47:49,273 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:49,273 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:49,274 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:49,275 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:47:49,273 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45934>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45934>: Stream is closed
2024-03-21 05:47:49,276 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45900; closing.
2024-03-21 05:47:49,276 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42363', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000069.276912')
2024-03-21 05:47:49,277 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45906; closing.
2024-03-21 05:47:49,277 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45948; closing.
2024-03-21 05:47:49,278 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46345', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000069.2780223')
2024-03-21 05:47:49,278 - distributed.nanny - INFO - Worker closed
2024-03-21 05:47:49,278 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36535', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000069.2784212')
2024-03-21 05:47:49,278 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45874; closing.
2024-03-21 05:47:49,279 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32883', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000069.279618')
2024-03-21 05:47:49,279 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:47:49,280 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45874>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-03-21 05:47:50,080 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:47:50,081 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:47:50,081 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:47:50,082 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:47:50,082 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-03-21 05:47:52,120 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:47:52,124 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:47:52,127 - distributed.scheduler - INFO - State start
2024-03-21 05:47:52,147 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:47:52,148 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:47:52,149 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:47:52,149 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:47:52,243 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43217'
2024-03-21 05:47:52,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34397'
2024-03-21 05:47:52,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43117'
2024-03-21 05:47:52,277 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39525'
2024-03-21 05:47:52,281 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38307'
2024-03-21 05:47:52,289 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33783'
2024-03-21 05:47:52,300 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39623'
2024-03-21 05:47:52,309 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42947'
2024-03-21 05:47:53,619 - distributed.scheduler - INFO - Receive client connection: Client-8d98f93b-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:47:53,631 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59332
2024-03-21 05:47:53,907 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:53,908 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:53,912 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:53,912 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41159
2024-03-21 05:47:53,913 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41159
2024-03-21 05:47:53,913 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33731
2024-03-21 05:47:53,913 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:53,913 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:53,913 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:53,913 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:53,913 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f2a95nwr
2024-03-21 05:47:53,913 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e7e03e27-42bd-4b8f-9469-357448fdef8c
2024-03-21 05:47:53,913 - distributed.worker - INFO - Starting Worker plugin PreImport-cc011970-b01d-4c2d-9ec7-5f8f3930344f
2024-03-21 05:47:53,913 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e6727193-7615-4734-9e3a-5c96a0d67779
2024-03-21 05:47:54,149 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:54,149 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:54,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:54,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:54,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:54,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:54,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:54,150 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:54,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:54,150 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:54,153 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:54,153 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:54,154 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:54,154 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:54,154 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:54,155 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:54,155 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33395
2024-03-21 05:47:54,155 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44785
2024-03-21 05:47:54,155 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44785
2024-03-21 05:47:54,155 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33395
2024-03-21 05:47:54,155 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43013
2024-03-21 05:47:54,155 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33851
2024-03-21 05:47:54,155 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38069
2024-03-21 05:47:54,155 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43013
2024-03-21 05:47:54,155 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33997
2024-03-21 05:47:54,155 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:54,155 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:54,156 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:54,156 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:54,156 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:54,156 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:54,156 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:54,156 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:54,156 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:54,156 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:54,156 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:54,156 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-case8rqf
2024-03-21 05:47:54,156 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_wx2a9nb
2024-03-21 05:47:54,156 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:54,156 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5r5olmov
2024-03-21 05:47:54,156 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a26ebaf-96a8-4b03-9045-8b6217987fa0
2024-03-21 05:47:54,156 - distributed.worker - INFO - Starting Worker plugin PreImport-08562a33-362e-4df1-a032-605dd5705413
2024-03-21 05:47:54,156 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0ced5353-edfb-4a10-854d-da08126b5d6d
2024-03-21 05:47:54,156 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29ce0fc5-9877-4478-b034-79fd2029c670
2024-03-21 05:47:54,156 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39029
2024-03-21 05:47:54,156 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39029
2024-03-21 05:47:54,156 - distributed.worker - INFO - Starting Worker plugin PreImport-3b4d1a3f-92ae-4f88-b85a-7acc258acd47
2024-03-21 05:47:54,156 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39899
2024-03-21 05:47:54,156 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:54,156 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0efa6da-0d69-434f-8b8c-da1aa1563cd1
2024-03-21 05:47:54,156 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:54,156 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:54,156 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:54,156 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kaahdjqb
2024-03-21 05:47:54,157 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:54,157 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a7955621-1d52-41e5-9613-a467e347872d
2024-03-21 05:47:54,157 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e3869642-44b1-440d-904a-2ae5fa9b1e2c
2024-03-21 05:47:54,157 - distributed.worker - INFO - Starting Worker plugin RMMSetup-31ea861a-a3f1-4862-97fa-45707c669539
2024-03-21 05:47:54,157 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46565
2024-03-21 05:47:54,158 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46565
2024-03-21 05:47:54,158 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34237
2024-03-21 05:47:54,158 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:54,158 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:54,158 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:54,158 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:54,158 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vf1h3alw
2024-03-21 05:47:54,158 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f939147-a24c-4b92-a051-a035d206903f
2024-03-21 05:47:54,158 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:54,159 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46723
2024-03-21 05:47:54,159 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46723
2024-03-21 05:47:54,159 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36873
2024-03-21 05:47:54,159 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:54,159 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:54,159 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:54,159 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:54,160 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hhltdxza
2024-03-21 05:47:54,160 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c6439a36-7106-41d3-b90a-232569c789f3
2024-03-21 05:47:54,160 - distributed.worker - INFO - Starting Worker plugin PreImport-0d13b93b-3a1c-4230-a757-7ceb560abfa4
2024-03-21 05:47:54,160 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8977788-eba1-4466-af8c-7bdc8c0aac07
2024-03-21 05:47:54,190 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:47:54,190 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:47:54,194 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:47:54,195 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43469
2024-03-21 05:47:54,195 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43469
2024-03-21 05:47:54,195 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40417
2024-03-21 05:47:54,196 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:47:54,196 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:54,196 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:47:54,196 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:47:54,196 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ox7vvegc
2024-03-21 05:47:54,196 - distributed.worker - INFO - Starting Worker plugin PreImport-79e630c1-e562-4156-a50a-2fd4603c8975
2024-03-21 05:47:54,196 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ffc3e77f-fd0f-4842-9993-235810b97f8e
2024-03-21 05:47:54,196 - distributed.worker - INFO - Starting Worker plugin RMMSetup-47a6308a-9f2d-4043-963f-a8c1d77b2308
2024-03-21 05:47:54,373 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:54,399 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41159', status: init, memory: 0, processing: 0>
2024-03-21 05:47:54,400 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41159
2024-03-21 05:47:54,400 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59350
2024-03-21 05:47:54,401 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:54,402 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:54,402 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:54,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:55,918 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5ad501f3-2553-48fe-aa33-16f3c0801551
2024-03-21 05:47:55,919 - distributed.worker - INFO - Starting Worker plugin PreImport-23fef8f2-b78b-4e85-bd4a-b1ecc4e91510
2024-03-21 05:47:55,920 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,921 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,921 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5738e468-66cc-4e07-8eca-6783c329b51b
2024-03-21 05:47:55,923 - distributed.worker - INFO - Starting Worker plugin PreImport-6e70732d-a47e-4bec-8937-571c89e63736
2024-03-21 05:47:55,924 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,924 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,929 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,929 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-21 05:47:55,932 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44785. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:47:55,932 - distributed.worker - INFO - Starting Worker plugin PreImport-9ea70ba4-1871-4ee8-9375-43d08290e21e
2024-03-21 05:47:55,933 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,932 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:47:55,940 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:47:55,948 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46723', status: init, memory: 0, processing: 0>
2024-03-21 05:47:55,949 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46723
2024-03-21 05:47:55,949 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59380
2024-03-21 05:47:55,950 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:55,950 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46565', status: init, memory: 0, processing: 0>
2024-03-21 05:47:55,951 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:55,951 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,951 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46565
2024-03-21 05:47:55,951 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59358
2024-03-21 05:47:55,952 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:55,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:55,953 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:55,953 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,954 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:55,955 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43013', status: init, memory: 0, processing: 0>
2024-03-21 05:47:55,956 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43013
2024-03-21 05:47:55,956 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59374
2024-03-21 05:47:55,957 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:55,957 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39029', status: init, memory: 0, processing: 0>
2024-03-21 05:47:55,958 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39029
2024-03-21 05:47:55,958 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59390
2024-03-21 05:47:55,958 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:55,958 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,959 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43469', status: init, memory: 0, processing: 0>
2024-03-21 05:47:55,959 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:55,959 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43469
2024-03-21 05:47:55,959 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59396
2024-03-21 05:47:55,960 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:55,960 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,960 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:55,960 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:55,961 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:55,961 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:55,963 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:55,964 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33395', status: init, memory: 0, processing: 0>
2024-03-21 05:47:55,965 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33395
2024-03-21 05:47:55,965 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:59404
2024-03-21 05:47:55,966 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:47:55,967 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:47:55,967 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:47:55,969 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:47:55,983 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:47:55,986 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43217'. Reason: nanny-instantiate-failed
2024-03-21 05:47:55,986 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:47:56,036 - distributed.nanny - INFO - Worker process 43244 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:47:56,037 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:59268'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:59268>: Stream is closed
2024-03-21 05:47:56,039 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43275 parent=43076 started daemon>
2024-03-21 05:47:56,039 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43271 parent=43076 started daemon>
2024-03-21 05:47:56,040 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43267 parent=43076 started daemon>
2024-03-21 05:47:56,040 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43264 parent=43076 started daemon>
2024-03-21 05:47:56,040 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43258 parent=43076 started daemon>
2024-03-21 05:47:56,040 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43253 parent=43076 started daemon>
2024-03-21 05:47:56,040 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43249 parent=43076 started daemon>
2024-03-21 05:47:56,061 - distributed.core - INFO - Connection to tcp://127.0.0.1:59396 has been closed.
2024-03-21 05:47:56,061 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43469', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000076.0613353')
2024-03-21 05:47:56,063 - distributed.core - INFO - Connection to tcp://127.0.0.1:59380 has been closed.
2024-03-21 05:47:56,063 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46723', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000076.0634634')
2024-03-21 05:47:56,064 - distributed.core - INFO - Connection to tcp://127.0.0.1:59350 has been closed.
2024-03-21 05:47:56,064 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41159', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000076.0649111')
2024-03-21 05:47:56,065 - distributed.core - INFO - Connection to tcp://127.0.0.1:59374 has been closed.
2024-03-21 05:47:56,065 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43013', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000076.0652614')
2024-03-21 05:47:56,065 - distributed.core - INFO - Connection to tcp://127.0.0.1:59358 has been closed.
2024-03-21 05:47:56,065 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46565', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000076.065869')
2024-03-21 05:47:56,066 - distributed.core - INFO - Connection to tcp://127.0.0.1:59390 has been closed.
2024-03-21 05:47:56,066 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39029', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000076.0666468')
2024-03-21 05:47:56,068 - distributed.core - INFO - Connection to tcp://127.0.0.1:59404 has been closed.
2024-03-21 05:47:56,068 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33395', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000076.0684524')
2024-03-21 05:47:56,068 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:47:56,276 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43258 exit status was already read will report exitcode 255
2024-03-21 05:47:56,333 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43264 exit status was already read will report exitcode 255
2024-03-21 05:48:09,635 - distributed.scheduler - INFO - Remove client Client-8d98f93b-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:09,635 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:59332; closing.
2024-03-21 05:48:09,636 - distributed.scheduler - INFO - Remove client Client-8d98f93b-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:09,636 - distributed.scheduler - INFO - Close client connection: Client-8d98f93b-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:09,637 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:48:09,637 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:48:09,638 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:48:09,639 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:48:09,639 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-03-21 05:48:11,703 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:11,707 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:48:11,711 - distributed.scheduler - INFO - State start
2024-03-21 05:48:11,712 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-kaahdjqb', purging
2024-03-21 05:48:11,713 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5r5olmov', purging
2024-03-21 05:48:11,713 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vf1h3alw', purging
2024-03-21 05:48:11,713 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-case8rqf', purging
2024-03-21 05:48:11,714 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-hhltdxza', purging
2024-03-21 05:48:11,714 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-f2a95nwr', purging
2024-03-21 05:48:11,714 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ox7vvegc', purging
2024-03-21 05:48:11,734 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:11,735 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:48:11,735 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:48:11,735 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:48:11,777 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40579'
2024-03-21 05:48:11,789 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40039'
2024-03-21 05:48:11,797 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33929'
2024-03-21 05:48:11,811 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46513'
2024-03-21 05:48:11,815 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35377'
2024-03-21 05:48:11,823 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35633'
2024-03-21 05:48:11,832 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46133'
2024-03-21 05:48:11,840 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42405'
2024-03-21 05:48:12,991 - distributed.scheduler - INFO - Receive client connection: Client-9b0919a2-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:13,003 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49200
2024-03-21 05:48:13,544 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:13,544 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:13,548 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:13,549 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35981
2024-03-21 05:48:13,549 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35981
2024-03-21 05:48:13,549 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36445
2024-03-21 05:48:13,549 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:13,549 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:13,549 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:13,549 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:13,549 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-n53nemv2
2024-03-21 05:48:13,549 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-312c1c71-a6dc-4d23-a84c-5db58daf262d
2024-03-21 05:48:13,549 - distributed.worker - INFO - Starting Worker plugin PreImport-b730a0f3-1212-4bc4-8820-6d2db9306890
2024-03-21 05:48:13,549 - distributed.worker - INFO - Starting Worker plugin RMMSetup-165f2cbc-31f5-42c0-8aef-181136563cc1
2024-03-21 05:48:13,770 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:13,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:13,772 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:13,772 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:13,775 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:13,776 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:13,776 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:13,776 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37291
2024-03-21 05:48:13,776 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37291
2024-03-21 05:48:13,776 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38333
2024-03-21 05:48:13,776 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:13,776 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:13,776 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:13,777 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:13,777 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ar88e9fk
2024-03-21 05:48:13,777 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:13,777 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce81c2c0-2c2c-45fc-9010-9d4c72d47ebc
2024-03-21 05:48:13,777 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2a211841-dfc3-4f05-95d9-d7f4c325a8a3
2024-03-21 05:48:13,778 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44805
2024-03-21 05:48:13,778 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44805
2024-03-21 05:48:13,778 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39123
2024-03-21 05:48:13,778 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:13,778 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:13,778 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:13,778 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:13,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-uozhupq4
2024-03-21 05:48:13,778 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3952c34c-0aa9-4403-8587-bdf8e600b328
2024-03-21 05:48:13,779 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ba7139f-05a1-45ef-a0dd-a2b636f05885
2024-03-21 05:48:13,781 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:13,782 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46791
2024-03-21 05:48:13,782 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46791
2024-03-21 05:48:13,782 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42769
2024-03-21 05:48:13,782 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:13,782 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:13,782 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:13,782 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:13,782 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iyrvcnxq
2024-03-21 05:48:13,782 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5942dd6a-c8cb-415e-82a2-8d6ef438ae46
2024-03-21 05:48:13,783 - distributed.worker - INFO - Starting Worker plugin PreImport-ff2e0148-eaa0-4ca7-908c-090a8134d517
2024-03-21 05:48:13,783 - distributed.worker - INFO - Starting Worker plugin RMMSetup-023c7709-373b-405e-ab94-dc52f9ffb07e
2024-03-21 05:48:13,787 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:13,787 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:13,792 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:13,793 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38969
2024-03-21 05:48:13,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38969
2024-03-21 05:48:13,793 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39203
2024-03-21 05:48:13,793 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:13,793 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:13,793 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:13,793 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:13,793 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0yy3nxp_
2024-03-21 05:48:13,793 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5bf5eff3-71ed-4708-8d9f-003e711aec6f
2024-03-21 05:48:13,803 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:13,803 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:13,807 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:13,808 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37747
2024-03-21 05:48:13,808 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37747
2024-03-21 05:48:13,808 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34493
2024-03-21 05:48:13,809 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:13,809 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:13,809 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:13,809 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:13,809 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3hx7qyba
2024-03-21 05:48:13,809 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a9c16647-e50b-4ae5-8c89-c1be9cdf5269
2024-03-21 05:48:13,809 - distributed.worker - INFO - Starting Worker plugin PreImport-41367977-680a-44cf-be99-d4a57b0a2417
2024-03-21 05:48:13,809 - distributed.worker - INFO - Starting Worker plugin RMMSetup-891c8f86-48fb-47ca-8a10-38ddd950dcb7
2024-03-21 05:48:13,811 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:13,811 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:13,816 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:13,816 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37207
2024-03-21 05:48:13,817 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37207
2024-03-21 05:48:13,817 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46629
2024-03-21 05:48:13,817 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:13,817 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:13,817 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:13,817 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:13,817 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ylct34pm
2024-03-21 05:48:13,817 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f58596fa-d3d3-411c-9ed1-a6fa91d463b0
2024-03-21 05:48:13,817 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5741d0f0-c66a-407b-abc5-a045e1a45811
2024-03-21 05:48:13,818 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:13,819 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:13,823 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:13,824 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36651
2024-03-21 05:48:13,824 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36651
2024-03-21 05:48:13,824 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42399
2024-03-21 05:48:13,824 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:13,824 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:13,825 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:13,825 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:13,825 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pzvcirf2
2024-03-21 05:48:13,825 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1ae1c4af-a502-4582-af95-0b4166241895
2024-03-21 05:48:13,825 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0d4e1355-5ef8-4484-9056-ada2afe01f1b
2024-03-21 05:48:14,048 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:14,075 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35981', status: init, memory: 0, processing: 0>
2024-03-21 05:48:14,076 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35981
2024-03-21 05:48:14,076 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49226
2024-03-21 05:48:14,077 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:14,078 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:14,078 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:14,080 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:15,665 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,694 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46791', status: init, memory: 0, processing: 0>
2024-03-21 05:48:15,695 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46791
2024-03-21 05:48:15,695 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49322
2024-03-21 05:48:15,696 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:15,697 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:15,697 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,699 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:15,710 - distributed.worker - INFO - Starting Worker plugin PreImport-6e1bf78d-fece-44a6-82cc-e33a40d047d6
2024-03-21 05:48:15,711 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,724 - distributed.worker - INFO - Starting Worker plugin PreImport-dd7cb3bd-2f14-4c20-bab9-e60fba531092
2024-03-21 05:48:15,725 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,725 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-d3c0c565-3f52-4550-b4fd-8db0e566bde6
2024-03-21 05:48:15,726 - distributed.worker - INFO - Starting Worker plugin PreImport-027a3a3b-668c-4143-86c4-2c087705b57a
2024-03-21 05:48:15,727 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,727 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,729 - distributed.worker - INFO - Starting Worker plugin PreImport-d9b3d5ee-19e1-43bc-a14a-0b4ddfc5e72a
2024-03-21 05:48:15,729 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,729 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory
2024-03-21 05:48:15,732 - distributed.worker - INFO - Starting Worker plugin PreImport-5ae5db86-7540-47e3-8949-2c307dbb36ec
2024-03-21 05:48:15,733 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44805. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:48:15,733 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:48:15,741 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37291', status: init, memory: 0, processing: 0>
2024-03-21 05:48:15,739 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:48:15,742 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37291
2024-03-21 05:48:15,742 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49334
2024-03-21 05:48:15,743 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:15,744 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:15,744 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,746 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:15,753 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37207', status: init, memory: 0, processing: 0>
2024-03-21 05:48:15,753 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37207
2024-03-21 05:48:15,754 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49376
2024-03-21 05:48:15,755 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:15,755 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36651', status: init, memory: 0, processing: 0>
2024-03-21 05:48:15,755 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:15,755 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,755 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36651
2024-03-21 05:48:15,755 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49350
2024-03-21 05:48:15,757 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37747', status: init, memory: 0, processing: 0>
2024-03-21 05:48:15,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:15,757 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:15,757 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37747
2024-03-21 05:48:15,757 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49366
2024-03-21 05:48:15,758 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:15,758 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:15,753 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:48:15,759 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40579'. Reason: nanny-instantiate-failed
2024-03-21 05:48:15,759 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:48:15,759 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:15,760 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:15,760 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,761 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38969', status: init, memory: 0, processing: 0>
2024-03-21 05:48:15,761 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:15,761 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38969
2024-03-21 05:48:15,761 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49372
2024-03-21 05:48:15,763 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:15,764 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:15,764 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:15,766 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:15,812 - distributed.nanny - INFO - Worker process 43528 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:112: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:48:15,815 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43559 parent=43360 started daemon>
2024-03-21 05:48:15,815 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43555 parent=43360 started daemon>
2024-03-21 05:48:15,815 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43552 parent=43360 started daemon>
2024-03-21 05:48:15,816 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43548 parent=43360 started daemon>
2024-03-21 05:48:15,816 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43543 parent=43360 started daemon>
2024-03-21 05:48:15,816 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43537 parent=43360 started daemon>
2024-03-21 05:48:15,816 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43532 parent=43360 started daemon>
2024-03-21 05:48:15,813 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:49142'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49142>: Stream is closed
2024-03-21 05:48:15,841 - distributed.core - INFO - Connection to tcp://127.0.0.1:49350 has been closed.
2024-03-21 05:48:15,842 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36651', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000095.842125')
2024-03-21 05:48:15,843 - distributed.core - INFO - Connection to tcp://127.0.0.1:49226 has been closed.
2024-03-21 05:48:15,843 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35981', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000095.8438673')
2024-03-21 05:48:15,844 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49226>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:49226>: Stream is closed
2024-03-21 05:48:15,846 - distributed.core - INFO - Connection to tcp://127.0.0.1:49376 has been closed.
2024-03-21 05:48:15,846 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37207', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000095.846143')
2024-03-21 05:48:15,846 - distributed.core - INFO - Connection to tcp://127.0.0.1:49372 has been closed.
2024-03-21 05:48:15,846 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38969', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000095.8467584')
2024-03-21 05:48:15,847 - distributed.core - INFO - Connection to tcp://127.0.0.1:49334 has been closed.
2024-03-21 05:48:15,847 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37291', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000095.8471477')
2024-03-21 05:48:15,847 - distributed.core - INFO - Connection to tcp://127.0.0.1:49366 has been closed.
2024-03-21 05:48:15,847 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37747', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000095.8478901')
2024-03-21 05:48:15,848 - distributed.core - INFO - Connection to tcp://127.0.0.1:49322 has been closed.
2024-03-21 05:48:15,848 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46791', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000095.848268')
2024-03-21 05:48:15,848 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:48:15,976 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43532 exit status was already read will report exitcode 255
2024-03-21 05:48:16,006 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43543 exit status was already read will report exitcode 255
2024-03-21 05:48:16,046 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43537 exit status was already read will report exitcode 255
2024-03-21 05:48:16,076 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43548 exit status was already read will report exitcode 255
2024-03-21 05:48:16,108 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43552 exit status was already read will report exitcode 255
2024-03-21 05:48:17,651 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42535', status: init, memory: 0, processing: 0>
2024-03-21 05:48:17,652 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42535
2024-03-21 05:48:17,652 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49388
2024-03-21 05:48:19,140 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36727', status: init, memory: 0, processing: 0>
2024-03-21 05:48:19,140 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36727
2024-03-21 05:48:19,140 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49402
2024-03-21 05:48:19,180 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36759', status: init, memory: 0, processing: 0>
2024-03-21 05:48:19,181 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36759
2024-03-21 05:48:19,181 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49414
2024-03-21 05:48:19,243 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42129', status: init, memory: 0, processing: 0>
2024-03-21 05:48:19,244 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42129
2024-03-21 05:48:19,244 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49424
2024-03-21 05:48:19,245 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44099', status: init, memory: 0, processing: 0>
2024-03-21 05:48:19,245 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44099
2024-03-21 05:48:19,245 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49426
2024-03-21 05:48:19,288 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41221', status: init, memory: 0, processing: 0>
2024-03-21 05:48:19,288 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41221
2024-03-21 05:48:19,288 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49432
2024-03-21 05:48:19,306 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41069', status: init, memory: 0, processing: 0>
2024-03-21 05:48:19,306 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41069
2024-03-21 05:48:19,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49440
2024-03-21 05:48:19,316 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36523', status: init, memory: 0, processing: 0>
2024-03-21 05:48:19,317 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36523
2024-03-21 05:48:19,317 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:49456
2024-03-21 05:48:19,345 - distributed.scheduler - INFO - Remove client Client-9b0919a2-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:19,346 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49200; closing.
2024-03-21 05:48:19,346 - distributed.scheduler - INFO - Remove client Client-9b0919a2-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:19,346 - distributed.scheduler - INFO - Close client connection: Client-9b0919a2-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:19,352 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49424; closing.
2024-03-21 05:48:19,352 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42129', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000099.3523023')
2024-03-21 05:48:19,353 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49426; closing.
2024-03-21 05:48:19,354 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44099', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000099.3542042')
2024-03-21 05:48:19,354 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49456; closing.
2024-03-21 05:48:19,354 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49414; closing.
2024-03-21 05:48:19,355 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36523', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000099.3556619')
2024-03-21 05:48:19,356 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49402; closing.
2024-03-21 05:48:19,356 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36759', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000099.3562005')
2024-03-21 05:48:19,357 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36727', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000099.3571723')
2024-03-21 05:48:19,357 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49432; closing.
2024-03-21 05:48:19,357 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49388; closing.
2024-03-21 05:48:19,358 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41221', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000099.358315')
2024-03-21 05:48:19,358 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42535', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000099.3586524')
2024-03-21 05:48:19,359 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:49440; closing.
2024-03-21 05:48:19,359 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41069', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000099.3594317')
2024-03-21 05:48:19,359 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:48:20,320 - distributed.scheduler - INFO - Receive client connection: Client-9f678f49-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:20,321 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56428
2024-03-21 05:48:20,838 - distributed.scheduler - INFO - Receive client connection: Client-994252d2-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:20,838 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56448
2024-03-21 05:48:24,245 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43021', status: init, memory: 0, processing: 0>
2024-03-21 05:48:24,245 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43021
2024-03-21 05:48:24,246 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56562
2024-03-21 05:48:25,783 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37963', status: init, memory: 0, processing: 0>
2024-03-21 05:48:25,783 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37963
2024-03-21 05:48:25,783 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56588
2024-03-21 05:48:25,788 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36187', status: init, memory: 0, processing: 0>
2024-03-21 05:48:25,789 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36187
2024-03-21 05:48:25,789 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56596
2024-03-21 05:48:25,817 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42929', status: init, memory: 0, processing: 0>
2024-03-21 05:48:25,817 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42929
2024-03-21 05:48:25,817 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56628
2024-03-21 05:48:25,818 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46847', status: init, memory: 0, processing: 0>
2024-03-21 05:48:25,819 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46847
2024-03-21 05:48:25,819 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56610
2024-03-21 05:48:25,820 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33245', status: init, memory: 0, processing: 0>
2024-03-21 05:48:25,821 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33245
2024-03-21 05:48:25,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56622
2024-03-21 05:48:25,830 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46687', status: init, memory: 0, processing: 0>
2024-03-21 05:48:25,830 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46687
2024-03-21 05:48:25,831 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:56634
2024-03-21 05:48:25,868 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:56468'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56468>: Stream is closed
2024-03-21 05:48:25,894 - distributed.core - INFO - Connection to tcp://127.0.0.1:56562 has been closed.
2024-03-21 05:48:25,894 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43021', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000105.8947322')
2024-03-21 05:48:25,896 - distributed.core - INFO - Connection to tcp://127.0.0.1:56596 has been closed.
2024-03-21 05:48:25,896 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36187', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000105.8964252')
2024-03-21 05:48:25,896 - distributed.core - INFO - Connection to tcp://127.0.0.1:56588 has been closed.
2024-03-21 05:48:25,896 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37963', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000105.896835')
2024-03-21 05:48:25,897 - distributed.core - INFO - Connection to tcp://127.0.0.1:56628 has been closed.
2024-03-21 05:48:25,897 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42929', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000105.8972082')
2024-03-21 05:48:25,897 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56596>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56596>: Stream is closed
2024-03-21 05:48:25,897 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56588>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56588>: Stream is closed
2024-03-21 05:48:25,898 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56628>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:56628>: Stream is closed
2024-03-21 05:48:25,899 - distributed.core - INFO - Connection to tcp://127.0.0.1:56622 has been closed.
2024-03-21 05:48:25,899 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33245', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000105.8991396')
2024-03-21 05:48:25,899 - distributed.core - INFO - Connection to tcp://127.0.0.1:56634 has been closed.
2024-03-21 05:48:25,899 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46687', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000105.8995261')
2024-03-21 05:48:25,900 - distributed.core - INFO - Connection to tcp://127.0.0.1:56610 has been closed.
2024-03-21 05:48:25,900 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46847', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000105.9002023')
2024-03-21 05:48:25,900 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:48:36,399 - distributed.scheduler - INFO - Remove client Client-9f678f49-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:36,400 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56428; closing.
2024-03-21 05:48:36,400 - distributed.scheduler - INFO - Remove client Client-9f678f49-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:36,400 - distributed.scheduler - INFO - Close client connection: Client-9f678f49-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:36,527 - distributed.scheduler - INFO - Receive client connection: Client-a910904d-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:48:36,528 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53458
2024-03-21 05:48:36,890 - distributed.scheduler - INFO - Remove client Client-994252d2-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:36,890 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:56448; closing.
2024-03-21 05:48:36,891 - distributed.scheduler - INFO - Remove client Client-994252d2-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:36,891 - distributed.scheduler - INFO - Close client connection: Client-994252d2-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:48:36,893 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:48:36,893 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:48:36,894 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:48:36,896 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:48:36,896 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-03-21 05:48:38,997 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:39,002 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40587 instead
  warnings.warn(
2024-03-21 05:48:39,006 - distributed.scheduler - INFO - State start
2024-03-21 05:48:39,007 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0yy3nxp_', purging
2024-03-21 05:48:39,008 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ylct34pm', purging
2024-03-21 05:48:39,008 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-pzvcirf2', purging
2024-03-21 05:48:39,009 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-iyrvcnxq', purging
2024-03-21 05:48:39,009 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ar88e9fk', purging
2024-03-21 05:48:39,010 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-3hx7qyba', purging
2024-03-21 05:48:39,010 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-n53nemv2', purging
2024-03-21 05:48:39,029 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:39,030 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:48:39,030 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:48:39,031 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-21 05:48:39,358 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37655'
2024-03-21 05:48:39,369 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38381'
2024-03-21 05:48:39,383 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38871'
2024-03-21 05:48:39,386 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43277'
2024-03-21 05:48:39,397 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41475'
2024-03-21 05:48:39,409 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34463'
2024-03-21 05:48:39,424 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45841'
2024-03-21 05:48:39,434 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39873'
2024-03-21 05:48:41,320 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:41,320 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:41,323 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:41,324 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:41,325 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:41,326 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34711
2024-03-21 05:48:41,326 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34711
2024-03-21 05:48:41,326 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35767
2024-03-21 05:48:41,326 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:41,326 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:41,326 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:41,326 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:41,326 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h_fc81ha
2024-03-21 05:48:41,326 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4661a2e5-a22a-41d6-84f4-d6fa9cee5501
2024-03-21 05:48:41,327 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fdf8a972-f16b-4924-bb7e-036bf9198216
2024-03-21 05:48:41,329 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:41,329 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35843
2024-03-21 05:48:41,329 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35843
2024-03-21 05:48:41,330 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39711
2024-03-21 05:48:41,330 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:41,330 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:41,330 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:41,330 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:41,330 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zgrulfvr
2024-03-21 05:48:41,330 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-69956fdc-10b3-4530-8f14-fbab9cb23dab
2024-03-21 05:48:41,331 - distributed.worker - INFO - Starting Worker plugin PreImport-58336920-fd7f-4303-8156-4affc186adac
2024-03-21 05:48:41,332 - distributed.worker - INFO - Starting Worker plugin RMMSetup-72d806b5-76ce-4c3c-aa24-ac484726c3da
2024-03-21 05:48:41,506 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:41,506 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:41,507 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:41,507 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:41,511 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:41,511 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:41,512 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45213
2024-03-21 05:48:41,512 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45213
2024-03-21 05:48:41,512 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34891
2024-03-21 05:48:41,512 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45673
2024-03-21 05:48:41,512 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:41,512 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45673
2024-03-21 05:48:41,512 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:41,512 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:41,512 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38775
2024-03-21 05:48:41,512 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:41,512 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:41,513 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:41,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-refjrhej
2024-03-21 05:48:41,513 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:41,513 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:41,513 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-311j0aqj
2024-03-21 05:48:41,513 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e486df9a-a1a0-4cfc-82b2-5421156e80e3
2024-03-21 05:48:41,513 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3a4f6ee1-fb47-469c-aad1-5f2e632cffd5
2024-03-21 05:48:41,513 - distributed.worker - INFO - Starting Worker plugin PreImport-0ba3a22c-6454-490a-94ec-e1c99c194ef9
2024-03-21 05:48:41,514 - distributed.worker - INFO - Starting Worker plugin RMMSetup-65f1d223-279c-4be7-8772-af0fa72a6681
2024-03-21 05:48:41,527 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:41,527 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:41,531 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:41,532 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39619
2024-03-21 05:48:41,532 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39619
2024-03-21 05:48:41,532 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45721
2024-03-21 05:48:41,532 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:41,532 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:41,532 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:41,532 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:41,532 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m26vt_vb
2024-03-21 05:48:41,533 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6ee02193-e216-44cd-aa19-1b356e0a8c39
2024-03-21 05:48:41,533 - distributed.worker - INFO - Starting Worker plugin PreImport-066e3a46-285a-4d73-b79b-fd1a51023ead
2024-03-21 05:48:41,533 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f0c7d323-69fd-4dae-97c1-00a899cd27c4
2024-03-21 05:48:41,537 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:41,537 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:41,541 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:41,542 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42643
2024-03-21 05:48:41,542 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42643
2024-03-21 05:48:41,542 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39471
2024-03-21 05:48:41,542 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:41,543 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:41,543 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:41,543 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:41,543 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tis7qgup
2024-03-21 05:48:41,543 - distributed.worker - INFO - Starting Worker plugin PreImport-b0b9e39f-8910-4ce4-9eec-bdbcb7b0a88e
2024-03-21 05:48:41,543 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5bfaa00e-0f15-4e9b-89b6-f096f8e1c46d
2024-03-21 05:48:41,543 - distributed.worker - INFO - Starting Worker plugin RMMSetup-564949be-7131-491b-bb77-e44e5dc2808e
2024-03-21 05:48:41,703 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:41,703 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:41,710 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:41,712 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34857
2024-03-21 05:48:41,712 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34857
2024-03-21 05:48:41,712 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34713
2024-03-21 05:48:41,712 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:41,712 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:41,712 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:41,712 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:41,712 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-465u1p6w
2024-03-21 05:48:41,713 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e7aaddf5-ca9b-40a5-99cd-cb17162499d7
2024-03-21 05:48:41,713 - distributed.worker - INFO - Starting Worker plugin PreImport-034234a2-efb7-4098-aa7d-e40373840765
2024-03-21 05:48:41,713 - distributed.worker - INFO - Starting Worker plugin RMMSetup-738c1023-3d3d-49c7-9814-97905c795718
2024-03-21 05:48:41,721 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:41,721 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:41,727 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:41,729 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33795
2024-03-21 05:48:41,729 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33795
2024-03-21 05:48:41,729 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42751
2024-03-21 05:48:41,729 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:41,729 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:41,729 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:41,729 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:48:41,729 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yptrlu2y
2024-03-21 05:48:41,729 - distributed.worker - INFO - Starting Worker plugin PreImport-a1be6c17-245f-4335-aa69-6a5d99d41c63
2024-03-21 05:48:41,730 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f34b9ac1-0b82-4616-b6eb-079e47d47b17
2024-03-21 05:48:41,731 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8da6ae4a-b356-4e34-876b-374d24b46d50
2024-03-21 05:48:43,755 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,760 - distributed.worker - INFO - Starting Worker plugin PreImport-547ac405-d652-4d1a-9d2e-955fc575327b
2024-03-21 05:48:43,761 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,776 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cbb0ae54-00b6-499d-8174-21c85a0d6755
2024-03-21 05:48:43,778 - distributed.worker - INFO - Starting Worker plugin PreImport-6820301b-af64-4c32-9a3a-6e629c8c4d78
2024-03-21 05:48:43,779 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,786 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:43,787 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:43,787 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,788 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:43,794 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:43,795 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:43,796 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,797 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:43,804 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,809 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,813 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:43,814 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:43,814 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,816 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:43,824 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,827 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,831 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:43,831 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:43,832 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,833 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:43,842 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:43,843 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:43,843 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,845 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:43,845 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:43,845 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:43,845 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,847 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:43,849 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:48:43,852 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42643. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:48:43,853 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:48:43,860 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:48:43,862 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:43,863 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:43,863 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:43,865 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:43,872 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:48:43,875 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37655'. Reason: nanny-instantiate-failed
2024-03-21 05:48:43,875 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:48:43,926 - distributed.nanny - INFO - Worker process 43812 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:48:43,930 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43843 parent=43644 started daemon>
2024-03-21 05:48:43,930 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43839 parent=43644 started daemon>
2024-03-21 05:48:43,930 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43835 parent=43644 started daemon>
2024-03-21 05:48:43,930 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43832 parent=43644 started daemon>
2024-03-21 05:48:43,930 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43827 parent=43644 started daemon>
2024-03-21 05:48:43,931 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43821 parent=43644 started daemon>
2024-03-21 05:48:43,931 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=43816 parent=43644 started daemon>
2024-03-21 05:48:44,082 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43835 exit status was already read will report exitcode 255
2024-03-21 05:48:44,110 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43821 exit status was already read will report exitcode 255
2024-03-21 05:48:44,173 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43832 exit status was already read will report exitcode 255
2024-03-21 05:48:44,199 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 43843 exit status was already read will report exitcode 255
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-03-21 05:48:57,381 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:57,386 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42139 instead
  warnings.warn(
2024-03-21 05:48:57,390 - distributed.scheduler - INFO - State start
2024-03-21 05:48:57,391 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-h_fc81ha', purging
2024-03-21 05:48:57,392 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-311j0aqj', purging
2024-03-21 05:48:57,392 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-refjrhej', purging
2024-03-21 05:48:57,393 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-yptrlu2y', purging
2024-03-21 05:48:57,393 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-zgrulfvr', purging
2024-03-21 05:48:57,393 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-465u1p6w', purging
2024-03-21 05:48:57,394 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-m26vt_vb', purging
2024-03-21 05:48:57,446 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:48:57,447 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:48:57,447 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:48:57,448 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-21 05:48:57,534 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44861'
2024-03-21 05:48:59,085 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:48:59,085 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:48:59,589 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:48:59,592 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45647
2024-03-21 05:48:59,592 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45647
2024-03-21 05:48:59,592 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-03-21 05:48:59,592 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:48:59,592 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:59,592 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:48:59,592 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:48:59,592 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zg9fjodt
2024-03-21 05:48:59,593 - distributed.worker - INFO - Starting Worker plugin PreImport-c5ea8660-748d-499b-b5fd-29320573ab85
2024-03-21 05:48:59,594 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ba049e62-8381-4949-a759-daded204505d
2024-03-21 05:48:59,594 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c7a70fd5-11b3-4fcf-b0ad-b2cd59066e7b
2024-03-21 05:48:59,594 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:59,644 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:48:59,645 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:48:59,645 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:48:59,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:48:59,691 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:48:59,695 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44861'. Reason: nanny-close
2024-03-21 05:48:59,696 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:48:59,697 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45647. Reason: nanny-close
2024-03-21 05:48:59,699 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:48:59,700 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-03-21 05:49:03,874 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:03,878 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44967 instead
  warnings.warn(
2024-03-21 05:49:03,881 - distributed.scheduler - INFO - State start
2024-03-21 05:49:03,902 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:03,902 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:49:03,903 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:03,904 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
2024-03-21 05:49:04,007 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35113'
2024-03-21 05:49:05,576 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:05,576 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:06,083 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:06,084 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41917
2024-03-21 05:49:06,084 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41917
2024-03-21 05:49:06,084 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45785
2024-03-21 05:49:06,085 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:06,085 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:06,085 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:06,085 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:49:06,085 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lf5djomz
2024-03-21 05:49:06,085 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57f61974-e3b4-420f-96f9-bbe510454df1
2024-03-21 05:49:06,086 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f3d125a4-7b68-4901-8216-ac8256d81cee
2024-03-21 05:49:06,086 - distributed.worker - INFO - Starting Worker plugin PreImport-e9807342-adc7-460f-88ae-aaae0117958b
2024-03-21 05:49:06,087 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:06,146 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:06,147 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:06,147 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:06,148 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:06,209 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:49:06,213 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35113'. Reason: nanny-close
2024-03-21 05:49:06,213 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:49:06,214 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41917. Reason: nanny-close
2024-03-21 05:49:06,216 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-03-21 05:49:06,218 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-03-21 05:49:08,616 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:08,620 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42897 instead
  warnings.warn(
2024-03-21 05:49:08,624 - distributed.scheduler - INFO - State start
2024-03-21 05:49:08,649 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:08,650 - distributed.scheduler - INFO - Scheduler closing due to failure-to-start-<class 'OSError'>...
2024-03-21 05:49:08,651 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:08,652 - distributed.scheduler - INFO - End scheduler
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 628, in start
    sockets = netutil.bind_sockets(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/netutil.py", line 162, in bind_sockets
    sock.bind(sockaddr)
OSError: [Errno 98] Address already in use

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 251, in main
    asyncio_run(run(), loop_factory=get_loop_factory())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/compatibility.py", line 236, in asyncio_run
    return loop.run_until_complete(main)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/base_events.py", line 647, in run_until_complete
    return future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in run
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 247, in <listcomp>
    [task.result() for task in done]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/cli/dask_scheduler.py", line 227, in wait_for_scheduler_to_finish
    await scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-03-21 05:49:12,843 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:12,847 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37047 instead
  warnings.warn(
2024-03-21 05:49:12,851 - distributed.scheduler - INFO - State start
2024-03-21 05:49:12,871 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:12,872 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-03-21 05:49:12,873 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:37047/status
2024-03-21 05:49:12,873 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:49:13,032 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33243'
2024-03-21 05:49:13,280 - distributed.scheduler - INFO - Receive client connection: Client-bdc12587-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:13,291 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54828
2024-03-21 05:49:14,558 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:14,558 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:14,561 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:14,562 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40071
2024-03-21 05:49:14,562 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40071
2024-03-21 05:49:14,562 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46459
2024-03-21 05:49:14,562 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-03-21 05:49:14,562 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:14,562 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:14,563 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:49:14,563 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-m82wgai2
2024-03-21 05:49:14,563 - distributed.worker - INFO - Starting Worker plugin PreImport-21d50039-28a5-4b07-b209-cde944e3a8ab
2024-03-21 05:49:14,563 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b5af537f-69bf-40d5-8de2-8f376430a571
2024-03-21 05:49:14,563 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dcb560ec-f4a4-4225-b0eb-693f330c497d
2024-03-21 05:49:14,563 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:14,608 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40071', status: init, memory: 0, processing: 0>
2024-03-21 05:49:14,609 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40071
2024-03-21 05:49:14,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54858
2024-03-21 05:49:14,609 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:14,610 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-03-21 05:49:14,610 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:14,611 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-03-21 05:49:14,613 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:49:14,615 - distributed.scheduler - INFO - Remove client Client-bdc12587-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:14,616 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54828; closing.
2024-03-21 05:49:14,616 - distributed.scheduler - INFO - Remove client Client-bdc12587-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:14,616 - distributed.scheduler - INFO - Close client connection: Client-bdc12587-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:14,617 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33243'. Reason: nanny-close
2024-03-21 05:49:14,655 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-03-21 05:49:14,657 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40071. Reason: nanny-close
2024-03-21 05:49:14,658 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-03-21 05:49:14,658 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:54858; closing.
2024-03-21 05:49:14,658 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40071', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000154.658868')
2024-03-21 05:49:14,659 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:49:14,659 - distributed.nanny - INFO - Worker closed
2024-03-21 05:49:15,082 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:49:15,082 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:49:15,082 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:15,083 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-03-21 05:49:15,083 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-03-21 05:49:17,086 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:17,090 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:49:17,093 - distributed.scheduler - INFO - State start
2024-03-21 05:49:17,114 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:17,114 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:49:17,115 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:49:17,115 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:49:17,216 - distributed.scheduler - INFO - Receive client connection: Client-c08970c1-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:17,226 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36284
2024-03-21 05:49:17,234 - distributed.scheduler - INFO - Receive client connection: Client-c03b7e97-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:17,234 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36308
2024-03-21 05:49:17,241 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40353'
2024-03-21 05:49:17,252 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41923'
2024-03-21 05:49:17,260 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40951'
2024-03-21 05:49:17,274 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42471'
2024-03-21 05:49:17,278 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36741'
2024-03-21 05:49:17,287 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38359'
2024-03-21 05:49:17,297 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42189'
2024-03-21 05:49:17,307 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36547'
2024-03-21 05:49:18,954 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:18,954 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:18,958 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:18,959 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39257
2024-03-21 05:49:18,960 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39257
2024-03-21 05:49:18,960 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40903
2024-03-21 05:49:18,960 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:18,960 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:18,960 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:18,960 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:18,960 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ha8cxhgj
2024-03-21 05:49:18,960 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-303e3ec3-385e-4d71-8e8a-7f410283972e
2024-03-21 05:49:18,961 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4ffc9d88-a1ac-4f47-969b-df2060c317bc
2024-03-21 05:49:19,189 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:19,189 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:19,193 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:19,194 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41233
2024-03-21 05:49:19,194 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41233
2024-03-21 05:49:19,194 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33709
2024-03-21 05:49:19,194 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:19,195 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:19,195 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:19,195 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:19,195 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nkullzqz
2024-03-21 05:49:19,195 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ce07d738-ecc9-4d58-bfd0-8c2067a65daf
2024-03-21 05:49:19,195 - distributed.worker - INFO - Starting Worker plugin PreImport-b643eb09-38a7-4bef-8fed-495a73fd63b7
2024-03-21 05:49:19,195 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f257d158-7c93-4eb1-abf4-fe7661c6f681
2024-03-21 05:49:19,198 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:19,199 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:19,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:19,203 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:19,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:19,203 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:19,203 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:19,204 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42777
2024-03-21 05:49:19,204 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42777
2024-03-21 05:49:19,204 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45549
2024-03-21 05:49:19,204 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:19,204 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:19,204 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:19,204 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:19,204 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j24r9mh6
2024-03-21 05:49:19,205 - distributed.worker - INFO - Starting Worker plugin PreImport-dd03f685-fd39-406d-97e9-f45c0e46121d
2024-03-21 05:49:19,205 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-85db4228-1ce6-4ec0-acdc-817e504426a7
2024-03-21 05:49:19,205 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ab88f0e4-e72c-479e-8b52-586964d020a5
2024-03-21 05:49:19,206 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:19,206 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:19,207 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:19,208 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:19,208 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41249
2024-03-21 05:49:19,208 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41249
2024-03-21 05:49:19,208 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40445
2024-03-21 05:49:19,208 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:19,208 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:19,209 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:19,209 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33435
2024-03-21 05:49:19,209 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:19,209 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zqfmi1xy
2024-03-21 05:49:19,209 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33435
2024-03-21 05:49:19,209 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42113
2024-03-21 05:49:19,209 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:19,209 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:19,209 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:19,209 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-15e9c0a1-9abf-4b45-ac36-aa23140de68d
2024-03-21 05:49:19,209 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:19,209 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d2dypkrp
2024-03-21 05:49:19,209 - distributed.worker - INFO - Starting Worker plugin PreImport-96490db7-cee5-4bca-a196-486a95cb3427
2024-03-21 05:49:19,209 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10dd4752-a929-4bc0-a923-c2b49fc4e5b5
2024-03-21 05:49:19,209 - distributed.worker - INFO - Starting Worker plugin PreImport-c3980d98-6d81-43be-b0ec-5286996d00d4
2024-03-21 05:49:19,209 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-e3924c6b-eeba-4623-97b2-4c38232df6e1
2024-03-21 05:49:19,211 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:19,211 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:32851
2024-03-21 05:49:19,211 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:32851
2024-03-21 05:49:19,212 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32957
2024-03-21 05:49:19,211 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5867b20e-ff36-4a3c-bb23-b21a781ca1d1
2024-03-21 05:49:19,212 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:19,212 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:19,212 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:19,212 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:19,212 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vvifc2mj
2024-03-21 05:49:19,212 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4fc43234-a70f-4767-8eb5-de8706f3aae5
2024-03-21 05:49:19,212 - distributed.worker - INFO - Starting Worker plugin PreImport-76f18518-6fca-49bf-845f-0fc41e951c62
2024-03-21 05:49:19,212 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f76df8a5-9ec4-4323-b4cc-f7c0a7dc3f1e
2024-03-21 05:49:19,230 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:19,230 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:19,231 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:19,231 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:19,234 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:19,235 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:19,235 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33639
2024-03-21 05:49:19,235 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33639
2024-03-21 05:49:19,235 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43309
2024-03-21 05:49:19,236 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:19,236 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:19,236 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:19,236 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:19,236 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1fuuo57k
2024-03-21 05:49:19,236 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7379827c-070d-479b-84d8-31be844cf20a
2024-03-21 05:49:19,236 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46297
2024-03-21 05:49:19,236 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46297
2024-03-21 05:49:19,236 - distributed.worker - INFO - Starting Worker plugin PreImport-bb6b6f09-7cc2-4dc7-81d4-5c10a3279473
2024-03-21 05:49:19,236 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42107
2024-03-21 05:49:19,236 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:19,236 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1fb484e-f214-4936-81ba-71bc5edf4014
2024-03-21 05:49:19,236 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:19,236 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:19,236 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 05:49:19,236 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9bgdy1pq
2024-03-21 05:49:19,237 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85dea419-cc6d-4878-a042-b764275a2440
2024-03-21 05:49:19,388 - distributed.worker - INFO - Starting Worker plugin PreImport-ea6f2b14-f1a6-4b41-98fa-6fe70db4c5c0
2024-03-21 05:49:19,389 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:19,415 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39257', status: init, memory: 0, processing: 0>
2024-03-21 05:49:19,417 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39257
2024-03-21 05:49:19,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36380
2024-03-21 05:49:19,418 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:19,419 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:19,419 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:19,421 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:19,479 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:49:19,482 - distributed.scheduler - INFO - Remove client Client-c08970c1-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:19,482 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36284; closing.
2024-03-21 05:49:19,483 - distributed.scheduler - INFO - Remove client Client-c08970c1-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:19,483 - distributed.scheduler - INFO - Close client connection: Client-c08970c1-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:20,900 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:20,930 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41233', status: init, memory: 0, processing: 0>
2024-03-21 05:49:20,930 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41233
2024-03-21 05:49:20,931 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47236
2024-03-21 05:49:20,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:20,933 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:20,933 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:20,935 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:20,953 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:20,962 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46303', status: init, memory: 0, processing: 0>
2024-03-21 05:49:20,962 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46303
2024-03-21 05:49:20,962 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36384
2024-03-21 05:49:20,974 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:32851', status: init, memory: 0, processing: 0>
2024-03-21 05:49:20,975 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:32851
2024-03-21 05:49:20,975 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47242
2024-03-21 05:49:20,976 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:20,977 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:20,977 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:20,978 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:20,985 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:21,008 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-021df4af-7614-4575-b768-d43a8dd34a62
2024-03-21 05:49:21,009 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:21,009 - distributed.worker - INFO - Starting Worker plugin PreImport-881120c0-d52d-41ff-9534-7d3fab7ed951
2024-03-21 05:49:21,010 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:21,014 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33435', status: init, memory: 0, processing: 0>
2024-03-21 05:49:21,015 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33435
2024-03-21 05:49:21,015 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47252
2024-03-21 05:49:21,016 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:21,017 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:21,017 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:21,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:21,020 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36384; closing.
2024-03-21 05:49:21,020 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46303', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000161.0203738')
2024-03-21 05:49:21,023 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:21,030 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46297', status: init, memory: 0, processing: 0>
2024-03-21 05:49:21,031 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46297
2024-03-21 05:49:21,031 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47276
2024-03-21 05:49:21,032 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33639', status: init, memory: 0, processing: 0>
2024-03-21 05:49:21,032 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:21,032 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33639
2024-03-21 05:49:21,032 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47262
2024-03-21 05:49:21,032 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:21,033 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:21,033 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:21,034 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:21,034 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:21,034 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:21,035 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:21,054 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41249', status: init, memory: 0, processing: 0>
2024-03-21 05:49:21,055 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41249
2024-03-21 05:49:21,055 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47282
2024-03-21 05:49:21,056 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:49:21,057 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-03-21 05:49:21,057 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:21,059 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-03-21 05:49:21,065 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:49:21,069 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42777. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:49:21,069 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:49:21,075 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:49:21,088 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:49:21,091 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40353'. Reason: nanny-instantiate-failed
2024-03-21 05:49:21,091 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:49:21,145 - distributed.nanny - INFO - Worker process 44908 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:49:21,146 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:36320'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:36320>: Stream is closed
2024-03-21 05:49:21,149 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44939 parent=44739 started daemon>
2024-03-21 05:49:21,149 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44935 parent=44739 started daemon>
2024-03-21 05:49:21,149 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44931 parent=44739 started daemon>
2024-03-21 05:49:21,149 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44928 parent=44739 started daemon>
2024-03-21 05:49:21,150 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44922 parent=44739 started daemon>
2024-03-21 05:49:21,150 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44917 parent=44739 started daemon>
2024-03-21 05:49:21,150 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=44912 parent=44739 started daemon>
2024-03-21 05:49:21,173 - distributed.core - INFO - Connection to tcp://127.0.0.1:47276 has been closed.
2024-03-21 05:49:21,173 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46297', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000161.1731799')
2024-03-21 05:49:21,174 - distributed.core - INFO - Connection to tcp://127.0.0.1:47262 has been closed.
2024-03-21 05:49:21,174 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33639', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000161.1746964')
2024-03-21 05:49:21,175 - distributed.core - INFO - Connection to tcp://127.0.0.1:47242 has been closed.
2024-03-21 05:49:21,175 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:32851', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000161.1750824')
2024-03-21 05:49:21,175 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47242>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47242>: Stream is closed
2024-03-21 05:49:21,176 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47262>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 297, in write
    raise StreamClosedError()
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 307, in write
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:47262>: Stream is closed
2024-03-21 05:49:21,177 - distributed.core - INFO - Connection to tcp://127.0.0.1:36380 has been closed.
2024-03-21 05:49:21,177 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39257', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000161.1774669')
2024-03-21 05:49:21,178 - distributed.core - INFO - Connection to tcp://127.0.0.1:47282 has been closed.
2024-03-21 05:49:21,178 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41249', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000161.1782763')
2024-03-21 05:49:21,178 - distributed.core - INFO - Connection to tcp://127.0.0.1:47252 has been closed.
2024-03-21 05:49:21,178 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33435', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000161.1786563')
2024-03-21 05:49:21,178 - distributed.core - INFO - Connection to tcp://127.0.0.1:47236 has been closed.
2024-03-21 05:49:21,179 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41233', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000161.1790144')
2024-03-21 05:49:21,179 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:49:21,268 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44922 exit status was already read will report exitcode 255
2024-03-21 05:49:21,383 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44928 exit status was already read will report exitcode 255
2024-03-21 05:49:21,406 - distributed.process - WARNING - [<AsyncProcess Dask Worker process (from Nanny)>] process 44912 exit status was already read will report exitcode 255
2024-03-21 05:49:23,541 - distributed.scheduler - INFO - Receive client connection: Client-c51655d6-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:23,541 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47288
2024-03-21 05:49:27,417 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44695', status: init, memory: 0, processing: 0>
2024-03-21 05:49:27,417 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44695
2024-03-21 05:49:27,417 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47304
2024-03-21 05:49:27,497 - distributed.scheduler - INFO - Remove client Client-c51655d6-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:27,497 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47288; closing.
2024-03-21 05:49:27,498 - distributed.scheduler - INFO - Remove client Client-c51655d6-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:27,498 - distributed.scheduler - INFO - Close client connection: Client-c51655d6-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:27,502 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47304; closing.
2024-03-21 05:49:27,503 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44695', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000167.5029793')
2024-03-21 05:49:27,503 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:49:31,426 - distributed.scheduler - INFO - Remove client Client-c03b7e97-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:31,426 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36308; closing.
2024-03-21 05:49:31,426 - distributed.scheduler - INFO - Remove client Client-c03b7e97-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:31,426 - distributed.scheduler - INFO - Close client connection: Client-c03b7e97-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:31,427 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:49:31,428 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:49:31,428 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:31,429 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:49:31,430 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-03-21 05:49:33,445 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:33,449 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:49:33,452 - distributed.scheduler - INFO - State start
2024-03-21 05:49:33,453 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1fuuo57k', purging
2024-03-21 05:49:33,454 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-zqfmi1xy', purging
2024-03-21 05:49:33,454 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-vvifc2mj', purging
2024-03-21 05:49:33,455 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ha8cxhgj', purging
2024-03-21 05:49:33,455 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-d2dypkrp', purging
2024-03-21 05:49:33,455 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-nkullzqz', purging
2024-03-21 05:49:33,455 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-9bgdy1pq', purging
2024-03-21 05:49:33,475 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:33,476 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:49:33,476 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:49:33,477 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:49:33,532 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43263'
2024-03-21 05:49:35,076 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:35,076 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:35,079 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:35,080 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41185
2024-03-21 05:49:35,080 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41185
2024-03-21 05:49:35,080 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36983
2024-03-21 05:49:35,080 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:35,080 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:35,080 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:35,080 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:49:35,080 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2_tk0v4b
2024-03-21 05:49:35,081 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-72723327-8ab3-494a-8c3e-c8da380debfc
2024-03-21 05:49:35,081 - distributed.worker - INFO - Starting Worker plugin RMMSetup-76a52a8c-6f49-41e4-aad5-b4c415164397
2024-03-21 05:49:35,363 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:49:35,364 - distributed.worker - INFO - Starting Worker plugin PreImport-c9e0806e-20fd-4596-aae7-46b823848179
2024-03-21 05:49:35,364 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41185. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:49:35,364 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:49:35,366 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:49:35,407 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:49:35,410 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43263'. Reason: nanny-instantiate-failed
2024-03-21 05:49:35,410 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:49:35,464 - distributed.nanny - INFO - Worker process 45191 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:49:35,466 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:50528'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:50528>: Stream is closed
2024-03-21 05:49:35,556 - distributed.scheduler - INFO - Receive client connection: Client-ca01b67d-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:35,568 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50530
2024-03-21 05:49:36,544 - distributed.scheduler - INFO - Receive client connection: Client-ccd675aa-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:36,545 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:50558
2024-03-21 05:49:42,142 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44373', status: init, memory: 0, processing: 0>
2024-03-21 05:49:42,143 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44373
2024-03-21 05:49:42,143 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53276
2024-03-21 05:49:42,165 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:49:42,166 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:49:42,169 - distributed.scheduler - INFO - Remove client Client-ca01b67d-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:42,170 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:50530; closing.
2024-03-21 05:49:42,170 - distributed.scheduler - INFO - Remove client Client-ca01b67d-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:42,170 - distributed.scheduler - INFO - Close client connection: Client-ca01b67d-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:42,171 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:49:42,171 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:49:42,172 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:42,173 - distributed.core - INFO - Connection to tcp://127.0.0.1:53276 has been closed.
2024-03-21 05:49:42,173 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44373', status: running, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1711000182.1732163')
2024-03-21 05:49:42,173 - distributed.scheduler - INFO - Lost all workers
2024-03-21 05:49:42,175 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:49:42,176 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-03-21 05:49:44,183 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:44,187 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-03-21 05:49:44,190 - distributed.scheduler - INFO - State start
2024-03-21 05:49:44,210 - distributed.scheduler - INFO - -----------------------------------------------
2024-03-21 05:49:44,211 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-03-21 05:49:44,211 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-03-21 05:49:44,212 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-03-21 05:49:44,305 - distributed.scheduler - INFO - Receive client connection: Client-ccd675aa-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:44,316 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53882
2024-03-21 05:49:44,342 - distributed.scheduler - INFO - Receive client connection: Client-d0725135-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:44,343 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:53900
2024-03-21 05:49:44,349 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43553'
2024-03-21 05:49:45,953 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:49:45,953 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:49:45,957 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:49:45,957 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46697
2024-03-21 05:49:45,957 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46697
2024-03-21 05:49:45,958 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42423
2024-03-21 05:49:45,958 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-03-21 05:49:45,958 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:49:45,958 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:49:45,958 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 05:49:45,958 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-m_lvlxer
2024-03-21 05:49:45,958 - distributed.worker - INFO - Starting Worker plugin PreImport-15d032c9-ad96-487f-9089-8a43db9c3ac2
2024-03-21 05:49:45,958 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9c4cd14-d5aa-4abe-9bd7-dcc80ee3cb19
2024-03-21 05:49:45,958 - distributed.worker - INFO - Starting Worker plugin RMMSetup-5f9dc56d-95d9-4e02-b46e-7f3981e8653c
2024-03-21 05:49:46,235 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:49:46,236 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46697. Reason: failure-to-start-<class 'MemoryError'>
2024-03-21 05:49:46,236 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 05:49:46,238 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:49:46,274 - distributed.nanny - ERROR - Failed to start process
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:49:46,277 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43553'. Reason: nanny-instantiate-failed
2024-03-21 05:49:46,277 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-instantiate-failed
2024-03-21 05:49:46,321 - distributed.nanny - INFO - Worker process 45375 was killed by signal 15
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/bin/dask", line 10, in <module>
    sys.exit(main())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/__main__.py", line 7, in main
    run_cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/cli.py", line 129, in run_cli
    cli()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1078, in main
    rv = self.invoke(ctx)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1688, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 442, in worker
    loop.run_sync(run)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 539, in run_sync
    return future_cell[0].result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cli.py", line 434, in run
    await worker
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/cuda_worker.py", line 242, in _wait
    await asyncio.gather(*self.nannies)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
2024-03-21 05:49:46,323 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:53914'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:53914>: Stream is closed
2024-03-21 05:49:52,608 - distributed.scheduler - INFO - Remove client Client-ccd675aa-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:52,608 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53882; closing.
2024-03-21 05:49:52,609 - distributed.scheduler - INFO - Remove client Client-ccd675aa-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:52,609 - distributed.scheduler - INFO - Close client connection: Client-ccd675aa-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:52,725 - distributed.scheduler - INFO - Receive client connection: Client-d67b5565-e746-11ee-a6b2-d8c49764f6bb
2024-03-21 05:49:52,725 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:54988
2024-03-21 05:49:54,358 - distributed.scheduler - INFO - Remove client Client-d0725135-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:54,358 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:53900; closing.
2024-03-21 05:49:54,358 - distributed.scheduler - INFO - Remove client Client-d0725135-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:54,359 - distributed.scheduler - INFO - Close client connection: Client-d0725135-e746-11ee-a471-d8c49764f6bb
2024-03-21 05:49:54,359 - distributed._signals - INFO - Received signal SIGINT (2)
2024-03-21 05:49:54,360 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-03-21 05:49:54,360 - distributed.scheduler - INFO - Scheduler closing all comms
2024-03-21 05:49:54,362 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-03-21 05:49:54,362 - distributed.scheduler - INFO - End scheduler
FAILED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42457 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35237 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] SKIPPED (could ...)
dask_cuda/tests/test_dgx.py::test_tcp_only PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] 2024-03-21 05:50:24,773 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:50:24,784 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:24,792 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:27,498 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f2fb5c627c0>>, <Task finished name='Task-31' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-540' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-552' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39795 instead
  warnings.warn(
2024-03-21 05:50:37,246 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-5:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 172, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] 2024-03-21 05:50:48,636 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:50:48,642 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:48,677 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:50:49,127 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f85b9aca9d0>>, <Task finished name='Task-41' coro=<SpecCluster._correct_state_internal() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:346> exception=RuntimeError('Nanny failed to start.')>)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Nanny failed to start.
Task exception was never retrieved
future: <Task finished name='Task-828' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Task exception was never retrieved
future: <Task finished name='Task-840' coro=<_wrap_awaitable() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py:124> exception=RuntimeError('Worker failed to start.')>
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 125, in _wrap_awaitable
    return await aw
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 653, in start
    raise self.__startup_exc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45971 instead
  warnings.warn(
2024-03-21 05:50:57,818 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-7:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 172, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] SKIPPED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44035 instead
  warnings.warn(
2024-03-21 05:51:02,448 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-8:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_dgx.py", line 172, in _test_ucx_infiniband_nvlink
    with LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] SKIPPED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40231 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35601 instead
  warnings.warn(
2024-03-21 05:51:22,489 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-11:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 37, in _test_local_cluster
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46669 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43101 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39379 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38749 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46681 instead
  warnings.warn(
Process SpawnProcess-16:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40959 instead
  warnings.warn(
Process SpawnProcess-17:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35579 instead
  warnings.warn(
Process SpawnProcess-18:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43513 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40939 instead
  warnings.warn(
[1711000368.922584] [dgx13:49417:0]            sock.c:470  UCX  ERROR bind(fd=133 addr=0.0.0.0:44463) failed: Address already in use
[1711000370.179499] [dgx13:49505:0]            sock.c:470  UCX  ERROR bind(fd=122 addr=0.0.0.0:54058) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43309 instead
  warnings.warn(
[1711000386.673253] [dgx13:49831:0]            sock.c:470  UCX  ERROR bind(fd=128 addr=0.0.0.0:43995) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40255 instead
  warnings.warn(
Process SpawnProcess-22:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38135 instead
  warnings.warn(
Process SpawnProcess-23:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39251 instead
  warnings.warn(
Process SpawnProcess-24:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39167 instead
  warnings.warn(
2024-03-21 05:53:49,585 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-25:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39261 instead
  warnings.warn(
2024-03-21 05:53:51,598 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-26:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38945 instead
  warnings.warn(
2024-03-21 05:53:53,745 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-27:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] 2024-03-21 05:53:58,322 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-28:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32857 instead
  warnings.warn(
2024-03-21 05:54:01,474 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-29:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] 2024-03-21 05:54:04,546 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-30:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42627 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45635 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44989 instead
  warnings.warn(
Process SpawnProcess-34:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35455 instead
  warnings.warn(
Process SpawnProcess-35:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] Process SpawnProcess-36:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33643 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40211 instead
  warnings.warn(
Process SpawnProcess-40:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43403 instead
  warnings.warn(
Process SpawnProcess-41:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45649 instead
  warnings.warn(
[1711000566.772502] [dgx13:54733:0]            sock.c:470  UCX  ERROR bind(fd=128 addr=0.0.0.0:50608) failed: Address already in use
Process SpawnProcess-42:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 120, in _test_dataframe_shuffle
    df = cudf.DataFrame.from_pandas(df)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1961, in as_column
    data = as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1827, in as_column
    mask = libcudf.transform.nans_to_nulls(col.fillna(np.nan))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 545, in fillna
    col = self.nans_to_nulls()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 309, in nans_to_nulls
    if self.dtype.kind != "f" or self.nan_count == 0:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/numerical.py", line 432, in nan_count
    nan_col = libcudf.unary.is_nan(self)
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "unary.pyx", line 51, in cudf._lib.unary.is_nan
  File "unary.pyx", line 113, in cudf._lib.pylibcudf.unary.is_nan
  File "unary.pyx", line 131, in cudf._lib.pylibcudf.unary.is_nan
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] 2024-03-21 05:56:12,393 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-43:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] 2024-03-21 05:56:14,362 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-44:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39363 instead
  warnings.warn(
2024-03-21 05:56:16,407 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-45:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41313 instead
  warnings.warn(
2024-03-21 05:56:19,429 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-46:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34205 instead
  warnings.warn(
2024-03-21 05:56:22,393 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-47:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37681 instead
  warnings.warn(
2024-03-21 05:56:25,376 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-48:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 106, in _test_dataframe_shuffle
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40429 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36271 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33509 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] Process SpawnProcess-54:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 276, in _test_dataframe_shuffle_merge
    ddf1 = dd.from_pandas(df1, npartitions=n_workers + 1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/io/io.py", line 278, in from_pandas
    name = name or ("from_pandas-" + tokenize(data, chunksize, npartitions))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 1035, in tokenize
    hasher = _md5(str(tuple(map(normalize_token, args))).encode())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 767, in __call__
    return meth(arg, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 1119, in normalize_object
    return method()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/utils.py", line 356, in wrapper
    return fn(self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 6316, in __dask_tokenize__
    normalize_token(self.hash_values().values_host),
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 2895, in hash_values
    {None: libcudf.hash.hash([*self._columns], method, seed)},
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "hash.pyx", line 58, in cudf._lib.hash.hash
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] Process SpawnProcess-55:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 276, in _test_dataframe_shuffle_merge
    ddf1 = dd.from_pandas(df1, npartitions=n_workers + 1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/io/io.py", line 278, in from_pandas
    name = name or ("from_pandas-" + tokenize(data, chunksize, npartitions))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 1035, in tokenize
    hasher = _md5(str(tuple(map(normalize_token, args))).encode())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 767, in __call__
    return meth(arg, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 1119, in normalize_object
    return method()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/utils.py", line 356, in wrapper
    return fn(self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 6316, in __dask_tokenize__
    normalize_token(self.hash_values().values_host),
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 2895, in hash_values
    {None: libcudf.hash.hash([*self._columns], method, seed)},
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "hash.pyx", line 58, in cudf._lib.hash.hash
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] Process SpawnProcess-56:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 276, in _test_dataframe_shuffle_merge
    ddf1 = dd.from_pandas(df1, npartitions=n_workers + 1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/dataframe/io/io.py", line 278, in from_pandas
    name = name or ("from_pandas-" + tokenize(data, chunksize, npartitions))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 1035, in tokenize
    hasher = _md5(str(tuple(map(normalize_token, args))).encode())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/utils.py", line 767, in __call__
    return meth(arg, *args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask/base.py", line 1119, in normalize_object
    return method()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/utils/utils.py", line 356, in wrapper
    return fn(self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 6316, in __dask_tokenize__
    normalize_token(self.hash_values().values_host),
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/indexed_frame.py", line 2895, in hash_values
    {None: libcudf.hash.hash([*self._columns], method, seed)},
  File "/opt/conda/envs/gdf/lib/python3.9/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "hash.pyx", line 58, in cudf._lib.hash.hash
RuntimeError: Fatal CUDA error encountered at: /opt/conda/conda-bld/work/cpp/src/bitmask/null_mask.cu:92: 2 cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] [1711000628.667742] [dgx13:56869:0]            sock.c:470  UCX  ERROR bind(fd=122 addr=0.0.0.0:44049) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39971 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39143 instead
  warnings.warn(
Process SpawnProcess-60:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 273, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44033 instead
  warnings.warn(
Process SpawnProcess-61:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 273, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46097 instead
  warnings.warn(
Process SpawnProcess-62:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 273, in _test_dataframe_shuffle_merge
    df1 = cudf.DataFrame.from_pandas(df1)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38205 instead
  warnings.warn(
2024-03-21 05:57:49,284 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-63:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] 2024-03-21 05:57:51,261 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-64:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38337 instead
  warnings.warn(
2024-03-21 05:57:53,167 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-65:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] 2024-03-21 05:57:56,036 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-66:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33891 instead
  warnings.warn(
2024-03-21 05:57:58,957 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-67:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] 2024-03-21 05:58:01,855 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-68:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 252, in _test_dataframe_shuffle_merge
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34547 instead
  warnings.warn(
Process SpawnProcess-69:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 312, in _test_jit_unspill
    df = cudf.DataFrame.from_pandas(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5383, in from_pandas
    data = {
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/dataframe.py", line 5384, in <dictcomp>
    col_name: column.as_column(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/column/column.py", line 1960, in as_column
    arbitrary = cupy.asarray(arbitrary)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cupy/_creation/from_data.py", line 88, in asarray
    return _core.array(a, dtype, False, order, blocking=blocking)
  File "cupy/_core/core.pyx", line 2379, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2406, in cupy._core.core.array
  File "cupy/_core/core.pyx", line 2548, in cupy._core.core._array_default
  File "cupy/_core/core.pyx", line 132, in cupy._core.core.ndarray.__new__
  File "cupy/_core/core.pyx", line 220, in cupy._core.core._ndarray_base._init
  File "cupy/cuda/memory.pyx", line 738, in cupy.cuda.memory.alloc
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/allocators/cupy.py", line 37, in rmm_cupy_allocator
    buf = librmm.device_buffer.DeviceBuffer(size=nbytes, stream=stream)
  File "device_buffer.pyx", line 85, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44703 instead
  warnings.warn(
2024-03-21 05:58:11,447 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-70:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 302, in _test_jit_unspill
    with dask_cuda.LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37179 instead
  warnings.warn(
2024-03-21 05:58:14,332 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-71:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 409, in listen
    backend = registry.get_backend(scheme)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/registry.py", line 103, in get_backend
    raise ValueError(
ValueError: unknown address scheme 'ucxx' (known schemes: ['inproc', 'tcp', 'tls', 'ucx', 'ws', 'wss'])

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_explicit_comms.py", line 302, in _test_jit_unspill
    with dask_cuda.LocalCUDACluster(
  File "/usr/src/dask-cuda/dask_cuda/local_cuda_cluster.py", line 352, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1711000709.159733] [dgx13:42097:0]            sock.c:470  UCX  ERROR bind(fd=175 addr=0.0.0.0:43975) failed: Address already in use
2024-03-21 05:58:33,173 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:58:33,178 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] SKIPPED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2024-03-21 05:58:43,440 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-107:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 31, in _test_initialize_ucx_tcp
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] Unable to start CUDA Context
Traceback (most recent call last):
  File "/usr/src/dask-cuda/dask_cuda/initialize.py", line 35, in _create_cuda_context
    distributed.comm.ucx.init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
2024-03-21 05:58:46,066 - distributed.deploy.spec - WARNING - Cluster closed without starting up
Process SpawnProcess-108:
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4039, in start_unsafe
    await self.listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 325, in _start
    self.scheduler = await self.scheduler
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Scheduler failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 78, in _test_initialize_ucx_nvlink
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 284, in __init__
    self.sync(self._start)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 335, in _start
    raise RuntimeError(f"Cluster failed to start: {e}") from e
RuntimeError: Cluster failed to start: Scheduler failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] 2024-03-21 05:58:49,996 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:58:50,000 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:58:50,016 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Process SpawnProcess-109:
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 126, in _test_initialize_ucx_infiniband
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 286, in __init__
    self.sync(self._correct_state)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: IncreasedCloseTimeoutNanny failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] SKIPPED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] 2024-03-21 05:58:54,831 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:58:54,836 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-03-21 05:58:54,864 - distributed.nanny - ERROR - Failed to start process
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
Process SpawnProcess-110:
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 369, in start_unsafe
    response = await self.instantiate()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 455, in instantiate
    result = await self.process.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 762, in start
    msg = await self._wait_until_connected(uid)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 903, in _wait_until_connected
    raise msg["exception"]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/opt/conda/envs/gdf/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/src/dask-cuda/dask_cuda/tests/test_initialize.py", line 176, in _test_initialize_ucx_all
    with LocalCluster(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/local.py", line 253, in __init__
    super().__init__(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 286, in __init__
    self.sync(self._correct_state)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 358, in sync
    return sync(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 434, in sync
    raise error
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 408, in f
    result = yield future
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/deploy/spec.py", line 390, in _correct_state_internal
    await asyncio.gather(*worker_futs)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 688, in _wrap_awaitable
    return (yield from awaitable.__await__())
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: IncreasedCloseTimeoutNanny failed to start.
FAILED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] 2024-03-21 05:59:03,538 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:59:03,545 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] 2024-03-21 05:59:11,922 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 05:59:11,930 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-03-21 05:59:26,825 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:59:26,825 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:59:26,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:59:26,844 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:59:26,966 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:59:26,966 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:59:27,028 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:59:27,028 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:59:27,029 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:59:27,029 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:59:27,040 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:59:27,041 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:59:27,090 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:59:27,090 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:59:27,097 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 05:59:27,097 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 05:59:27,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:59:27,491 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44091
2024-03-21 05:59:27,491 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44091
2024-03-21 05:59:27,492 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43209
2024-03-21 05:59:27,492 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,492 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,492 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:59:27,492 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jpdbzw17
2024-03-21 05:59:27,492 - distributed.worker - INFO - Starting Worker plugin RMMSetup-85b69fe7-0a34-4708-8fb1-f835f9a5db26
2024-03-21 05:59:27,492 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f67f1561-ef0f-461b-bea5-35a73287ef90
2024-03-21 05:59:27,492 - distributed.worker - INFO - Starting Worker plugin PreImport-547ca3fd-868e-4776-897c-2ac0c0b78241
2024-03-21 05:59:27,493 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,497 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:59:27,498 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37461
2024-03-21 05:59:27,498 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37461
2024-03-21 05:59:27,498 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36797
2024-03-21 05:59:27,498 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,498 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,498 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:59:27,499 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bnzbbksg
2024-03-21 05:59:27,499 - distributed.worker - INFO - Starting Worker plugin PreImport-94fa44e6-defb-41cc-bc05-dc73dd768286
2024-03-21 05:59:27,499 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4d773895-13de-45ba-a483-fef5c4444553
2024-03-21 05:59:27,501 - distributed.worker - INFO - Starting Worker plugin RMMSetup-01f33319-e608-480a-9a35-a26240874624
2024-03-21 05:59:27,501 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,573 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:59:27,574 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,574 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,575 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33595
2024-03-21 05:59:27,579 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:59:27,580 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,580 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,581 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33595
2024-03-21 05:59:27,626 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:59:27,627 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39535
2024-03-21 05:59:27,627 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39535
2024-03-21 05:59:27,627 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32917
2024-03-21 05:59:27,627 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,627 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,627 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:59:27,627 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ivjvwoh8
2024-03-21 05:59:27,627 - distributed.worker - INFO - Starting Worker plugin RMMSetup-504b724d-0080-4c67-9354-eaf365ebb0ff
2024-03-21 05:59:27,627 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7bf56f83-2465-4896-abf6-8a7692836289
2024-03-21 05:59:27,628 - distributed.worker - INFO - Starting Worker plugin PreImport-ec029c68-65af-4ab8-83c3-09689a73b2c2
2024-03-21 05:59:27,628 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,685 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:59:27,686 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,686 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,687 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33595
2024-03-21 05:59:27,691 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:59:27,692 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37547
2024-03-21 05:59:27,692 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37547
2024-03-21 05:59:27,692 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38191
2024-03-21 05:59:27,692 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,692 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,692 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:59:27,692 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2hwlv1pi
2024-03-21 05:59:27,693 - distributed.worker - INFO - Starting Worker plugin PreImport-07e4421b-ac21-42e1-a7e0-d824886bb367
2024-03-21 05:59:27,693 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a69f0fb8-d57d-477e-a504-b7e50a2aeb83
2024-03-21 05:59:27,693 - distributed.worker - INFO - Starting Worker plugin RMMSetup-771a81fc-8eb9-4e78-9408-20258266e72e
2024-03-21 05:59:27,693 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,699 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:59:27,700 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41449
2024-03-21 05:59:27,700 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41449
2024-03-21 05:59:27,700 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33227
2024-03-21 05:59:27,700 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,701 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,701 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:59:27,701 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h2hgs7j4
2024-03-21 05:59:27,701 - distributed.worker - INFO - Starting Worker plugin RMMSetup-acd1e8cf-b8b0-4914-8b19-e9d593ece4fd
2024-03-21 05:59:27,701 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-95f37084-abfb-438a-8b6d-5b687ed5b63c
2024-03-21 05:59:27,703 - distributed.worker - INFO - Starting Worker plugin PreImport-7781e35e-aa6e-4faf-9561-5f3cb8f66800
2024-03-21 05:59:27,704 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,713 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:59:27,714 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36077
2024-03-21 05:59:27,714 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36077
2024-03-21 05:59:27,714 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37213
2024-03-21 05:59:27,714 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,714 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,714 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:59:27,714 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oawhpywg
2024-03-21 05:59:27,714 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9c2f1d9c-4499-4368-af7e-4e69bc1ae642
2024-03-21 05:59:27,714 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4999ae03-c84e-4498-af9f-64b01384e90c
2024-03-21 05:59:27,715 - distributed.worker - INFO - Starting Worker plugin PreImport-6ae6ea51-34ed-4818-bf26-63c8fee62d90
2024-03-21 05:59:27,715 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,745 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:59:27,746 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38263
2024-03-21 05:59:27,746 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38263
2024-03-21 05:59:27,746 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39357
2024-03-21 05:59:27,746 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,746 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,746 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:59:27,746 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-zfzs3v0s
2024-03-21 05:59:27,747 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0dfa6816-0bed-4ce2-ae91-bf670721adb0
2024-03-21 05:59:27,747 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-58953729-687a-43f0-b985-d27e19b8cc9c
2024-03-21 05:59:27,747 - distributed.worker - INFO - Starting Worker plugin PreImport-6cda386e-0d8e-41de-b045-5d200b9cede0
2024-03-21 05:59:27,747 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,795 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 05:59:27,796 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46251
2024-03-21 05:59:27,796 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46251
2024-03-21 05:59:27,796 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43197
2024-03-21 05:59:27,796 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,796 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,796 - distributed.worker - INFO -               Threads:                          1
2024-03-21 05:59:27,797 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1nqh65xo
2024-03-21 05:59:27,797 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2d19637b-528c-4ab0-8956-d5243d8760e2
2024-03-21 05:59:27,797 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ffd3fd14-e73b-4bfa-8363-6b577ff9ceed
2024-03-21 05:59:27,797 - distributed.worker - INFO - Starting Worker plugin PreImport-c8a5176f-4d74-4e9c-b868-5bc6920287ab
2024-03-21 05:59:27,797 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,807 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:59:27,807 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,808 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,809 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33595
2024-03-21 05:59:27,832 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:59:27,833 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,833 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,835 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33595
2024-03-21 05:59:27,850 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:59:27,851 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,851 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,852 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33595
2024-03-21 05:59:27,872 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:59:27,873 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,873 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,874 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33595
2024-03-21 05:59:27,886 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 05:59:27,887 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:33595
2024-03-21 05:59:27,887 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 05:59:27,888 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:33595
2024-03-21 05:59:27,917 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:59:27,918 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:59:27,918 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:59:27,918 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:59:27,918 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:59:27,919 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:59:27,919 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:59:27,919 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-03-21 05:59:27,924 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44091. Reason: nanny-close
2024-03-21 05:59:27,925 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37461. Reason: nanny-close
2024-03-21 05:59:27,925 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36077. Reason: nanny-close
2024-03-21 05:59:27,926 - distributed.core - INFO - Connection to tcp://127.0.0.1:33595 has been closed.
2024-03-21 05:59:27,927 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39535. Reason: nanny-close
2024-03-21 05:59:27,927 - distributed.core - INFO - Connection to tcp://127.0.0.1:33595 has been closed.
2024-03-21 05:59:27,928 - distributed.nanny - INFO - Worker closed
2024-03-21 05:59:27,928 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37547. Reason: nanny-close
2024-03-21 05:59:27,928 - distributed.core - INFO - Connection to tcp://127.0.0.1:33595 has been closed.
2024-03-21 05:59:27,928 - distributed.nanny - INFO - Worker closed
2024-03-21 05:59:27,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41449. Reason: nanny-close
2024-03-21 05:59:27,929 - distributed.core - INFO - Connection to tcp://127.0.0.1:33595 has been closed.
2024-03-21 05:59:27,929 - distributed.nanny - INFO - Worker closed
2024-03-21 05:59:27,929 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46251. Reason: nanny-close
2024-03-21 05:59:27,930 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38263. Reason: nanny-close
2024-03-21 05:59:27,930 - distributed.nanny - INFO - Worker closed
2024-03-21 05:59:27,930 - distributed.core - INFO - Connection to tcp://127.0.0.1:33595 has been closed.
2024-03-21 05:59:27,931 - distributed.core - INFO - Connection to tcp://127.0.0.1:33595 has been closed.
2024-03-21 05:59:27,931 - distributed.core - INFO - Connection to tcp://127.0.0.1:33595 has been closed.
2024-03-21 05:59:27,931 - distributed.core - INFO - Connection to tcp://127.0.0.1:33595 has been closed.
2024-03-21 05:59:27,932 - distributed.nanny - INFO - Worker closed
2024-03-21 05:59:27,932 - distributed.nanny - INFO - Worker closed
2024-03-21 05:59:27,933 - distributed.nanny - INFO - Worker closed
2024-03-21 05:59:27,933 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool 2024-03-21 05:59:35,882 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:59:35,890 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async 2024-03-21 05:59:45,776 - distributed.worker - ERROR - CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
2024-03-21 05:59:45,785 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size 2024-03-21 05:59:51,119 - distributed.worker - ERROR - CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
2024-03-21 05:59:51,128 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 338, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging 2024-03-21 05:59:56,465 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 05:59:56,474 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1002, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-03-21 06:00:01,341 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:00:01,341 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:00:01,345 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:00:01,346 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44397
2024-03-21 06:00:01,346 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44397
2024-03-21 06:00:01,346 - distributed.worker - INFO -           Worker name:                          0
2024-03-21 06:00:01,346 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43143
2024-03-21 06:00:01,346 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:40957
2024-03-21 06:00:01,346 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:01,346 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:00:01,346 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-03-21 06:00:01,346 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gmjd0jkd
2024-03-21 06:00:01,347 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7a0cbb89-0827-4e73-9a17-2aadba7a317d
2024-03-21 06:00:01,347 - distributed.worker - INFO - Starting Worker plugin PreImport-4a5bc0bd-ea0c-4356-b344-2b2e98dfbb03
2024-03-21 06:00:01,352 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-03-21 06:00:01,353 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4e7c194-befe-4a27-a3d7-6669e04bb15d
2024-03-21 06:00:01,353 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44397. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-03-21 06:00:01,353 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-03-21 06:00:01,355 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-03-21 06:00:05,392 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:00:05,392 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:00:05,427 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:00:05,427 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:00:05,428 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:00:05,428 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:00:05,429 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:00:05,429 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:00:05,549 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:00:05,550 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:00:05,550 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:00:05,551 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:00:05,604 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:00:05,604 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:00:05,621 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-03-21 06:00:05,621 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-03-21 06:00:06,066 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:00:06,067 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46675
2024-03-21 06:00:06,067 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46675
2024-03-21 06:00:06,067 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33509
2024-03-21 06:00:06,067 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,068 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,068 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:00:06,068 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:00:06,068 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-syt4iq5g
2024-03-21 06:00:06,068 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ac56d90b-4402-47e0-9167-a9e2797aa9a1
2024-03-21 06:00:06,068 - distributed.worker - INFO - Starting Worker plugin RMMSetup-87c4e5ca-c3b9-4c25-a12f-71a657c752da
2024-03-21 06:00:06,068 - distributed.worker - INFO - Starting Worker plugin PreImport-4d2c71bd-8a66-46a9-bb32-7203a4f40ddc
2024-03-21 06:00:06,069 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,075 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:00:06,076 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42719
2024-03-21 06:00:06,076 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42719
2024-03-21 06:00:06,076 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35059
2024-03-21 06:00:06,076 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,076 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,076 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:00:06,076 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:00:06,076 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-78hjt9kg
2024-03-21 06:00:06,076 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-5c349758-1dc3-4f50-9928-add67d035a0f
2024-03-21 06:00:06,077 - distributed.worker - INFO - Starting Worker plugin RMMSetup-feb9396e-43b7-4feb-bd5d-3a240f36d671
2024-03-21 06:00:06,077 - distributed.worker - INFO - Starting Worker plugin PreImport-ba73199a-f428-4539-a10d-b50af55d5d2c
2024-03-21 06:00:06,077 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,086 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:00:06,087 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41185
2024-03-21 06:00:06,087 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41185
2024-03-21 06:00:06,087 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34393
2024-03-21 06:00:06,087 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,087 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,087 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:00:06,087 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:00:06,087 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ato3lolc
2024-03-21 06:00:06,088 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cea0237e-0a4f-4710-ade0-50102174951d
2024-03-21 06:00:06,088 - distributed.worker - INFO - Starting Worker plugin PreImport-36150dd3-467d-4452-8c56-695bb5841519
2024-03-21 06:00:06,088 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25251cc6-6a96-42de-b91e-ba6bf854c17c
2024-03-21 06:00:06,088 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,118 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:00:06,119 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36991
2024-03-21 06:00:06,119 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36991
2024-03-21 06:00:06,119 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40309
2024-03-21 06:00:06,119 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,119 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,119 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:00:06,119 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:00:06,119 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_ahul1ru
2024-03-21 06:00:06,120 - distributed.worker - INFO - Starting Worker plugin PreImport-b99d1d71-f851-47c5-b023-406ea4944c5c
2024-03-21 06:00:06,120 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-98de6776-756f-4d94-a956-3553e48433c6
2024-03-21 06:00:06,120 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a18b7d6f-ee38-4ea8-b1bb-76a09f582288
2024-03-21 06:00:06,120 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,196 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:00:06,197 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34009
2024-03-21 06:00:06,197 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34009
2024-03-21 06:00:06,197 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44019
2024-03-21 06:00:06,197 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,197 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,197 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:00:06,197 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:00:06,197 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3d39_lob
2024-03-21 06:00:06,197 - distributed.worker - INFO - Starting Worker plugin PreImport-1cc01748-4775-4e9d-8c8c-bd2d87ec729b
2024-03-21 06:00:06,198 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-65467e08-6629-4c69-8abf-a3713a3d420e
2024-03-21 06:00:06,198 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6191d72-c937-495e-8ba9-30a67fd4b47e
2024-03-21 06:00:06,198 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:00:06,198 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,199 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,199 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,200 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38519
2024-03-21 06:00:06,201 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:00:06,202 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,202 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38519
2024-03-21 06:00:06,214 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:00:06,215 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,215 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,216 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38519
2024-03-21 06:00:06,221 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:00:06,222 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34569
2024-03-21 06:00:06,222 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34569
2024-03-21 06:00:06,222 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39021
2024-03-21 06:00:06,222 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,222 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,222 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:00:06,222 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:00:06,222 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z3hxw0wf
2024-03-21 06:00:06,223 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-78ca1ccc-ed2b-453e-b819-24da4e570152
2024-03-21 06:00:06,223 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8e004f58-6260-4d03-8eb8-081e727ce9b7
2024-03-21 06:00:06,223 - distributed.worker - INFO - Starting Worker plugin PreImport-3deaa5ae-0d25-4146-80b6-8b74a926e805
2024-03-21 06:00:06,223 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,230 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:00:06,231 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,231 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,233 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38519
2024-03-21 06:00:06,252 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:00:06,253 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38459
2024-03-21 06:00:06,253 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38459
2024-03-21 06:00:06,253 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36279
2024-03-21 06:00:06,253 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,253 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,253 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:00:06,253 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:00:06,253 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-cwj62enf
2024-03-21 06:00:06,253 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-87726398-13e0-42be-bb1d-96dda1ca295e
2024-03-21 06:00:06,254 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9e262453-74fa-40b9-80bd-4f779b592467
2024-03-21 06:00:06,254 - distributed.worker - INFO - Starting Worker plugin PreImport-0c8a58c7-71e4-4740-99e5-c9c566154811
2024-03-21 06:00:06,254 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,273 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-03-21 06:00:06,274 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37535
2024-03-21 06:00:06,274 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37535
2024-03-21 06:00:06,274 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42733
2024-03-21 06:00:06,274 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,274 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,274 - distributed.worker - INFO -               Threads:                          1
2024-03-21 06:00:06,274 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-03-21 06:00:06,274 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fdqd__ow
2024-03-21 06:00:06,275 - distributed.worker - INFO - Starting Worker plugin PreImport-b7d37fe3-04e2-4346-8232-dafffc66232d
2024-03-21 06:00:06,275 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4bb5046-3fce-41ee-ae4c-04baaca00141
2024-03-21 06:00:06,275 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eba94c49-8d14-416a-8054-caff35589658
2024-03-21 06:00:06,275 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,277 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:00:06,278 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,278 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,280 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38519
2024-03-21 06:00:06,325 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:00:06,326 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,326 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,327 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38519
2024-03-21 06:00:06,350 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:00:06,351 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,351 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,352 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38519
2024-03-21 06:00:06,365 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-03-21 06:00:06,366 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:38519
2024-03-21 06:00:06,366 - distributed.worker - INFO - -------------------------------------------------
2024-03-21 06:00:06,367 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:38519
2024-03-21 06:00:06,391 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46675. Reason: nanny-close
2024-03-21 06:00:06,391 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41185. Reason: nanny-close
2024-03-21 06:00:06,392 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42719. Reason: nanny-close
2024-03-21 06:00:06,393 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36991. Reason: nanny-close
2024-03-21 06:00:06,393 - distributed.core - INFO - Connection to tcp://127.0.0.1:38519 has been closed.
2024-03-21 06:00:06,394 - distributed.core - INFO - Connection to tcp://127.0.0.1:38519 has been closed.
2024-03-21 06:00:06,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34569. Reason: nanny-close
2024-03-21 06:00:06,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34009. Reason: nanny-close
2024-03-21 06:00:06,394 - distributed.core - INFO - Connection to tcp://127.0.0.1:38519 has been closed.
2024-03-21 06:00:06,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38459. Reason: nanny-close
2024-03-21 06:00:06,394 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37535. Reason: nanny-close
2024-03-21 06:00:06,395 - distributed.nanny - INFO - Worker closed
2024-03-21 06:00:06,395 - distributed.core - INFO - Connection to tcp://127.0.0.1:38519 has been closed.
2024-03-21 06:00:06,395 - distributed.nanny - INFO - Worker closed
2024-03-21 06:00:06,395 - distributed.nanny - INFO - Worker closed
2024-03-21 06:00:06,396 - distributed.core - INFO - Connection to tcp://127.0.0.1:38519 has been closed.
2024-03-21 06:00:06,396 - distributed.core - INFO - Connection to tcp://127.0.0.1:38519 has been closed.
2024-03-21 06:00:06,396 - distributed.core - INFO - Connection to tcp://127.0.0.1:38519 has been closed.
2024-03-21 06:00:06,396 - distributed.nanny - INFO - Worker closed
2024-03-21 06:00:06,396 - distributed.core - INFO - Connection to tcp://127.0.0.1:38519 has been closed.
2024-03-21 06:00:06,397 - distributed.nanny - INFO - Worker closed
2024-03-21 06:00:06,397 - distributed.nanny - INFO - Worker closed
2024-03-21 06:00:06,397 - distributed.nanny - INFO - Worker closed
2024-03-21 06:00:06,397 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations 2024-03-21 06:00:14,177 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 06:00:14,184 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration 2024-03-21 06:00:17,368 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 06:00:17,376 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits 2024-03-21 06:00:20,081 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 06:00:20,089 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] 2024-03-21 06:00:23,492 - distributed.worker - ERROR - Failed to log closing event
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1537, in close
    self.log_event(self.address, {"action": "closing-worker", "reason": reason})
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 771, in address
    raise ValueError("cannot get address of non-running Server")
ValueError: cannot get address of non-running Server
2024-03-21 06:00:23,496 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1388, in start_unsafe
    await self.listen(start_address, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 859, in listen
    listener = await listen(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/core.py", line 256, in _
    await self.start()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 527, in start
    init_once()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/ucx.py", line 138, in init_once
    numba.cuda.current_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 220, in get_context
    return _runtime.get_or_create_context(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 155, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/devices.py", line 177, in _activate_context_for
    newctx = gpu.get_primary_context()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 671, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 327, in safe_cuda_api_call
    self._check_ctypes_error(fname, retcode)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/numba/cuda/cudadrv/driver.py", line 395, in _check_ctypes_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
FAILED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] 2024-03-21 06:02:06,804 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:02:06,941 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:35965'. Shutting down.
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] 2024-03-21 06:02:13,296 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:02:13,304 - distributed.worker - ERROR - Scheduler was unaware of this worker 'tcp://127.0.0.1:42097'. Shutting down.
2024-03-21 06:02:13,306 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x150a9b903730>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:02:15,310 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] 2024-03-21 06:02:33,716 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:02:33,724 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x14b9e7074760>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:02:35,728 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] 2024-03-21 06:03:04,327 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:03:04,333 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x14b2ddb31760>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:03:06,337 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-03-21 06:03:34,745 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:03:34,752 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x154df8867790>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:03:36,756 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-03-21 06:04:05,374 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:04:05,381 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x14c8097fa760>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:04:07,385 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-03-21 06:04:36,932 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:04:36,939 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x153638653760>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:04:38,943 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-03-21 06:05:06,408 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:05:06,415 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x147b83450760>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 750, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 774, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory
2024-03-21 06:05:08,418 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-03-21 06:05:40,626 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded
2024-03-21 06:05:40,630 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/usr/src/dask-cuda/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 946, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1006, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 381, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:424: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 12 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
