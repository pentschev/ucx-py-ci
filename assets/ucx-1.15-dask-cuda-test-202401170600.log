============================= test session starts ==============================
platform linux -- Python 3.9.18, pytest-7.4.4, pluggy-1.3.0 -- /opt/conda/envs/gdf/bin/python3.9
cachedir: .pytest_cache
rootdir: /usr/src/dask-cuda
configfile: pyproject.toml
plugins: asyncio-0.23.3
asyncio: mode=strict
collecting ... collected 1246 items

dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_is_spillable_object_when_cudf_spilling_enabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_when_cudf_spilling_is_disabled PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_cudf_builtin_spilling.py::test_proxify_host_file PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_and_memory_limit_and_nthreads 2024-01-17 06:37:19,574 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:19,579 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34971 instead
  warnings.warn(
2024-01-17 06:37:19,583 - distributed.scheduler - INFO - State start
2024-01-17 06:37:19,621 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:19,622 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-17 06:37:19,624 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34971/status
2024-01-17 06:37:19,624 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:37:19,869 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35381'
2024-01-17 06:37:19,888 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42753'
2024-01-17 06:37:19,891 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33499'
2024-01-17 06:37:19,901 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45573'
2024-01-17 06:37:21,620 - distributed.scheduler - INFO - Receive client connection: Client-dbe7bfde-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:21,637 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48884
2024-01-17 06:37:21,679 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:21,679 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:21,683 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:21,684 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36747
2024-01-17 06:37:21,684 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36747
2024-01-17 06:37:21,684 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36975
2024-01-17 06:37:21,684 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-17 06:37:21,684 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,684 - distributed.worker - INFO -               Threads:                          4
2024-01-17 06:37:21,684 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-17 06:37:21,684 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-ywbc4yix
2024-01-17 06:37:21,685 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8fa2d709-d5a8-4beb-a273-07f7fea2ac76
2024-01-17 06:37:21,685 - distributed.worker - INFO - Starting Worker plugin PreImport-b7e89bb9-96b2-4e75-8e98-abc1278e4f16
2024-01-17 06:37:21,685 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ac92d3e2-1970-435d-9f3a-088b3d0bb677
2024-01-17 06:37:21,685 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:21,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:21,693 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:21,693 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:21,697 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:21,697 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:21,698 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35047
2024-01-17 06:37:21,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35047
2024-01-17 06:37:21,698 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44659
2024-01-17 06:37:21,698 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39065
2024-01-17 06:37:21,698 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44659
2024-01-17 06:37:21,698 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-17 06:37:21,698 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34613
2024-01-17 06:37:21,698 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,698 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-17 06:37:21,698 - distributed.worker - INFO -               Threads:                          4
2024-01-17 06:37:21,698 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,698 - distributed.worker - INFO -               Threads:                          4
2024-01-17 06:37:21,698 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-17 06:37:21,698 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-17 06:37:21,698 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-yj2wqgr9
2024-01-17 06:37:21,698 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-xr7n__8j
2024-01-17 06:37:21,698 - distributed.worker - INFO - Starting Worker plugin PreImport-59999da4-3641-4a88-8e84-de9cd1736bad
2024-01-17 06:37:21,698 - distributed.worker - INFO - Starting Worker plugin PreImport-952b93e0-fedf-41d1-b6eb-9b25807ad1b0
2024-01-17 06:37:21,698 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b251bcbb-8a05-4dd3-b54e-6fe0e296ae1d
2024-01-17 06:37:21,698 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-debd79f3-11fa-4b01-bbc9-13c0199895ae
2024-01-17 06:37:21,698 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-aefe0423-25c6-45c4-8cf9-78758130815b
2024-01-17 06:37:21,699 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,699 - distributed.worker - INFO - Starting Worker plugin RMMSetup-7f8c7d94-8506-4184-9c46-eeae1329f233
2024-01-17 06:37:21,699 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,761 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36747', status: init, memory: 0, processing: 0>
2024-01-17 06:37:21,762 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36747
2024-01-17 06:37:21,762 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48906
2024-01-17 06:37:21,763 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:21,763 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-17 06:37:21,763 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,764 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-17 06:37:21,774 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:21,774 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:21,778 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:21,779 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41095
2024-01-17 06:37:21,779 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41095
2024-01-17 06:37:21,779 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33353
2024-01-17 06:37:21,779 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-17 06:37:21,779 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,779 - distributed.worker - INFO -               Threads:                          4
2024-01-17 06:37:21,779 - distributed.worker - INFO -                Memory:                 251.94 GiB
2024-01-17 06:37:21,779 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-j69cpr5h
2024-01-17 06:37:21,779 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f2c0aaa1-dcfe-4e11-af49-bef39d8ba9a3
2024-01-17 06:37:21,781 - distributed.worker - INFO - Starting Worker plugin PreImport-569e6b3a-b20c-4498-b756-5d3576f7b455
2024-01-17 06:37:21,781 - distributed.worker - INFO - Starting Worker plugin RMMSetup-3186d75c-666c-44b5-b181-03be9b2c2b33
2024-01-17 06:37:21,781 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,794 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44659', status: init, memory: 0, processing: 0>
2024-01-17 06:37:21,795 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44659
2024-01-17 06:37:21,795 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48914
2024-01-17 06:37:21,796 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:21,797 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-17 06:37:21,797 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,797 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35047', status: init, memory: 0, processing: 0>
2024-01-17 06:37:21,798 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35047
2024-01-17 06:37:21,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48916
2024-01-17 06:37:21,798 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-17 06:37:21,799 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:21,800 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-17 06:37:21,800 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:21,801 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-17 06:37:22,716 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41095', status: init, memory: 0, processing: 0>
2024-01-17 06:37:22,717 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41095
2024-01-17 06:37:22,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:48920
2024-01-17 06:37:22,718 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:22,719 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-17 06:37:22,719 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:22,720 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-17 06:37:22,810 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-17 06:37:22,810 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-17 06:37:22,811 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-17 06:37:22,834 - distributed.worker - INFO - Run out-of-band function 'get_visible_devices'
2024-01-17 06:37:22,839 - distributed.scheduler - INFO - Remove client Client-dbe7bfde-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:22,839 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48884; closing.
2024-01-17 06:37:22,840 - distributed.scheduler - INFO - Remove client Client-dbe7bfde-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:22,840 - distributed.scheduler - INFO - Close client connection: Client-dbe7bfde-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:22,841 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35381'. Reason: nanny-close
2024-01-17 06:37:22,841 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:22,842 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42753'. Reason: nanny-close
2024-01-17 06:37:22,842 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:22,842 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33499'. Reason: nanny-close
2024-01-17 06:37:22,843 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:22,842 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41095. Reason: nanny-close
2024-01-17 06:37:22,843 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45573'. Reason: nanny-close
2024-01-17 06:37:22,843 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36747. Reason: nanny-close
2024-01-17 06:37:22,843 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:22,843 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35047. Reason: nanny-close
2024-01-17 06:37:22,844 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44659. Reason: nanny-close
2024-01-17 06:37:22,844 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-17 06:37:22,845 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48920; closing.
2024-01-17 06:37:22,845 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-17 06:37:22,845 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41095', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473442.8456595')
2024-01-17 06:37:22,845 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-17 06:37:22,845 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-17 06:37:22,846 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48914; closing.
2024-01-17 06:37:22,846 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:22,846 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:22,846 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44659', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473442.84681')
2024-01-17 06:37:22,847 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:22,847 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:22,847 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48906; closing.
2024-01-17 06:37:22,847 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36747', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473442.8478627')
2024-01-17 06:37:22,848 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:48916; closing.
2024-01-17 06:37:22,848 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9359 remote=tcp://127.0.0.1:48914>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-17 06:37:22,850 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35047', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473442.8502827')
2024-01-17 06:37:22,850 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:37:23,607 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:37:23,607 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:37:23,608 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:37:23,609 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-17 06:37:23,609 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_pool 2024-01-17 06:37:26,029 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:26,034 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 45841 instead
  warnings.warn(
2024-01-17 06:37:26,038 - distributed.scheduler - INFO - State start
2024-01-17 06:37:26,062 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:26,063 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:37:26,064 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:45841/status
2024-01-17 06:37:26,064 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:37:26,305 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40483'
2024-01-17 06:37:26,318 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39959'
2024-01-17 06:37:26,327 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46557'
2024-01-17 06:37:26,345 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39383'
2024-01-17 06:37:26,348 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41959'
2024-01-17 06:37:26,359 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33333'
2024-01-17 06:37:26,372 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44991'
2024-01-17 06:37:26,385 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36551'
2024-01-17 06:37:27,527 - distributed.scheduler - INFO - Receive client connection: Client-dfa304bf-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:27,543 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:36196
2024-01-17 06:37:28,194 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:28,194 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:28,199 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:28,200 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43025
2024-01-17 06:37:28,200 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43025
2024-01-17 06:37:28,200 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42923
2024-01-17 06:37:28,200 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:28,200 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:28,200 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:28,200 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:28,200 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h9rkk_kh
2024-01-17 06:37:28,201 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fa9c9b06-8dd3-4410-b6fa-7237661cbcbd
2024-01-17 06:37:28,250 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:28,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:28,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:28,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41053
2024-01-17 06:37:28,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41053
2024-01-17 06:37:28,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42197
2024-01-17 06:37:28,256 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:28,256 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:28,256 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:28,256 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:28,256 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-0x0d9t3k
2024-01-17 06:37:28,256 - distributed.worker - INFO - Starting Worker plugin RMMSetup-24b8f241-1d14-471a-91c1-30387ac0c0ae
2024-01-17 06:37:28,266 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:28,266 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:28,271 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:28,271 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45615
2024-01-17 06:37:28,272 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45615
2024-01-17 06:37:28,272 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40135
2024-01-17 06:37:28,272 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:28,272 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:28,272 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:28,272 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:28,272 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vn5o8nce
2024-01-17 06:37:28,272 - distributed.worker - INFO - Starting Worker plugin RMMSetup-60786e9d-c73e-4ed0-81c0-52ba980df0ba
2024-01-17 06:37:28,281 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:28,281 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:28,286 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:28,286 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:28,286 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:28,286 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40827
2024-01-17 06:37:28,286 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40827
2024-01-17 06:37:28,287 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46189
2024-01-17 06:37:28,287 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:28,287 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:28,287 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:28,287 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:28,287 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yqj3fvxr
2024-01-17 06:37:28,287 - distributed.worker - INFO - Starting Worker plugin RMMSetup-cd09a69e-cd2b-4780-8765-dd429afd1138
2024-01-17 06:37:28,290 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:28,291 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40013
2024-01-17 06:37:28,291 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40013
2024-01-17 06:37:28,291 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43777
2024-01-17 06:37:28,292 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:28,292 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:28,292 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:28,292 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:28,292 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k0rctbd7
2024-01-17 06:37:28,292 - distributed.worker - INFO - Starting Worker plugin PreImport-97503156-53e9-4baa-a469-1accd8f20431
2024-01-17 06:37:28,292 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2e4f2f85-6dc6-4536-8e0a-de9a2c292c7f
2024-01-17 06:37:28,293 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:28,293 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:28,293 - distributed.worker - INFO - Starting Worker plugin RMMSetup-33a1383b-812a-45f7-ba2a-42c5672a788a
2024-01-17 06:37:28,298 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:28,299 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44105
2024-01-17 06:37:28,299 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44105
2024-01-17 06:37:28,299 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43475
2024-01-17 06:37:28,299 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:28,299 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:28,299 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:28,299 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:28,299 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-e5xfat15
2024-01-17 06:37:28,299 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0c94bae2-bf34-4302-8b22-232586a56a41
2024-01-17 06:37:28,531 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:28,531 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:28,536 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:28,537 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45135
2024-01-17 06:37:28,537 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45135
2024-01-17 06:37:28,537 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41195
2024-01-17 06:37:28,537 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:28,537 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:28,537 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:28,537 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:28,537 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ukkrwzo0
2024-01-17 06:37:28,537 - distributed.worker - INFO - Starting Worker plugin RMMSetup-8d8c088c-41a6-4bc9-a999-a0f5cebd9ad9
2024-01-17 06:37:28,557 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:28,557 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:28,562 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:28,563 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35149
2024-01-17 06:37:28,563 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35149
2024-01-17 06:37:28,563 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33547
2024-01-17 06:37:28,563 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:28,563 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:28,563 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:28,563 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:28,563 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t97_gbuz
2024-01-17 06:37:28,563 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e002ef79-22c9-4bb0-a090-248c60e35bb4
2024-01-17 06:37:30,441 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-451e3837-81b1-4683-9d09-a1fef35c7361
2024-01-17 06:37:30,442 - distributed.worker - INFO - Starting Worker plugin PreImport-748b11e9-37d8-432a-ac15-ee290a62eb98
2024-01-17 06:37:30,443 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,475 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43025', status: init, memory: 0, processing: 0>
2024-01-17 06:37:30,476 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43025
2024-01-17 06:37:30,477 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51056
2024-01-17 06:37:30,478 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:30,479 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:30,479 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,481 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:30,580 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6a2423fd-859d-4765-afa5-ea59c913b15c
2024-01-17 06:37:30,582 - distributed.worker - INFO - Starting Worker plugin PreImport-f6855f01-4b37-4bbf-8625-a3b389c451af
2024-01-17 06:37:30,582 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,588 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f8b96c5-0f72-434b-a7c8-4dea87ab161f
2024-01-17 06:37:30,589 - distributed.worker - INFO - Starting Worker plugin PreImport-8c8384ba-2571-453c-9a0b-abecd0419217
2024-01-17 06:37:30,591 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,617 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45615', status: init, memory: 0, processing: 0>
2024-01-17 06:37:30,618 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45615
2024-01-17 06:37:30,618 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51058
2024-01-17 06:37:30,619 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:30,621 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:30,621 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,623 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:30,642 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41053', status: init, memory: 0, processing: 0>
2024-01-17 06:37:30,642 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41053
2024-01-17 06:37:30,642 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51068
2024-01-17 06:37:30,644 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:30,645 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:30,645 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:30,687 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c1a8cbc5-f3cb-47ec-b4ac-578f16a4f783
2024-01-17 06:37:30,688 - distributed.worker - INFO - Starting Worker plugin PreImport-9f820d9b-be52-4b1d-bf2d-30a356768534
2024-01-17 06:37:30,688 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,711 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-29ff2570-ded6-467e-8081-81468686cc10
2024-01-17 06:37:30,712 - distributed.worker - INFO - Starting Worker plugin PreImport-0ac08428-2946-4504-baab-7913f85742c9
2024-01-17 06:37:30,713 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35149', status: init, memory: 0, processing: 0>
2024-01-17 06:37:30,713 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35149
2024-01-17 06:37:30,714 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,714 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51082
2024-01-17 06:37:30,713 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,714 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:30,715 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:30,715 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,716 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2455ac56-c25e-478f-b79c-6412895abe76
2024-01-17 06:37:30,717 - distributed.worker - INFO - Starting Worker plugin PreImport-6b823df1-b4b9-4b39-9436-63c09580d093
2024-01-17 06:37:30,717 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:30,717 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,722 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-fdf6f5b1-9041-4bd1-a834-486f5a7eaad5
2024-01-17 06:37:30,724 - distributed.worker - INFO - Starting Worker plugin PreImport-86544e3d-d495-41c3-8aba-163e73446a57
2024-01-17 06:37:30,724 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,742 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40827', status: init, memory: 0, processing: 0>
2024-01-17 06:37:30,742 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40827
2024-01-17 06:37:30,743 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51116
2024-01-17 06:37:30,744 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:30,744 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:30,745 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,746 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:30,750 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40013', status: init, memory: 0, processing: 0>
2024-01-17 06:37:30,751 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40013
2024-01-17 06:37:30,751 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51112
2024-01-17 06:37:30,753 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:30,754 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:30,754 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,756 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44105', status: init, memory: 0, processing: 0>
2024-01-17 06:37:30,756 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44105
2024-01-17 06:37:30,756 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51098
2024-01-17 06:37:30,757 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45135', status: init, memory: 0, processing: 0>
2024-01-17 06:37:30,758 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45135
2024-01-17 06:37:30,758 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51124
2024-01-17 06:37:30,758 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:30,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:30,759 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:30,759 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,760 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:30,760 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:30,761 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:30,761 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:30,762 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:30,788 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:30,789 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:30,789 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:30,789 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:30,789 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:30,789 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:30,790 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:30,794 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:30,800 - distributed.scheduler - INFO - Remove client Client-dfa304bf-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:30,800 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:36196; closing.
2024-01-17 06:37:30,800 - distributed.scheduler - INFO - Remove client Client-dfa304bf-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:30,800 - distributed.scheduler - INFO - Close client connection: Client-dfa304bf-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:30,802 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40483'. Reason: nanny-close
2024-01-17 06:37:30,802 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:30,802 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39959'. Reason: nanny-close
2024-01-17 06:37:30,803 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:30,803 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46557'. Reason: nanny-close
2024-01-17 06:37:30,803 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43025. Reason: nanny-close
2024-01-17 06:37:30,804 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:30,804 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39383'. Reason: nanny-close
2024-01-17 06:37:30,804 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41053. Reason: nanny-close
2024-01-17 06:37:30,804 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:30,804 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41959'. Reason: nanny-close
2024-01-17 06:37:30,804 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40827. Reason: nanny-close
2024-01-17 06:37:30,805 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:30,805 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33333'. Reason: nanny-close
2024-01-17 06:37:30,805 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45615. Reason: nanny-close
2024-01-17 06:37:30,805 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:30,805 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44991'. Reason: nanny-close
2024-01-17 06:37:30,806 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44105. Reason: nanny-close
2024-01-17 06:37:30,806 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:30,806 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:30,806 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51056; closing.
2024-01-17 06:37:30,806 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36551'. Reason: nanny-close
2024-01-17 06:37:30,806 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40013. Reason: nanny-close
2024-01-17 06:37:30,806 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43025', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473450.8064353')
2024-01-17 06:37:30,806 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:30,806 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:30,806 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:30,806 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45135. Reason: nanny-close
2024-01-17 06:37:30,807 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:30,807 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35149. Reason: nanny-close
2024-01-17 06:37:30,807 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:30,808 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:30,808 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51058; closing.
2024-01-17 06:37:30,808 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:30,808 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:30,808 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51068; closing.
2024-01-17 06:37:30,808 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:30,808 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51116; closing.
2024-01-17 06:37:30,808 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:30,809 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45615', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473450.8093393')
2024-01-17 06:37:30,809 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:30,809 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:30,809 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41053', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473450.8097558')
2024-01-17 06:37:30,810 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:30,810 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40827', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473450.8101301')
2024-01-17 06:37:30,810 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:30,810 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:30,811 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51098; closing.
2024-01-17 06:37:30,811 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:30,811 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51124; closing.
2024-01-17 06:37:30,811 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44105', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473450.8118165')
2024-01-17 06:37:30,812 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45135', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473450.8122268')
2024-01-17 06:37:30,812 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51112; closing.
2024-01-17 06:37:30,812 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51082; closing.
2024-01-17 06:37:30,813 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40013', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473450.8133216')
2024-01-17 06:37:30,813 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35149', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473450.8137186')
2024-01-17 06:37:30,813 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:37:31,718 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:37:31,718 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:37:31,719 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:37:31,720 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:37:31,720 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_managed 2024-01-17 06:37:33,838 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:33,843 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41661 instead
  warnings.warn(
2024-01-17 06:37:33,847 - distributed.scheduler - INFO - State start
2024-01-17 06:37:33,869 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:33,870 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:37:33,871 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:41661/status
2024-01-17 06:37:33,871 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:37:34,229 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43055'
2024-01-17 06:37:34,253 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45335'
2024-01-17 06:37:34,255 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44305'
2024-01-17 06:37:34,263 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39301'
2024-01-17 06:37:34,272 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35203'
2024-01-17 06:37:34,281 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33553'
2024-01-17 06:37:34,293 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42663'
2024-01-17 06:37:34,303 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43471'
2024-01-17 06:37:35,806 - distributed.scheduler - INFO - Receive client connection: Client-e461063a-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:35,821 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51252
2024-01-17 06:37:36,136 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:36,136 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:36,140 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:36,141 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34407
2024-01-17 06:37:36,141 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34407
2024-01-17 06:37:36,141 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43909
2024-01-17 06:37:36,141 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:36,142 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:36,142 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:36,142 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:36,142 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ijqzp2dg
2024-01-17 06:37:36,142 - distributed.worker - INFO - Starting Worker plugin PreImport-31874574-973f-4753-b600-a5f8e287fe2a
2024-01-17 06:37:36,142 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-4c4f1b37-6249-42e0-808c-8ff0eed56e25
2024-01-17 06:37:36,142 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0eb02438-ff89-42dc-aced-55edd669eb7c
2024-01-17 06:37:36,158 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:36,158 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:36,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:36,158 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:36,162 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:36,162 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:36,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43151
2024-01-17 06:37:36,163 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35653
2024-01-17 06:37:36,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43151
2024-01-17 06:37:36,163 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35653
2024-01-17 06:37:36,163 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46267
2024-01-17 06:37:36,163 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32773
2024-01-17 06:37:36,163 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:36,163 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:36,163 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:36,163 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:36,163 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:36,164 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:36,164 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:36,164 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bil31m55
2024-01-17 06:37:36,164 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:36,164 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v8bed97d
2024-01-17 06:37:36,164 - distributed.worker - INFO - Starting Worker plugin PreImport-8f125c8c-6885-4897-bfa8-b7911f4f9bc0
2024-01-17 06:37:36,164 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2da85886-feb6-449f-ace3-c87241f87fb6
2024-01-17 06:37:36,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c5c947f6-8dfc-4686-8951-c437ddbd9f93
2024-01-17 06:37:36,164 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a5ab10de-83cb-42e0-a830-498f2f279884
2024-01-17 06:37:36,172 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:36,172 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:36,176 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:36,177 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34999
2024-01-17 06:37:36,177 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34999
2024-01-17 06:37:36,177 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40495
2024-01-17 06:37:36,177 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:36,177 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:36,177 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:36,178 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:36,178 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ko3cqui
2024-01-17 06:37:36,178 - distributed.worker - INFO - Starting Worker plugin PreImport-5e161c2a-b97f-417e-999f-3e97c9cf3954
2024-01-17 06:37:36,178 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ff1a39b0-deb0-4155-a31d-3d88f92d0429
2024-01-17 06:37:36,178 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e71b57ce-db7f-4c4d-8b40-328639af045f
2024-01-17 06:37:36,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:36,214 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:36,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:36,214 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:36,214 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:36,215 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:36,218 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:36,219 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:36,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45867
2024-01-17 06:37:36,219 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:36,219 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45867
2024-01-17 06:37:36,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44899
2024-01-17 06:37:36,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:36,219 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:36,219 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:36,219 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:36,219 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45451
2024-01-17 06:37:36,219 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p_m9do1k
2024-01-17 06:37:36,219 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45451
2024-01-17 06:37:36,219 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37433
2024-01-17 06:37:36,219 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:36,220 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:36,220 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d428f1aa-75b8-4af7-85ab-a1fa5d392aa7
2024-01-17 06:37:36,220 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:36,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:36,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-lgf6qacd
2024-01-17 06:37:36,220 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45189
2024-01-17 06:37:36,220 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45189
2024-01-17 06:37:36,220 - distributed.worker - INFO - Starting Worker plugin RMMSetup-632095ec-c24a-4a49-ba4e-fd8c744e55b6
2024-01-17 06:37:36,220 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34669
2024-01-17 06:37:36,220 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:36,220 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:36,220 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:36,220 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:36,220 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-iz84ldac
2024-01-17 06:37:36,220 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fe0e5229-1450-4fcd-93fe-4834c3ee55ec
2024-01-17 06:37:36,242 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:36,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:36,247 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:36,248 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38365
2024-01-17 06:37:36,248 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38365
2024-01-17 06:37:36,248 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38693
2024-01-17 06:37:36,248 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:36,248 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:36,248 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:36,248 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:36,248 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-fdmyuivl
2024-01-17 06:37:36,248 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e91c9a6e-3c80-42bc-bbe9-52874f62c05f
2024-01-17 06:37:37,599 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:37,625 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34407', status: init, memory: 0, processing: 0>
2024-01-17 06:37:37,627 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34407
2024-01-17 06:37:37,627 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51264
2024-01-17 06:37:37,628 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:37,629 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:37,629 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:37,630 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:38,098 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f83d6659-4604-42d6-90e4-8891091f5452
2024-01-17 06:37:38,099 - distributed.worker - INFO - Starting Worker plugin PreImport-a6389155-a5b9-4b75-a7df-03a1c6812475
2024-01-17 06:37:38,100 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,133 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,135 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43151', status: init, memory: 0, processing: 0>
2024-01-17 06:37:38,136 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43151
2024-01-17 06:37:38,136 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51266
2024-01-17 06:37:38,137 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:38,138 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:38,139 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,141 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:38,155 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35653', status: init, memory: 0, processing: 0>
2024-01-17 06:37:38,155 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35653
2024-01-17 06:37:38,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51280
2024-01-17 06:37:38,156 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:38,157 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:38,157 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,158 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:38,191 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,224 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34999', status: init, memory: 0, processing: 0>
2024-01-17 06:37:38,225 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34999
2024-01-17 06:37:38,225 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51282
2024-01-17 06:37:38,226 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:38,227 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:38,227 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,229 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:38,257 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e238270-521e-4120-8b97-1e3c03691fa1
2024-01-17 06:37:38,258 - distributed.worker - INFO - Starting Worker plugin PreImport-002b54a7-8cd7-44e8-8fd1-4ec4a44632fc
2024-01-17 06:37:38,258 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,282 - distributed.worker - INFO - Starting Worker plugin PreImport-f159993a-eca8-4c76-8363-c89cd0dc15fd
2024-01-17 06:37:38,282 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-092ab62b-cc3f-41f0-a0ee-19022f677633
2024-01-17 06:37:38,282 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,285 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45867', status: init, memory: 0, processing: 0>
2024-01-17 06:37:38,286 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45867
2024-01-17 06:37:38,286 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51290
2024-01-17 06:37:38,287 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:38,287 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-94d8eefe-dd96-4f1d-8884-b88e8dfc26ca
2024-01-17 06:37:38,288 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:38,288 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,288 - distributed.worker - INFO - Starting Worker plugin PreImport-9a36283c-cf85-41af-9998-4abfdd23caea
2024-01-17 06:37:38,289 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:38,289 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,290 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-ba5ff4b6-7355-4b3d-81c6-b4ff24034879
2024-01-17 06:37:38,291 - distributed.worker - INFO - Starting Worker plugin PreImport-c76c17ff-5ef6-4c6f-b945-7425c03bc062
2024-01-17 06:37:38,293 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,306 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45451', status: init, memory: 0, processing: 0>
2024-01-17 06:37:38,306 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45451
2024-01-17 06:37:38,306 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51298
2024-01-17 06:37:38,307 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:38,308 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:38,308 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,310 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:38,323 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45189', status: init, memory: 0, processing: 0>
2024-01-17 06:37:38,324 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45189
2024-01-17 06:37:38,324 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51308
2024-01-17 06:37:38,325 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38365', status: init, memory: 0, processing: 0>
2024-01-17 06:37:38,326 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38365
2024-01-17 06:37:38,326 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51312
2024-01-17 06:37:38,326 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:38,327 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:38,327 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,327 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:38,328 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:38,328 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:38,329 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:38,330 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:38,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:38,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:38,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:38,383 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:38,384 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:38,384 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:38,384 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:38,388 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:38,394 - distributed.scheduler - INFO - Remove client Client-e461063a-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:38,395 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51252; closing.
2024-01-17 06:37:38,395 - distributed.scheduler - INFO - Remove client Client-e461063a-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:38,395 - distributed.scheduler - INFO - Close client connection: Client-e461063a-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:38,396 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43055'. Reason: nanny-close
2024-01-17 06:37:38,397 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:38,397 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45335'. Reason: nanny-close
2024-01-17 06:37:38,398 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:38,398 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44305'. Reason: nanny-close
2024-01-17 06:37:38,398 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:38,398 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45189. Reason: nanny-close
2024-01-17 06:37:38,398 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39301'. Reason: nanny-close
2024-01-17 06:37:38,398 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:38,398 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34999. Reason: nanny-close
2024-01-17 06:37:38,399 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35203'. Reason: nanny-close
2024-01-17 06:37:38,399 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35653. Reason: nanny-close
2024-01-17 06:37:38,399 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:38,399 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33553'. Reason: nanny-close
2024-01-17 06:37:38,399 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45867. Reason: nanny-close
2024-01-17 06:37:38,399 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:38,400 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42663'. Reason: nanny-close
2024-01-17 06:37:38,400 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:38,400 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43151. Reason: nanny-close
2024-01-17 06:37:38,400 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43471'. Reason: nanny-close
2024-01-17 06:37:38,400 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:38,400 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38365. Reason: nanny-close
2024-01-17 06:37:38,400 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:38,401 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45451. Reason: nanny-close
2024-01-17 06:37:38,401 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:38,401 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:38,401 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51308; closing.
2024-01-17 06:37:38,401 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:38,401 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34407. Reason: nanny-close
2024-01-17 06:37:38,401 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51290; closing.
2024-01-17 06:37:38,402 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45189', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473458.402057')
2024-01-17 06:37:38,402 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:38,402 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51280; closing.
2024-01-17 06:37:38,402 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:38,402 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:38,402 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51282; closing.
2024-01-17 06:37:38,402 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:38,402 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:38,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:38,403 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:38,403 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45867', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473458.403162')
2024-01-17 06:37:38,403 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:38,403 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35653', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473458.4038315')
2024-01-17 06:37:38,404 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:38,404 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34999', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473458.4042058')
2024-01-17 06:37:38,404 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:38,404 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:38,405 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:38,405 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51266; closing.
2024-01-17 06:37:38,405 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51298; closing.
2024-01-17 06:37:38,406 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51282>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-17 06:37:38,407 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:51280>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-17 06:37:38,408 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43151', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473458.40825')
2024-01-17 06:37:38,408 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45451', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473458.4086492')
2024-01-17 06:37:38,409 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51312; closing.
2024-01-17 06:37:38,409 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51264; closing.
2024-01-17 06:37:38,409 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38365', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473458.409502')
2024-01-17 06:37:38,409 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34407', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473458.4098768')
2024-01-17 06:37:38,410 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:37:39,413 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:37:39,413 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:37:39,413 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:37:39,415 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:37:39,415 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async 2024-01-17 06:37:41,800 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:41,809 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34383 instead
  warnings.warn(
2024-01-17 06:37:41,816 - distributed.scheduler - INFO - State start
2024-01-17 06:37:41,849 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:41,850 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:37:41,851 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:34383/status
2024-01-17 06:37:41,851 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:37:42,023 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35903'
2024-01-17 06:37:42,037 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34097'
2024-01-17 06:37:42,046 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34705'
2024-01-17 06:37:42,061 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45227'
2024-01-17 06:37:42,064 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45479'
2024-01-17 06:37:42,073 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41115'
2024-01-17 06:37:42,083 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45371'
2024-01-17 06:37:42,092 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:41251'
2024-01-17 06:37:42,645 - distributed.scheduler - INFO - Receive client connection: Client-e90d89d3-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:42,661 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45638
2024-01-17 06:37:43,949 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:43,949 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:43,954 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:43,955 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43621
2024-01-17 06:37:43,955 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43621
2024-01-17 06:37:43,955 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35033
2024-01-17 06:37:43,955 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:43,955 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:43,955 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:43,955 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:43,955 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j50vxbb5
2024-01-17 06:37:43,955 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f5af67d2-2ab1-4ac2-8d33-4d8ec7d6d05d
2024-01-17 06:37:44,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:44,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:44,475 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:44,476 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39161
2024-01-17 06:37:44,476 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39161
2024-01-17 06:37:44,476 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37685
2024-01-17 06:37:44,476 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:44,476 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:44,476 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:44,476 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:44,476 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hmxsmki5
2024-01-17 06:37:44,476 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e9dd5820-7165-4c1a-8a86-fc6b9ea8f792
2024-01-17 06:37:44,482 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:44,483 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:44,484 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:44,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:44,486 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:44,486 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:44,486 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:44,486 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:44,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:44,488 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:44,488 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:44,489 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43221
2024-01-17 06:37:44,489 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43221
2024-01-17 06:37:44,489 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35105
2024-01-17 06:37:44,489 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:44,489 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:44,489 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:44,489 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:44,489 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ch0o50s2
2024-01-17 06:37:44,489 - distributed.worker - INFO - Starting Worker plugin PreImport-332a6bed-b3dd-4ec3-bc0d-000bd9cf433a
2024-01-17 06:37:44,489 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:44,489 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6fa4007f-2a05-42b4-bd69-956b05175663
2024-01-17 06:37:44,490 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45879
2024-01-17 06:37:44,490 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45879
2024-01-17 06:37:44,490 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41037
2024-01-17 06:37:44,490 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:44,490 - distributed.worker - INFO - Starting Worker plugin RMMSetup-80066d5e-854c-44b8-8ca3-6deed618a9ed
2024-01-17 06:37:44,490 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:44,490 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:44,491 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:44,491 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jf00kwqk
2024-01-17 06:37:44,491 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1fa791cb-7495-4f73-9deb-bff535ed2450
2024-01-17 06:37:44,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:44,491 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:44,492 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46473
2024-01-17 06:37:44,492 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46473
2024-01-17 06:37:44,492 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39091
2024-01-17 06:37:44,492 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:44,492 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:44,492 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:44,492 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44851
2024-01-17 06:37:44,492 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:44,492 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44851
2024-01-17 06:37:44,492 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-10h9wzrf
2024-01-17 06:37:44,492 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46417
2024-01-17 06:37:44,492 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:44,492 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:44,492 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:44,492 - distributed.worker - INFO - Starting Worker plugin PreImport-1faafe53-ffec-40e0-ac55-a7602565e447
2024-01-17 06:37:44,492 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:44,493 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-i3okw5pa
2024-01-17 06:37:44,493 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1c88d808-677c-425f-9984-a3c8c060d1b9
2024-01-17 06:37:44,493 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4d59b39b-da5b-4239-a645-4502890599f8
2024-01-17 06:37:44,493 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:44,493 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:44,493 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:44,493 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4b4debb6-5427-48b9-83dd-a8ebfe49e0a4
2024-01-17 06:37:44,494 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46051
2024-01-17 06:37:44,494 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46051
2024-01-17 06:37:44,494 - distributed.worker - INFO -          dashboard at:            127.0.0.1:33465
2024-01-17 06:37:44,495 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:44,495 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:44,495 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:44,495 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:44,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1_dr8wu7
2024-01-17 06:37:44,495 - distributed.worker - INFO - Starting Worker plugin RMMSetup-32bb5c70-aee1-4911-8d8d-a3fb28687838
2024-01-17 06:37:44,498 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:44,499 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42341
2024-01-17 06:37:44,499 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42341
2024-01-17 06:37:44,499 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39445
2024-01-17 06:37:44,499 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:44,499 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:44,499 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:44,499 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:44,500 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d62ad0f3
2024-01-17 06:37:44,500 - distributed.worker - INFO - Starting Worker plugin RMMSetup-9001dac5-6ad0-4055-8624-a12f4557245c
2024-01-17 06:37:47,080 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a48d09d1-9ada-416a-b8c7-abad1f052c02
2024-01-17 06:37:47,081 - distributed.worker - INFO - Starting Worker plugin PreImport-a1bfc930-789b-41d1-bc53-26693ac01aa7
2024-01-17 06:37:47,081 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,113 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43621', status: init, memory: 0, processing: 0>
2024-01-17 06:37:47,115 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43621
2024-01-17 06:37:47,115 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45660
2024-01-17 06:37:47,116 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:47,116 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:47,117 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,118 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:47,140 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-57ed65e8-f02e-4864-a57c-55b5bc6829ea
2024-01-17 06:37:47,141 - distributed.worker - INFO - Starting Worker plugin PreImport-b61de99d-80eb-4f2b-9f1d-ffe521c83ea3
2024-01-17 06:37:47,142 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,148 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-13c07e7a-e2f6-401b-ab77-6fae28c7e374
2024-01-17 06:37:47,149 - distributed.worker - INFO - Starting Worker plugin PreImport-2ba3ef4f-b245-4c84-84d8-a42822dff164
2024-01-17 06:37:47,149 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,153 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1f657935-1ca7-42f8-86d4-9f21bc71e673
2024-01-17 06:37:47,154 - distributed.worker - INFO - Starting Worker plugin PreImport-758e69be-9766-46ea-ac22-736161118049
2024-01-17 06:37:47,155 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,158 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2c87f5b9-53f1-429b-a95c-159f8dd94bd9
2024-01-17 06:37:47,159 - distributed.worker - INFO - Starting Worker plugin PreImport-0efc10ca-046a-41b6-88d9-5af6c71ac284
2024-01-17 06:37:47,159 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,160 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,161 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,163 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-68ae549c-a52b-418f-9cdc-96809118e54b
2024-01-17 06:37:47,165 - distributed.worker - INFO - Starting Worker plugin PreImport-777cdcc7-b6ca-4728-8c93-2cf38370fa59
2024-01-17 06:37:47,166 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,178 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46051', status: init, memory: 0, processing: 0>
2024-01-17 06:37:47,179 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46051
2024-01-17 06:37:47,179 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45664
2024-01-17 06:37:47,180 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:47,181 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:47,181 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,182 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:47,182 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42341', status: init, memory: 0, processing: 0>
2024-01-17 06:37:47,183 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42341
2024-01-17 06:37:47,183 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45668
2024-01-17 06:37:47,184 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44851', status: init, memory: 0, processing: 0>
2024-01-17 06:37:47,184 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:47,184 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44851
2024-01-17 06:37:47,184 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45662
2024-01-17 06:37:47,184 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:47,184 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,185 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45879', status: init, memory: 0, processing: 0>
2024-01-17 06:37:47,185 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45879
2024-01-17 06:37:47,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45672
2024-01-17 06:37:47,186 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:47,186 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:47,186 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:47,187 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:47,187 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,187 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:47,187 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,188 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:47,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:47,194 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46473', status: init, memory: 0, processing: 0>
2024-01-17 06:37:47,194 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46473
2024-01-17 06:37:47,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45682
2024-01-17 06:37:47,196 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:47,197 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:47,197 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,198 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:43221', status: init, memory: 0, processing: 0>
2024-01-17 06:37:47,199 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:43221
2024-01-17 06:37:47,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45686
2024-01-17 06:37:47,199 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:47,200 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:47,201 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:47,201 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,203 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:47,204 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39161', status: init, memory: 0, processing: 0>
2024-01-17 06:37:47,205 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39161
2024-01-17 06:37:47,205 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45700
2024-01-17 06:37:47,206 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:47,207 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:47,208 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:47,209 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:47,227 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:47,227 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:47,227 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:47,227 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:47,227 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:47,228 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:47,228 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:47,228 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:37:47,245 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:47,245 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:47,245 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:47,245 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:47,245 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:47,245 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:47,245 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:47,246 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:47,255 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:47,257 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:47,259 - distributed.scheduler - INFO - Remove client Client-e90d89d3-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:47,259 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45638; closing.
2024-01-17 06:37:47,260 - distributed.scheduler - INFO - Remove client Client-e90d89d3-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:47,260 - distributed.scheduler - INFO - Close client connection: Client-e90d89d3-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:47,261 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35903'. Reason: nanny-close
2024-01-17 06:37:47,262 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:47,262 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34097'. Reason: nanny-close
2024-01-17 06:37:47,263 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:47,263 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34705'. Reason: nanny-close
2024-01-17 06:37:47,264 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43221. Reason: nanny-close
2024-01-17 06:37:47,264 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:47,264 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45227'. Reason: nanny-close
2024-01-17 06:37:47,264 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39161. Reason: nanny-close
2024-01-17 06:37:47,264 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:47,264 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45479'. Reason: nanny-close
2024-01-17 06:37:47,265 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45879. Reason: nanny-close
2024-01-17 06:37:47,265 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:47,265 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41115'. Reason: nanny-close
2024-01-17 06:37:47,265 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43621. Reason: nanny-close
2024-01-17 06:37:47,265 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:47,265 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45371'. Reason: nanny-close
2024-01-17 06:37:47,266 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44851. Reason: nanny-close
2024-01-17 06:37:47,266 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:47,266 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:47,266 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:41251'. Reason: nanny-close
2024-01-17 06:37:47,266 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45686; closing.
2024-01-17 06:37:47,266 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46473. Reason: nanny-close
2024-01-17 06:37:47,266 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:47,266 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43221', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473467.2665987')
2024-01-17 06:37:47,266 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:47,266 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42341. Reason: nanny-close
2024-01-17 06:37:47,267 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:47,267 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:47,267 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46051. Reason: nanny-close
2024-01-17 06:37:47,267 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:47,268 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:47,268 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:47,268 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:47,268 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:47,268 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45660; closing.
2024-01-17 06:37:47,268 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45700; closing.
2024-01-17 06:37:47,268 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:47,268 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45672; closing.
2024-01-17 06:37:47,269 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:47,269 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:47,269 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:47,269 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:43621', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473467.269879')
2024-01-17 06:37:47,270 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:47,270 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39161', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473467.2702725')
2024-01-17 06:37:47,270 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:47,270 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45879', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473467.2706683')
2024-01-17 06:37:47,271 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45662; closing.
2024-01-17 06:37:47,271 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:47,271 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44851', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473467.2716732')
2024-01-17 06:37:47,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45682; closing.
2024-01-17 06:37:47,272 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45668; closing.
2024-01-17 06:37:47,272 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46473', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473467.2727203')
2024-01-17 06:37:47,273 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42341', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473467.2731612')
2024-01-17 06:37:47,273 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45664; closing.
2024-01-17 06:37:47,274 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46051', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473467.2740715')
2024-01-17 06:37:47,274 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:37:48,328 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:37:48,328 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:37:48,329 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:37:48,330 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:37:48,330 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_async_with_maximum_pool_size 2024-01-17 06:37:50,681 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:50,686 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46209 instead
  warnings.warn(
2024-01-17 06:37:50,691 - distributed.scheduler - INFO - State start
2024-01-17 06:37:50,714 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:50,715 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:37:50,715 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:46209/status
2024-01-17 06:37:50,715 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:37:50,813 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44721'
2024-01-17 06:37:50,826 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33287'
2024-01-17 06:37:50,835 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46737'
2024-01-17 06:37:50,849 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32925'
2024-01-17 06:37:50,852 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42129'
2024-01-17 06:37:50,861 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36025'
2024-01-17 06:37:50,871 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:45995'
2024-01-17 06:37:50,880 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:43103'
2024-01-17 06:37:51,653 - distributed.scheduler - INFO - Receive client connection: Client-ee5233b9-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:51,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39574
2024-01-17 06:37:52,717 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:52,718 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:52,719 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:52,719 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:52,722 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:52,723 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34421
2024-01-17 06:37:52,723 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34421
2024-01-17 06:37:52,723 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43983
2024-01-17 06:37:52,723 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:52,723 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:52,723 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:52,723 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:52,723 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:52,723 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3s_75b09
2024-01-17 06:37:52,723 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e1743811-3b02-4d06-ae56-c7438f8c3084
2024-01-17 06:37:52,724 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36689
2024-01-17 06:37:52,724 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36689
2024-01-17 06:37:52,724 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45831
2024-01-17 06:37:52,724 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:52,724 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:52,724 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:52,724 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:52,724 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ne8cfuwy
2024-01-17 06:37:52,724 - distributed.worker - INFO - Starting Worker plugin PreImport-a381409a-e5a2-4d3f-ac6a-974a8c8831e7
2024-01-17 06:37:52,725 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7725989d-d0c0-4ce6-9b4e-15dd6a171953
2024-01-17 06:37:52,725 - distributed.worker - INFO - Starting Worker plugin RMMSetup-616b082a-0ace-4e1f-ae2e-e1042f710173
2024-01-17 06:37:52,771 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:52,771 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:52,776 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:52,777 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34019
2024-01-17 06:37:52,777 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34019
2024-01-17 06:37:52,777 - distributed.worker - INFO -          dashboard at:            127.0.0.1:45851
2024-01-17 06:37:52,777 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:52,777 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:52,777 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:52,777 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:52,778 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5bgrcsj2
2024-01-17 06:37:52,778 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e7661f1f-26a0-45f9-b364-9b2cef57238c
2024-01-17 06:37:52,792 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:52,792 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:52,797 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:52,797 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33327
2024-01-17 06:37:52,798 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33327
2024-01-17 06:37:52,798 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42627
2024-01-17 06:37:52,798 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:52,798 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:52,798 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:52,798 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:52,798 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qsn9p3b6
2024-01-17 06:37:52,798 - distributed.worker - INFO - Starting Worker plugin PreImport-bdc30371-1849-4cd9-b617-a59d120b3522
2024-01-17 06:37:52,798 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eebdd196-ebbd-49cb-beb8-9937331875ee
2024-01-17 06:37:52,799 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b399b012-54c9-44c6-b912-56b1f0bdadbf
2024-01-17 06:37:52,820 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:52,820 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:52,824 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:52,825 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33333
2024-01-17 06:37:52,825 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33333
2024-01-17 06:37:52,825 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32769
2024-01-17 06:37:52,825 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:52,826 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:52,826 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:52,826 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:52,826 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9_fzgpl6
2024-01-17 06:37:52,826 - distributed.worker - INFO - Starting Worker plugin PreImport-b9edddc3-baba-408c-b8b7-44796a6b7ec5
2024-01-17 06:37:52,826 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-baa6050a-fc61-4a6a-99f4-090b37f11036
2024-01-17 06:37:52,826 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a2d6ae0e-5341-4b34-aa37-55a4ead6f80b
2024-01-17 06:37:52,827 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:52,827 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:52,832 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:52,832 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35877
2024-01-17 06:37:52,833 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35877
2024-01-17 06:37:52,833 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39593
2024-01-17 06:37:52,833 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:52,833 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:52,833 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:52,833 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:52,833 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bothde8k
2024-01-17 06:37:52,833 - distributed.worker - INFO - Starting Worker plugin RMMSetup-4fc9ff07-fb69-4473-8d6b-a87c26b922c1
2024-01-17 06:37:53,006 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:53,007 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:53,012 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:53,013 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39155
2024-01-17 06:37:53,013 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39155
2024-01-17 06:37:53,013 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35171
2024-01-17 06:37:53,013 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:53,013 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:53,013 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:53,013 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:53,013 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-oee1a17_
2024-01-17 06:37:53,013 - distributed.worker - INFO - Starting Worker plugin RMMSetup-25f7de77-dc4c-46a0-aec4-dcf569b363de
2024-01-17 06:37:53,207 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:37:53,207 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:37:53,212 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:37:53,213 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34801
2024-01-17 06:37:53,213 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34801
2024-01-17 06:37:53,214 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34295
2024-01-17 06:37:53,214 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:37:53,214 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:53,214 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:37:53,214 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:37:53,214 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-y2on173z
2024-01-17 06:37:53,214 - distributed.worker - INFO - Starting Worker plugin RMMSetup-96f48a26-b297-4bb0-ad25-46a14e3e06a6
2024-01-17 06:37:54,614 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:54,642 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36689', status: init, memory: 0, processing: 0>
2024-01-17 06:37:54,643 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36689
2024-01-17 06:37:54,644 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39594
2024-01-17 06:37:54,645 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:54,645 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:54,645 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:54,647 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:54,913 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-48642267-31b0-405b-a9a5-1460eac61202
2024-01-17 06:37:54,914 - distributed.worker - INFO - Starting Worker plugin PreImport-1563f722-eb9e-49b6-9672-6f6cba095355
2024-01-17 06:37:54,915 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:54,950 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34421', status: init, memory: 0, processing: 0>
2024-01-17 06:37:54,951 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34421
2024-01-17 06:37:54,951 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39610
2024-01-17 06:37:54,952 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:54,953 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:54,953 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:54,955 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:55,009 - distributed.worker - INFO - Starting Worker plugin PreImport-0c888913-f933-42dd-a4a4-9eab09cc461c
2024-01-17 06:37:55,010 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-086c5572-64ad-479e-956f-5f49ff81e174
2024-01-17 06:37:55,011 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,010 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81617915-3530-48d6-8312-6e8e18e72756
2024-01-17 06:37:55,012 - distributed.worker - INFO - Starting Worker plugin PreImport-19839847-db73-4903-bc28-c04da3ff8230
2024-01-17 06:37:55,012 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,035 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:35877', status: init, memory: 0, processing: 0>
2024-01-17 06:37:55,035 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:35877
2024-01-17 06:37:55,035 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39614
2024-01-17 06:37:55,036 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:55,037 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:55,037 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,038 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:55,044 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-db038ffe-24e3-481f-8ee5-7c0681e29f2d
2024-01-17 06:37:55,044 - distributed.worker - INFO - Starting Worker plugin PreImport-1415da69-6ae6-434e-94e0-0d980f52a69a
2024-01-17 06:37:55,045 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,046 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34019', status: init, memory: 0, processing: 0>
2024-01-17 06:37:55,047 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34019
2024-01-17 06:37:55,047 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39618
2024-01-17 06:37:55,048 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:55,049 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:55,049 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,051 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:55,055 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-b52dbc91-2700-4895-b7bf-fc459c7d46ce
2024-01-17 06:37:55,056 - distributed.worker - INFO - Starting Worker plugin PreImport-20ac6b85-a4a7-4bd7-be22-a7502aefd4ee
2024-01-17 06:37:55,057 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,062 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,068 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:34801', status: init, memory: 0, processing: 0>
2024-01-17 06:37:55,068 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:34801
2024-01-17 06:37:55,068 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39624
2024-01-17 06:37:55,069 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,069 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:55,070 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:55,070 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,071 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:55,087 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33333', status: init, memory: 0, processing: 0>
2024-01-17 06:37:55,088 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33333
2024-01-17 06:37:55,088 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39648
2024-01-17 06:37:55,089 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:55,089 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:55,089 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,091 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:55,094 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:39155', status: init, memory: 0, processing: 0>
2024-01-17 06:37:55,095 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:39155
2024-01-17 06:37:55,095 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39636
2024-01-17 06:37:55,096 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:55,097 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:55,097 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,099 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:55,106 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33327', status: init, memory: 0, processing: 0>
2024-01-17 06:37:55,107 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33327
2024-01-17 06:37:55,107 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39654
2024-01-17 06:37:55,109 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:37:55,110 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:37:55,110 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:37:55,112 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:37:55,167 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:55,168 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:55,168 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:55,168 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:55,168 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:55,168 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:55,169 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:55,248 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:55,259 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:55,259 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:55,259 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:55,260 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:55,260 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:55,260 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:55,260 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:55,260 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:37:55,268 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:55,270 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:37:55,273 - distributed.scheduler - INFO - Remove client Client-ee5233b9-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:55,273 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39574; closing.
2024-01-17 06:37:55,273 - distributed.scheduler - INFO - Remove client Client-ee5233b9-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:55,274 - distributed.scheduler - INFO - Close client connection: Client-ee5233b9-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:37:55,275 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44721'. Reason: nanny-close
2024-01-17 06:37:55,275 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:55,275 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33287'. Reason: nanny-close
2024-01-17 06:37:55,276 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:55,276 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46737'. Reason: nanny-close
2024-01-17 06:37:55,276 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34421. Reason: nanny-close
2024-01-17 06:37:55,276 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:55,277 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32925'. Reason: nanny-close
2024-01-17 06:37:55,277 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33327. Reason: nanny-close
2024-01-17 06:37:55,277 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:55,277 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36689. Reason: nanny-close
2024-01-17 06:37:55,277 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42129'. Reason: nanny-close
2024-01-17 06:37:55,278 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:55,278 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36025'. Reason: nanny-close
2024-01-17 06:37:55,278 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34801. Reason: nanny-close
2024-01-17 06:37:55,278 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:55,278 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:45995'. Reason: nanny-close
2024-01-17 06:37:55,278 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39155. Reason: nanny-close
2024-01-17 06:37:55,278 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:55,279 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:43103'. Reason: nanny-close
2024-01-17 06:37:55,279 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:55,279 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34019. Reason: nanny-close
2024-01-17 06:37:55,279 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:37:55,279 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:55,279 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39610; closing.
2024-01-17 06:37:55,279 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:55,279 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35877. Reason: nanny-close
2024-01-17 06:37:55,279 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34421', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473475.2797992')
2024-01-17 06:37:55,279 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33333. Reason: nanny-close
2024-01-17 06:37:55,279 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:55,280 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39594; closing.
2024-01-17 06:37:55,280 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:55,281 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:55,281 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:55,281 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:55,281 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:55,281 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:55,281 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36689', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473475.2814665')
2024-01-17 06:37:55,281 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:55,282 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39654; closing.
2024-01-17 06:37:55,282 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:37:55,282 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:55,283 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:55,283 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:55,283 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33327', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473475.283583')
2024-01-17 06:37:55,283 - distributed.nanny - INFO - Worker closed
2024-01-17 06:37:55,284 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39624; closing.
2024-01-17 06:37:55,285 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:39594>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/batched.py", line 115, in _background_send
    nbytes = yield coro
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/gen.py", line 767, in run
    value = future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 262, in write
    raise CommClosedError()
distributed.comm.core.CommClosedError
2024-01-17 06:37:55,288 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34801', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473475.2883')
2024-01-17 06:37:55,288 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39636; closing.
2024-01-17 06:37:55,289 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39618; closing.
2024-01-17 06:37:55,289 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39614; closing.
2024-01-17 06:37:55,289 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39648; closing.
2024-01-17 06:37:55,290 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:39155', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473475.290322')
2024-01-17 06:37:55,290 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:34019', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473475.2908497')
2024-01-17 06:37:55,291 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:35877', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473475.2913842')
2024-01-17 06:37:55,292 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33333', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473475.2919393')
2024-01-17 06:37:55,292 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:37:56,291 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:37:56,291 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:37:56,292 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:37:56,293 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:37:56,294 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_logging 2024-01-17 06:37:58,525 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:58,530 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42681 instead
  warnings.warn(
2024-01-17 06:37:58,534 - distributed.scheduler - INFO - State start
2024-01-17 06:37:58,732 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:37:58,734 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:37:58,735 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42681/status
2024-01-17 06:37:58,735 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:37:58,863 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34003'
2024-01-17 06:37:58,877 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:42191'
2024-01-17 06:37:58,892 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38111'
2024-01-17 06:37:58,904 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34423'
2024-01-17 06:37:58,907 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:36931'
2024-01-17 06:37:58,917 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39667'
2024-01-17 06:37:58,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44359'
2024-01-17 06:37:58,937 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:46539'
2024-01-17 06:38:00,688 - distributed.scheduler - INFO - Receive client connection: Client-f30d6ef7-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:00,704 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39002
2024-01-17 06:38:00,758 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:00,758 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:00,762 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:00,762 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:00,763 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:00,764 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45099
2024-01-17 06:38:00,764 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45099
2024-01-17 06:38:00,764 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37143
2024-01-17 06:38:00,764 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:00,764 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:00,764 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:00,764 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:00,764 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-gd8e4ldd
2024-01-17 06:38:00,764 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a10c0bb1-ec3e-4cf3-9adc-596a32c8b08a
2024-01-17 06:38:00,766 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:00,767 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40173
2024-01-17 06:38:00,767 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40173
2024-01-17 06:38:00,767 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42511
2024-01-17 06:38:00,767 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:00,767 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:00,767 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:00,767 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:00,767 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-j4bh11ke
2024-01-17 06:38:00,768 - distributed.worker - INFO - Starting Worker plugin PreImport-57a555ff-b908-4b6b-8d6b-d8cc14923d5c
2024-01-17 06:38:00,768 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-31917d8c-2fec-487a-887d-013ecce88581
2024-01-17 06:38:00,769 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b2112461-0ec6-40ff-9c08-77dc27411b44
2024-01-17 06:38:00,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:00,828 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:00,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:00,828 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:00,829 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:00,829 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:00,832 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:00,832 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:00,833 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:00,833 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40583
2024-01-17 06:38:00,833 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33941
2024-01-17 06:38:00,833 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40583
2024-01-17 06:38:00,833 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33941
2024-01-17 06:38:00,833 - distributed.worker - INFO -          dashboard at:            127.0.0.1:40641
2024-01-17 06:38:00,834 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42471
2024-01-17 06:38:00,834 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:00,834 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:00,834 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:00,834 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:00,834 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:00,834 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:00,834 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:00,834 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:00,834 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-vh29vl_l
2024-01-17 06:38:00,834 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-3hgkz6tw
2024-01-17 06:38:00,834 - distributed.worker - INFO - Starting Worker plugin PreImport-63855f21-3839-418e-a2bb-fafe4c8a6b02
2024-01-17 06:38:00,834 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a0739f89-74f8-45ae-ba53-55aed746b948
2024-01-17 06:38:00,834 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-de55bab7-f700-4387-866f-838522490b06
2024-01-17 06:38:00,834 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37097
2024-01-17 06:38:00,834 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37097
2024-01-17 06:38:00,834 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44749
2024-01-17 06:38:00,834 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:00,834 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:00,834 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:00,835 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:00,835 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-q8a8echr
2024-01-17 06:38:00,835 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c4e2667d-392e-42b5-8bde-28c8963558c7
2024-01-17 06:38:00,835 - distributed.worker - INFO - Starting Worker plugin RMMSetup-483088f9-a2fd-4969-a039-5b29d77b642d
2024-01-17 06:38:00,843 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:00,843 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:00,848 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:00,849 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45701
2024-01-17 06:38:00,849 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45701
2024-01-17 06:38:00,850 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35355
2024-01-17 06:38:00,850 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:00,850 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:00,850 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:00,850 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:00,850 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jp5jg6bk
2024-01-17 06:38:00,850 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b6fd9dd1-479d-4a34-8bb2-5ceac923250e
2024-01-17 06:38:00,855 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:00,855 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:00,859 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:00,860 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38023
2024-01-17 06:38:00,860 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38023
2024-01-17 06:38:00,860 - distributed.worker - INFO -          dashboard at:            127.0.0.1:37063
2024-01-17 06:38:00,860 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:00,860 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:00,861 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:00,861 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:00,861 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5h_iv5j6
2024-01-17 06:38:00,861 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e4011366-b5fb-4fdb-ae48-f586e9f5b870
2024-01-17 06:38:01,030 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:01,030 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:01,035 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:01,036 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:36447
2024-01-17 06:38:01,036 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:36447
2024-01-17 06:38:01,036 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36865
2024-01-17 06:38:01,036 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:01,036 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:01,036 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:01,036 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:01,036 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ahl3ig5o
2024-01-17 06:38:01,037 - distributed.worker - INFO - Starting Worker plugin RMMSetup-42de6cba-5b7c-4b2a-924f-ad59bc156c5c
2024-01-17 06:38:02,843 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:02,876 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40173', status: init, memory: 0, processing: 0>
2024-01-17 06:38:02,877 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40173
2024-01-17 06:38:02,877 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39018
2024-01-17 06:38:02,878 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0c274be7-fa1f-470f-ba10-8f679c133a52
2024-01-17 06:38:02,879 - distributed.worker - INFO - Starting Worker plugin PreImport-7158921a-5bb6-4e28-9581-a2ad2d7601d3
2024-01-17 06:38:02,879 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:02,880 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:02,880 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:02,880 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:02,882 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:02,913 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45099', status: init, memory: 0, processing: 0>
2024-01-17 06:38:02,913 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45099
2024-01-17 06:38:02,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39032
2024-01-17 06:38:02,915 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:02,916 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:02,916 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:02,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:02,974 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7259a0d6-bda3-40a1-b74b-772b3a0a8038
2024-01-17 06:38:02,975 - distributed.worker - INFO - Starting Worker plugin PreImport-7a17e48f-836a-464a-a9de-55c86a84bcda
2024-01-17 06:38:02,976 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:02,994 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-71783ed5-1b9a-4406-9987-d38783dda757
2024-01-17 06:38:02,994 - distributed.worker - INFO - Starting Worker plugin PreImport-1de3ce14-6210-4578-ac9d-8095fd0ae71d
2024-01-17 06:38:02,995 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,000 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33941', status: init, memory: 0, processing: 0>
2024-01-17 06:38:03,000 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33941
2024-01-17 06:38:03,001 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39044
2024-01-17 06:38:03,001 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:03,002 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:03,002 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,004 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:03,013 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-357428f6-622a-4183-a717-bee6153f1c5d
2024-01-17 06:38:03,014 - distributed.worker - INFO - Starting Worker plugin PreImport-6de4fd9c-7912-4050-bc21-8015d862bbb8
2024-01-17 06:38:03,015 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,018 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37097', status: init, memory: 0, processing: 0>
2024-01-17 06:38:03,018 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37097
2024-01-17 06:38:03,019 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39052
2024-01-17 06:38:03,020 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:03,020 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:03,020 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,022 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:03,024 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-3f0a3def-3245-42f2-bc29-3db7191e3dca
2024-01-17 06:38:03,026 - distributed.worker - INFO - Starting Worker plugin PreImport-fceccc96-e504-4454-8e58-1f7a509bab7a
2024-01-17 06:38:03,027 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,042 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38023', status: init, memory: 0, processing: 0>
2024-01-17 06:38:03,043 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38023
2024-01-17 06:38:03,043 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39058
2024-01-17 06:38:03,044 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:03,045 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:03,045 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,047 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:03,047 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,050 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9259532f-0094-4031-b145-4968784ca04a
2024-01-17 06:38:03,051 - distributed.worker - INFO - Starting Worker plugin PreImport-9b4ab048-21be-4964-8598-c5ef6f5e5449
2024-01-17 06:38:03,051 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,058 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45701', status: init, memory: 0, processing: 0>
2024-01-17 06:38:03,058 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45701
2024-01-17 06:38:03,058 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39068
2024-01-17 06:38:03,060 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:03,061 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:03,061 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,067 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:03,072 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:36447', status: init, memory: 0, processing: 0>
2024-01-17 06:38:03,073 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:36447
2024-01-17 06:38:03,073 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39090
2024-01-17 06:38:03,074 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:03,075 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:03,075 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,076 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:03,080 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40583', status: init, memory: 0, processing: 0>
2024-01-17 06:38:03,081 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40583
2024-01-17 06:38:03,081 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39074
2024-01-17 06:38:03,082 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:03,083 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:03,083 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:03,085 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:03,114 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:03,114 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:03,115 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:03,115 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:03,115 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:03,116 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:03,117 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:03,124 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:03,128 - distributed.scheduler - INFO - Remove client Client-f30d6ef7-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:03,129 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39002; closing.
2024-01-17 06:38:03,129 - distributed.scheduler - INFO - Remove client Client-f30d6ef7-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:03,129 - distributed.scheduler - INFO - Close client connection: Client-f30d6ef7-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:03,130 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34003'. Reason: nanny-close
2024-01-17 06:38:03,130 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:03,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:42191'. Reason: nanny-close
2024-01-17 06:38:03,131 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:03,131 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38111'. Reason: nanny-close
2024-01-17 06:38:03,132 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40173. Reason: nanny-close
2024-01-17 06:38:03,132 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:03,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34423'. Reason: nanny-close
2024-01-17 06:38:03,132 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:03,132 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45099. Reason: nanny-close
2024-01-17 06:38:03,132 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:36931'. Reason: nanny-close
2024-01-17 06:38:03,132 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37097. Reason: nanny-close
2024-01-17 06:38:03,133 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:03,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39667'. Reason: nanny-close
2024-01-17 06:38:03,133 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:36447. Reason: nanny-close
2024-01-17 06:38:03,133 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:03,133 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44359'. Reason: nanny-close
2024-01-17 06:38:03,133 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:03,133 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45701. Reason: nanny-close
2024-01-17 06:38:03,134 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:46539'. Reason: nanny-close
2024-01-17 06:38:03,134 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40583. Reason: nanny-close
2024-01-17 06:38:03,134 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:03,134 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33941. Reason: nanny-close
2024-01-17 06:38:03,134 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:03,134 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39052; closing.
2024-01-17 06:38:03,135 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37097', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473483.1351235')
2024-01-17 06:38:03,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:03,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:03,135 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38023. Reason: nanny-close
2024-01-17 06:38:03,135 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:03,136 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:03,136 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:03,136 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39018; closing.
2024-01-17 06:38:03,136 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:03,136 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:03,136 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:03,137 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:03,137 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39068; closing.
2024-01-17 06:38:03,137 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:03,137 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:03,137 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40173', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473483.1377285')
2024-01-17 06:38:03,137 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:03,138 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:03,138 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39090; closing.
2024-01-17 06:38:03,138 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:03,138 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39032; closing.
2024-01-17 06:38:03,139 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45701', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473483.1393373')
2024-01-17 06:38:03,139 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:03,139 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:36447', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473483.1397688')
2024-01-17 06:38:03,140 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39074; closing.
2024-01-17 06:38:03,140 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45099', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473483.1403391')
2024-01-17 06:38:03,140 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39044; closing.
2024-01-17 06:38:03,141 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40583', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473483.14122')
2024-01-17 06:38:03,141 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33941', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473483.141633')
2024-01-17 06:38:03,142 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39058; closing.
2024-01-17 06:38:03,142 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38023', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473483.1424491')
2024-01-17 06:38:03,142 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:38:04,196 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:38:04,197 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:38:04,197 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:38:04,198 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:38:04,199 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_dashboard_address 2024-01-17 06:38:06,321 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:06,326 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35681 instead
  warnings.warn(
2024-01-17 06:38:06,330 - distributed.scheduler - INFO - State start
2024-01-17 06:38:06,352 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:06,353 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:38:06,354 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:35681/status
2024-01-17 06:38:06,354 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:38:06,464 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:40787'
2024-01-17 06:38:07,656 - distributed.scheduler - INFO - Receive client connection: Client-f7b91d0c-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:07,671 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39202
2024-01-17 06:38:08,262 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:08,263 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:08,846 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:08,847 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46679
2024-01-17 06:38:08,847 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46679
2024-01-17 06:38:08,847 - distributed.worker - INFO -          dashboard at:             127.0.0.1:9370
2024-01-17 06:38:08,847 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:08,847 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:08,847 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:08,848 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-17 06:38:08,848 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z8yk_0k7
2024-01-17 06:38:08,848 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0bd32542-3bbd-4934-bd6e-14d04eb4c221
2024-01-17 06:38:08,848 - distributed.worker - INFO - Starting Worker plugin PreImport-5e539d9d-e461-487b-a077-71c45eefc6e4
2024-01-17 06:38:08,848 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-0fcbb476-dcec-4a9a-a435-2834d8f44b7c
2024-01-17 06:38:08,848 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:08,912 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46679', status: init, memory: 0, processing: 0>
2024-01-17 06:38:08,913 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46679
2024-01-17 06:38:08,913 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:39216
2024-01-17 06:38:08,914 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:08,916 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:08,916 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:08,918 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:08,999 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:09,002 - distributed.scheduler - INFO - Remove client Client-f7b91d0c-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:09,002 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39202; closing.
2024-01-17 06:38:09,003 - distributed.scheduler - INFO - Remove client Client-f7b91d0c-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:09,003 - distributed.scheduler - INFO - Close client connection: Client-f7b91d0c-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:09,004 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:40787'. Reason: nanny-close
2024-01-17 06:38:09,004 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:09,005 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46679. Reason: nanny-close
2024-01-17 06:38:09,007 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:09,007 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:39216; closing.
2024-01-17 06:38:09,007 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46679', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473489.0077412')
2024-01-17 06:38:09,008 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:38:09,008 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:09,720 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:38:09,720 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:38:09,721 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:38:09,721 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:38:09,722 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_unknown_argument PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import 2024-01-17 06:38:14,030 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:14,034 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42913 instead
  warnings.warn(
2024-01-17 06:38:14,038 - distributed.scheduler - INFO - State start
2024-01-17 06:38:14,061 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:14,062 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:38:14,062 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:42913/status
2024-01-17 06:38:14,063 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:38:14,122 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:37731'
2024-01-17 06:38:14,816 - distributed.scheduler - INFO - Receive client connection: Client-fc4b1b99-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:14,830 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45396
2024-01-17 06:38:15,841 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:15,841 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:16,366 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:16,367 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44037
2024-01-17 06:38:16,367 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44037
2024-01-17 06:38:16,367 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43699
2024-01-17 06:38:16,367 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:16,367 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:16,368 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:16,368 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-17 06:38:16,368 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-1c40154c
2024-01-17 06:38:16,368 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dea8fdc1-d189-4e9b-88ef-079da17b1e10
2024-01-17 06:38:16,368 - distributed.worker - INFO - Starting Worker plugin PreImport-ee122ef0-407a-4cea-8833-f0f305d13fc7
2024-01-17 06:38:16,369 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7f49cd7b-0d36-4b86-bce4-1bc2ae03c3d8
2024-01-17 06:38:16,370 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:16,436 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:44037', status: init, memory: 0, processing: 0>
2024-01-17 06:38:16,437 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:44037
2024-01-17 06:38:16,437 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:45404
2024-01-17 06:38:16,439 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:16,440 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:16,440 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:16,442 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:16,461 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:16,463 - distributed.scheduler - INFO - Remove client Client-fc4b1b99-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:16,464 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45396; closing.
2024-01-17 06:38:16,464 - distributed.scheduler - INFO - Remove client Client-fc4b1b99-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:16,464 - distributed.scheduler - INFO - Close client connection: Client-fc4b1b99-b502-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:16,465 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:37731'. Reason: nanny-close
2024-01-17 06:38:16,466 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:16,467 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44037. Reason: nanny-close
2024-01-17 06:38:16,469 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:16,469 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:45404; closing.
2024-01-17 06:38:16,469 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:44037', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473496.4698815')
2024-01-17 06:38:16,470 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:38:16,471 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:17,231 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:38:17,231 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:38:17,232 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:38:17,233 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:38:17,233 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_pre_import_not_found 2024-01-17 06:38:19,400 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:19,405 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40171 instead
  warnings.warn(
2024-01-17 06:38:19,409 - distributed.scheduler - INFO - State start
2024-01-17 06:38:19,432 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:19,434 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:38:19,434 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:40171/status
2024-01-17 06:38:19,435 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:38:21,850 - distributed.core - INFO - Lost connection to 'tcp://127.0.0.1:45408'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 970, in _handle_comm
    result = await result
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/scheduler.py", line 4440, in add_nanny
    await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed)  local=tcp://127.0.0.1:9369 remote=tcp://127.0.0.1:45408>: Stream is closed
2024-01-17 06:38:22,134 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:38:22,134 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:38:22,134 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:38:22,135 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:38:22,136 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_mig_visible_devices_and_memory_limit_and_nthreads SKIPPED
dask_cuda/tests/test_dask_cuda_worker.py::test_cuda_visible_devices_uuid 2024-01-17 06:38:24,373 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:24,378 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-17 06:38:24,381 - distributed.scheduler - INFO - State start
2024-01-17 06:38:24,403 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:24,404 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9359
2024-01-17 06:38:24,405 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-17 06:38:24,405 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:38:24,510 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34113'
2024-01-17 06:38:25,412 - distributed.scheduler - INFO - Receive client connection: Client-02708f4a-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:25,427 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47782
2024-01-17 06:38:26,105 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:26,105 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:26,109 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:26,110 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33905
2024-01-17 06:38:26,110 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33905
2024-01-17 06:38:26,110 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41535
2024-01-17 06:38:26,110 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9359
2024-01-17 06:38:26,110 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:26,110 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:26,110 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-17 06:38:26,110 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/mockworker-3ts7g1r1
2024-01-17 06:38:26,110 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1c524363-7a79-4e8f-8923-6d796a9c40ed
2024-01-17 06:38:26,110 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f06b07bd-b03e-47a7-903f-8a6933208b7e
2024-01-17 06:38:26,110 - distributed.worker - INFO - Starting Worker plugin PreImport-06f18281-46a6-4d26-ac32-67b5c8c35719
2024-01-17 06:38:26,111 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:26,168 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33905', status: init, memory: 0, processing: 0>
2024-01-17 06:38:26,169 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33905
2024-01-17 06:38:26,169 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47804
2024-01-17 06:38:26,170 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:26,171 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9359
2024-01-17 06:38:26,171 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:26,172 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9359
2024-01-17 06:38:26,250 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:26,253 - distributed.scheduler - INFO - Remove client Client-02708f4a-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:26,254 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47782; closing.
2024-01-17 06:38:26,254 - distributed.scheduler - INFO - Remove client Client-02708f4a-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:26,254 - distributed.scheduler - INFO - Close client connection: Client-02708f4a-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:26,255 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34113'. Reason: nanny-close
2024-01-17 06:38:26,255 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:26,257 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33905. Reason: nanny-close
2024-01-17 06:38:26,258 - distributed.core - INFO - Connection to tcp://127.0.0.1:9359 has been closed.
2024-01-17 06:38:26,258 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:47804; closing.
2024-01-17 06:38:26,259 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33905', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473506.2591774')
2024-01-17 06:38:26,259 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:38:26,260 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:26,870 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:38:26,871 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:38:26,873 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:38:26,875 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9359'
2024-01-17 06:38:26,876 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_rmm_track_allocations 2024-01-17 06:38:29,140 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:29,144 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-17 06:38:29,147 - distributed.scheduler - INFO - State start
2024-01-17 06:38:29,301 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:29,302 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:38:29,303 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-17 06:38:29,303 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:38:29,448 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38293'
2024-01-17 06:38:29,466 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34713'
2024-01-17 06:38:29,480 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:32913'
2024-01-17 06:38:29,494 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:34809'
2024-01-17 06:38:29,496 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:35607'
2024-01-17 06:38:29,507 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:33641'
2024-01-17 06:38:29,518 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:38195'
2024-01-17 06:38:29,529 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39435'
2024-01-17 06:38:30,432 - distributed.scheduler - INFO - Receive client connection: Client-0545e4ec-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:30,450 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52140
2024-01-17 06:38:31,251 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:31,251 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:31,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:31,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:42505
2024-01-17 06:38:31,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:42505
2024-01-17 06:38:31,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44553
2024-01-17 06:38:31,256 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:31,256 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:31,256 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:31,256 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:31,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-jkcqic6w
2024-01-17 06:38:31,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d4f54198-03b1-4ed9-bb46-f92073e9c100
2024-01-17 06:38:31,483 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:31,484 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:31,485 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:31,485 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:31,486 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:31,486 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:31,487 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:31,487 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:31,488 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:31,489 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:31,489 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33479
2024-01-17 06:38:31,489 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:31,489 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33479
2024-01-17 06:38:31,489 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39069
2024-01-17 06:38:31,489 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:31,490 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:31,490 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:31,490 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:31,490 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ejtxo7rb
2024-01-17 06:38:31,490 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:31,490 - distributed.worker - INFO - Starting Worker plugin RMMSetup-c42672f8-b398-47b8-b40c-6761cbcb8560
2024-01-17 06:38:31,490 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:31,491 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45255
2024-01-17 06:38:31,491 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45255
2024-01-17 06:38:31,491 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35449
2024-01-17 06:38:31,491 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:31,491 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:31,491 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:31,491 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:31,491 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:40247
2024-01-17 06:38:31,491 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tolh2ygx
2024-01-17 06:38:31,491 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:40247
2024-01-17 06:38:31,491 - distributed.worker - INFO -          dashboard at:            127.0.0.1:32829
2024-01-17 06:38:31,491 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:31,491 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:31,491 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:31,491 - distributed.worker - INFO - Starting Worker plugin RMMSetup-10f1f7b7-ad91-49d2-829e-a06db79506e4
2024-01-17 06:38:31,491 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:31,491 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mwx4001t
2024-01-17 06:38:31,492 - distributed.worker - INFO - Starting Worker plugin PreImport-94691701-7efe-497b-b70a-b2919a711038
2024-01-17 06:38:31,492 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:31,492 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-bb3311a8-0922-4a7b-b5e0-0f4ae8bdd374
2024-01-17 06:38:31,492 - distributed.worker - INFO - Starting Worker plugin RMMSetup-1b2fc768-c180-4584-8911-fdb4f605ccbd
2024-01-17 06:38:31,493 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38149
2024-01-17 06:38:31,493 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38149
2024-01-17 06:38:31,493 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42671
2024-01-17 06:38:31,493 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:31,493 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:31,493 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:31,493 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:31,493 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-k4q0ydlu
2024-01-17 06:38:31,493 - distributed.worker - INFO - Starting Worker plugin PreImport-0ff48fe0-f928-4225-9efe-0f17c3ad1b29
2024-01-17 06:38:31,494 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-874fc870-5868-4d74-ba2e-bae2444d8a83
2024-01-17 06:38:31,494 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:31,495 - distributed.worker - INFO - Starting Worker plugin RMMSetup-331c448e-59ac-43e4-ad1d-0867de6f2e72
2024-01-17 06:38:31,495 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41263
2024-01-17 06:38:31,495 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41263
2024-01-17 06:38:31,495 - distributed.worker - INFO -          dashboard at:            127.0.0.1:39387
2024-01-17 06:38:31,495 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:31,495 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:31,495 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:31,495 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:31,495 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rvi5usp8
2024-01-17 06:38:31,496 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d8187b40-1612-4d3b-98dc-1f4c76e49f53
2024-01-17 06:38:31,512 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:31,512 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:31,517 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:31,518 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33365
2024-01-17 06:38:31,518 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33365
2024-01-17 06:38:31,518 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44343
2024-01-17 06:38:31,518 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:31,518 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:31,518 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:31,518 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:31,518 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-x8g7l_k1
2024-01-17 06:38:31,518 - distributed.worker - INFO - Starting Worker plugin RMMSetup-17618051-c445-4fc9-9429-bbfc4b94585c
2024-01-17 06:38:31,559 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:31,559 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:31,564 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:31,565 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:41443
2024-01-17 06:38:31,565 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:41443
2024-01-17 06:38:31,566 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34113
2024-01-17 06:38:31,566 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:31,566 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:31,566 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:31,566 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 06:38:31,566 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-18ymzj6f
2024-01-17 06:38:31,566 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b27d4528-d07c-45f7-aa5a-66a4499b21e4
2024-01-17 06:38:31,676 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-2d418685-cc55-4400-8c76-f33da2db7f6f
2024-01-17 06:38:31,677 - distributed.worker - INFO - Starting Worker plugin PreImport-5d409619-362e-4e41-b9c9-bc21a11e2b83
2024-01-17 06:38:31,677 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:31,701 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:42505', status: init, memory: 0, processing: 0>
2024-01-17 06:38:31,703 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:42505
2024-01-17 06:38:31,703 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52154
2024-01-17 06:38:31,704 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:31,705 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:31,705 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:31,706 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:33,325 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,358 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:40247', status: init, memory: 0, processing: 0>
2024-01-17 06:38:33,359 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:40247
2024-01-17 06:38:33,359 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52166
2024-01-17 06:38:33,361 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:33,362 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:33,362 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,364 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:33,372 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7dfebde4-a5f2-4372-bd69-804522148e38
2024-01-17 06:38:33,373 - distributed.worker - INFO - Starting Worker plugin PreImport-52396fb2-d9e7-4f02-881d-9ad931367c28
2024-01-17 06:38:33,373 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8ece31c5-9959-4d33-88dd-b2d3aa0eb02c
2024-01-17 06:38:33,379 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-6f64e781-e93d-4b44-b1fc-a0e72078c22b
2024-01-17 06:38:33,380 - distributed.worker - INFO - Starting Worker plugin PreImport-7e354632-4808-4160-ba30-6a0aa1f03e48
2024-01-17 06:38:33,380 - distributed.worker - INFO - Starting Worker plugin PreImport-a7474bf4-8000-4a97-ab75-77618fd097fe
2024-01-17 06:38:33,380 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,380 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,391 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a552eee3-b4ab-44aa-96ac-dc5bb776b7aa
2024-01-17 06:38:33,391 - distributed.worker - INFO - Starting Worker plugin PreImport-29dd9b41-f648-4cfd-a19d-949d314fbbc8
2024-01-17 06:38:33,392 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,398 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:45255', status: init, memory: 0, processing: 0>
2024-01-17 06:38:33,399 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:45255
2024-01-17 06:38:33,399 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52172
2024-01-17 06:38:33,400 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:33,401 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:33,401 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,402 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:33,404 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41263', status: init, memory: 0, processing: 0>
2024-01-17 06:38:33,404 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41263
2024-01-17 06:38:33,404 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52180
2024-01-17 06:38:33,405 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:33,406 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:33,406 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,407 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33365', status: init, memory: 0, processing: 0>
2024-01-17 06:38:33,408 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33365
2024-01-17 06:38:33,408 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:33,408 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52174
2024-01-17 06:38:33,409 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:33,409 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:33,410 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,411 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:33,415 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:41443', status: init, memory: 0, processing: 0>
2024-01-17 06:38:33,415 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:41443
2024-01-17 06:38:33,415 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52186
2024-01-17 06:38:33,415 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,416 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:33,417 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:33,417 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,418 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:33,420 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1799b92e-fd9b-4282-8f70-5824cbd4a8aa
2024-01-17 06:38:33,421 - distributed.worker - INFO - Starting Worker plugin PreImport-4f6412c2-c2cb-4f4a-ab5a-360921e9aefc
2024-01-17 06:38:33,422 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,450 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:38149', status: init, memory: 0, processing: 0>
2024-01-17 06:38:33,451 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:38149
2024-01-17 06:38:33,451 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52190
2024-01-17 06:38:33,452 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:33,453 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:33,453 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,455 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:33,458 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:33479', status: init, memory: 0, processing: 0>
2024-01-17 06:38:33,458 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:33479
2024-01-17 06:38:33,459 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52204
2024-01-17 06:38:33,460 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:33,461 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:33,461 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:33,463 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:33,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:33,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:33,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:33,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:33,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:33,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:33,472 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:33,474 - distributed.worker - INFO - Run out-of-band function 'get_current_device_resource_type'
2024-01-17 06:38:33,492 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:33,493 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:33,493 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:33,493 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:33,493 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:33,493 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:33,493 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:33,493 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:33,497 - distributed.scheduler - INFO - Remove client Client-0545e4ec-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:33,497 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52140; closing.
2024-01-17 06:38:33,498 - distributed.scheduler - INFO - Remove client Client-0545e4ec-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:33,498 - distributed.scheduler - INFO - Close client connection: Client-0545e4ec-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:33,499 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38293'. Reason: nanny-close
2024-01-17 06:38:33,499 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:33,500 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34713'. Reason: nanny-close
2024-01-17 06:38:33,500 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:33,500 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:32913'. Reason: nanny-close
2024-01-17 06:38:33,500 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38149. Reason: nanny-close
2024-01-17 06:38:33,501 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:33,501 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:34809'. Reason: nanny-close
2024-01-17 06:38:33,501 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33479. Reason: nanny-close
2024-01-17 06:38:33,501 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:33,501 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:35607'. Reason: nanny-close
2024-01-17 06:38:33,501 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41263. Reason: nanny-close
2024-01-17 06:38:33,502 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:33,502 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:33641'. Reason: nanny-close
2024-01-17 06:38:33,502 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45255. Reason: nanny-close
2024-01-17 06:38:33,502 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:33,502 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:38195'. Reason: nanny-close
2024-01-17 06:38:33,502 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:42505. Reason: nanny-close
2024-01-17 06:38:33,502 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:33,503 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39435'. Reason: nanny-close
2024-01-17 06:38:33,503 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52190; closing.
2024-01-17 06:38:33,503 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:33,503 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:33,503 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:33,503 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38149', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473513.5035198')
2024-01-17 06:38:33,503 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:40247. Reason: nanny-close
2024-01-17 06:38:33,503 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33365. Reason: nanny-close
2024-01-17 06:38:33,503 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:41443. Reason: nanny-close
2024-01-17 06:38:33,504 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:33,504 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:33,504 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52180; closing.
2024-01-17 06:38:33,504 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:33,504 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:33,505 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:33,505 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:33,505 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41263', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473513.505882')
2024-01-17 06:38:33,505 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:33,505 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:33,506 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52172; closing.
2024-01-17 06:38:33,506 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:33,506 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52204; closing.
2024-01-17 06:38:33,506 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:33,507 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:33,507 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:45255', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473513.5073533')
2024-01-17 06:38:33,507 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:33,507 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52154; closing.
2024-01-17 06:38:33,507 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:33,507 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33479', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473513.5079067')
2024-01-17 06:38:33,508 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:42505', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473513.5086956')
2024-01-17 06:38:33,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52174; closing.
2024-01-17 06:38:33,509 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52186; closing.
2024-01-17 06:38:33,509 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:33365', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473513.5097656')
2024-01-17 06:38:33,510 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:41443', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473513.5101323')
2024-01-17 06:38:33,510 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52166; closing.
2024-01-17 06:38:33,510 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:33,510 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:40247', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473513.5109217')
2024-01-17 06:38:33,511 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:38:34,515 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:38:34,516 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:38:34,516 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:38:34,517 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:38:34,518 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_get_cluster_configuration 2024-01-17 06:38:36,761 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:36,766 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-17 06:38:36,769 - distributed.scheduler - INFO - State start
2024-01-17 06:38:36,790 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:36,791 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:38:36,792 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-17 06:38:36,792 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:38:36,858 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:44691'
2024-01-17 06:38:38,104 - distributed.scheduler - INFO - Receive client connection: Client-09d76ccf-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:38,119 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52340
2024-01-17 06:38:38,530 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:38,530 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:38,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:38,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:37391
2024-01-17 06:38:38,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:37391
2024-01-17 06:38:38,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35001
2024-01-17 06:38:38,535 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:38,535 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:38,535 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:38,535 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-17 06:38:38,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-odbeqk6h
2024-01-17 06:38:38,535 - distributed.worker - INFO - Starting Worker plugin RMMSetup-0a6c4fa3-73cc-4686-b4e4-8dee64cc8fef
2024-01-17 06:38:38,828 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f5091e55-8387-4c66-a2c4-ade5edea6b7d
2024-01-17 06:38:38,828 - distributed.worker - INFO - Starting Worker plugin PreImport-c5cd9da1-f6b1-43f8-affb-c2cc6f6fc49a
2024-01-17 06:38:38,828 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:38,890 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:37391', status: init, memory: 0, processing: 0>
2024-01-17 06:38:38,891 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:37391
2024-01-17 06:38:38,891 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:52356
2024-01-17 06:38:38,892 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:38,893 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:38,893 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:38,894 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:38,939 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:38:38,945 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:38,947 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:38,949 - distributed.scheduler - INFO - Remove client Client-09d76ccf-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:38,949 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52340; closing.
2024-01-17 06:38:38,950 - distributed.scheduler - INFO - Remove client Client-09d76ccf-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:38,950 - distributed.scheduler - INFO - Close client connection: Client-09d76ccf-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:38,951 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:44691'. Reason: nanny-close
2024-01-17 06:38:38,951 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:38,952 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:37391. Reason: nanny-close
2024-01-17 06:38:38,954 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:38,954 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:52356; closing.
2024-01-17 06:38:38,954 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:37391', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473518.954705')
2024-01-17 06:38:38,954 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:38:38,955 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:39,666 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:38:39,666 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:38:39,667 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:38:39,668 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:38:39,668 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_fraction_limits 2024-01-17 06:38:41,947 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:41,951 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy
2024-01-17 06:38:41,955 - distributed.scheduler - INFO - State start
2024-01-17 06:38:41,978 - distributed.scheduler - INFO - -----------------------------------------------
2024-01-17 06:38:41,979 - distributed.scheduler - INFO -   Scheduler at:  tcp://10.33.227.163:9369
2024-01-17 06:38:41,980 - distributed.scheduler - INFO -   dashboard at:  http://10.33.227.163:8787/status
2024-01-17 06:38:41,980 - distributed.scheduler - INFO - Registering Worker plugin shuffle
2024-01-17 06:38:42,013 - distributed.nanny - INFO -         Start Nanny at: 'tcp://127.0.0.1:39435'
2024-01-17 06:38:43,808 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 06:38:43,808 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 06:38:43,812 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 06:38:43,813 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46505
2024-01-17 06:38:43,813 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46505
2024-01-17 06:38:43,814 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36179
2024-01-17 06:38:43,814 - distributed.worker - INFO - Waiting to connect to:       tcp://127.0.0.1:9369
2024-01-17 06:38:43,814 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:43,814 - distributed.worker - INFO -               Threads:                          1
2024-01-17 06:38:43,814 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-17 06:38:43,814 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2z1tv2zj
2024-01-17 06:38:43,814 - distributed.worker - INFO - Starting Worker plugin RMMSetup-fecda336-d1c3-4d19-89a1-47bed7af0ba6
2024-01-17 06:38:44,124 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-eb9b7a3c-970d-4394-8f22-9261224cc33d
2024-01-17 06:38:44,124 - distributed.worker - INFO - Starting Worker plugin PreImport-f241ffcd-b9d2-484f-83f2-2039e609663e
2024-01-17 06:38:44,125 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:44,175 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://127.0.0.1:46505', status: init, memory: 0, processing: 0>
2024-01-17 06:38:44,190 - distributed.scheduler - INFO - Starting worker compute stream, tcp://127.0.0.1:46505
2024-01-17 06:38:44,190 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51882
2024-01-17 06:38:44,191 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 06:38:44,192 - distributed.worker - INFO -         Registered to:       tcp://127.0.0.1:9369
2024-01-17 06:38:44,192 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 06:38:44,194 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:9369
2024-01-17 06:38:48,078 - distributed.scheduler - INFO - Receive client connection: Client-0ce8465b-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:48,079 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:51904
2024-01-17 06:38:48,085 - distributed.worker - INFO - Run out-of-band function 'get_device_total_memory'
2024-01-17 06:38:48,089 - distributed.worker - INFO - Run out-of-band function 'get_worker_config'
2024-01-17 06:38:48,092 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:48,094 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 06:38:48,096 - distributed.scheduler - INFO - Remove client Client-0ce8465b-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:48,096 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51904; closing.
2024-01-17 06:38:48,096 - distributed.scheduler - INFO - Remove client Client-0ce8465b-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:48,097 - distributed.scheduler - INFO - Close client connection: Client-0ce8465b-b503-11ee-b0d8-d8c49764f6bb
2024-01-17 06:38:48,098 - distributed.nanny - INFO - Closing Nanny at 'tcp://127.0.0.1:39435'. Reason: nanny-close
2024-01-17 06:38:48,098 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close
2024-01-17 06:38:48,099 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46505. Reason: nanny-close
2024-01-17 06:38:48,101 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:51882; closing.
2024-01-17 06:38:48,101 - distributed.core - INFO - Connection to tcp://127.0.0.1:9369 has been closed.
2024-01-17 06:38:48,101 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:46505', status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1705473528.101561')
2024-01-17 06:38:48,101 - distributed.scheduler - INFO - Lost all workers
2024-01-17 06:38:48,102 - distributed.nanny - INFO - Worker closed
2024-01-17 06:38:48,663 - distributed._signals - INFO - Received signal SIGINT (2)
2024-01-17 06:38:48,663 - distributed.scheduler - INFO - Scheduler closing due to signal-2...
2024-01-17 06:38:48,664 - distributed.scheduler - INFO - Scheduler closing all comms
2024-01-17 06:38:48,665 - distributed.scheduler - INFO - Stopped scheduler at 'tcp://10.33.227.163:9369'
2024-01-17 06:38:48,665 - distributed.scheduler - INFO - End scheduler
PASSED
dask_cuda/tests/test_dask_cuda_worker.py::test_worker_timeout PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range0-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range1-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-1-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-10-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-1] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-10] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_short[array_size_range2-100-100] PASSED
dask_cuda/tests/test_device_host_file.py::test_device_host_file_step_by_step PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[10-6-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-0-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-1-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-3-tuple] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-dict] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-list] PASSED
dask_cuda/tests/test_device_host_file.py::test_serialize_cupy_collection[value1-6-tuple] PASSED
dask_cuda/tests/test_dgx.py::test_default /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44209 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38705 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_over_ucx[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42703 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_tcp_only /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35379 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37605 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params0-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42569 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40391 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params1-ucxx] PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43567 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params2-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40053 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38503 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params3-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40485 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44471 instead
  warnings.warn(
[1705473675.337030] [dgx13:68000:0]            sock.c:470  UCX  ERROR bind(fd=165 addr=0.0.0.0:56745) failed: Address already in use
PASSED
dask_cuda/tests/test_dgx.py::test_ucx_infiniband_nvlink[params4-ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44085 instead
  warnings.warn(
[1705473688.023569] [dgx13:68231:0]            sock.c:470  UCX  ERROR bind(fd=163 addr=0.0.0.0:40046) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[tcp] PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46225 instead
  warnings.warn(
[1705473705.943531] [dgx13:68601:0]            sock.c:470  UCX  ERROR bind(fd=132 addr=0.0.0.0:38210) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_local_cluster[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33465 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_merge_empty_partitions /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46867 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46267 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40405 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41737 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33097 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46271 instead
  warnings.warn(
2024-01-17 06:43:37,849 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:45268 remote=tcp://127.0.0.1:44183>: Stream is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39481 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38623 instead
  warnings.warn(
[1705473849.292828] [dgx13:71276:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:43262) failed: Address already in use
2024-01-17 06:44:19,947 - distributed.worker - ERROR - Unexpected exception during heartbeat. Closing worker.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
2024-01-17 06:44:19,955 - tornado.application - ERROR - Exception in callback <bound method Worker.heartbeat of <Worker 'ucx://127.0.0.1:42634', name: 0, status: closed, stored: 0, running: 0/1, ready: 0, comm: 0, waiting: 0>>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 921, in _run
    await val
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1392, in send_recv_from_rpc
    comm = await self.pool.connect(self.addr)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1591, in connect
    raise RuntimeError("ConnectionPool is closed")
RuntimeError: ConnectionPool is closed
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37811 instead
  warnings.warn(
[1705473865.526069] [dgx13:71573:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:32903) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42163 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46011 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34091 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33211 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37471 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34527 instead
  warnings.warn(
[1705474062.461808] [dgx13:74631:0]            sock.c:470  UCX  ERROR bind(fd=155 addr=0.0.0.0:47120) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-2] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[True-ucxx-cudf-3] [1705474136.112423] [dgx13:75343:0]            sock.c:470  UCX  ERROR bind(fd=155 addr=0.0.0.0:35670) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37737 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35183 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44253 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 32933 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38149 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-tcp-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46579 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40905 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37939 instead
  warnings.warn(
[1705474304.265363] [dgx13:78169:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:50582) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33577 instead
  warnings.warn(
[1705474318.809502] [dgx13:78407:0]            sock.c:470  UCX  ERROR bind(fd=134 addr=0.0.0.0:41466) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41631 instead
  warnings.warn(
[1705474342.331097] [dgx13:79020:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:46590) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39471 instead
  warnings.warn(
[1705474365.264885] [dgx13:79315:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:42192) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41151 instead
  warnings.warn(
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7f677db9e180, tag: 0x1c954ede90954534>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7f677db9e180, tag: 0x1c954ede90954534>: 
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-5392' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40071 instead
  warnings.warn(
[1705474427.735508] [dgx13:80080:0]            sock.c:470  UCX  ERROR bind(fd=126 addr=0.0.0.0:52996) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36687 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-pandas-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35713 instead
  warnings.warn(
[1705474483.036393] [dgx13:80716:0]            sock.c:470  UCX  ERROR bind(fd=157 addr=0.0.0.0:42822) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36937 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35263 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle[False-ucxx-cudf-3] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 41755 instead
  warnings.warn(
[1705474588.064856] [dgx13:81772:0]            sock.c:470  UCX  ERROR bind(fd=160 addr=0.0.0.0:36426) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[True] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35129 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dask_use_explicit_comms[False] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43263 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36081 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42519 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-1] PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 37991 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[tcp-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40991 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39995 instead
  warnings.warn(
[1705474733.053171] [dgx13:83697:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:53952) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 35033 instead
  warnings.warn(
[1705474740.493232] [dgx13:83801:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:33165) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 44571 instead
  warnings.warn(
[1705474750.431300] [dgx13:83931:0]            sock.c:470  UCX  ERROR bind(fd=130 addr=0.0.0.0:47704) failed: Address already in use
Future exception was never retrieved
future: <Future finished exception=UCXCanceled('<[Recv shutdown] ep: 0x7ff482a76180, tag: 0xdfb33852f2f1c6fb>: ')>
ucp._libs.exceptions.UCXCanceled: <[Recv shutdown] ep: 0x7ff482a76180, tag: 0xdfb33852f2f1c6fb>: 
sys:1: RuntimeWarning: coroutine 'BlockingMode._arm_worker' was never awaited
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
Task was destroyed but it is pending!
task: <Task cancelling name='Task-1185' coro=<BlockingMode._arm_worker() running at /opt/conda/envs/gdf/lib/python3.9/site-packages/ucp/continuous_ucx_progress.py:88>>
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40047 instead
  warnings.warn(
[1705474766.600407] [dgx13:84037:0]            sock.c:470  UCX  ERROR bind(fd=160 addr=0.0.0.0:44458) failed: Address already in use
[1705474767.983435] [dgx13:84206:0]            sock.c:470  UCX  ERROR bind(fd=124 addr=0.0.0.0:51006) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40937 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36751 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46263 instead
  warnings.warn(
[1705474824.403562] [dgx13:84728:0]            sock.c:470  UCX  ERROR bind(fd=159 addr=0.0.0.0:46228) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34715 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-pandas-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 39389 instead
  warnings.warn(
[1705474846.566086] [dgx13:85052:0]            sock.c:470  UCX  ERROR bind(fd=155 addr=0.0.0.0:52093) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-1] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46053 instead
  warnings.warn(
2024-01-17 07:01:12,528 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
2024-01-17 07:01:12,531 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 396, in read
    await self.ep.recv(msg)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 505, in ep
    raise CommClosedError("UCX Endpoint is closed")
distributed.comm.core.CommClosedError: UCX Endpoint is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 414, in read
    raise CommClosedError(
distributed.comm.core.CommClosedError: Connection closed by writer.
Inner exception: CommClosedError('UCX Endpoint is closed')
unpack(b) received extra data.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 450, in read
    return await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
2024-01-17 07:01:12,535 - distributed.core - ERROR - Exception while reading from ucxx://127.0.0.1:32996
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 900, in _handle_comm
    msg = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 450, in read
    return await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
Task exception was never retrieved
future: <Task finished name='Task-435' coro=<Server._handle_comm() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py:876> exception=ExtraData(1, b'\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00pd\\\xb0u\x7f\x00\x00pd\\\xb0u\x7f\x00\x00\xe4\x03\x01\x00S\x001\x00\x00\x00\x00\x00\x00\x00\x00\x00Inter-|   Receive                                                |  Transmit\n face |bytes    packets errs drop fifo frame compressed multicast|bytes    packets errs drop fifo colls carrier compressed\n   ib1: 4856177428 48561771    0    0    0     0          0         0   205056    3416    0    0    0     0       0          0\nvethc73a07d: 4540384   33172    0    0    0     0          0         0 471882430   64500    0    0    0     0       0          0\n    lo: 768584172327415 28274136914    0    0    0     0          0         0 768584172327415 28274136914    0    0    0     0       0          0\n   ib2: 4856179428 48561791    0    0    0     0          0         0   205476    3423    0    0    0     0       0          0\n   ib0: 4856177420 48561771    0    0    0     0          0         0   204876    3413    0    0    0     0       0          0\nveth810e024: 8626145   78777    0    0    0     0          0         0 2089437189  262540    0    0    0     0       0          0\nenp1s0f1:       0       0    0    0    0     0          0         0        0       0    0    0    0     0       0          0\ndocker0: 1420394267 10685193    0    0    0     0          0         0 185292660825 18195039    0    0    0     0       0          0\nveth7c28c21: 12506094  187338    0    0    0     0          0         0 1096145573  314089    0    0    0     0       0          0\n   ib3: 4856181068 48561809    0    0    0     0          0         0   205476    3423    0    0    0     0       0          0\nveth5e4636f: 8499222  126811    0    0    0     0          0         0 968665155  237372    0    0    0     0       0          0\nenp1s0f0: 4373087666003 3897449645    0 43720    0     0          0   7728545 2575869677656 2885112269    0    0    0     0       0          0\n\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00Q\x04\x00\x00\x00\x00\x00\x00\x80\x00\x00\xb0u\x7f\x00\x00\x80\xc3\xaf\xb1u\x7f\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 900, in _handle_comm
    msg = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed_ucxx/ucxx.py", line 450, in read
    return await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 201, in msgpack._cmsgpack.unpackb
msgpack.exceptions.ExtraData: unpack(b) received extra data.
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-2] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42911 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_dataframe_shuffle_merge[ucxx-cudf-4] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46743 instead
  warnings.warn(
[1705474902.755615] [dgx13:85754:0]            sock.c:470  UCX  ERROR bind(fd=157 addr=0.0.0.0:54984) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[tcp] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 42801 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33027 instead
  warnings.warn(
[1705474931.189392] [dgx13:86234:0]            sock.c:470  UCX  ERROR bind(fd=164 addr=0.0.0.0:57054) failed: Address already in use
PASSED
dask_cuda/tests/test_explicit_comms.py::test_jit_unspill[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 38033 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_explicit_comms.py::test_lock_workers PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucx] [1705474967.323811] [dgx13:61656:0]            sock.c:470  UCX  ERROR bind(fd=181 addr=0.0.0.0:34064) failed: Address already in use
[1705474971.840678] [dgx13:86836:0]            sock.c:470  UCX  ERROR bind(fd=165 addr=0.0.0.0:36630) failed: Address already in use
PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[ucxx] PASSED
dask_cuda/tests/test_from_array.py::test_ucx_from_array[tcp] PASSED
dask_cuda/tests/test_gds.py::test_gds[True-cupy] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-cudf] SKIPPED (GDS not av...)
dask_cuda/tests/test_gds.py::test_gds[True-numba.cuda] SKIPPED (GDS ...)
dask_cuda/tests/test_gds.py::test_gds[False-cupy] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-cudf] PASSED
dask_cuda/tests/test_gds.py::test_gds[False-numba.cuda] PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 43495 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_tcp[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46341 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 34105 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_nvlink[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 46689 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 36469 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_infiniband[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40803 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 33173 instead
  warnings.warn(
PASSED
dask_cuda/tests/test_initialize.py::test_initialize_ucx_all[ucxx] /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.
Perhaps you already have a cluster running?
Hosting the HTTP server on port 40295 instead
  warnings.warn(
[1705475036.362116] [dgx13:87441:0]            sock.c:470  UCX  ERROR bind(fd=164 addr=0.0.0.0:40698) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_local_cuda_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_with_subset_of_cuda_visible_devices PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucx] [1705475050.626584] [dgx13:87621:0]            sock.c:470  UCX  ERROR bind(fd=164 addr=0.0.0.0:58658) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol[ucxx] [1705475052.816379] [dgx13:61656:1]            sock.c:470  UCX  ERROR bind(fd=257 addr=0.0.0.0:48700) failed: Address already in use
[1705475052.816459] [dgx13:61656:1]            sock.c:470  UCX  ERROR bind(fd=257 addr=0.0.0.0:40661) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucx] [1705475061.232475] [dgx13:61656:1]            sock.c:470  UCX  ERROR bind(fd=259 addr=0.0.0.0:54508) failed: Address already in use
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_explicit_ucx_with_protocol_none[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_ucx_protocol_type_error[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_n_workers PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_threads_per_worker_and_memory_limit PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cluster PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_no_memory_limits_cudaworker 2024-01-17 07:04:47,450 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:04:47,451 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:04:47,470 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:04:47,470 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:04:47,500 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:04:47,500 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:04:47,577 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:04:47,577 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:04:47,586 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:04:47,586 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:04:47,660 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:04:47,661 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:04:47,664 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:04:47,664 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:04:47,711 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:04:47,711 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:04:48,053 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:04:48,053 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44847
2024-01-17 07:04:48,054 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44847
2024-01-17 07:04:48,054 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35343
2024-01-17 07:04:48,054 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,054 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,054 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:04:48,054 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-18yivrrf
2024-01-17 07:04:48,054 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8e7935bf-5ef1-4ba0-96ef-f8d811887e79
2024-01-17 07:04:48,054 - distributed.worker - INFO - Starting Worker plugin PreImport-e80117cf-15e0-4bc8-a6f1-99d04cddfebe
2024-01-17 07:04:48,055 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e17710cf-99f4-4444-931d-bd2167fee8a6
2024-01-17 07:04:48,055 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,064 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:04:48,065 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44105
2024-01-17 07:04:48,065 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44105
2024-01-17 07:04:48,065 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38951
2024-01-17 07:04:48,066 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,066 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,066 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:04:48,066 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-o2c2hlpl
2024-01-17 07:04:48,066 - distributed.worker - INFO - Starting Worker plugin PreImport-fcd14a77-aa62-4278-b5f7-c1671240c4d7
2024-01-17 07:04:48,066 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-1a7407d9-96f6-4b84-a2a5-1a7a653a75cd
2024-01-17 07:04:48,067 - distributed.worker - INFO - Starting Worker plugin RMMSetup-45a74c40-944d-4c31-a27f-dc5a4977b421
2024-01-17 07:04:48,067 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,077 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:04:48,078 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:46809
2024-01-17 07:04:48,078 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:46809
2024-01-17 07:04:48,078 - distributed.worker - INFO -          dashboard at:            127.0.0.1:46765
2024-01-17 07:04:48,078 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,078 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,078 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:04:48,078 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2d5u8_mc
2024-01-17 07:04:48,078 - distributed.worker - INFO - Starting Worker plugin PreImport-dbc0c4e2-4b59-4f47-b960-be50029c7292
2024-01-17 07:04:48,078 - distributed.worker - INFO - Starting Worker plugin RMMSetup-12fa9e8a-961a-40a7-9a95-c394f2754e0f
2024-01-17 07:04:48,079 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-73b3bf3d-6894-4b87-907a-1b0a4da47dfd
2024-01-17 07:04:48,079 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,154 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:04:48,155 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,155 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,156 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44335
2024-01-17 07:04:48,168 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:04:48,169 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39853
2024-01-17 07:04:48,169 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39853
2024-01-17 07:04:48,169 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42219
2024-01-17 07:04:48,170 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,170 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,170 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:04:48,170 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-2oo725xn
2024-01-17 07:04:48,170 - distributed.worker - INFO - Starting Worker plugin PreImport-ffb05501-3175-450b-8ff4-7158ab41603d
2024-01-17 07:04:48,170 - distributed.worker - INFO - Starting Worker plugin RMMSetup-2ed9b215-d0cc-412d-b2f0-631dc4c2fcaa
2024-01-17 07:04:48,170 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:04:48,170 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-c9c1507d-b007-4e96-9cda-07969506763e
2024-01-17 07:04:48,171 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,171 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38591
2024-01-17 07:04:48,171 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38591
2024-01-17 07:04:48,171 - distributed.worker - INFO -          dashboard at:            127.0.0.1:38871
2024-01-17 07:04:48,171 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,171 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,171 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:04:48,171 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d_3asveo
2024-01-17 07:04:48,172 - distributed.worker - INFO - Starting Worker plugin PreImport-4a311059-ae25-4dfe-8743-5cffea62cd7a
2024-01-17 07:04:48,172 - distributed.worker - INFO - Starting Worker plugin RMMSetup-dc3e416a-67f8-4136-b2f7-bc4a66ed407f
2024-01-17 07:04:48,172 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-cf7bac9a-be8a-4b05-88bd-a900e21ee71c
2024-01-17 07:04:48,172 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,183 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:04:48,184 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,184 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,185 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44335
2024-01-17 07:04:48,187 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:04:48,188 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,188 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,189 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44335
2024-01-17 07:04:48,246 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:04:48,247 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39357
2024-01-17 07:04:48,247 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39357
2024-01-17 07:04:48,247 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34781
2024-01-17 07:04:48,247 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,247 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,247 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:04:48,247 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-h84iw5xx
2024-01-17 07:04:48,248 - distributed.worker - INFO - Starting Worker plugin RMMSetup-922c4b91-49b1-4604-9abf-688028b1a6e9
2024-01-17 07:04:48,248 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-8559b49f-8584-411e-8221-73f35effc5ab
2024-01-17 07:04:48,248 - distributed.worker - INFO - Starting Worker plugin PreImport-9139fec8-4ced-4937-9d80-b07110670e5f
2024-01-17 07:04:48,248 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,255 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:04:48,256 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:39601
2024-01-17 07:04:48,256 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:39601
2024-01-17 07:04:48,256 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36491
2024-01-17 07:04:48,257 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,257 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,257 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:04:48,257 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p9nqnnyu
2024-01-17 07:04:48,257 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:04:48,257 - distributed.worker - INFO - Starting Worker plugin PreImport-19ffbdf9-7475-4a8b-90e6-48fb9ce36a89
2024-01-17 07:04:48,257 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f7382d97-e3e5-4448-88c1-545834471550
2024-01-17 07:04:48,257 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a6d28714-6eb7-4459-96af-4abdad035c25
2024-01-17 07:04:48,258 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,258 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,258 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,259 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44335
2024-01-17 07:04:48,262 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:04:48,263 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,263 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,264 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44335
2024-01-17 07:04:48,277 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:04:48,278 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:34111
2024-01-17 07:04:48,278 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:34111
2024-01-17 07:04:48,278 - distributed.worker - INFO -          dashboard at:            127.0.0.1:36091
2024-01-17 07:04:48,278 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,279 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,279 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:04:48,279 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-476eig0f
2024-01-17 07:04:48,279 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-81a0ecd3-f482-4f69-ac41-4eae6d76609e
2024-01-17 07:04:48,279 - distributed.worker - INFO - Starting Worker plugin RMMSetup-f543c271-7839-4a00-a662-13f6b1423563
2024-01-17 07:04:48,279 - distributed.worker - INFO - Starting Worker plugin PreImport-a97a294a-e534-48fa-be3b-78bc3ba62e6a
2024-01-17 07:04:48,279 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,331 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:04:48,332 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,332 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,333 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44335
2024-01-17 07:04:48,372 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:04:48,373 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,373 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,374 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44335
2024-01-17 07:04:48,377 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:04:48,378 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:44335
2024-01-17 07:04:48,378 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:04:48,379 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:44335
2024-01-17 07:04:48,413 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 07:04:48,413 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 07:04:48,413 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 07:04:48,414 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 07:04:48,414 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 07:04:48,414 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 07:04:48,414 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 07:04:48,414 - distributed.worker - INFO - Run out-of-band function 'lambda'
2024-01-17 07:04:48,419 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44847. Reason: nanny-close
2024-01-17 07:04:48,420 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44105. Reason: nanny-close
2024-01-17 07:04:48,420 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:46809. Reason: nanny-close
2024-01-17 07:04:48,421 - distributed.core - INFO - Connection to tcp://127.0.0.1:44335 has been closed.
2024-01-17 07:04:48,421 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38591. Reason: nanny-close
2024-01-17 07:04:48,422 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39853. Reason: nanny-close
2024-01-17 07:04:48,423 - distributed.core - INFO - Connection to tcp://127.0.0.1:44335 has been closed.
2024-01-17 07:04:48,423 - distributed.nanny - INFO - Worker closed
2024-01-17 07:04:48,423 - distributed.core - INFO - Connection to tcp://127.0.0.1:44335 has been closed.
2024-01-17 07:04:48,423 - distributed.core - INFO - Connection to tcp://127.0.0.1:44335 has been closed.
2024-01-17 07:04:48,424 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39357. Reason: nanny-close
2024-01-17 07:04:48,424 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:39601. Reason: nanny-close
2024-01-17 07:04:48,424 - distributed.nanny - INFO - Worker closed
2024-01-17 07:04:48,424 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:34111. Reason: nanny-close
2024-01-17 07:04:48,424 - distributed.core - INFO - Connection to tcp://127.0.0.1:44335 has been closed.
2024-01-17 07:04:48,425 - distributed.nanny - INFO - Worker closed
2024-01-17 07:04:48,425 - distributed.nanny - INFO - Worker closed
2024-01-17 07:04:48,425 - distributed.core - INFO - Connection to tcp://127.0.0.1:44335 has been closed.
2024-01-17 07:04:48,426 - distributed.nanny - INFO - Worker closed
2024-01-17 07:04:48,426 - distributed.core - INFO - Connection to tcp://127.0.0.1:44335 has been closed.
2024-01-17 07:04:48,426 - distributed.core - INFO - Connection to tcp://127.0.0.1:44335 has been closed.
2024-01-17 07:04:48,426 - distributed.nanny - INFO - Worker closed
2024-01-17 07:04:48,427 - distributed.nanny - INFO - Worker closed
2024-01-17 07:04:48,427 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_all_to_all PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_pool PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_maximum_poolsize_without_poolsize_error PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_managed PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_async_with_maximum_pool_size 2024-01-17 07:05:11,922 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:128: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 345, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:128: cudaErrorMemoryAllocation out of memory
2024-01-17 07:05:11,926 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 345, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:128: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-01-17 07:05:12,075 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:128: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 345, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:128: cudaErrorMemoryAllocation out of memory
2024-01-17 07:05:12,079 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 345, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:128: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-01-17 07:05:12,096 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:128: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 345, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:128: cudaErrorMemoryAllocation out of memory
2024-01-17 07:05:12,100 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 345, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_async_view_memory_resource.hpp:128: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-01-17 07:05:12,144 - distributed.worker - ERROR - CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 345, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory
2024-01-17 07:05:12,147 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 64, in setup
    mr = rmm.mr.CudaAsyncMemoryResource(
  File "memory_resource.pyx", line 345, in rmm._lib.memory_resource.CudaAsyncMemoryResource.__cinit__
RuntimeError: CUDA error at: /opt/conda/conda-bld/work/include/rmm/cuda_device.hpp:115: cudaErrorMemoryAllocation out of memory

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_logging 2024-01-17 07:05:17,158 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1004, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-17 07:05:17,162 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1004, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-01-17 07:05:17,181 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1004, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-17 07:05:17,185 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1004, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-01-17 07:05:17,729 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1004, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-17 07:05:17,732 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1004, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-01-17 07:05:17,766 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1004, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-17 07:05:17,771 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1004, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_pre_import_not_found 2024-01-17 07:05:22,269 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:05:22,269 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:05:22,273 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:05:22,274 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35273
2024-01-17 07:05:22,274 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35273
2024-01-17 07:05:22,274 - distributed.worker - INFO -           Worker name:                          0
2024-01-17 07:05:22,274 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34685
2024-01-17 07:05:22,274 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:34553
2024-01-17 07:05:22,274 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:22,274 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:05:22,274 - distributed.worker - INFO -                Memory:                   0.98 TiB
2024-01-17 07:05:22,274 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tulf22ki
2024-01-17 07:05:22,275 - distributed.worker - INFO - Starting Worker plugin RMMSetup-d49925b1-9d07-43a6-aa14-0b8cdfe81783
2024-01-17 07:05:22,275 - distributed.worker - INFO - Starting Worker plugin PreImport-a8833486-cc09-4fee-a7dd-cbd9ff99a0f5
2024-01-17 07:05:22,278 - distributed.worker - ERROR - No module named 'my_module'
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'
2024-01-17 07:05:22,279 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-54d1d234-9bb9-41bf-95f5-5419a51d908b
2024-01-17 07:05:22,279 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35273. Reason: failure-to-start-<class 'ModuleNotFoundError'>
2024-01-17 07:05:22,279 - distributed.worker - INFO - Closed worker has not yet started: Status.init
2024-01-17 07:05:22,282 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 122, in setup
    importlib.import_module(l)
  File "/opt/conda/envs/gdf/lib/python3.9/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'my_module'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
XFAIL
dask_cuda/tests/test_local_cuda_cluster.py::test_cluster_worker 2024-01-17 07:05:26,910 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:05:26,910 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:05:26,951 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:05:26,951 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:05:27,026 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:05:27,026 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:05:27,055 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:05:27,055 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:05:27,081 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:05:27,081 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:05:27,102 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:05:27,102 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:05:27,173 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:05:27,173 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:05:27,243 - distributed.preloading - INFO - Creating preload: dask_cuda.initialize
2024-01-17 07:05:27,243 - distributed.preloading - INFO - Import preload module: dask_cuda.initialize
2024-01-17 07:05:27,534 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:05:27,535 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43131
2024-01-17 07:05:27,535 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43131
2024-01-17 07:05:27,535 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43317
2024-01-17 07:05:27,535 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,535 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,535 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:05:27,535 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 07:05:27,535 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-t7aifmlc
2024-01-17 07:05:27,536 - distributed.worker - INFO - Starting Worker plugin RMMSetup-a8fea891-1d38-46b5-99ad-8571370803ca
2024-01-17 07:05:27,536 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-7c94e437-8554-4e7e-8235-c6bf256e4047
2024-01-17 07:05:27,537 - distributed.worker - INFO - Starting Worker plugin PreImport-c5771f5b-cba6-4180-992e-b00fe8d7e863
2024-01-17 07:05:27,537 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,568 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:05:27,569 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:44745
2024-01-17 07:05:27,569 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:44745
2024-01-17 07:05:27,569 - distributed.worker - INFO -          dashboard at:            127.0.0.1:44373
2024-01-17 07:05:27,569 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,569 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,569 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:05:27,569 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 07:05:27,569 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-5ulbtp22
2024-01-17 07:05:27,569 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-f4415f6c-2a5f-4c70-b2d9-edb34e274393
2024-01-17 07:05:27,571 - distributed.worker - INFO - Starting Worker plugin RMMSetup-28ed492e-c772-4cc9-8f70-29b58ffc721c
2024-01-17 07:05:27,571 - distributed.worker - INFO - Starting Worker plugin PreImport-81e1eca9-b37d-4944-a633-9fb21cb6ae23
2024-01-17 07:05:27,571 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,607 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:05:27,608 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,608 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,609 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43995
2024-01-17 07:05:27,639 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:05:27,640 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,640 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,641 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43995
2024-01-17 07:05:27,687 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:05:27,688 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35199
2024-01-17 07:05:27,688 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35199
2024-01-17 07:05:27,688 - distributed.worker - INFO -          dashboard at:            127.0.0.1:35439
2024-01-17 07:05:27,689 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,689 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,689 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:05:27,689 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 07:05:27,689 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-qkjsucw0
2024-01-17 07:05:27,689 - distributed.worker - INFO - Starting Worker plugin PreImport-487954f7-a570-4f64-b872-27ef39f3c2ca
2024-01-17 07:05:27,689 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-a4019bde-7b33-41ff-a88e-c2ab8c54ebae
2024-01-17 07:05:27,689 - distributed.worker - INFO - Starting Worker plugin RMMSetup-ae52f65f-acf7-4291-b5ed-6d0291b53f47
2024-01-17 07:05:27,690 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,714 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:05:27,715 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:35095
2024-01-17 07:05:27,715 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:35095
2024-01-17 07:05:27,715 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42553
2024-01-17 07:05:27,715 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,715 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,715 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:05:27,715 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 07:05:27,715 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-hbyqkivu
2024-01-17 07:05:27,715 - distributed.worker - INFO - Starting Worker plugin PreImport-63098b9e-e5c5-4dd6-b95c-c96a7046f211
2024-01-17 07:05:27,715 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9b1cd768-df88-4bb1-9fd6-f6b7d5d7cfc4
2024-01-17 07:05:27,716 - distributed.worker - INFO - Starting Worker plugin RMMSetup-926da7a6-b28a-4f69-a09c-2ce2fae7af8e
2024-01-17 07:05:27,716 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,722 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:05:27,723 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:33479
2024-01-17 07:05:27,723 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:33479
2024-01-17 07:05:27,723 - distributed.worker - INFO -          dashboard at:            127.0.0.1:41005
2024-01-17 07:05:27,724 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,724 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,724 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:05:27,724 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 07:05:27,724 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-krout37n
2024-01-17 07:05:27,724 - distributed.worker - INFO - Starting Worker plugin RMMSetup-b066ad6e-03db-4734-bd4e-0afd000abe44
2024-01-17 07:05:27,724 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9cefa8a9-f494-41f5-b3d4-398c184926b5
2024-01-17 07:05:27,726 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:05:27,727 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:45795
2024-01-17 07:05:27,727 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:45795
2024-01-17 07:05:27,727 - distributed.worker - INFO -          dashboard at:            127.0.0.1:43439
2024-01-17 07:05:27,727 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,727 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,727 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:05:27,727 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 07:05:27,727 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-pdz597c_
2024-01-17 07:05:27,727 - distributed.worker - INFO - Starting Worker plugin RMMSetup-eecf6e10-dbfe-4958-96bd-706309bc7a82
2024-01-17 07:05:27,727 - distributed.worker - INFO - Starting Worker plugin PreImport-db776a8e-d58a-46a4-b149-2bae350cf1ca
2024-01-17 07:05:27,728 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-64d8721c-d9ac-4be9-b1fa-a944dc185307
2024-01-17 07:05:27,728 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,729 - distributed.worker - INFO - Starting Worker plugin PreImport-9eeccb79-0af2-416e-a565-0902dea62a6a
2024-01-17 07:05:27,729 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,759 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:05:27,760 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,760 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,761 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43995
2024-01-17 07:05:27,792 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:05:27,793 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:43875
2024-01-17 07:05:27,793 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:43875
2024-01-17 07:05:27,794 - distributed.worker - INFO -          dashboard at:            127.0.0.1:34311
2024-01-17 07:05:27,794 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,794 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,794 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:05:27,794 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 07:05:27,794 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yzu692y3
2024-01-17 07:05:27,794 - distributed.worker - INFO - Starting Worker plugin RMMSetup-e49cf5c1-bc0c-4038-90fe-bc25fdfc0169
2024-01-17 07:05:27,794 - distributed.worker - INFO - Starting Worker plugin PreImport-06090506-89bb-4783-a3c3-3d49dcd45c23
2024-01-17 07:05:27,794 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-9f78ceab-45e1-4604-b067-24b366d6077d
2024-01-17 07:05:27,795 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,858 - distributed.preloading - INFO - Run preload setup: dask_cuda.initialize
2024-01-17 07:05:27,859 - distributed.worker - INFO -       Start worker at:      tcp://127.0.0.1:38953
2024-01-17 07:05:27,859 - distributed.worker - INFO -          Listening to:      tcp://127.0.0.1:38953
2024-01-17 07:05:27,859 - distributed.worker - INFO -          dashboard at:            127.0.0.1:42967
2024-01-17 07:05:27,859 - distributed.worker - INFO - Waiting to connect to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,859 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,859 - distributed.worker - INFO -               Threads:                          1
2024-01-17 07:05:27,859 - distributed.worker - INFO -                Memory:                 125.97 GiB
2024-01-17 07:05:27,859 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-z1hnmvtr
2024-01-17 07:05:27,859 - distributed.worker - INFO - Starting Worker plugin RMMSetup-6e8733d0-d6fc-4e46-ac50-f99fdbc7ca12
2024-01-17 07:05:27,860 - distributed.worker - INFO - Starting Worker plugin CPUAffinity-821001b7-7fd7-402b-ad51-e9cf54065d8a
2024-01-17 07:05:27,860 - distributed.worker - INFO - Starting Worker plugin PreImport-a2dbf021-acfe-42bf-b92c-1edc3816e506
2024-01-17 07:05:27,860 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,864 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:05:27,865 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,865 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,866 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43995
2024-01-17 07:05:27,928 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:05:27,929 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,929 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,930 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43995
2024-01-17 07:05:27,932 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:05:27,932 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,932 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,934 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43995
2024-01-17 07:05:27,949 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:05:27,950 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,950 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,951 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43995
2024-01-17 07:05:27,965 - distributed.worker - INFO - Starting Worker plugin shuffle
2024-01-17 07:05:27,966 - distributed.worker - INFO -         Registered to:      tcp://127.0.0.1:43995
2024-01-17 07:05:27,966 - distributed.worker - INFO - -------------------------------------------------
2024-01-17 07:05:27,967 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:43995
2024-01-17 07:05:28,011 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43131. Reason: nanny-close
2024-01-17 07:05:28,012 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:44745. Reason: nanny-close
2024-01-17 07:05:28,012 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35199. Reason: nanny-close
2024-01-17 07:05:28,013 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:35095. Reason: nanny-close
2024-01-17 07:05:28,013 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:33479. Reason: nanny-close
2024-01-17 07:05:28,013 - distributed.core - INFO - Connection to tcp://127.0.0.1:43995 has been closed.
2024-01-17 07:05:28,014 - distributed.core - INFO - Connection to tcp://127.0.0.1:43995 has been closed.
2024-01-17 07:05:28,014 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:43875. Reason: nanny-close
2024-01-17 07:05:28,015 - distributed.core - INFO - Connection to tcp://127.0.0.1:43995 has been closed.
2024-01-17 07:05:28,015 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:45795. Reason: nanny-close
2024-01-17 07:05:28,015 - distributed.worker - INFO - Stopping worker at tcp://127.0.0.1:38953. Reason: nanny-close
2024-01-17 07:05:28,015 - distributed.nanny - INFO - Worker closed
2024-01-17 07:05:28,015 - distributed.core - INFO - Connection to tcp://127.0.0.1:43995 has been closed.
2024-01-17 07:05:28,016 - distributed.nanny - INFO - Worker closed
2024-01-17 07:05:28,016 - distributed.core - INFO - Connection to tcp://127.0.0.1:43995 has been closed.
2024-01-17 07:05:28,016 - distributed.core - INFO - Connection to tcp://127.0.0.1:43995 has been closed.
2024-01-17 07:05:28,016 - distributed.nanny - INFO - Worker closed
2024-01-17 07:05:28,017 - distributed.core - INFO - Connection to tcp://127.0.0.1:43995 has been closed.
2024-01-17 07:05:28,017 - distributed.nanny - INFO - Worker closed
2024-01-17 07:05:28,017 - distributed.core - INFO - Connection to tcp://127.0.0.1:43995 has been closed.
2024-01-17 07:05:28,017 - distributed.nanny - INFO - Worker closed
2024-01-17 07:05:28,017 - distributed.nanny - INFO - Worker closed
2024-01-17 07:05:28,018 - distributed.nanny - INFO - Worker closed
2024-01-17 07:05:28,019 - distributed.nanny - INFO - Worker closed
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_available_mig_workers SKIPPED
dask_cuda/tests/test_local_cuda_cluster.py::test_gpu_uuid PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_rmm_track_allocations 2024-01-17 07:05:36,833 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-17 07:05:36,840 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-01-17 07:05:36,849 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-17 07:05:36,856 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
2024-01-17 07:05:37,386 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-17 07:05:37,390 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_get_cluster_configuration PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_worker_fraction_limits PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_print_cluster_config[ucxx] PASSED
dask_cuda/tests/test_local_cuda_cluster.py::test_death_timeout_raises XFAIL
dask_cuda/tests/test_proxify_host_file.py::test_one_dev_item_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_one_item_host_limit PASSED
dask_cuda/tests/test_proxify_host_file.py::test_spill_on_demand PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[True] PASSED
dask_cuda/tests/test_proxify_host_file.py::test_local_cuda_cluster[False] 2024-01-17 07:06:09,937 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:06:09,945 - distributed.worker - ERROR - Failed to communicate with scheduler during heartbeat.
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 225, in read
    frames_nosplit_nbytes_bin = await stream.read_bytes(fmt_size)
tornado.iostream.StreamClosedError: Stream is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1252, in heartbeat
    response = await retry_operation(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 455, in retry_operation
    return await retry(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils_comm.py", line 434, in retry
    return await coro()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1395, in send_recv_from_rpc
    return await send_recv(comm=comm, op=key, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1154, in send_recv
    response = await comm.read(deserializers=deserializers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 236, in read
    convert_stream_closed_error(self, e)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 142, in convert_stream_closed_error
    raise CommClosedError(f"in {obj}: {exc}") from exc
distributed.comm.core.CommClosedError: in <TCP (closed) ConnectionPool.heartbeat_worker local=tcp://127.0.0.1:55546 remote=tcp://127.0.0.1:39537>: Stream is closed
2024-01-17 07:06:09,947 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f7dc7f319a0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_dataframes_share_dev_mem PASSED
dask_cuda/tests/test_proxify_host_file.py::test_cudf_get_device_memory_objects PASSED
dask_cuda/tests/test_proxify_host_file.py::test_externals PASSED
dask_cuda/tests/test_proxify_host_file.py::test_incompatible_types PASSED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-1] 2024-01-17 07:06:29,658 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:06:29,665 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fcf4ceea9a0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:06:31,669 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-2] 2024-01-17 07:07:00,158 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:07:00,165 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fdd23f02a00>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:07:02,169 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[True-3] 2024-01-17 07:07:30,671 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:07:30,677 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7ffb52f2c9d0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:07:32,681 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-1] 2024-01-17 07:08:01,169 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:08:01,175 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fa06d557a00>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:08:03,178 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-2] 2024-01-17 07:08:31,727 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:08:31,734 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7fe798eaba00>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:08:33,737 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_compatibility_mode_dataframe_shuffle[False-3] 2024-01-17 07:09:02,111 - distributed.protocol.core - CRITICAL - Failed to deserialize
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:09:02,118 - tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <tornado.platform.asyncio.AsyncIOMainLoop object at 0x7f7d91a0c9a0>>, <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>)
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
unhandled exception during asyncio.run() shutdown
task: <Task finished name='Task-8' coro=<Worker.handle_scheduler() done, defined at /opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py:203> exception=MemoryError('std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory')>
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 738, in _run_callback
    ret = callback()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/tornado/ioloop.py", line 762, in _discard_future_result
    future.result()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 206, in wrapper
    return await method(self, *args, **kwargs)  # type: ignore
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1299, in handle_scheduler
    await self.handle_stream(comm)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 1025, in handle_stream
    msgs = await comm.read()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/tcp.py", line 247, in read
    msg = await from_frames(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 78, in from_frames
    res = _from_frames()
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/comm/utils.py", line 61, in _from_frames
    return protocol.loads(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 175, in loads
    return msgpack.loads(
  File "msgpack/_unpacker.pyx", line 194, in msgpack._cmsgpack.unpackb
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/core.py", line 172, in _decode_default
    return pickle.loads(sub_header["pickled-obj"], buffers=sub_frames)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/protocol/pickle.py", line 94, in loads
    return pickle.loads(x, buffers=buffers)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 178, in host_deserialize
    frames = [
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/abc.py", line 179, in <listcomp>
    cudf.core.buffer.as_buffer(f) if c else f
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/utils.py", line 136, in as_buffer
    return buffer_class(owner=owner_class._from_host_memory(data))
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/cudf/core/buffer/buffer.py", line 197, in _from_host_memory
    buf = rmm.DeviceBuffer(ptr=ptr, size=size)
  File "device_buffer.pyx", line 87, in rmm._lib.device_buffer.DeviceBuffer.__cinit__
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /opt/conda/conda-bld/work/include/rmm/mr/device/cuda_memory_resource.hpp:76: cudaErrorMemoryAllocation out of memory
2024-01-17 07:09:04,122 - distributed.nanny - ERROR - Worker process died unexpectedly
FAILED
dask_cuda/tests/test_proxify_host_file.py::test_worker_force_spill_to_disk FAILED
dask_cuda/tests/test_proxify_host_file.py::test_on_demand_debug_info 2024-01-17 07:09:36,342 - distributed.worker - ERROR - std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded
2024-01-17 07:09:36,346 - distributed.nanny - ERROR - Failed to start worker
Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 664, in start
    await wait_for(self.start_unsafe(), timeout=timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 1940, in wait_for
    return await asyncio.wait_for(fut, timeout)
  File "/opt/conda/envs/gdf/lib/python3.9/asyncio/tasks.py", line 442, in wait_for
    return await fut
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1473, in start_unsafe
    raise plugins_exceptions[0]
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/utils.py", line 832, in wrapper
    return await func(*args, **kwargs)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/worker.py", line 1873, in plugin_add
    result = plugin.setup(worker=self)
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/dask_cuda/plugins.py", line 95, in setup
    rmm.reinitialize(
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/rmm/rmm.py", line 81, in reinitialize
    mr._initialize(
  File "memory_resource.pyx", line 948, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 1008, in rmm._lib.memory_resource._initialize
  File "memory_resource.pyx", line 388, in rmm._lib.memory_resource.PoolMemoryResource.__cinit__
MemoryError: std::bad_alloc: out_of_memory: RMM failure at:/opt/conda/conda-bld/work/include/rmm/mr/device/pool_memory_resource.hpp:320: Maximum pool size exceeded

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/nanny.py", line 967, in run
    async with worker:
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 678, in __aenter__
    await self
  File "/opt/conda/envs/gdf/lib/python3.9/site-packages/distributed/core.py", line 672, in start
    raise RuntimeError(f"{type(self).__name__} failed to start.") from exc
RuntimeError: Worker failed to start.
/opt/conda/envs/gdf/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
